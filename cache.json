{"2023-07-12T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2305.12493v5","updated":"2023-07-12T17:41:53Z","published":"2023-05-21T16:08:04Z","title":"Contextualized End-to-End Speech Recognition with Contextual Phrase\n  Prediction Network","summary":"  Contextual information plays a crucial role in speech recognition\ntechnologies and incorporating it into the end-to-end speech recognition models\nhas drawn immense interest recently. However, previous deep bias methods lacked\nexplicit supervision for bias tasks. In this study, we introduce a contextual\nphrase prediction network for an attention-based deep bias method. This network\npredicts context phrases in utterances using contextual embeddings and\ncalculates bias loss to assist in the training of the contextualized model. Our\nmethod achieved a significant word error rate (WER) reduction across various\nend-to-end speech recognition models. Experiments on the LibriSpeech corpus\nshow that our proposed model obtains a 12.1% relative WER improvement over the\nbaseline model, and the WER of the context phrases decreases relatively by\n40.5%. Moreover, by applying a context phrase filtering strategy, we also\neffectively eliminate the WER degradation when using a larger biasing list.\n","authors":["Kaixun Huang","Ao Zhang","Zhanheng Yang","Pengcheng Guo","Bingshen Mu","Tianyi Xu","Lei Xie"],"pdf_url":"https://arxiv.org/pdf/2305.12493v5.pdf","comment":"Accepted by interspeech2023"},{"id":"http://arxiv.org/abs/2307.06290v1","updated":"2023-07-12T16:37:31Z","published":"2023-07-12T16:37:31Z","title":"Instruction Mining: High-Quality Instruction Data Selection for Large\n  Language Models","summary":"  Large language models typically undergo two training stages, pretraining and\nfinetuning. Despite that large-scale pretraining endows the model with strong\ncapabilities to generate natural language responses, these pretrained models\ncan still fail to understand human instructions at times. To enhance language\nmodels' ability of interpreting and responding to instructions, instruction\nfinetuning has emerged as a critical method in this area. Recent studies found\nthat large language models can be finetuned to perform well even with a small\namount of high-quality instruction-following data. However, the selection of\nhigh-quality datasets for finetuning language models still lacks clear\nguidelines to follow. In this paper, we propose InstructMining, a linear rule\nfor evaluating instruction-following data quality. We formulate InstructMining\nusing specific natural language indicators. To investigate the relationship\nbetween data quality and these indicators, we further conduct extensive\nfinetuning experiments. The experiment results are then applied to estimating\nparameters in InstructMining. To further investigate its performance, we use\nInstructMining to select high-quality data from unseen datasets. Results\ndemonstrate that InstructMining can help select relatively high-quality samples\nfrom various instruction-following datasets. Compared to models finetuned on\nunfiltered datasets, models finetuned on InstructMining selected datasets\nperform better on 42.5% cases.\n","authors":["Yihan Cao","Yanbin Kang","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2307.06290v1.pdf","comment":"Work in progress. 12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2212.09746v3","updated":"2023-07-12T16:29:28Z","published":"2022-12-19T18:59:45Z","title":"Evaluating Human-Language Model Interaction","summary":"  Many real-world applications of language models (LMs), such as writing\nassistance and code autocomplete, involve human-LM interaction. However, most\nbenchmarks are non-interactive in that a model produces output without human\ninvolvement. To evaluate human-LM interaction, we develop a new framework,\nHuman-AI Language-based Interaction Evaluation (HALIE), that defines the\ncomponents of interactive systems and dimensions to consider when designing\nevaluation metrics. Compared to standard, non-interactive evaluation, HALIE\ncaptures (i) the interactive process, not only the final output; (ii) the\nfirst-person subjective experience, not just a third-party assessment; and\n(iii) notions of preference beyond quality (e.g., enjoyment and ownership). We\nthen design five tasks to cover different forms of interaction: social\ndialogue, question answering, crossword puzzles, summarization, and metaphor\ngeneration. With four state-of-the-art LMs (three variants of OpenAI's GPT-3\nand AI21 Labs' Jurassic-1), we find that better non-interactive performance\ndoes not always translate to better human-LM interaction. In particular, we\nhighlight three cases where the results from non-interactive and interactive\nmetrics diverge and underscore the importance of human-LM interaction for LM\nevaluation.\n","authors":["Mina Lee","Megha Srivastava","Amelia Hardy","John Thickstun","Esin Durmus","Ashwin Paranjape","Ines Gerard-Ursin","Xiang Lisa Li","Faisal Ladhak","Frieda Rong","Rose E. Wang","Minae Kwon","Joon Sung Park","Hancheng Cao","Tony Lee","Rishi Bommasani","Michael Bernstein","Percy Liang"],"pdf_url":"https://arxiv.org/pdf/2212.09746v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06281v1","updated":"2023-07-12T16:23:09Z","published":"2023-07-12T16:23:09Z","title":"MMBench: Is Your Multi-modal Model an All-around Player?","summary":"  Large vision-language models have recently achieved remarkable progress,\nexhibiting great perception and reasoning abilities concerning visual\ninformation. However, how to effectively evaluate these large vision-language\nmodels remains a major obstacle, hindering future model development.\nTraditional benchmarks like VQAv2 or COCO Caption provide quantitative\nperformance measurements but suffer from a lack of fine-grained ability\nassessment and non-robust evaluation metrics. Recent subjective benchmarks,\nsuch as OwlEval, offer comprehensive evaluations of a model's abilities by\nincorporating human labor, but they are not scalable and display significant\nbias. In response to these challenges, we propose MMBench, a novel\nmulti-modality benchmark. MMBench methodically develops a comprehensive\nevaluation pipeline, primarily comprised of two elements. The first element is\na meticulously curated dataset that surpasses existing similar benchmarks in\nterms of the number and variety of evaluation questions and abilities. The\nsecond element introduces a novel CircularEval strategy and incorporates the\nuse of ChatGPT. This implementation is designed to convert free-form\npredictions into pre-defined choices, thereby facilitating a more robust\nevaluation of the model's predictions. MMBench is a systematically-designed\nobjective benchmark for robustly evaluating the various abilities of\nvision-language models. We hope MMBench will assist the research community in\nbetter evaluating their models and encourage future advancements in this\ndomain. Project page: https://opencompass.org.cn/mmbench.\n","authors":["Yuan Liu","Haodong Duan","Yuanhan Zhang","Bo Li","Songyang Zhang","Wangbo Zhao","Yike Yuan","Jiaqi Wang","Conghui He","Ziwei Liu","Kai Chen","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2307.06281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.13636v2","updated":"2023-07-12T16:15:57Z","published":"2022-09-27T19:05:22Z","title":"Local Grammar-Based Coding Revisited","summary":"  We revisit the problem of minimal local grammar-based coding. In this\nsetting, the local grammar encoder encodes grammars symbol by symbol, whereas\nthe minimal grammar transform minimizes the grammar length in a preset class of\ngrammars as given by the length of local grammar encoding. It has been known\nthat such minimal codes are strongly universal for a strictly positive entropy\nrate, whereas the number of rules in the minimal grammar constitutes an upper\nbound for the mutual information of the source. Whereas the fully minimal code\nis likely intractable, the constrained minimal block code can be efficiently\ncomputed. In this article, we present a new, simpler, and more general proof of\nstrong universality of the minimal block code, regardless of the entropy rate.\nThe proof is based on a simple Zipfian bound for ranked probabilities. By the\nway, we also show empirically that the number of rules in the minimal block\ncode cannot clearly discriminate between long-memory and memoryless sources,\nsuch as a text in English and a random permutation of its characters. This\ncontradicts our previous expectations.\n","authors":["Łukasz Dębowski"],"pdf_url":"https://arxiv.org/pdf/2209.13636v2.pdf","comment":"21 pages, 1 figure"},{"id":"http://arxiv.org/abs/2307.03109v3","updated":"2023-07-12T15:43:03Z","published":"2023-07-06T16:28:35Z","title":"A Survey on Evaluation of Large Language Models","summary":"  Large language models (LLMs) are gaining increasing popularity in both\nacademia and industry, owing to their unprecedented performance in various\napplications. As LLMs continue to play a vital role in both research and daily\nuse, their evaluation becomes increasingly critical, not only at the task\nlevel, but also at the society level for better understanding of their\npotential risks. Over the past years, significant efforts have been made to\nexamine LLMs from various perspectives. This paper presents a comprehensive\nreview of these evaluation methods for LLMs, focusing on three key dimensions:\nwhat to evaluate, where to evaluate, and how to evaluate. Firstly, we provide\nan overview from the perspective of evaluation tasks, encompassing general\nnatural language processing tasks, reasoning, medical usage, ethics,\neducations, natural and social sciences, agent applications, and other areas.\nSecondly, we answer the `where' and `how' questions by diving into the\nevaluation methods and benchmarks, which serve as crucial components in\nassessing performance of LLMs. Then, we summarize the success and failure cases\nof LLMs in different tasks. Finally, we shed light on several future challenges\nthat lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to\nresearchers in the realm of LLMs evaluation, thereby aiding the development of\nmore proficient LLMs. Our key point is that evaluation should be treated as an\nessential discipline to better assist the development of LLMs. We consistently\nmaintain the related open-source materials at:\nhttps://github.com/MLGroupJLU/LLM-eval-survey.\n","authors":["Yupeng Chang","Xu Wang","Jindong Wang","Yuan Wu","Kaijie Zhu","Hao Chen","Linyi Yang","Xiaoyuan Yi","Cunxiang Wang","Yidong Wang","Wei Ye","Yue Zhang","Yi Chang","Philip S. Yu","Qiang Yang","Xing Xie"],"pdf_url":"https://arxiv.org/pdf/2307.03109v3.pdf","comment":"24 pages; code is at https://github.com/MLGroupJLU/LLM-eval-survey"},{"id":"http://arxiv.org/abs/2307.06218v1","updated":"2023-07-12T15:07:16Z","published":"2023-07-12T15:07:16Z","title":"Ashaar: Automatic Analysis and Generation of Arabic Poetry Using Deep\n  Learning Approaches","summary":"  Poetry holds immense significance within the cultural and traditional fabric\nof any nation. It serves as a vehicle for poets to articulate their emotions,\npreserve customs, and convey the essence of their culture. Arabic poetry is no\nexception, having played a cherished role in the heritage of the Arabic\ncommunity throughout history and maintaining its relevance in the present era.\nTypically, comprehending Arabic poetry necessitates the expertise of a linguist\nwho can analyze its content and assess its quality. This paper presents the\nintroduction of a framework called \\textit{Ashaar}\nhttps://github.com/ARBML/Ashaar, which encompasses a collection of datasets and\npre-trained models designed specifically for the analysis and generation of\nArabic poetry. The pipeline established within our proposed approach\nencompasses various aspects of poetry, such as meter, theme, and era\nclassification. It also incorporates automatic poetry diacritization, enabling\nmore intricate analyses like automated extraction of the \\textit{Arudi} style.\nAdditionally, we explore the feasibility of generating conditional poetry\nthrough the pre-training of a character-based GPT model. Furthermore, as part\nof this endeavor, we provide four datasets: one for poetry generation, another\nfor diacritization, and two for Arudi-style prediction. These datasets aim to\nfacilitate research and development in the field of Arabic poetry by enabling\nresearchers and enthusiasts to delve into the nuances of this rich literary\ntradition.\n","authors":["Zaid Alyafeai","Maged S. Al-Shaibani","Moataz Ahmed"],"pdf_url":"https://arxiv.org/pdf/2307.06218v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06187v1","updated":"2023-07-12T14:26:46Z","published":"2023-07-12T14:26:46Z","title":"Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems","summary":"  In autonomic computing, self-adaptation has been proposed as a fundamental\nparadigm to manage the complexity of multiagent systems (MASs). This achieved\nby extending a system with support to monitor and adapt itself to achieve\nspecific concerns of interest. Communication in these systems is key given that\nin scenarios involving agent interaction, it enhances cooperation and reduces\ncoordination challenges by enabling direct, clear information exchange.\nHowever, improving the expressiveness of the interaction communication with\nMASs is not without challenges. In this sense, the interplay between\nself-adaptive systems and effective communication is crucial for future MAS\nadvancements. In this paper, we propose the integration of large language\nmodels (LLMs) such as GPT-based technologies into multiagent systems. We anchor\nour methodology on the MAPE-K model, which is renowned for its robust support\nin monitoring, analyzing, planning, and executing system adaptations in\nresponse to dynamic environments. We also present a practical illustration of\nthe proposed approach, in which we implement and assess a basic MAS-based\napplication. The approach significantly advances the state-of-the-art of\nself-adaptive systems by proposing a new paradigm for MAS self-adaptation of\nautonomous systems based on LLM capabilities.\n","authors":["Nathalia Nascimento","Paulo Alencar","Donald Cowan"],"pdf_url":"https://arxiv.org/pdf/2307.06187v1.pdf","comment":"6 pages, submitted"},{"id":"http://arxiv.org/abs/2307.03042v2","updated":"2023-07-12T13:57:41Z","published":"2023-07-06T15:06:41Z","title":"Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain","summary":"  Adapting pretrained language models to novel domains, such as clinical\napplications, traditionally involves retraining their entire set of parameters.\nHowever, this approach is increasingly proven to be impractical owing to the\nsubstantial computational requirements associated with training such large\nlanguage models. To address this issue, Parameter-Efficient Fine-Tuning (PEFT)\ntechniques offer a viable solution by selectively fine-tuning a small subset of\nadditional parameters, significantly reducing the computational requirements\nfor domain adaptation. In this study, we propose Clinical LLaMA-LoRA, a PEFT\nadapter layer built upon the open-sourced LLaMA model. Clinical LLaMA-LoRA is\ntrained using clinical notes obtained from the MIMIC-IV database, thereby\ncreating a specialised adapter designed for the clinical domain. Additionally,\nwe propose a two-step PEFT framework which fuses Clinical LLaMA-LoRA with\nDownstream LLaMA-LoRA, another PEFT adapter specialised for downstream tasks.\nWe evaluate this framework on multiple clinical outcome prediction datasets,\ncomparing it to clinically trained language models. Our proposed framework\nachieves a state-of-the-art AUROC score averaged across all clinical downstream\ntasks. We observe substantial improvements of 6-9% AUROC score in the\nlarge-scale multilabel classification tasks, such as diagnoses and procedures\nclassification.\n","authors":["Aryo Pradipta Gema","Luke Daines","Pasquale Minervini","Beatrice Alex"],"pdf_url":"https://arxiv.org/pdf/2307.03042v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.04518v2","updated":"2023-07-12T12:30:12Z","published":"2023-07-10T12:31:27Z","title":"On the Computational Modeling of Meaning: Embodied Cognition Intertwined\n  with Emotion","summary":"  This document chronicles this author's attempt to explore how words come to\nmean what they do, with a particular focus on child language acquisition and\nwhat that means for models of language understanding.\\footnote{I say\n\\emph{historical} because I synthesize the ideas based on when I discovered\nthem and how those ideas influenced my later thinking.} I explain the setting\nfor child language learning, how embodiment -- being able to perceive and enact\nin the world, including knowledge of concrete and abstract concepts -- is\ncrucial, and how emotion and cognition relate to each other and the language\nlearning process. I end with what I think are some of the requirements for a\nlanguage-learning agent that learns language in a setting similar to that of\nchildren. This paper can act as a potential guide for ongoing and future work\nin modeling language.\n","authors":["Casey Kennington"],"pdf_url":"https://arxiv.org/pdf/2307.04518v2.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2211.00923v3","updated":"2023-07-12T12:28:56Z","published":"2022-11-02T07:13:30Z","title":"SpeechBlender: Speech Augmentation Framework for Mispronunciation Data\n  Generation","summary":"  The lack of labeled second language (L2) speech data is a major challenge in\ndesigning mispronunciation detection models. We introduce SpeechBlender - a\nfine-grained data augmentation pipeline for generating mispronunciation errors\nto overcome such data scarcity. The SpeechBlender utilizes varieties of masks\nto target different regions of phonetic units, and use the mixing factors to\nlinearly interpolate raw speech signals while augmenting pronunciation. The\nmasks facilitate smooth blending of the signals, generating more effective\nsamples than the `Cut/Paste' method. Our proposed technique achieves\nstate-of-the-art results, with Speechocean762, on ASR dependent\nmispronunciation detection models at phoneme level, with a 2.0% gain in Pearson\nCorrelation Coefficient (PCC) compared to the previous state-of-the-art [1].\nAdditionally, we demonstrate a 5.0% improvement at the phoneme level compared\nto our baseline. We also observed a 4.6% increase in F1-score with Arabic\nAraVoiceL2 testset.\n","authors":["Yassine El Kheir","Shammur Absar Chowdhury","Ahmed Ali","Hamdy Mubarak","Shazia Afzal"],"pdf_url":"https://arxiv.org/pdf/2211.00923v3.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2307.06124v1","updated":"2023-07-12T12:25:03Z","published":"2023-07-12T12:25:03Z","title":"Enhancing Portuguese Sign Language Animation with Dynamic Timing and\n  Mouthing","summary":"  Current signing avatars are often described as unnatural as they cannot\naccurately reproduce all the subtleties of synchronized body behaviors of a\nhuman signer. In this paper, we propose a new dynamic approach for transitions\nbetween signs, focusing on mouthing animations for Portuguese Sign Language.\nAlthough native signers preferred animations with dynamic transitions, we did\nnot find significant differences in comprehension and perceived naturalness\nscores. On the other hand, we show that including mouthing behaviors improved\ncomprehension and perceived naturalness for novice sign language learners.\nResults have implications in computational linguistics, human-computer\ninteraction, and synthetic animation of signing avatars.\n","authors":["Inês Lacerda","Hugo Nicolau","Luisa Coheur"],"pdf_url":"https://arxiv.org/pdf/2307.06124v1.pdf","comment":"11 pages, 8 figures"},{"id":"http://arxiv.org/abs/2307.06082v1","updated":"2023-07-12T11:08:24Z","published":"2023-07-12T11:08:24Z","title":"VELMA: Verbalization Embodiment of LLM Agents for Vision and Language\n  Navigation in Street View","summary":"  Incremental decision making in real-world environments is one of the most\nchallenging tasks in embodied artificial intelligence. One particularly\ndemanding scenario is Vision and Language Navigation~(VLN) which requires\nvisual and natural language understanding as well as spatial and temporal\nreasoning capabilities. The embodied agent needs to ground its understanding of\nnavigation instructions in observations of a real-world environment like Street\nView. Despite the impressive results of LLMs in other research areas, it is an\nongoing problem of how to best connect them with an interactive visual\nenvironment. In this work, we propose VELMA, an embodied LLM agent that uses a\nverbalization of the trajectory and of visual environment observations as\ncontextual prompt for the next action. Visual information is verbalized by a\npipeline that extracts landmarks from the human written navigation instructions\nand uses CLIP to determine their visibility in the current panorama view. We\nshow that VELMA is able to successfully follow navigation instructions in\nStreet View with only two in-context examples. We further finetune the LLM\nagent on a few thousand examples and achieve 25%-30% relative improvement in\ntask completion over the previous state-of-the-art for two datasets.\n","authors":["Raphael Schumann","Wanrong Zhu","Weixi Feng","Tsu-Jui Fu","Stefan Riezler","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2307.06082v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06060v1","updated":"2023-07-12T10:22:28Z","published":"2023-07-12T10:22:28Z","title":"Interpreting deep embeddings for disease progression clustering","summary":"  We propose a novel approach for interpreting deep embeddings in the context\nof patient clustering. We evaluate our approach on a dataset of participants\nwith type 2 diabetes from the UK Biobank, and demonstrate clinically meaningful\ninsights into disease progression patterns.\n","authors":["Anna Munoz-Farre","Antonios Poulakakis-Daktylidis","Dilini Mahesha Kothalawala","Andrea Rodriguez-Martinez"],"pdf_url":"https://arxiv.org/pdf/2307.06060v1.pdf","comment":"Workshop on Interpretable ML in Healthcare at International\n  Conference on Machine Learning (ICML), Honolulu, Hawaii, USA. 2023"},{"id":"http://arxiv.org/abs/2307.06050v1","updated":"2023-07-12T10:10:24Z","published":"2023-07-12T10:10:24Z","title":"A Study on the Appropriate size of the Mongolian general corpus","summary":"  This study aims to determine the appropriate size of the Mongolian general\ncorpus. This study used the Heaps function and Type Token Ratio to determine\nthe appropriate size of the Mongolian general corpus. The sample corpus of\n906,064 tokens comprised texts from 10 domains of newspaper politics, economy,\nsociety, culture, sports, world articles and laws, middle and high school\nliterature textbooks, interview articles, and podcast transcripts. First, we\nestimated the Heaps function with this sample corpus. Next, we observed changes\nin the number of types and TTR values while increasing the number of tokens by\none million using the estimated Heaps function. As a result of observation, we\nfound that the TTR value hardly changed when the number of tokens exceeded from\n39 to 42 million. Thus, we conclude that an appropriate size for a Mongolian\ngeneral corpus is from 39 to 42 million tokens.\n","authors":["Sunsoo Choi","Ganbat Tsend"],"pdf_url":"https://arxiv.org/pdf/2307.06050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.04003v2","updated":"2023-07-12T09:24:34Z","published":"2023-05-06T10:36:39Z","title":"ANTONIO: Towards a Systematic Method of Generating NLP Benchmarks for\n  Verification","summary":"  Verification of machine learning models used in Natural Language Processing\n(NLP) is known to be a hard problem. In particular, many known neural network\nverification methods that work for computer vision and other numeric datasets\ndo not work for NLP. Here, we study technical reasons that underlie this\nproblem. Based on this analysis, we propose practical methods and heuristics\nfor preparing NLP datasets and models in a way that renders them amenable to\nknown verification methods based on abstract interpretation. We implement these\nmethods as a Python library called ANTONIO that links to the neural network\nverifiers ERAN and Marabou. We perform evaluation of the tool using an NLP\ndataset R-U-A-Robot suggested as a benchmark for verifying legally critical NLP\napplications. We hope that, thanks to its general applicability, this work will\nopen novel possibilities for including NLP verification problems into neural\nnetwork verification competitions, and will popularise NLP problems within this\ncommunity.\n","authors":["Marco Casadio","Luca Arnaboldi","Matthew L. Daggitt","Omri Isac","Tanvi Dinkar","Daniel Kienitz","Verena Rieser","Ekaterina Komendantskaya"],"pdf_url":"https://arxiv.org/pdf/2305.04003v2.pdf","comment":"To appear in proceedings of 6th Workshop on Formal Methods for\n  ML-Enabled Autonomous Systems (Affiliated with CAV 2023)"},{"id":"http://arxiv.org/abs/2307.06029v1","updated":"2023-07-12T09:23:41Z","published":"2023-07-12T09:23:41Z","title":"Pluggable Neural Machine Translation Models via Memory-augmented\n  Adapters","summary":"  Although neural machine translation (NMT) models perform well in the general\ndomain, it remains rather challenging to control their generation behavior to\nsatisfy the requirement of different users. Given the expensive training cost\nand the data scarcity challenge of learning a new model from scratch for each\nuser requirement, we propose a memory-augmented adapter to steer pretrained NMT\nmodels in a pluggable manner. Specifically, we construct a multi-granular\nmemory based on the user-provided text samples and propose a new adapter\narchitecture to combine the model representations and the retrieved results. We\nalso propose a training strategy using memory dropout to reduce spurious\ndependencies between the NMT model and the memory. We validate our approach on\nboth style- and domain-specific experiments and the results indicate that our\nmethod can outperform several representative pluggable baselines.\n","authors":["Yuzhuang Xu","Shuo Wang","Peng Li","Xuebo Liu","Xiaolong Wang","Weidong Liu","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2307.06029v1.pdf","comment":"12 pages, 8 figures, 8 tables"},{"id":"http://arxiv.org/abs/2302.09582v3","updated":"2023-07-12T09:04:14Z","published":"2023-02-19T14:21:33Z","title":"Language-Specific Representation of Emotion-Concept Knowledge Causally\n  Supports Emotion Inference","summary":"  Understanding how language supports emotion inference remains a topic of\ndebate in emotion science. The present study investigated whether\nlanguage-derived emotion-concept knowledge would causally support emotion\ninference by manipulating the language-specific knowledge representations in\nlarge language models. Using the prompt technique, 14 attributes of emotion\nconcepts were found to be represented by distinct artificial neuron\npopulations. By manipulating these attribute-related neurons, the majority of\nthe emotion inference tasks showed performance deterioration compared to random\nmanipulations. The attribute-specific performance deterioration was related to\nthe importance of different attributes in human mental space. Our findings\nprovide causal evidence in support of a language-based mechanism for emotion\ninference and highlight the contributions of emotion-concept knowledge.\n","authors":["Ming Li","Yusheng Su","Hsiu-Yuan Huang","Jiali Cheng","Xin Hu","Xinmiao Zhang","Huadong Wang","Yujia Qin","Xiaozhi Wang","Zhiyuan Liu","Dan Zhang"],"pdf_url":"https://arxiv.org/pdf/2302.09582v3.pdf","comment":"39 pages, 13 figures, 2 tables, major revisions over previous\n  versions"},{"id":"http://arxiv.org/abs/2307.06018v1","updated":"2023-07-12T09:00:37Z","published":"2023-07-12T09:00:37Z","title":"PolyLM: An Open Source Polyglot Large Language Model","summary":"  Large language models (LLMs) demonstrate remarkable ability to comprehend,\nreason, and generate following nature language instructions. However, the\ndevelopment of LLMs has been primarily focused on high-resource languages, such\nas English, thereby limiting their applicability and research in other\nlanguages. Consequently, we present PolyLM, a multilingual LLM trained on 640\nbillion (B) tokens, avaliable in two model sizes: 1.7B and 13B. To enhance its\nmultilingual capabilities, we 1) integrate bilingual data into training data;\nand 2) adopt a curriculum learning strategy that increases the proportion of\nnon-English data from 30% in the first stage to 60% in the final stage during\npre-training. Further, we propose a multilingual self-instruct method which\nautomatically generates 132.7K diverse multilingual instructions for model\nfine-tuning. To assess the model's performance, we collect several existing\nmultilingual tasks, including multilingual understanding, question answering,\ngeneration, and translation. Extensive experiments show that PolyLM surpasses\nother open-source models such as LLaMA and BLOOM on multilingual tasks while\nmaintaining comparable performance in English. Our models, alone with the\ninstruction data and multilingual benchmark, are available at:\n\\url{https://modelscope.cn/models/damo/nlp_polylm_13b_text_generation}.\n","authors":["Xiangpeng Wei","Haoran Wei","Huan Lin","Tianhao Li","Pei Zhang","Xingzhang Ren","Mei Li","Yu Wan","Zhiwei Cao","Binbin Xie","Tianxiang Hu","Shangjie Li","Binyuan Hui","Bowen Yu","Dayiheng Liu","Baosong Yang","Fei Huang","Jun Xie"],"pdf_url":"https://arxiv.org/pdf/2307.06018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06005v1","updated":"2023-07-12T08:33:16Z","published":"2023-07-12T08:33:16Z","title":"DDNAS: Discretized Differentiable Neural Architecture Search for Text\n  Classification","summary":"  Neural Architecture Search (NAS) has shown promising capability in learning\ntext representation. However, existing text-based NAS neither performs a\nlearnable fusion of neural operations to optimize the architecture, nor encodes\nthe latent hierarchical categorization behind text input. This paper presents a\nnovel NAS method, Discretized Differentiable Neural Architecture Search\n(DDNAS), for text representation learning and classification. With the\ncontinuous relaxation of architecture representation, DDNAS can use gradient\ndescent to optimize the search. We also propose a novel discretization layer\nvia mutual information maximization, which is imposed on every search node to\nmodel the latent hierarchical categorization in text representation. Extensive\nexperiments conducted on eight diverse real datasets exhibit that DDNAS can\nconsistently outperform the state-of-the-art NAS methods. While DDNAS relies on\nonly three basic operations, i.e., convolution, pooling, and none, to be the\ncandidates of NAS building blocks, its promising performance is noticeable and\nextensible to obtain further improvement by adding more different operations.\n","authors":["Kuan-Chun Chen","Cheng-Te Li","Kuo-Jung Lee"],"pdf_url":"https://arxiv.org/pdf/2307.06005v1.pdf","comment":"ACM Trans. Intell. Syst. Technol. (TIST) 2023"},{"id":"http://arxiv.org/abs/2307.05973v1","updated":"2023-07-12T07:40:48Z","published":"2023-07-12T07:40:48Z","title":"VoxPoser: Composable 3D Value Maps for Robotic Manipulation with\n  Language Models","summary":"  Large language models (LLMs) are shown to possess a wealth of actionable\nknowledge that can be extracted for robot manipulation in the form of reasoning\nand planning. Despite the progress, most still rely on pre-defined motion\nprimitives to carry out the physical interactions with the environment, which\nremains a major bottleneck. In this work, we aim to synthesize robot\ntrajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a\nlarge variety of manipulation tasks given an open-set of instructions and an\nopen-set of objects. We achieve this by first observing that LLMs excel at\ninferring affordances and constraints given a free-form language instruction.\nMore importantly, by leveraging their code-writing capabilities, they can\ninteract with a visual-language model (VLM) to compose 3D value maps to ground\nthe knowledge into the observation space of the agent. The composed value maps\nare then used in a model-based planning framework to zero-shot synthesize\nclosed-loop robot trajectories with robustness to dynamic perturbations. We\nfurther demonstrate how the proposed framework can benefit from online\nexperiences by efficiently learning a dynamics model for scenes that involve\ncontact-rich interactions. We present a large-scale study of the proposed\nmethod in both simulated and real-robot environments, showcasing the ability to\nperform a large variety of everyday manipulation tasks specified in free-form\nnatural language. Project website: https://voxposer.github.io\n","authors":["Wenlong Huang","Chen Wang","Ruohan Zhang","Yunzhu Li","Jiajun Wu","Li Fei-Fei"],"pdf_url":"https://arxiv.org/pdf/2307.05973v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05972v1","updated":"2023-07-12T07:38:24Z","published":"2023-07-12T07:38:24Z","title":"Self-Distilled Quantization: Achieving High Compression Rates in\n  Transformer-Based Language Models","summary":"  We investigate the effects of post-training quantization and\nquantization-aware training on the generalization of Transformer language\nmodels. We present a new method called self-distilled quantization (SDQ) that\nminimizes accumulative quantization errors and outperforms baselines. We apply\nSDQ to multilingual models XLM-R-Base and InfoXLM-Base and demonstrate that\nboth models can be reduced from 32-bit floating point weights to 8-bit integer\nweights while maintaining a high level of performance on the XGLUE benchmark.\nOur results also highlight the challenges of quantizing multilingual models,\nwhich must generalize to languages they were not fine-tuned on.\n","authors":["James O' Neill","Sourav Dutta"],"pdf_url":"https://arxiv.org/pdf/2307.05972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05942v1","updated":"2023-07-12T06:14:36Z","published":"2023-07-12T06:14:36Z","title":"Prototypical Contrastive Transfer Learning for Multimodal Language\n  Understanding","summary":"  Although domestic service robots are expected to assist individuals who\nrequire support, they cannot currently interact smoothly with people through\nnatural language. For example, given the instruction \"Bring me a bottle from\nthe kitchen,\" it is difficult for such robots to specify the bottle in an\nindoor environment. Most conventional models have been trained on real-world\ndatasets that are labor-intensive to collect, and they have not fully leveraged\nsimulation data through a transfer learning framework. In this study, we\npropose a novel transfer learning approach for multimodal language\nunderstanding called Prototypical Contrastive Transfer Learning (PCTL), which\nuses a new contrastive loss called Dual ProtoNCE. We introduce PCTL to the task\nof identifying target objects in domestic environments according to free-form\nnatural language instructions. To validate PCTL, we built new real-world and\nsimulation datasets. Our experiment demonstrated that PCTL outperformed\nexisting methods. Specifically, PCTL achieved an accuracy of 78.1%, whereas\nsimple fine-tuning achieved an accuracy of 73.4%.\n","authors":["Seitaro Otsuki","Shintaro Ishikawa","Komei Sugiura"],"pdf_url":"https://arxiv.org/pdf/2307.05942v1.pdf","comment":"Accepted for presentation at IROS23"},{"id":"http://arxiv.org/abs/2306.05320v3","updated":"2023-07-12T04:41:47Z","published":"2023-06-08T16:13:20Z","title":"KIT's Multilingual Speech Translation System for IWSLT 2023","summary":"  Many existing speech translation benchmarks focus on native-English speech in\nhigh-quality recording conditions, which often do not match the conditions in\nreal-life use-cases. In this paper, we describe our speech translation system\nfor the multilingual track of IWSLT 2023, which evaluates translation quality\non scientific conference talks. The test condition features accented input\nspeech and terminology-dense contents. The task requires translation into 10\nlanguages of varying amounts of resources. In absence of training data from the\ntarget domain, we use a retrieval-based approach (kNN-MT) for effective\nadaptation (+0.8 BLEU for speech translation). We also use adapters to easily\nintegrate incremental training data from data augmentation, and show that it\nmatches the performance of re-training. We observe that cascaded systems are\nmore easily adaptable towards specific target domains, due to their separate\nmodules. Our cascaded speech system substantially outperforms its end-to-end\ncounterpart on scientific talk translation, although their performance remains\nsimilar on TED talks.\n","authors":["Danni Liu","Thai Binh Nguyen","Sai Koneru","Enes Yavuz Ugan","Ngoc-Quan Pham","Tuan-Nam Nguyen","Tu Anh Dinh","Carlos Mullov","Alexander Waibel","Jan Niehues"],"pdf_url":"https://arxiv.org/pdf/2306.05320v3.pdf","comment":"IWSLT 2023"},{"id":"http://arxiv.org/abs/2307.05908v1","updated":"2023-07-12T04:28:41Z","published":"2023-07-12T04:28:41Z","title":"Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM\n  Decoding","summary":"  This paper presents \"Predictive Pipelined Decoding (PPD),\" an approach that\nspeeds up greedy decoding in Large Language Models (LLMs) while maintaining the\nexact same output as the original decoding. Unlike conventional strategies, PPD\nemploys additional compute resources to parallelize the initiation of\nsubsequent token decoding during the current token decoding. This innovative\nmethod reduces decoding latency and reshapes the understanding of trade-offs in\nLLM decoding strategies. We have developed a theoretical framework that allows\nus to analyze the trade-off between computation and latency. Using this\nframework, we can analytically estimate the potential reduction in latency\nassociated with our proposed method, achieved through the assessment of the\nmatch rate, represented as p_correct. The results demonstrate that the use of\nextra computational resources has the potential to accelerate LLM greedy\ndecoding.\n","authors":["Seongjun Yang","Gibbeum Lee","Jaewoong Cho","Dimitris Papailiopoulos","Kangwook Lee"],"pdf_url":"https://arxiv.org/pdf/2307.05908v1.pdf","comment":"ES-FoMo Workshop at ICML 2023"},{"id":"http://arxiv.org/abs/2212.09747v2","updated":"2023-07-12T02:41:46Z","published":"2022-12-19T18:59:56Z","title":"Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023?","summary":"  The CoNLL-2003 English named entity recognition (NER) dataset has been widely\nused to train and evaluate NER models for almost 20 years. However, it is\nunclear how well models that are trained on this 20-year-old data and developed\nover a period of decades using the same test set will perform when applied on\nmodern data. In this paper, we evaluate the generalization of over 20 different\nmodels trained on CoNLL-2003, and show that NER models have very different\ngeneralization. Surprisingly, we find no evidence of performance degradation in\npre-trained Transformers, such as RoBERTa and T5, even when fine-tuned using\ndecades-old data. We investigate why some models generalize well to new data\nwhile others do not, and attempt to disentangle the effects of temporal drift\nand overfitting due to test reuse. Our analysis suggests that most\ndeterioration is due to temporal mismatch between the pre-training corpora and\nthe downstream test sets. We found that four factors are important for good\ngeneralization: model architecture, number of parameters, time period of the\npre-training corpus, in addition to the amount of fine-tuning data. We suggest\ncurrent evaluation methods have, in some sense, underestimated progress on NER\nover the past 20 years, as NER models have not only improved on the original\nCoNLL-2003 test set, but improved even more on modern data. Our datasets can be\nfound at https://github.com/ShuhengL/acl2023_conllpp.\n","authors":["Shuheng Liu","Alan Ritter"],"pdf_url":"https://arxiv.org/pdf/2212.09747v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.00470v2","updated":"2023-07-12T02:36:05Z","published":"2023-07-02T04:32:41Z","title":"PatternGPT :A Pattern-Driven Framework for Large Language Model Text\n  Generation","summary":"  Large language models(LLMS) have shown excellent text generation\ncapabilities,capable of generating fluent responses for many downstream tasks.\nHowever,applying large language models to real-world critical tasks remains\nchallenging due to their susceptibility to hallucinations and inability to\ndirectly use external knowledge. To address the above challenges,this paper\nproposes PatternGPT, a pattern-driven text generation framework for large\nlanguage models. First,the framework utilizes the extraction capabilities of\nlarge language models to generate rich and diverse patterns and later draws on\nthe idea of federated learning. Using multiple agents to achieve sharing to\nobtain more diverse patterns. Finally, it searches for high-quality patterns\nusing judgment criteria and optimization algorithms and uses the searched\npatterns to guide the model for generation. This framework has the advantages\nof generating diversified patterns, protecting data privacy,combining external\nknowledge, and improving the quality of generation, which provides an effective\nmethod to optimize the text generation capability of large language models,and\nmake it better applied to the field of intelligent dialogue and content\ngeneration.\n","authors":["Le Xiao","Xin Shan"],"pdf_url":"https://arxiv.org/pdf/2307.00470v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.06331v2","updated":"2023-07-12T01:56:52Z","published":"2023-06-10T02:01:02Z","title":"Investigating the Effectiveness of ChatGPT in Mathematical Reasoning and\n  Problem Solving: Evidence from the Vietnamese National High School Graduation\n  Examination","summary":"  This study offers a complete analysis of ChatGPT's mathematics abilities in\nresponding to multiple-choice questions for the Vietnamese National High School\nGraduation Examination (VNHSGE) on a range of subjects and difficulty levels.\nThe dataset included 250 questions divided into four levels: knowledge (K),\ncomprehension (C), application (A), and high application (H), and it included\nten themes that covered diverse mathematical concepts. The outcomes demonstrate\nthat ChatGPT's performance varies depending on the difficulty level and\nsubject. It performed best on questions at Level (K), with an accuracy rate of\n$83\\%$; but, as the difficulty level rose, it scored poorly, with an accuracy\nrate of $10\\%$. The study has also shown that ChatGPT significantly succeeds in\nproviding responses to questions on subjects including exponential and\nlogarithmic functions, geometric progression, and arithmetic progression. The\nstudy found that ChatGPT had difficulty correctly answering questions on topics\nincluding derivatives and applications, spatial geometry, and Oxyz spatial\ncalculus. Additionally, this study contrasted ChatGPT outcomes with Vietnamese\nstudents in VNHSGE and in other math competitions. ChatGPT dominated in the SAT\nMath competition with a success rate of $70\\%$, followed by VNHSGE mathematics\n($58.8\\%)$. However, its success rates were lower on other exams, such as AP\nStatistics, the GRE Quantitative, AMC 10, AMC 12, and AP Calculus BC. These\nresults suggest that ChatGPT has the potential to be an effective teaching tool\nfor mathematics, but more work is needed to enhance its handling of graphical\ndata and address the challenges presented by questions that are getting more\nchallenging.\n","authors":["Xuan-Quy Dao","Ngoc-Bich Le"],"pdf_url":"https://arxiv.org/pdf/2306.06331v2.pdf","comment":"17 pages, 13 images"},{"id":"http://arxiv.org/abs/2306.05685v2","updated":"2023-07-12T01:42:26Z","published":"2023-06-09T05:55:52Z","title":"Judging LLM-as-a-judge with MT-Bench and Chatbot Arena","summary":"  Evaluating large language model (LLM) based chat assistants is challenging\ndue to their broad capabilities and the inadequacy of existing benchmarks in\nmeasuring human preferences. To address this, we explore using strong LLMs as\njudges to evaluate these models on more open-ended questions. We examine the\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\nself-enhancement biases, as well as limited reasoning ability, and propose\nsolutions to mitigate some of them. We then verify the agreement between LLM\njudges and human preferences by introducing two benchmarks: MT-bench, a\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\ncrowdsourced human preferences well, achieving over 80\\% agreement, the same\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\nexplainable way to approximate human preferences, which are otherwise very\nexpensive to obtain. Additionally, we show our benchmark and traditional\nbenchmarks complement each other by evaluating several variants of LLaMA and\nVicuna. We will publicly release MT-bench questions, 3K expert votes, and 30K\nconversations with human preferences from Chatbot Arena.\n","authors":["Lianmin Zheng","Wei-Lin Chiang","Ying Sheng","Siyuan Zhuang","Zhanghao Wu","Yonghao Zhuang","Zi Lin","Zhuohan Li","Dacheng Li","Eric. P Xing","Hao Zhang","Joseph E. Gonzalez","Ion Stoica"],"pdf_url":"https://arxiv.org/pdf/2306.05685v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.04492v7","updated":"2023-07-12T01:04:28Z","published":"2023-05-08T06:36:46Z","title":"MGR: Multi-generator Based Rationalization","summary":"  Rationalization is to employ a generator and a predictor to construct a\nself-explaining NLP model in which the generator selects a subset of\nhuman-intelligible pieces of the input text to the following predictor.\nHowever, rationalization suffers from two key challenges, i.e., spurious\ncorrelation and degeneration, where the predictor overfits the spurious or\nmeaningless pieces solely selected by the not-yet well-trained generator and in\nturn deteriorates the generator. Although many studies have been proposed to\naddress the two challenges, they are usually designed separately and do not\ntake both of them into account. In this paper, we propose a simple yet\neffective method named MGR to simultaneously solve the two problems. The key\nidea of MGR is to employ multiple generators such that the occurrence stability\nof real pieces is improved and more meaningful pieces are delivered to the\npredictor. Empirically, we show that MGR improves the F1 score by up to 20.9%\nas compared to state-of-the-art methods. Codes are available at\nhttps://github.com/jugechengzi/Rationalization-MGR .\n","authors":["Wei Liu","Haozhao Wang","Jun Wang","Ruixuan Li","Xinyang Li","Yuankai Zhang","Yang Qiu"],"pdf_url":"https://arxiv.org/pdf/2305.04492v7.pdf","comment":"ACL 2023, oral presentation. arXiv admin note: text overlap with\n  arXiv:2209.08285"},{"id":"http://arxiv.org/abs/2307.05034v2","updated":"2023-07-12T00:52:15Z","published":"2023-07-11T06:18:07Z","title":"Synthetic Dataset for Evaluating Complex Compositional Knowledge for\n  Natural Language Inference","summary":"  We introduce a synthetic dataset called Sentences Involving Complex\nCompositional Knowledge (SICCK) and a novel analysis that investigates the\nperformance of Natural Language Inference (NLI) models to understand\ncompositionality in logic. We produce 1,304 sentence pairs by modifying 15\nexamples from the SICK dataset (Marelli et al., 2014). To this end, we modify\nthe original texts using a set of phrases - modifiers that correspond to\nuniversal quantifiers, existential quantifiers, negation, and other concept\nmodifiers in Natural Logic (NL) (MacCartney, 2009). We use these phrases to\nmodify the subject, verb, and object parts of the premise and hypothesis.\nLastly, we annotate these modified texts with the corresponding entailment\nlabels following NL rules. We conduct a preliminary verification of how well\nthe change in the structural and semantic composition is captured by neural NLI\nmodels, in both zero-shot and fine-tuned scenarios. We found that the\nperformance of NLI models under the zero-shot setting is poor, especially for\nmodified sentences with negation and existential quantifiers. After fine-tuning\nthis dataset, we observe that models continue to perform poorly over negation,\nexistential and universal modifiers.\n","authors":["Sushma Anand Akoju","Robert Vacareanu","Haris Riaz","Eduardo Blanco","Mihai Surdeanu"],"pdf_url":"https://arxiv.org/pdf/2307.05034v2.pdf","comment":"Accepted to Natural Language Reasoning and Structured Explanations\n  (NLRSE) Workshop, ACL 2023. For dataset, please refer\n  https://github.com/clulab/releases/tree/master/acl2023-nlrse-sicck and\n  https://github.com/sushmaakoju/natural-logic"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2305.12529v2","updated":"2023-07-12T17:58:59Z","published":"2023-05-21T17:59:39Z","title":"DreamWaltz: Make a Scene with Complex 3D Animatable Avatars","summary":"  We present DreamWaltz, a novel framework for generating and animating complex\n3D avatars given text guidance and parametric human body prior. While recent\nmethods have shown encouraging results for text-to-3D generation of common\nobjects, creating high-quality and animatable 3D avatars remains challenging.\nTo create high-quality 3D avatars, DreamWaltz proposes 3D-consistent\nocclusion-aware Score Distillation Sampling (SDS) to optimize implicit neural\nrepresentations with canonical poses. It provides view-aligned supervision via\n3D-aware skeleton conditioning which enables complex avatar generation without\nartifacts and multiple faces. For animation, our method learns an animatable\nand generalizable avatar representation which could map arbitrary poses to the\ncanonical pose representation. Extensive evaluations demonstrate that\nDreamWaltz is an effective and robust approach for creating 3D avatars that can\ntake on complex shapes and appearances as well as novel poses for animation.\nThe proposed framework further enables the creation of complex scenes with\ndiverse compositions, including avatar-avatar, avatar-object and avatar-scene\ninteractions. See https://dreamwaltz3d.github.io/ for more vivid 3D avatar and\nanimation results.\n","authors":["Yukun Huang","Jianan Wang","Ailing Zeng","He Cao","Xianbiao Qi","Yukai Shi","Zheng-Jun Zha","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.12529v2.pdf","comment":"project page at https://dreamwaltz3d.github.io/"},{"id":"http://arxiv.org/abs/2307.06335v1","updated":"2023-07-12T17:56:09Z","published":"2023-07-12T17:56:09Z","title":"Neural Free-Viewpoint Relighting for Glossy Indirect Illumination","summary":"  Precomputed Radiance Transfer (PRT) remains an attractive solution for\nreal-time rendering of complex light transport effects such as glossy global\nillumination. After precomputation, we can relight the scene with new\nenvironment maps while changing viewpoint in real-time. However, practical PRT\nmethods are usually limited to low-frequency spherical harmonic lighting.\nAll-frequency techniques using wavelets are promising but have so far had\nlittle practical impact. The curse of dimensionality and much higher data\nrequirements have typically limited them to relighting with fixed view or only\ndirect lighting with triple product integrals. In this paper, we demonstrate a\nhybrid neural-wavelet PRT solution to high-frequency indirect illumination,\nincluding glossy reflection, for relighting with changing view. Specifically,\nwe seek to represent the light transport function in the Haar wavelet basis.\nFor global illumination, we learn the wavelet transport using a small\nmulti-layer perceptron (MLP) applied to a feature field as a function of\nspatial location and wavelet index, with reflected direction and material\nparameters being other MLP inputs. We optimize/learn the feature field\n(compactly represented by a tensor decomposition) and MLP parameters from\nmultiple images of the scene under different lighting and viewing conditions.\nWe demonstrate real-time (512 x 512 at 24 FPS, 800 x 600 at 13 FPS) precomputed\nrendering of challenging scenes involving view-dependent reflections and even\ncaustics.\n","authors":["Nithin Raghavan","Yan Xiao","Kai-En Lin","Tiancheng Sun","Sai Bi","Zexiang Xu","Tzu-Mao Li","Ravi Ramamoorthi"],"pdf_url":"https://arxiv.org/pdf/2307.06335v1.pdf","comment":"13 pages, 9 figures, to appear in cgf proceedings of egsr 2023"},{"id":"http://arxiv.org/abs/2307.03190v2","updated":"2023-07-12T17:45:01Z","published":"2023-07-06T17:59:31Z","title":"Synthesizing Artistic Cinemagraphs from Text","summary":"  We introduce Text2Cinemagraph, a fully automated method for creating\ncinemagraphs from text descriptions - an especially challenging task when\nprompts feature imaginary elements and artistic styles, given the complexity of\ninterpreting the semantics and motions of these images. Existing single-image\nanimation methods fall short on artistic inputs, and recent text-based video\nmethods frequently introduce temporal inconsistencies, struggling to keep\ncertain regions static. To address these challenges, we propose an idea of\nsynthesizing image twins from a single text prompt - a pair of an artistic\nimage and its pixel-aligned corresponding natural-looking twin. While the\nartistic image depicts the style and appearance detailed in our text prompt,\nthe realistic counterpart greatly simplifies layout and motion analysis.\nLeveraging existing natural image and video datasets, we can accurately segment\nthe realistic image and predict plausible motion given the semantic\ninformation. The predicted motion can then be transferred to the artistic image\nto create the final cinemagraph. Our method outperforms existing approaches in\ncreating cinemagraphs for natural landscapes as well as artistic and\nother-worldly scenes, as validated by automated metrics and user studies.\nFinally, we demonstrate two extensions: animating existing paintings and\ncontrolling motion directions using text.\n","authors":["Aniruddha Mahapatra","Aliaksandr Siarohin","Hsin-Ying Lee","Sergey Tulyakov","Jun-Yan Zhu"],"pdf_url":"https://arxiv.org/pdf/2307.03190v2.pdf","comment":"Project website: https://text2cinemagraph.github.io/website/"},{"id":"http://arxiv.org/abs/2307.06322v1","updated":"2023-07-12T17:37:46Z","published":"2023-07-12T17:37:46Z","title":"Deep Learning of Crystalline Defects from TEM images: A Solution for the\n  Problem of \"Never Enough Training Data\"","summary":"  Crystalline defects, such as line-like dislocations, play an important role\nfor the performance and reliability of many metallic devices. Their interaction\nand evolution still poses a multitude of open questions to materials science\nand materials physics. In-situ TEM experiments can provide important insights\ninto how dislocations behave and move. During such experiments, the dislocation\nmicrostructure is captured in form of videos. The analysis of individual video\nframes can provide useful insights but is limited by the capabilities of\nautomated identification, digitization, and quantitative extraction of the\ndislocations as curved objects. The vast amount of data also makes manual\nannotation very time consuming, thereby limiting the use of Deep\nLearning-based, automated image analysis and segmentation of the dislocation\nmicrostructure. In this work, a parametric model for generating synthetic\ntraining data for segmentation of dislocations is developed. Even though domain\nscientists might dismiss synthetic training images sometimes as too artificial,\nour findings show that they can result in superior performance, particularly\nregarding the generalizing of the Deep Learning models with respect to\ndifferent microstructures and imaging conditions. Additionally, we propose an\nenhanced deep learning method optimized for segmenting overlapping or\nintersecting dislocation lines. Upon testing this framework on four distinct\nreal datasets, we find that our synthetic training data are able to yield\nhigh-quality results also on real images-even more so if fine-tune on a few\nreal images was done.\n","authors":["Kishan Govind","Daniela Oliveros","Antonin Dlouhy","Marc Legros","Stefan Sandfeld"],"pdf_url":"https://arxiv.org/pdf/2307.06322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06312v1","updated":"2023-07-12T17:20:05Z","published":"2023-07-12T17:20:05Z","title":"Correlation-Aware Mutual Learning for Semi-supervised Medical Image\n  Segmentation","summary":"  Semi-supervised learning has become increasingly popular in medical image\nsegmentation due to its ability to leverage large amounts of unlabeled data to\nextract additional information. However, most existing semi-supervised\nsegmentation methods only focus on extracting information from unlabeled data,\ndisregarding the potential of labeled data to further improve the performance\nof the model. In this paper, we propose a novel Correlation Aware Mutual\nLearning (CAML) framework that leverages labeled data to guide the extraction\nof information from unlabeled data. Our approach is based on a mutual learning\nstrategy that incorporates two modules: the Cross-sample Mutual Attention\nModule (CMA) and the Omni-Correlation Consistency Module (OCC). The CMA module\nestablishes dense cross-sample correlations among a group of samples, enabling\nthe transfer of label prior knowledge to unlabeled data. The OCC module\nconstructs omni-correlations between the unlabeled and labeled datasets and\nregularizes dual models by constraining the omni-correlation matrix of each\nsub-model to be consistent. Experiments on the Atrial Segmentation Challenge\ndataset demonstrate that our proposed approach outperforms state-of-the-art\nmethods, highlighting the effectiveness of our framework in medical image\nsegmentation tasks. The codes, pre-trained weights, and data are publicly\navailable.\n","authors":["Shengbo Gao","Ziji Zhang","Jiechao Ma","Zihao Li","Shu Zhang"],"pdf_url":"https://arxiv.org/pdf/2307.06312v1.pdf","comment":"MICCAI2023 early accepted, camera ready version"},{"id":"http://arxiv.org/abs/2307.06307v1","updated":"2023-07-12T17:09:18Z","published":"2023-07-12T17:09:18Z","title":"Facial Reenactment Through a Personalized Generator","summary":"  In recent years, the role of image generative models in facial reenactment\nhas been steadily increasing. Such models are usually subject-agnostic and\ntrained on domain-wide datasets. The appearance of the reenacted individual is\nlearned from a single image, and hence, the entire breadth of the individual's\nappearance is not entirely captured, leading these methods to resort to\nunfaithful hallucination. Thanks to recent advancements, it is now possible to\ntrain a personalized generative model tailored specifically to a given\nindividual. In this paper, we propose a novel method for facial reenactment\nusing a personalized generator. We train the generator using frames from a\nshort, yet varied, self-scan video captured using a simple commodity camera.\nImages synthesized by the personalized generator are guaranteed to preserve\nidentity. The premise of our work is that the task of reenactment is thus\nreduced to accurately mimicking head poses and expressions. To this end, we\nlocate the desired frames in the latent space of the personalized generator\nusing carefully designed latent optimization. Through extensive evaluation, we\ndemonstrate state-of-the-art performance for facial reenactment. Furthermore,\nwe show that since our reenactment takes place in a semantic latent space, it\ncan be semantically edited and stylized in post-processing.\n","authors":["Ariel Elazary","Yotam Nitzan","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2307.06307v1.pdf","comment":"Project webpage: https://arielazary.github.io/PGR/"},{"id":"http://arxiv.org/abs/2307.06304v1","updated":"2023-07-12T17:01:03Z","published":"2023-07-12T17:01:03Z","title":"Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and\n  Resolution","summary":"  The ubiquitous and demonstrably suboptimal choice of resizing images to a\nfixed resolution before processing them with computer vision models has not yet\nbeen successfully challenged. However, models such as the Vision Transformer\n(ViT) offer flexible sequence-based modeling, and hence varying input sequence\nlengths. We take advantage of this with NaViT (Native Resolution ViT) which\nuses sequence packing during training to process inputs of arbitrary\nresolutions and aspect ratios. Alongside flexible model usage, we demonstrate\nimproved training efficiency for large-scale supervised and contrastive\nimage-text pretraining. NaViT can be efficiently transferred to standard tasks\nsuch as image and video classification, object detection, and semantic\nsegmentation and leads to improved results on robustness and fairness\nbenchmarks. At inference time, the input resolution flexibility can be used to\nsmoothly navigate the test-time cost-performance trade-off. We believe that\nNaViT marks a departure from the standard, CNN-designed, input and modelling\npipeline used by most computer vision models, and represents a promising\ndirection for ViTs.\n","authors":["Mostafa Dehghani","Basil Mustafa","Josip Djolonga","Jonathan Heek","Matthias Minderer","Mathilde Caron","Andreas Steiner","Joan Puigcerver","Robert Geirhos","Ibrahim Alabdulmohsin","Avital Oliver","Piotr Padlewski","Alexey Gritsenko","Mario Lučić","Neil Houlsby"],"pdf_url":"https://arxiv.org/pdf/2307.06304v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06298v1","updated":"2023-07-12T16:52:40Z","published":"2023-07-12T16:52:40Z","title":"Improved Real-time Image Smoothing with Weak Structures Preserved and\n  High-contrast Details Removed","summary":"  Image smoothing is by reducing pixel-wise gradients to smooth out details. As\nexisting methods always rely on gradients to determine smoothing manners, it is\ndifficult to distinguish structures and details to handle distinctively due to\nthe overlapped ranges of gradients for structures and details. Thus, it is\nstill challenging to achieve high-quality results, especially on preserving\nweak structures and removing high-contrast details. In this paper, we address\nthis challenge by improving the real-time optimization-based method via\niterative least squares (called ILS). We observe that 1) ILS uses gradients as\nthe independent variable in its penalty function for determining smoothing\nmanners, and 2) the framework of ILS can still work for image smoothing when we\nuse some values instead of gradients in the penalty function. Thus,\ncorresponding to the properties of pixels on structures or not, we compute some\nvalues to use in the penalty function to determine smoothing manners, and so we\ncan handle structures and details distinctively, no matter whether their\ngradients are high or low. As a result, we can conveniently remove\nhigh-contrast details while preserving weak structures. Moreover, such values\ncan be adjusted to accelerate optimization computation, so that we can use\nfewer iterations than the original ILS method for efficiency. This also reduces\nthe changes onto structures to help structure preservation. Experimental\nresults show our advantages over existing methods on efficiency and quality.\n","authors":["Shengchun Wang","Wencheng Wang","Fei Hou"],"pdf_url":"https://arxiv.org/pdf/2307.06298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06281v1","updated":"2023-07-12T16:23:09Z","published":"2023-07-12T16:23:09Z","title":"MMBench: Is Your Multi-modal Model an All-around Player?","summary":"  Large vision-language models have recently achieved remarkable progress,\nexhibiting great perception and reasoning abilities concerning visual\ninformation. However, how to effectively evaluate these large vision-language\nmodels remains a major obstacle, hindering future model development.\nTraditional benchmarks like VQAv2 or COCO Caption provide quantitative\nperformance measurements but suffer from a lack of fine-grained ability\nassessment and non-robust evaluation metrics. Recent subjective benchmarks,\nsuch as OwlEval, offer comprehensive evaluations of a model's abilities by\nincorporating human labor, but they are not scalable and display significant\nbias. In response to these challenges, we propose MMBench, a novel\nmulti-modality benchmark. MMBench methodically develops a comprehensive\nevaluation pipeline, primarily comprised of two elements. The first element is\na meticulously curated dataset that surpasses existing similar benchmarks in\nterms of the number and variety of evaluation questions and abilities. The\nsecond element introduces a novel CircularEval strategy and incorporates the\nuse of ChatGPT. This implementation is designed to convert free-form\npredictions into pre-defined choices, thereby facilitating a more robust\nevaluation of the model's predictions. MMBench is a systematically-designed\nobjective benchmark for robustly evaluating the various abilities of\nvision-language models. We hope MMBench will assist the research community in\nbetter evaluating their models and encourage future advancements in this\ndomain. Project page: https://opencompass.org.cn/mmbench.\n","authors":["Yuan Liu","Haodong Duan","Yuanhan Zhang","Bo Li","Songyang Zhang","Wangbo Zhao","Yike Yuan","Jiaqi Wang","Conghui He","Ziwei Liu","Kai Chen","Dahua Lin"],"pdf_url":"https://arxiv.org/pdf/2307.06281v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06277v1","updated":"2023-07-12T16:20:08Z","published":"2023-07-12T16:20:08Z","title":"Stochastic Light Field Holography","summary":"  The Visual Turing Test is the ultimate goal to evaluate the realism of\nholographic displays. Previous studies have focused on addressing challenges\nsuch as limited \\'etendue and image quality over a large focal volume, but they\nhave not investigated the effect of pupil sampling on the viewing experience in\nfull 3D holograms. In this work, we tackle this problem with a novel hologram\ngeneration algorithm motivated by matching the projection operators of\nincoherent Light Field and coherent Wigner Function light transport. To this\nend, we supervise hologram computation using synthesized photographs, which are\nrendered on-the-fly using Light Field refocusing from stochastically sampled\npupil states during optimization. The proposed method produces holograms with\ncorrect parallax and focus cues, which are important for passing the Visual\nTuring Test. We validate that our approach compares favorably to\nstate-of-the-art CGH algorithms that use Light Field and Focal Stack\nsupervision. Our experiments demonstrate that our algorithm significantly\nimproves the realism of the viewing experience for a variety of different pupil\nstates.\n","authors":["Florian Schiffers","Praneeth Chakravarthula","Nathan Matsuda","Grace Kuo","Ethan Tseng","Douglas Lanman","Felix Heide","Oliver Cossairt"],"pdf_url":"https://arxiv.org/pdf/2307.06277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06272v1","updated":"2023-07-12T16:16:37Z","published":"2023-07-12T16:16:37Z","title":"Exposing the Fake: Effective Diffusion-Generated Images Detection","summary":"  Image synthesis has seen significant advancements with the advent of\ndiffusion-based generative models like Denoising Diffusion Probabilistic Models\n(DDPM) and text-to-image diffusion models. Despite their efficacy, there is a\ndearth of research dedicated to detecting diffusion-generated images, which\ncould pose potential security and privacy risks. This paper addresses this gap\nby proposing a novel detection method called Stepwise Error for\nDiffusion-generated Image Detection (SeDID). Comprising statistical-based\n$\\text{SeDID}_{\\text{Stat}}$ and neural network-based\n$\\text{SeDID}_{\\text{NNs}}$, SeDID exploits the unique attributes of diffusion\nmodels, namely deterministic reverse and deterministic denoising computation\nerrors. Our evaluations demonstrate SeDID's superior performance over existing\nmethods when applied to diffusion models. Thus, our work makes a pivotal\ncontribution to distinguishing diffusion model-generated images, marking a\nsignificant step in the domain of artificial intelligence security.\n","authors":["Ruipeng Ma","Jinhao Duan","Fei Kong","Xiaoshuang Shi","Kaidi Xu"],"pdf_url":"https://arxiv.org/pdf/2307.06272v1.pdf","comment":"AdvML-Frontiers@ICML 2023"},{"id":"http://arxiv.org/abs/2307.06260v1","updated":"2023-07-12T16:01:56Z","published":"2023-07-12T16:01:56Z","title":"UGCANet: A Unified Global Context-Aware Transformer-based Network with\n  Feature Alignment for Endoscopic Image Analysis","summary":"  Gastrointestinal endoscopy is a medical procedure that utilizes a flexible\ntube equipped with a camera and other instruments to examine the digestive\ntract. This minimally invasive technique allows for diagnosing and managing\nvarious gastrointestinal conditions, including inflammatory bowel disease,\ngastrointestinal bleeding, and colon cancer. The early detection and\nidentification of lesions in the upper gastrointestinal tract and the\nidentification of malignant polyps that may pose a risk of cancer development\nare critical components of gastrointestinal endoscopy's diagnostic and\ntherapeutic applications. Therefore, enhancing the detection rates of\ngastrointestinal disorders can significantly improve a patient's prognosis by\nincreasing the likelihood of timely medical intervention, which may prolong the\npatient's lifespan and improve overall health outcomes. This paper presents a\nnovel Transformer-based deep neural network designed to perform multiple tasks\nsimultaneously, thereby enabling accurate identification of both upper\ngastrointestinal tract lesions and colon polyps. Our approach proposes a unique\nglobal context-aware module and leverages the powerful MiT backbone, along with\na feature alignment block, to enhance the network's representation capability.\nThis novel design leads to a significant improvement in performance across\nvarious endoscopic diagnosis tasks. Extensive experiments demonstrate the\nsuperior performance of our method compared to other state-of-the-art\napproaches.\n","authors":["Pham Vu Hung","Nguyen Duy Manh","Nguyen Thi Oanh","Nguyen Thi Thuy","Dinh Viet Sang"],"pdf_url":"https://arxiv.org/pdf/2307.06260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05350v2","updated":"2023-07-12T15:56:15Z","published":"2023-07-07T01:10:18Z","title":"Dividing and Conquering a BlackBox to a Mixture of Interpretable Models:\n  Route, Interpret, Repeat","summary":"  ML model design either starts with an interpretable model or a Blackbox and\nexplains it post hoc. Blackbox models are flexible but difficult to explain,\nwhile interpretable models are inherently explainable. Yet, interpretable\nmodels require extensive ML knowledge and tend to be less flexible and\nunderperforming than their Blackbox variants. This paper aims to blur the\ndistinction between a post hoc explanation of a Blackbox and constructing\ninterpretable models. Beginning with a Blackbox, we iteratively carve out a\nmixture of interpretable experts (MoIE) and a residual network. Each\ninterpretable model specializes in a subset of samples and explains them using\nFirst Order Logic (FOL), providing basic reasoning on concepts from the\nBlackbox. We route the remaining samples through a flexible residual. We repeat\nthe method on the residual network until all the interpretable models explain\nthe desired proportion of data. Our extensive experiments show that our route,\ninterpret, and repeat approach (1) identifies a diverse set of\ninstance-specific concepts with high concept completeness via MoIE without\ncompromising in performance, (2) identifies the relatively ``harder'' samples\nto explain via residuals, (3) outperforms the interpretable by-design models by\nsignificant margins during test-time interventions, and (4) fixes the shortcut\nlearned by the original Blackbox. The code for MoIE is publicly available at:\n\\url{https://github.com/batmanlab/ICML-2023-Route-interpret-repeat}\n","authors":["Shantanu Ghosh","Ke Yu","Forough Arabshahi","Kayhan Batmanghelich"],"pdf_url":"https://arxiv.org/pdf/2307.05350v2.pdf","comment":"appeared as v5 of arXiv:2302.10289 which was replaced in error, which\n  drifted into a different work, accepted in ICML 2023"},{"id":"http://arxiv.org/abs/2307.04149v2","updated":"2023-07-12T15:49:41Z","published":"2023-07-09T10:56:44Z","title":"Latent Graph Attention for Enhanced Spatial Context","summary":"  Global contexts in images are quite valuable in image-to-image translation\nproblems. Conventional attention-based and graph-based models capture the\nglobal context to a large extent, however, these are computationally expensive.\nMoreover, the existing approaches are limited to only learning the pairwise\nsemantic relation between any two points on the image. In this paper, we\npresent Latent Graph Attention (LGA) a computationally inexpensive (linear to\nthe number of nodes) and stable, modular framework for incorporating the global\ncontext in the existing architectures, especially empowering small-scale\narchitectures to give performance closer to large size architectures, thus\nmaking the light-weight architectures more useful for edge devices with lower\ncompute power and lower energy needs. LGA propagates information spatially\nusing a network of locally connected graphs, thereby facilitating to construct\na semantically coherent relation between any two spatially distant points that\nalso takes into account the influence of the intermediate pixels. Moreover, the\ndepth of the graph network can be used to adapt the extent of contextual spread\nto the target dataset, thereby being able to explicitly control the added\ncomputational cost. To enhance the learning mechanism of LGA, we also introduce\na novel contrastive loss term that helps our LGA module to couple well with the\noriginal architecture at the expense of minimal additional computational load.\nWe show that incorporating LGA improves the performance on three challenging\napplications, namely transparent object segmentation, image restoration for\ndehazing and optical flow estimation.\n","authors":["Ayush Singh","Yash Bhambhu","Himanshu Buckchash","Deepak K. Gupta","Dilip K. Prasad"],"pdf_url":"https://arxiv.org/pdf/2307.04149v2.pdf","comment":"20 pages, 7 figures"},{"id":"http://arxiv.org/abs/2307.06233v1","updated":"2023-07-12T15:26:04Z","published":"2023-07-12T15:26:04Z","title":"On the Importance of Denoising when Learning to Compress Images","summary":"  Image noise is ubiquitous in photography. However, image noise is not\ncompressible nor desirable, thus attempting to convey the noise in compressed\nimage bitstreams yields sub-par results in both rate and distortion. We propose\nto explicitly learn the image denoising task when training a codec. Therefore,\nwe leverage the Natural Image Noise Dataset, which offers a wide variety of\nscenes captured with various ISO numbers, leading to different noise levels,\nincluding insignificant ones. Given this training set, we supervise the codec\nwith noisy-clean image pairs, and show that a single model trained based on a\nmixture of images with variable noise levels appears to yield best-in-class\nresults with both noisy and clean images, achieving better rate-distortion than\na compression-only model or even than a pair of denoising-then-compression\nmodels with almost one order of magnitude fewer GMac operations.\n","authors":["Benoit Brummer","Christophe De Vleeschouwer"],"pdf_url":"https://arxiv.org/pdf/2307.06233v1.pdf","comment":"Published in 2023 IEEE/CVF Winter Conference on Applications of\n  Computer Vision (WACV)"},{"id":"http://arxiv.org/abs/2305.19767v3","updated":"2023-07-12T15:09:37Z","published":"2023-05-31T11:58:33Z","title":"Analytical reconstructions of full-scan multiple source-translation\n  computed tomography under large field of views","summary":"  This paper is to investigate the high-quality analytical reconstructions of\nmultiple source-translation computed tomography (mSTCT) under an extended field\nof view (FOV). Under the larger FOVs, the previously proposed backprojection\nfiltration (BPF) algorithms for mSTCT, including D-BPF and S-BPF (their\ndifferences are different derivate directions along the detector and source,\nrespectively), make some errors and artifacts in the reconstructed images due\nto a backprojection weighting factor and the half-scan mode, which deviates\nfrom the intention of mSTCT imaging. In this paper, to achieve reconstruction\nwith as little error as possible under the extremely extended FOV, we combine\nthe full-scan mSTCT (F-mSTCT) geometry with the previous BPF algorithms to\nstudy the performance and derive a suitable redundancy-weighted function for\nF-mSTCT. The experimental results indicate FS-BPF can get high-quality, stable\nimages under the extremely extended FOV of imaging a large object, though it\nrequires more projections than FD-BPF. Finally, for different practical\nrequirements in extending FOV imaging, we give suggestions on algorithm\nselection.\n","authors":["Zhisheng Wang","Yue Liu","Shunli Wang","Xingyuan Bian","Zongfeng Li","Junning Cui"],"pdf_url":"https://arxiv.org/pdf/2305.19767v3.pdf","comment":"17 pages, 9 figures"},{"id":"http://arxiv.org/abs/2307.04617v2","updated":"2023-07-12T15:04:16Z","published":"2023-07-10T15:02:13Z","title":"Weakly-supervised positional contrastive learning: application to\n  cirrhosis classification","summary":"  Large medical imaging datasets can be cheaply and quickly annotated with\nlow-confidence, weak labels (e.g., radiological scores). Access to\nhigh-confidence labels, such as histology-based diagnoses, is rare and costly.\nPretraining strategies, like contrastive learning (CL) methods, can leverage\nunlabeled or weakly-annotated datasets. These methods typically require large\nbatch sizes, which poses a difficulty in the case of large 3D images at full\nresolution, due to limited GPU memory. Nevertheless, volumetric positional\ninformation about the spatial context of each 2D slice can be very important\nfor some medical applications. In this work, we propose an efficient\nweakly-supervised positional (WSP) contrastive learning strategy where we\nintegrate both the spatial context of each 2D slice and a weak label via a\ngeneric kernel-based loss function. We illustrate our method on cirrhosis\nprediction using a large volume of weakly-labeled images, namely radiological\nlow-confidence annotations, and small strongly-labeled (i.e., high-confidence)\ndatasets. The proposed model improves the classification AUC by 5% with respect\nto a baseline model on our internal dataset, and by 26% on the public LIHC\ndataset from the Cancer Genome Atlas. The code is available at:\nhttps://github.com/Guerbet-AI/wsp-contrastive.\n","authors":["Emma Sarfati","Alexandre Bône","Marc-Michel Rohé","Pietro Gori","Isabelle Bloch"],"pdf_url":"https://arxiv.org/pdf/2307.04617v2.pdf","comment":"MICCAI 2023"},{"id":"http://arxiv.org/abs/2307.06206v1","updated":"2023-07-12T14:52:21Z","published":"2023-07-12T14:52:21Z","title":"SepVAE: a contrastive VAE to separate pathological patterns from healthy\n  ones","summary":"  Contrastive Analysis VAE (CA-VAEs) is a family of Variational auto-encoders\n(VAEs) that aims at separating the common factors of variation between a\nbackground dataset (BG) (i.e., healthy subjects) and a target dataset (TG)\n(i.e., patients) from the ones that only exist in the target dataset. To do so,\nthese methods separate the latent space into a set of salient features (i.e.,\nproper to the target dataset) and a set of common features (i.e., exist in both\ndatasets). Currently, all models fail to prevent the sharing of information\nbetween latent spaces effectively and to capture all salient factors of\nvariation. To this end, we introduce two crucial regularization losses: a\ndisentangling term between common and salient representations and a\nclassification term between background and target samples in the salient space.\nWe show a better performance than previous CA-VAEs methods on three medical\napplications and a natural images dataset (CelebA). Code and datasets are\navailable on GitHub https://github.com/neurospin-projects/2023_rlouiset_sepvae.\n","authors":["Robin Louiset","Edouard Duchesnay","Antoine Grigis","Benoit Dufumier","Pietro Gori"],"pdf_url":"https://arxiv.org/pdf/2307.06206v1.pdf","comment":"Workshop on Interpretable ML in Healthcare at International\n  Conference on Machine Learning (ICML), Honolulu, Hawaii, USA. 2023"},{"id":"http://arxiv.org/abs/2307.06182v1","updated":"2023-07-12T14:13:54Z","published":"2023-07-12T14:13:54Z","title":"CellGAN: Conditional Cervical Cell Synthesis for Augmenting\n  Cytopathological Image Classification","summary":"  Automatic examination of thin-prep cytologic test (TCT) slides can assist\npathologists in finding cervical abnormality for accurate and efficient cancer\nscreening. Current solutions mostly need to localize suspicious cells and\nclassify abnormality based on local patches, concerning the fact that whole\nslide images of TCT are extremely large. It thus requires many annotations of\nnormal and abnormal cervical cells, to supervise the training of the\npatch-level classifier for promising performance. In this paper, we propose\nCellGAN to synthesize cytopathological images of various cervical cell types\nfor augmenting patch-level cell classification. Built upon a lightweight\nbackbone, CellGAN is equipped with a non-linear class mapping network to\neffectively incorporate cell type information into image generation. We also\npropose the Skip-layer Global Context module to model the complex spatial\nrelationship of the cells, and attain high fidelity of the synthesized images\nthrough adversarial learning. Our experiments demonstrate that CellGAN can\nproduce visually plausible TCT cytopathological images for different cell\ntypes. We also validate the effectiveness of using CellGAN to greatly augment\npatch-level cell classification performance.\n","authors":["Zhenrong Shen","Maosong Cao","Sheng Wang","Lichi Zhang","Qian Wang"],"pdf_url":"https://arxiv.org/pdf/2307.06182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06179v1","updated":"2023-07-12T14:10:15Z","published":"2023-07-12T14:10:15Z","title":"Large Class Separation is not what you need for Relational\n  Reasoning-based OOD Detection","summary":"  Standard recognition approaches are unable to deal with novel categories at\ntest time. Their overconfidence on the known classes makes the predictions\nunreliable for safety-critical applications such as healthcare or autonomous\ndriving. Out-Of-Distribution (OOD) detection methods provide a solution by\nidentifying semantic novelty. Most of these methods leverage a learning stage\non the known data, which means training (or fine-tuning) a model to capture the\nconcept of normality. This process is clearly sensitive to the amount of\navailable samples and might be computationally expensive for on-board systems.\nA viable alternative is that of evaluating similarities in the embedding space\nproduced by large pre-trained models without any further learning effort. We\nfocus exactly on such a fine-tuning-free OOD detection setting. This works\npresents an in-depth analysis of the recently introduced relational reasoning\npre-training and investigates the properties of the learned embedding,\nhighlighting the existence of a correlation between the inter-class feature\ndistance and the OOD detection accuracy. As the class separation depends on the\nchosen pre-training objective, we propose an alternative loss function to\ncontrol the inter-class margin, and we show its advantage with thorough\nexperiments.\n","authors":["Lorenzo Li Lu","Giulia D'Ascenzi","Francesco Cappio Borlino","Tatiana Tommasi"],"pdf_url":"https://arxiv.org/pdf/2307.06179v1.pdf","comment":"Accepted for publication at ICIAP 2023"},{"id":"http://arxiv.org/abs/2307.06177v1","updated":"2023-07-12T14:04:12Z","published":"2023-07-12T14:04:12Z","title":"Smart Infrastructure: A Research Junction","summary":"  Complex inner-city junctions are among the most critical traffic areas for\ninjury and fatal accidents. The development of highly automated driving (HAD)\nsystems struggles with the complex and hectic everyday life within those areas.\nSensor-equipped smart infrastructures, which can communicate and cooperate with\nvehicles, are essential to enable a holistic scene understanding to resolve\nocclusions drivers and vehicle perception systems for themselves can not cover.\nWe introduce an intelligent research infrastructure equipped with visual sensor\ntechnology, located at a public inner-city junction in Aschaffenburg, Germany.\nA multiple-view camera system monitors the traffic situation to perceive road\nusers' behavior. Both motorized and non-motorized traffic is considered. The\nsystem is used for research in data generation, evaluating new HAD sensors\nsystems, algorithms, and Artificial Intelligence (AI) training strategies using\nreal-, synthetic- and augmented data. In addition, the junction features a\nhighly accurate digital twin. Real-world data can be taken into the digital\ntwin for simulation purposes and synthetic data generation.\n","authors":["Manuel Hetzel","Hannes Reichert","Konrad Doll","Bernhard Sick"],"pdf_url":"https://arxiv.org/pdf/2307.06177v1.pdf","comment":"IEEE International Smart Cities Conference (ISC2) 2021"},{"id":"http://arxiv.org/abs/2304.07139v2","updated":"2023-07-12T13:57:23Z","published":"2023-04-14T14:03:35Z","title":"Neuromorphic Optical Flow and Real-time Implementation with Event\n  Cameras","summary":"  Optical flow provides information on relative motion that is an important\ncomponent in many computer vision pipelines. Neural networks provide high\naccuracy optical flow, yet their complexity is often prohibitive for\napplication at the edge or in robots, where efficiency and latency play crucial\nrole. To address this challenge, we build on the latest developments in\nevent-based vision and spiking neural networks. We propose a new network\narchitecture, inspired by Timelens, that improves the state-of-the-art\nself-supervised optical flow accuracy when operated both in spiking and\nnon-spiking mode. To implement a real-time pipeline with a physical event\ncamera, we propose a methodology for principled model simplification based on\nactivity and latency analysis. We demonstrate high speed optical flow\nprediction with almost two orders of magnitude reduced complexity while\nmaintaining the accuracy, opening the path for real-time deployments.\n","authors":["Yannick Schnider","Stanislaw Wozniak","Mathias Gehrig","Jules Lecomte","Axel von Arnim","Luca Benini","Davide Scaramuzza","Angeliki Pantazi"],"pdf_url":"https://arxiv.org/pdf/2304.07139v2.pdf","comment":"Accepted for IEEE CVPRW, Vancouver 2023. Personal use of this\n  material is permitted. Permission from IEEE must be obtained for all other\n  uses, in any current or future media. Copyright 2023 IEEE"},{"id":"http://arxiv.org/abs/2307.06166v1","updated":"2023-07-12T13:46:28Z","published":"2023-07-12T13:46:28Z","title":"Can Vision-Language Models be a Good Guesser? Exploring VLMs for Times\n  and Location Reasoning","summary":"  Vision-Language Models (VLMs) are expected to be capable of reasoning with\ncommonsense knowledge as human beings. One example is that humans can reason\nwhere and when an image is taken based on their knowledge. This makes us wonder\nif, based on visual cues, Vision-Language Models that are pre-trained with\nlarge-scale image-text resources can achieve and even outperform human's\ncapability in reasoning times and location. To address this question, we\npropose a two-stage \\recognition\\space and \\reasoning\\space probing task,\napplied to discriminative and generative VLMs to uncover whether VLMs can\nrecognize times and location-relevant features and further reason about it. To\nfacilitate the investigation, we introduce WikiTiLo, a well-curated image\ndataset compromising images with rich socio-cultural cues. In the extensive\nexperimental studies, we find that although VLMs can effectively retain\nrelevant features in visual encoders, they still fail to make perfect\nreasoning. We will release our dataset and codes to facilitate future studies.\n","authors":["Gengyuan Zhang","Yurui Zhang","Kerui Zhang","Volker Tresp"],"pdf_url":"https://arxiv.org/pdf/2307.06166v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2307.06165v1","updated":"2023-07-12T13:46:20Z","published":"2023-07-12T13:46:20Z","title":"The IMPTC Dataset: An Infrastructural Multi-Person Trajectory and\n  Context Dataset","summary":"  Inner-city intersections are among the most critical traffic areas for injury\nand fatal accidents. Automated vehicles struggle with the complex and hectic\neveryday life within those areas. Sensor-equipped smart infrastructures, which\ncan cooperate with vehicles, can benefit automated traffic by extending the\nperception capabilities of drivers and vehicle perception systems.\nAdditionally, they offer the opportunity to gather reproducible and precise\ndata of a holistic scene understanding, including context information as a\nbasis for training algorithms for various applications in automated traffic.\nTherefore, we introduce the Infrastructural Multi-Person Trajectory and Context\nDataset (IMPTC). We use an intelligent public inner-city intersection in\nGermany with visual sensor technology. A multi-view camera and LiDAR system\nperceives traffic situations and road users' behavior. Additional sensors\nmonitor contextual information like weather, lighting, and traffic light signal\nstatus. The data acquisition system focuses on Vulnerable Road Users (VRUs) and\nmulti-agent interaction. The resulting dataset consists of eight hours of\nmeasurement data. It contains over 2,500 VRU trajectories, including\npedestrians, cyclists, e-scooter riders, strollers, and wheelchair users, and\nover 20,000 vehicle trajectories at different day times, weather conditions,\nand seasons. In addition, to enable the entire stack of research capabilities,\nthe dataset includes all data, starting from the sensor-, calibration- and\ndetection data until trajectory and context data. The dataset is continuously\nexpanded and is available online for non-commercial research at\nhttps://github.com/kav-institute/imptc-dataset.\n","authors":["Manuel Hetzel","Hannes Reichert","Günther Reitberger","Erich Fuchs","Konrad Doll","Bernhard Sick"],"pdf_url":"https://arxiv.org/pdf/2307.06165v1.pdf","comment":"IEEE Intelligent Vehicles Conference (IV) 2023"},{"id":"http://arxiv.org/abs/2307.06143v1","updated":"2023-07-12T12:58:03Z","published":"2023-07-12T12:58:03Z","title":"Learning Kernel-Modulated Neural Representation for Efficient Light\n  Field Compression","summary":"  Light field is a type of image data that captures the 3D scene information by\nrecording light rays emitted from a scene at various orientations. It offers a\nmore immersive perception than classic 2D images but at the cost of huge data\nvolume. In this paper, we draw inspiration from the visual characteristics of\nSub-Aperture Images (SAIs) of light field and design a compact neural network\nrepresentation for the light field compression task. The network backbone takes\nrandomly initialized noise as input and is supervised on the SAIs of the target\nlight field. It is composed of two types of complementary kernels: descriptive\nkernels (descriptors) that store scene description information learned during\ntraining, and modulatory kernels (modulators) that control the rendering of\ndifferent SAIs from the queried perspectives. To further enhance compactness of\nthe network meanwhile retain high quality of the decoded light field, we\naccordingly introduce modulator allocation and kernel tensor decomposition\nmechanisms, followed by non-uniform quantization and lossless entropy coding\ntechniques, to finally form an efficient compression pipeline. Extensive\nexperiments demonstrate that our method outperforms other state-of-the-art\n(SOTA) methods by a significant margin in the light field compression task.\nMoreover, after aligning descriptors, the modulators learned from one light\nfield can be transferred to new light fields for rendering dense views,\nindicating a potential solution for view synthesis task.\n","authors":["Jinglei Shi","Yihong Xu","Christine Guillemot"],"pdf_url":"https://arxiv.org/pdf/2307.06143v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.11474v3","updated":"2023-07-12T12:36:49Z","published":"2023-05-19T06:55:04Z","title":"RAMiT: Reciprocal Attention Mixing Transformer for Lightweight Image\n  Restoration","summary":"  Although many recent works have made advancements in the image restoration\n(IR) field, they often suffer from an excessive number of parameters. Another\nissue is that most Transformer-based IR methods focus only on either local or\nglobal features, leading to limited receptive fields or deficient parameter\nissues. To address these problems, we propose a lightweight IR network,\nReciprocal Attention Mixing Transformer (RAMiT). It employs our proposed\ndimensional reciprocal attention mixing Transformer (D-RAMiT) blocks, which\ncompute bi-dimensional (spatial and channel) self-attentions in parallel with\ndifferent numbers of multi-heads. The bi-dimensional attentions help each other\nto complement their counterpart's drawbacks and are then mixed. Additionally,\nwe introduce a hierarchical reciprocal attention mixing (H-RAMi) layer that\ncompensates for pixel-level information losses and utilizes semantic\ninformation while maintaining an efficient hierarchical structure. Furthermore,\nwe revisit and modify MobileNet V1 and V2 to attach efficient convolutions to\nour proposed components. The experimental results demonstrate that RAMiT\nachieves state-of-the-art performance on multiple lightweight IR tasks,\nincluding super-resolution, color denoising, grayscale denoising, low-light\nenhancement, and deraining. Codes are available at\nhttps://github.com/rami0205/RAMiT.\n","authors":["Haram Choi","Cheolwoong Na","Jihyeon Oh","Seungjae Lee","Jinseop Kim","Subeen Choe","Jeongmin Lee","Taehoon Kim","Jihoon Yang"],"pdf_url":"https://arxiv.org/pdf/2305.11474v3.pdf","comment":"Technical report. 9 pages for main contents + 14 pages for appendix +\n  6 pages for references. Codes are available at\n  https://github.com/rami0205/RAMiT"},{"id":"http://arxiv.org/abs/2307.06120v1","updated":"2023-07-12T12:20:04Z","published":"2023-07-12T12:20:04Z","title":"Recognizing student identification numbers from the matrix templates\n  using a modified U-net architecture","summary":"  This paper presents an innovative approach to student identification during\nexams and knowledge tests, which overcomes the limitations of the traditional\npersonal information entry method. The proposed method employs a matrix\ntemplate on the designated section of the exam, where squares containing\nnumbers are selectively blackened. The methodology involves the development of\na neural network specifically designed for recognizing students' personal\nidentification numbers. The neural network utilizes a specially adapted U-Net\narchitecture, trained on an extensive dataset comprising images of blackened\ntables. The network demonstrates proficiency in recognizing the patterns and\narrangement of blackened squares, accurately interpreting the information\ninscribed within them. Additionally, the model exhibits high accuracy in\ncorrectly identifying entered student personal numbers and effectively\ndetecting erroneous entries within the table. This approach offers multiple\nadvantages. Firstly, it significantly accelerates the exam marking process by\nautomatically extracting identifying information from the blackened tables,\neliminating the need for manual entry and minimizing the potential for errors.\nSecondly, the method automates the identification process, thereby reducing\nadministrative effort and expediting data processing. The introduction of this\ninnovative identification system represents a notable advancement in the field\nof exams and knowledge tests, replacing the conventional manual entry of\npersonal data with a streamlined, efficient, and accurate identification\nprocess.\n","authors":["Filip Pavičić"],"pdf_url":"https://arxiv.org/pdf/2307.06120v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06118v1","updated":"2023-07-12T12:19:36Z","published":"2023-07-12T12:19:36Z","title":"TreeFormer: a Semi-Supervised Transformer-based Framework for Tree\n  Counting from a Single High Resolution Image","summary":"  Automatic tree density estimation and counting using single aerial and\nsatellite images is a challenging task in photogrammetry and remote sensing,\nyet has an important role in forest management. In this paper, we propose the\nfirst semisupervised transformer-based framework for tree counting which\nreduces the expensive tree annotations for remote sensing images. Our method,\ntermed as TreeFormer, first develops a pyramid tree representation module based\non transformer blocks to extract multi-scale features during the encoding\nstage. Contextual attention-based feature fusion and tree density regressor\nmodules are further designed to utilize the robust features from the encoder to\nestimate tree density maps in the decoder. Moreover, we propose a pyramid\nlearning strategy that includes local tree density consistency and local tree\ncount ranking losses to utilize unlabeled images into the training process.\nFinally, the tree counter token is introduced to regulate the network by\ncomputing the global tree counts for both labeled and unlabeled images. Our\nmodel was evaluated on two benchmark tree counting datasets, Jiangsu, and\nYosemite, as well as a new dataset, KCL-London, created by ourselves. Our\nTreeFormer outperforms the state of the art semi-supervised methods under the\nsame setting and exceeds the fully-supervised methods using the same number of\nlabeled images. The codes and datasets are available at\nhttps://github.com/HAAClassic/TreeFormer.\n","authors":["Hamed Amini Amirkolaee","Miaojing Shi","Mark Mulligan"],"pdf_url":"https://arxiv.org/pdf/2307.06118v1.pdf","comment":"Accepted in IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING"},{"id":"http://arxiv.org/abs/2307.06099v1","updated":"2023-07-12T11:45:22Z","published":"2023-07-12T11:45:22Z","title":"RFENet: Towards Reciprocal Feature Evolution for Glass Segmentation","summary":"  Glass-like objects are widespread in daily life but remain intractable to be\nsegmented for most existing methods. The transparent property makes it\ndifficult to be distinguished from background, while the tiny separation\nboundary further impedes the acquisition of their exact contour. In this paper,\nby revealing the key co-evolution demand of semantic and boundary learning, we\npropose a Selective Mutual Evolution (SME) module to enable the reciprocal\nfeature learning between them. Then to exploit the global shape context, we\npropose a Structurally Attentive Refinement (SAR) module to conduct a\nfine-grained feature refinement for those ambiguous points around the boundary.\nFinally, to further utilize the multi-scale representation, we integrate the\nabove two modules into a cascaded structure and then introduce a Reciprocal\nFeature Evolution Network (RFENet) for effective glass-like object\nsegmentation. Extensive experiments demonstrate that our RFENet achieves\nstate-of-the-art performance on three popular public datasets.\n","authors":["Ke Fan","Changan Wang","Yabiao Wang","Chengjie Wang","Ran Yi","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2307.06099v1.pdf","comment":"Accepted by 2023 International Joint Conference on Artificial\n  Intelligence (IJCAI2023)"},{"id":"http://arxiv.org/abs/2307.06091v1","updated":"2023-07-12T11:32:02Z","published":"2023-07-12T11:32:02Z","title":"AICT: An Adaptive Image Compression Transformer","summary":"  Motivated by the efficiency investigation of the Tranformer-based transform\ncoding framework, namely SwinT-ChARM, we propose to enhance the latter, as\nfirst, with a more straightforward yet effective Tranformer-based channel-wise\nauto-regressive prior model, resulting in an absolute image compression\ntransformer (ICT). Current methods that still rely on ConvNet-based entropy\ncoding are limited in long-range modeling dependencies due to their local\nconnectivity and an increasing number of architectural biases and priors. On\nthe contrary, the proposed ICT can capture both global and local contexts from\nthe latent representations and better parameterize the distribution of the\nquantized latents. Further, we leverage a learnable scaling module with a\nsandwich ConvNeXt-based pre/post-processor to accurately extract more compact\nlatent representation while reconstructing higher-quality images. Extensive\nexperimental results on benchmark datasets showed that the proposed adaptive\nimage compression transformer (AICT) framework significantly improves the\ntrade-off between coding efficiency and decoder complexity over the versatile\nvideo coding (VVC) reference encoder (VTM-18.0) and the neural codec\nSwinT-ChARM.\n","authors":["Ahmed Ghorbel","Wassim Hamidouche","Luce Morin"],"pdf_url":"https://arxiv.org/pdf/2307.06091v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2307.02273"},{"id":"http://arxiv.org/abs/2307.02273v2","updated":"2023-07-12T11:20:58Z","published":"2023-07-05T13:17:14Z","title":"Joint Hierarchical Priors and Adaptive Spatial Resolution for Efficient\n  Neural Image Compression","summary":"  Recently, the performance of neural image compression (NIC) has steadily\nimproved thanks to the last line of study, reaching or outperforming\nstate-of-the-art conventional codecs. Despite significant progress, current NIC\nmethods still rely on ConvNet-based entropy coding, limited in modeling\nlong-range dependencies due to their local connectivity and the increasing\nnumber of architectural biases and priors, resulting in complex underperforming\nmodels with high decoding latency. Motivated by the efficiency investigation of\nthe Tranformer-based transform coding framework, namely SwinT-ChARM, we propose\nto enhance the latter, as first, with a more straightforward yet effective\nTranformer-based channel-wise auto-regressive prior model, resulting in an\nabsolute image compression transformer (ICT). Through the proposed ICT, we can\ncapture both global and local contexts from the latent representations and\nbetter parameterize the distribution of the quantized latents. Further, we\nleverage a learnable scaling module with a sandwich ConvNeXt-based\npre-/post-processor to accurately extract more compact latent codes while\nreconstructing higher-quality images. Extensive experimental results on\nbenchmark datasets showed that the proposed framework significantly improves\nthe trade-off between coding efficiency and decoder complexity over the\nversatile video coding (VVC) reference encoder (VTM-18.0) and the neural codec\nSwinT-ChARM. Moreover, we provide model scaling studies to verify the\ncomputational efficiency of our approach and conduct several objective and\nsubjective analyses to bring to the fore the performance gap between the\nadaptive image compression transformer (AICT) and the neural codec SwinT-ChARM.\n","authors":["Ahmed Ghorbel","Wassim Hamidouche","Luce Morin"],"pdf_url":"https://arxiv.org/pdf/2307.02273v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06082v1","updated":"2023-07-12T11:08:24Z","published":"2023-07-12T11:08:24Z","title":"VELMA: Verbalization Embodiment of LLM Agents for Vision and Language\n  Navigation in Street View","summary":"  Incremental decision making in real-world environments is one of the most\nchallenging tasks in embodied artificial intelligence. One particularly\ndemanding scenario is Vision and Language Navigation~(VLN) which requires\nvisual and natural language understanding as well as spatial and temporal\nreasoning capabilities. The embodied agent needs to ground its understanding of\nnavigation instructions in observations of a real-world environment like Street\nView. Despite the impressive results of LLMs in other research areas, it is an\nongoing problem of how to best connect them with an interactive visual\nenvironment. In this work, we propose VELMA, an embodied LLM agent that uses a\nverbalization of the trajectory and of visual environment observations as\ncontextual prompt for the next action. Visual information is verbalized by a\npipeline that extracts landmarks from the human written navigation instructions\nand uses CLIP to determine their visibility in the current panorama view. We\nshow that VELMA is able to successfully follow navigation instructions in\nStreet View with only two in-context examples. We further finetune the LLM\nagent on a few thousand examples and achieve 25%-30% relative improvement in\ntask completion over the previous state-of-the-art for two datasets.\n","authors":["Raphael Schumann","Wanrong Zhu","Weixi Feng","Tsu-Jui Fu","Stefan Riezler","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2307.06082v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.04132v2","updated":"2023-07-12T10:57:00Z","published":"2023-07-09T09:04:26Z","title":"Reasoning over the Behaviour of Objects in Video-Clips for Adverb-Type\n  Recognition","summary":"  In this work, following the intuition that adverbs describing scene-sequences\nare best identified by reasoning over high-level concepts of object-behavior,\nwe propose the design of a new framework that reasons over object-behaviours\nextracted from raw-video-clips to recognize the clip's corresponding\nadverb-types. Importantly, while previous works for general scene\nadverb-recognition assume knowledge of the clips underlying action-types, our\nmethod is directly applicable in the more general problem setting where the\naction-type of a video-clip is unknown. Specifically, we propose a novel\npipeline that extracts human-interpretable object-behaviour-facts from raw\nvideo clips and propose novel symbolic and transformer based reasoning methods\nthat operate over these extracted facts to identify adverb-types. Experiment\nresults demonstrate that our proposed methods perform favourably against the\nprevious state-of-the-art. Additionally, to support efforts in symbolic\nvideo-processing, we release two new datasets of object-behaviour-facts\nextracted from raw video clips - the MSR-VTT-ASP and ActivityNet-ASP datasets.\n","authors":["Amrit Diggavi Seshadri","Alessandra Russo"],"pdf_url":"https://arxiv.org/pdf/2307.04132v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.14618v2","updated":"2023-07-12T10:44:51Z","published":"2023-03-26T04:04:58Z","title":"BoxVIS: Video Instance Segmentation with Box Annotations","summary":"  It is expensive and labour-extensive to label the pixel-wise object masks in\na video. As a result, the amount of pixel-wise annotations in existing video\ninstance segmentation (VIS) datasets is small, limiting the generalization\ncapability of trained VIS models. An alternative but much cheaper solution is\nto use bounding boxes to label instances in videos. Inspired by the recent\nsuccess of box-supervised image instance segmentation, we adapt the\nstate-of-the-art pixel-supervised VIS models to a box-supervised VIS (BoxVIS)\nbaseline, and observe slight performance degradation. We consequently propose\nto improve the BoxVIS performance from two aspects. First, we propose a\nbox-center guided spatial-temporal pairwise affinity (STPA) loss to predict\ninstance masks for better spatial and temporal consistency. Second, we collect\na larger scale box-annotated VIS dataset (BVISD) by consolidating the videos\nfrom current VIS benchmarks and converting images from the COCO dataset to\nshort pseudo video clips. With the proposed BVISD and the STPA loss, our\ntrained BoxVIS model achieves 43.2\\% and 29.0\\% mask AP on the YouTube-VIS 2021\nand OVIS valid sets, respectively. It exhibits comparable instance mask\nprediction performance and better generalization ability than state-of-the-art\npixel-supervised VIS models by using only 16\\% of their annotation time and\ncost. Codes and data can be found at \\url{https://github.com/MinghanLi/BoxVIS}.\n","authors":["Minghan Li","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.14618v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06065v1","updated":"2023-07-12T10:29:40Z","published":"2023-07-12T10:29:40Z","title":"Operational Support Estimator Networks","summary":"  In this work, we propose a novel approach called Operational Support\nEstimator Networks (OSENs) for the support estimation task. Support Estimation\n(SE) is defined as finding the locations of non-zero elements in a sparse\nsignal. By its very nature, the mapping between the measurement and sparse\nsignal is a non-linear operation. Traditional support estimators rely on\ncomputationally expensive iterative signal recovery techniques to achieve such\nnon-linearity. Contrary to the convolution layers, the proposed OSEN approach\nconsists of operational layers that can learn such complex non-linearities\nwithout the need for deep networks. In this way, the performance of the\nnon-iterative support estimation is greatly improved. Moreover, the operational\nlayers comprise so-called generative \\textit{super neurons} with non-local\nkernels. The kernel location for each neuron/feature map is optimized jointly\nfor the SE task during the training. We evaluate the OSENs in three different\napplications: i. support estimation from Compressive Sensing (CS) measurements,\nii. representation-based classification, and iii. learning-aided CS\nreconstruction where the output of OSENs is used as prior knowledge to the CS\nalgorithm for an enhanced reconstruction. Experimental results show that the\nproposed approach achieves computational efficiency and outperforms competing\nmethods, especially at low measurement rates by a significant margin. The\nsoftware implementation is publicly shared at\nhttps://github.com/meteahishali/OSEN.\n","authors":["Mete Ahishali","Mehmet Yamac","Serkan Kiranyaz","Moncef Gabbouj"],"pdf_url":"https://arxiv.org/pdf/2307.06065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06052v1","updated":"2023-07-12T10:12:57Z","published":"2023-07-12T10:12:57Z","title":"Visualization for Multivariate Gaussian Anomaly Detection in Images","summary":"  This paper introduces a simplified variation of the PaDiM (Pixel-Wise Anomaly\nDetection through Instance Modeling) method for anomaly detection in images,\nfitting a single multivariate Gaussian (MVG) distribution to the feature\nvectors extracted from a backbone convolutional neural network (CNN) and using\ntheir Mahalanobis distance as the anomaly score. We introduce an intermediate\nstep in this framework by applying a whitening transformation to the feature\nvectors, which enables the generation of heatmaps capable of visually\nexplaining the features learned by the MVG. The proposed technique is evaluated\non the MVTec-AD dataset, and the results show the importance of visual model\nvalidation, providing insights into issues in this framework that were\notherwise invisible. The visualizations generated for this paper are publicly\navailable at https://doi.org/10.5281/zenodo.7937978.\n","authors":["Joao P C Bertoldo","David Arrustico"],"pdf_url":"https://arxiv.org/pdf/2307.06052v1.pdf","comment":"6 pages, 8 figures, accepted to 2023 Twelfth International Conference\n  on Image Processing Theory, Tools and Applications (IPTA)"},{"id":"http://arxiv.org/abs/2306.07591v2","updated":"2023-07-12T09:45:54Z","published":"2023-06-13T07:35:28Z","title":"I See Dead People: Gray-Box Adversarial Attack on Image-To-Text Models","summary":"  Modern image-to-text systems typically adopt the encoder-decoder framework,\nwhich comprises two main components: an image encoder, responsible for\nextracting image features, and a transformer-based decoder, used for generating\ncaptions. Taking inspiration from the analysis of neural networks' robustness\nagainst adversarial perturbations, we propose a novel gray-box algorithm for\ncreating adversarial examples in image-to-text models. Unlike image\nclassification tasks that have a finite set of class labels, finding visually\nsimilar adversarial examples in an image-to-text task poses greater challenges\nbecause the captioning system allows for a virtually infinite space of possible\ncaptions. In this paper, we present a gray-box adversarial attack on\nimage-to-text, both untargeted and targeted. We formulate the process of\ndiscovering adversarial perturbations as an optimization problem that uses only\nthe image-encoder component, meaning the proposed attack is language-model\nagnostic. Through experiments conducted on the ViT-GPT2 model, which is the\nmost-used image-to-text model in Hugging Face, and the Flickr30k dataset, we\ndemonstrate that our proposed attack successfully generates visually similar\nadversarial examples, both with untargeted and targeted captions. Notably, our\nattack operates in a gray-box manner, requiring no knowledge about the decoder\nmodule. We also show that our attacks fool the popular open-source platform\nHugging Face.\n","authors":["Raz Lapid","Moshe Sipper"],"pdf_url":"https://arxiv.org/pdf/2306.07591v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06038v1","updated":"2023-07-12T09:33:21Z","published":"2023-07-12T09:33:21Z","title":"Pyramid Deep Fusion Network for Two-Hand Reconstruction from RGB-D\n  Images","summary":"  Accurately recovering the dense 3D mesh of both hands from monocular images\nposes considerable challenges due to occlusions and projection ambiguity. Most\nof the existing methods extract features from color images to estimate the\nroot-aligned hand meshes, which neglect the crucial depth and scale information\nin the real world. Given the noisy sensor measurements with limited resolution,\ndepth-based methods predict 3D keypoints rather than a dense mesh. These\nlimitations motivate us to take advantage of these two complementary inputs to\nacquire dense hand meshes on a real-world scale. In this work, we propose an\nend-to-end framework for recovering dense meshes for both hands, which employ\nsingle-view RGB-D image pairs as input. The primary challenge lies in\neffectively utilizing two different input modalities to mitigate the blurring\neffects in RGB images and noises in depth images. Instead of directly treating\ndepth maps as additional channels for RGB images, we encode the depth\ninformation into the unordered point cloud to preserve more geometric details.\nSpecifically, our framework employs ResNet50 and PointNet++ to derive features\nfrom RGB and point cloud, respectively. Additionally, we introduce a novel\npyramid deep fusion network (PDFNet) to aggregate features at different scales,\nwhich demonstrates superior efficacy compared to previous fusion strategies.\nFurthermore, we employ a GCN-based decoder to process the fused features and\nrecover the corresponding 3D pose and dense mesh. Through comprehensive\nablation experiments, we have not only demonstrated the effectiveness of our\nproposed fusion algorithm but also outperformed the state-of-the-art approaches\non publicly available datasets. To reproduce the results, we will make our\nsource code and models publicly available at\n{\\url{https://github.com/zijinxuxu/PDFNet}}.\n","authors":["Jinwei Ren","Jianke Zhu"],"pdf_url":"https://arxiv.org/pdf/2307.06038v1.pdf","comment":"Submitted to TCSVT"},{"id":"http://arxiv.org/abs/2307.05288v2","updated":"2023-07-12T09:25:03Z","published":"2023-07-11T14:28:33Z","title":"Navigating Uncertainty: The Role of Short-Term Trajectory Prediction in\n  Autonomous Vehicle Safety","summary":"  Autonomous vehicles require accurate and reliable short-term trajectory\npredictions for safe and efficient driving. While most commercial automated\nvehicles currently use state machine-based algorithms for trajectory\nforecasting, recent efforts have focused on end-to-end data-driven systems.\nOften, the design of these models is limited by the availability of datasets,\nwhich are typically restricted to generic scenarios. To address this\nlimitation, we have developed a synthetic dataset for short-term trajectory\nprediction tasks using the CARLA simulator. This dataset is extensive and\nincorporates what is considered complex scenarios - pedestrians crossing the\nroad, vehicles overtaking - and comprises 6000 perspective view images with\ncorresponding IMU and odometry information for each frame. Furthermore, an\nend-to-end short-term trajectory prediction model using convolutional neural\nnetworks (CNN) and long short-term memory (LSTM) networks has also been\ndeveloped. This model can handle corner cases, such as slowing down near zebra\ncrossings and stopping when pedestrians cross the road, without the need for\nexplicit encoding of the surrounding environment. In an effort to accelerate\nthis research and assist others, we are releasing our dataset and model to the\nresearch community. Our datasets are publicly available on\nhttps://github.com/sharmasushil/Navigating-Uncertainty-Trajectory-Prediction .\n","authors":["Sushil Sharma","Ganesh Sistu","Lucie Yahiaoui","Arindam Das","Mark Halton","Ciarán Eising"],"pdf_url":"https://arxiv.org/pdf/2307.05288v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06026v1","updated":"2023-07-12T09:14:35Z","published":"2023-07-12T09:14:35Z","title":"Learning from Exemplary Explanations","summary":"  eXplanation Based Learning (XBL) is a form of Interactive Machine Learning\n(IML) that provides a model refining approach via user feedback collected on\nmodel explanations. Although the interactivity of XBL promotes model\ntransparency, XBL requires a huge amount of user interaction and can become\nexpensive as feedback is in the form of detailed annotation rather than simple\ncategory labelling which is more common in IML. This expense is exacerbated in\nhigh stakes domains such as medical image classification. To reduce the effort\nand expense of XBL we introduce a new approach that uses two input instances\nand their corresponding Gradient Weighted Class Activation Mapping (GradCAM)\nmodel explanations as exemplary explanations to implement XBL. Using a medical\nimage classification task, we demonstrate that, using minimal human input, our\napproach produces improved explanations (+0.02, +3%) and achieves reduced\nclassification performance (-0.04, -4%) when compared against a model trained\nwithout interactions.\n","authors":["Misgina Tsighe Hagos","Kathleen M. Curran","Brian Mac Namee"],"pdf_url":"https://arxiv.org/pdf/2307.06026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11615v2","updated":"2023-07-12T09:05:17Z","published":"2023-03-21T06:20:49Z","title":"Robust Table Structure Recognition with Dynamic Queries Enhanced\n  Detection Transformer","summary":"  We present a new table structure recognition (TSR) approach, called\nTSRFormer, to robustly recognizing the structures of complex tables with\ngeometrical distortions from various table images. Unlike previous methods, we\nformulate table separation line prediction as a line regression problem instead\nof an image segmentation problem and propose a new two-stage dynamic queries\nenhanced DETR based separation line regression approach, named DQ-DETR, to\npredict separation lines from table images directly. Compared to Vallina DETR,\nwe propose three improvements in DQ-DETR to make the two-stage DETR framework\nwork efficiently and effectively for the separation line prediction task: 1) A\nnew query design, named Dynamic Query, to decouple single line query into\nseparable point queries which could intuitively improve the localization\naccuracy for regression tasks; 2) A dynamic queries based progressive line\nregression approach to progressively regressing points on the line which\nfurther enhances localization accuracy for distorted tables; 3) A\nprior-enhanced matching strategy to solve the slow convergence issue of DETR.\nAfter separation line prediction, a simple relation network based cell merging\nmodule is used to recover spanning cells. With these new techniques, our\nTSRFormer achieves state-of-the-art performance on several benchmark datasets,\nincluding SciTSR, PubTabNet, WTW and FinTabNet. Furthermore, we have validated\nthe robustness and high localization accuracy of our approach to tables with\ncomplex structures, borderless cells, large blank spaces, empty or spanning\ncells as well as distorted or even curved shapes on a more challenging\nreal-world in-house dataset.\n","authors":["Jiawei Wang","Weihong Lin","Chixiang Ma","Mingze Li","Zheng Sun","Lei Sun","Qiang Huo"],"pdf_url":"https://arxiv.org/pdf/2303.11615v2.pdf","comment":"18 pages, 11 figures, PR2023. arXiv admin note: substantial text\n  overlap with arXiv:2208.04921"},{"id":"http://arxiv.org/abs/2307.06006v1","updated":"2023-07-12T08:35:24Z","published":"2023-07-12T08:35:24Z","title":"What Happens During Finetuning of Vision Transformers: An Invariance\n  Based Investigation","summary":"  The pretrain-finetune paradigm usually improves downstream performance over\ntraining a model from scratch on the same task, becoming commonplace across\nmany areas of machine learning. While pretraining is empirically observed to be\nbeneficial for a range of tasks, there is not a clear understanding yet of the\nreasons for this effect. In this work, we examine the relationship between\npretrained vision transformers and the corresponding finetuned versions on\nseveral benchmark datasets and tasks. We present new metrics that specifically\ninvestigate the degree to which invariances learned by a pretrained model are\nretained or forgotten during finetuning. Using these metrics, we present a\nsuite of empirical findings, including that pretraining induces transferable\ninvariances in shallow layers and that invariances from deeper pretrained\nlayers are compressed towards shallower layers during finetuning. Together,\nthese findings contribute to understanding some of the reasons for the\nsuccesses of pretrained models and the changes that a pretrained model\nundergoes when finetuned on a downstream task.\n","authors":["Gabriele Merlin","Vedant Nanda","Ruchit Rawal","Mariya Toneva"],"pdf_url":"https://arxiv.org/pdf/2307.06006v1.pdf","comment":"Accepted to CoLLAs 2023"},{"id":"http://arxiv.org/abs/2307.06003v1","updated":"2023-07-12T08:30:42Z","published":"2023-07-12T08:30:42Z","title":"Unsupervised Optical Flow Estimation with Dynamic Timing Representation\n  for Spike Camera","summary":"  Efficiently selecting an appropriate spike stream data length to extract\nprecise information is the key to the spike vision tasks. To address this\nissue, we propose a dynamic timing representation for spike streams. Based on\nmulti-layers architecture, it applies dilated convolutions on temporal\ndimension to extract features on multi-temporal scales with few parameters. And\nwe design layer attention to dynamically fuse these features. Moreover, we\npropose an unsupervised learning method for optical flow estimation in a\nspike-based manner to break the dependence on labeled data. In addition, to\nverify the robustness, we also build a spike-based synthetic validation dataset\nfor extreme scenarios in autonomous driving, denoted as SSES dataset. It\nconsists of various corner cases. Experiments show that our method can predict\noptical flow from spike streams in different high-speed scenes, including real\nscenes. For instance, our method gets $15\\%$ and $19\\%$ error reduction from\nthe best spike-based work, SCFlow, in $\\Delta t=10$ and $\\Delta t=20$\nrespectively which are the same settings as the previous works.\n","authors":["Lujie Xia","Ziluo Ding","Rui Zhao","Jiyuan Zhang","Lei Ma","Zhaofei Yu","Tiejun Huang","Ruiqin Xiong"],"pdf_url":"https://arxiv.org/pdf/2307.06003v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05999v1","updated":"2023-07-12T08:26:27Z","published":"2023-07-12T08:26:27Z","title":"Flexible and Fully Quantized Ultra-Lightweight TinyissimoYOLO for\n  Ultra-Low-Power Edge Systems","summary":"  This paper deploys and explores variants of TinyissimoYOLO, a highly flexible\nand fully quantized ultra-lightweight object detection network designed for\nedge systems with a power envelope of a few milliwatts. With experimental\nmeasurements, we present a comprehensive characterization of the network's\ndetection performance, exploring the impact of various parameters, including\ninput resolution, number of object classes, and hidden layer adjustments. We\ndeploy variants of TinyissimoYOLO on state-of-the-art ultra-low-power extreme\nedge platforms, presenting an in-depth a comparison on latency, energy\nefficiency, and their ability to efficiently parallelize the workload. In\nparticular, the paper presents a comparison between a novel parallel RISC-V\nprocessor (GAP9 from Greenwaves) with and without use of its on-chip hardware\naccelerator, an ARM Cortex-M7 core (STM32H7 from ST Microelectronics), two ARM\nCortex-M4 cores (STM32L4 from STM and Apollo4b from Ambiq), and a multi-core\nplatform with a CNN hardware accelerator (Analog Devices MAX78000).\nExperimental results show that the GAP9's hardware accelerator achieves the\nlowest inference latency and energy at 2.12ms and 150uJ respectively, which is\naround 2x faster and 20% more efficient than the next best platform, the\nMAX78000. The hardware accelerator of GAP9 can even run an increased resolution\nversion of TinyissimoYOLO with 112x112 pixels and 10 detection classes within\n3.2ms, consuming 245uJ. To showcase the competitiveness of a versatile\ngeneral-purpose system we also deployed and profiled a multi-core\nimplementation on GAP9 at different operating points, achieving 11.3ms with the\nlowest-latency and 490uJ with the most energy-efficient configuration. With\nthis paper, we demonstrate the suitability and flexibility of TinyissimoYOLO on\nstate-of-the-art detection datasets for real-time ultra-low-power edge\ninference.\n","authors":["Julian Moosmann","Hanna Mueller","Nicky Zimmerman","Georg Rutishauser","Luca Benini","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2307.05999v1.pdf","comment":"* All three authors contributed equally to this research"},{"id":"http://arxiv.org/abs/2211.11220v3","updated":"2023-07-12T08:19:00Z","published":"2022-11-21T07:29:24Z","title":"STGlow: A Flow-based Generative Framework with Dual Graphormer for\n  Pedestrian Trajectory Prediction","summary":"  The pedestrian trajectory prediction task is an essential component of\nintelligent systems. Its applications include but are not limited to autonomous\ndriving, robot navigation, and anomaly detection of monitoring systems. Due to\nthe diversity of motion behaviors and the complex social interactions among\npedestrians, accurately forecasting their future trajectory is challenging.\nExisting approaches commonly adopt GANs or CVAEs to generate diverse\ntrajectories. However, GAN-based methods do not directly model data in a latent\nspace, which may make them fail to have full support over the underlying data\ndistribution; CVAE-based methods optimize a lower bound on the log-likelihood\nof observations, which may cause the learned distribution to deviate from the\nunderlying distribution. The above limitations make existing approaches often\ngenerate highly biased or inaccurate trajectories. In this paper, we propose a\nnovel generative flow based framework with dual graphormer for pedestrian\ntrajectory prediction (STGlow). Different from previous approaches, our method\ncan more precisely model the underlying data distribution by optimizing the\nexact log-likelihood of motion behaviors. Besides, our method has clear\nphysical meanings for simulating the evolution of human motion behaviors. The\nforward process of the flow gradually degrades complex motion behavior into\nsimple behavior, while its reverse process represents the evolution of simple\nbehavior into complex motion behavior. Further, we introduce a dual graphormer\ncombining with the graph structure to more adequately model the temporal\ndependencies and the mutual spatial interactions. Experimental results on\nseveral benchmarks demonstrate that our method achieves much better performance\ncompared to previous state-of-the-art approaches.\n","authors":["Rongqin Liang","Yuanman Li","Jiantao Zhou","Xia Li"],"pdf_url":"https://arxiv.org/pdf/2211.11220v3.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2307.05979v1","updated":"2023-07-12T07:51:12Z","published":"2023-07-12T07:51:12Z","title":"Transformers in Reinforcement Learning: A Survey","summary":"  Transformers have significantly impacted domains like natural language\nprocessing, computer vision, and robotics, where they improve performance\ncompared to other neural networks. This survey explores how transformers are\nused in reinforcement learning (RL), where they are seen as a promising\nsolution for addressing challenges such as unstable training, credit\nassignment, lack of interpretability, and partial observability. We begin by\nproviding a brief domain overview of RL, followed by a discussion on the\nchallenges of classical RL algorithms. Next, we delve into the properties of\nthe transformer and its variants and discuss the characteristics that make them\nwell-suited to address the challenges inherent in RL. We examine the\napplication of transformers to various aspects of RL, including representation\nlearning, transition and reward function modeling, and policy optimization. We\nalso discuss recent research that aims to enhance the interpretability and\nefficiency of transformers in RL, using visualization techniques and efficient\ntraining strategies. Often, the transformer architecture must be tailored to\nthe specific needs of a given application. We present a broad overview of how\ntransformers have been adapted for several applications, including robotics,\nmedicine, language modeling, cloud computing, and combinatorial optimization.\nWe conclude by discussing the limitations of using transformers in RL and\nassess their potential for catalyzing future breakthroughs in this field.\n","authors":["Pranav Agarwal","Aamer Abdul Rahman","Pierre-Luc St-Charles","Simon J. D. Prince","Samira Ebrahimi Kahou"],"pdf_url":"https://arxiv.org/pdf/2307.05979v1.pdf","comment":"35 pages, 11 figures"},{"id":"http://arxiv.org/abs/2307.05977v1","updated":"2023-07-12T07:48:29Z","published":"2023-07-12T07:48:29Z","title":"Towards Safe Self-Distillation of Internet-Scale Text-to-Image Diffusion\n  Models","summary":"  Large-scale image generation models, with impressive quality made possible by\nthe vast amount of data available on the Internet, raise social concerns that\nthese models may generate harmful or copyrighted content. The biases and\nharmfulness arise throughout the entire training process and are hard to\ncompletely remove, which have become significant hurdles to the safe deployment\nof these models. In this paper, we propose a method called SDD to prevent\nproblematic content generation in text-to-image diffusion models. We\nself-distill the diffusion model to guide the noise estimate conditioned on the\ntarget removal concept to match the unconditional one. Compared to the previous\nmethods, our method eliminates a much greater proportion of harmful content\nfrom the generated images without degrading the overall image quality.\nFurthermore, our method allows the removal of multiple concepts at once,\nwhereas previous works are limited to removing a single concept at a time.\n","authors":["Sanghyun Kim","Seohyeon Jung","Balhae Kim","Moonseok Choi","Jinwoo Shin","Juho Lee"],"pdf_url":"https://arxiv.org/pdf/2307.05977v1.pdf","comment":"17 pages, 13 figures, ICML 2023 Workshop on Challenges in Deployable\n  Generative AI"},{"id":"http://arxiv.org/abs/2307.05973v1","updated":"2023-07-12T07:40:48Z","published":"2023-07-12T07:40:48Z","title":"VoxPoser: Composable 3D Value Maps for Robotic Manipulation with\n  Language Models","summary":"  Large language models (LLMs) are shown to possess a wealth of actionable\nknowledge that can be extracted for robot manipulation in the form of reasoning\nand planning. Despite the progress, most still rely on pre-defined motion\nprimitives to carry out the physical interactions with the environment, which\nremains a major bottleneck. In this work, we aim to synthesize robot\ntrajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a\nlarge variety of manipulation tasks given an open-set of instructions and an\nopen-set of objects. We achieve this by first observing that LLMs excel at\ninferring affordances and constraints given a free-form language instruction.\nMore importantly, by leveraging their code-writing capabilities, they can\ninteract with a visual-language model (VLM) to compose 3D value maps to ground\nthe knowledge into the observation space of the agent. The composed value maps\nare then used in a model-based planning framework to zero-shot synthesize\nclosed-loop robot trajectories with robustness to dynamic perturbations. We\nfurther demonstrate how the proposed framework can benefit from online\nexperiences by efficiently learning a dynamics model for scenes that involve\ncontact-rich interactions. We present a large-scale study of the proposed\nmethod in both simulated and real-robot environments, showcasing the ability to\nperform a large variety of everyday manipulation tasks specified in free-form\nnatural language. Project website: https://voxposer.github.io\n","authors":["Wenlong Huang","Chen Wang","Ruohan Zhang","Yunzhu Li","Jiajun Wu","Li Fei-Fei"],"pdf_url":"https://arxiv.org/pdf/2307.05973v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05963v1","updated":"2023-07-12T07:12:20Z","published":"2023-07-12T07:12:20Z","title":"GVCCI: Lifelong Learning of Visual Grounding for Language-Guided Robotic\n  Manipulation","summary":"  Language-Guided Robotic Manipulation (LGRM) is a challenging task as it\nrequires a robot to understand human instructions to manipulate everyday\nobjects. Recent approaches in LGRM rely on pre-trained Visual Grounding (VG)\nmodels to detect objects without adapting to manipulation environments. This\nresults in a performance drop due to a substantial domain gap between the\npre-training and real-world data. A straightforward solution is to collect\nadditional training data, but the cost of human-annotation is extortionate. In\nthis paper, we propose Grounding Vision to Ceaselessly Created Instructions\n(GVCCI), a lifelong learning framework for LGRM, which continuously learns VG\nwithout human supervision. GVCCI iteratively generates synthetic instruction\nvia object detection and trains the VG model with the generated data. We\nvalidate our framework in offline and online settings across diverse\nenvironments on different VG models. Experimental results show that\naccumulating synthetic data from GVCCI leads to a steady improvement in VG by\nup to 56.7% and improves resultant LGRM by up to 29.4%. Furthermore, the\nqualitative analysis shows that the unadapted VG model often fails to find\ncorrect objects due to a strong bias learned from the pre-training data.\nFinally, we introduce a novel VG dataset for LGRM, consisting of nearly 252k\ntriplets of image-object-instruction from diverse manipulation environments.\n","authors":["Junghyun Kim","Gi-Cheon Kang","Jaein Kim","Suyeon Shin","Byoung-Tak Zhang"],"pdf_url":"https://arxiv.org/pdf/2307.05963v1.pdf","comment":"Accepted at IROS2023"},{"id":"http://arxiv.org/abs/2306.07615v4","updated":"2023-07-12T07:00:14Z","published":"2023-06-13T08:19:14Z","title":"UOD: Universal One-shot Detection of Anatomical Landmarks","summary":"  One-shot medical landmark detection gains much attention and achieves great\nsuccess for its label-efficient training process. However, existing one-shot\nlearning methods are highly specialized in a single domain and suffer domain\npreference heavily in the situation of multi-domain unlabeled data. Moreover,\none-shot learning is not robust that it faces performance drop when annotating\na sub-optimal image. To tackle these issues, we resort to developing a\ndomain-adaptive one-shot landmark detection framework for handling multi-domain\nmedical images, named Universal One-shot Detection (UOD). UOD consists of two\nstages and two corresponding universal models which are designed as\ncombinations of domain-specific modules and domain-shared modules. In the first\nstage, a domain-adaptive convolution model is self-supervised learned to\ngenerate pseudo landmark labels. In the second stage, we design a\ndomain-adaptive transformer to eliminate domain preference and build the global\ncontext for multi-domain data. Even though only one annotated sample from each\ndomain is available for training, the domain-shared modules help UOD aggregate\nall one-shot samples to detect more robust and accurate landmarks. We\ninvestigated both qualitatively and quantitatively the proposed UOD on three\nwidely-used public X-ray datasets in different anatomical domains (i.e., head,\nhand, chest) and obtained state-of-the-art performances in each domain. The\ncode is available at\nhttps://github.com/heqin-zhu/UOD_universal_oneshot_detection.\n","authors":["Heqin Zhu","Quan Quan","Qingsong Yao","Zaiyi Liu","S. kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2306.07615v4.pdf","comment":"Eealy accepted by MICCAI 2023. 11pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2307.05945v1","updated":"2023-07-12T06:22:51Z","published":"2023-07-12T06:22:51Z","title":"YOGA: Deep Object Detection in the Wild with Lightweight Feature\n  Learning and Multiscale Attention","summary":"  We introduce YOGA, a deep learning based yet lightweight object detection\nmodel that can operate on low-end edge devices while still achieving\ncompetitive accuracy. The YOGA architecture consists of a two-phase feature\nlearning pipeline with a cheap linear transformation, which learns feature maps\nusing only half of the convolution filters required by conventional\nconvolutional neural networks. In addition, it performs multi-scale feature\nfusion in its neck using an attention mechanism instead of the naive\nconcatenation used by conventional detectors. YOGA is a flexible model that can\nbe easily scaled up or down by several orders of magnitude to fit a broad range\nof hardware constraints. We evaluate YOGA on COCO-val and COCO-testdev datasets\nwith other over 10 state-of-the-art object detectors. The results show that\nYOGA strikes the best trade-off between model size and accuracy (up to 22%\nincrease of AP and 23-34% reduction of parameters and FLOPs), making it an\nideal choice for deployment in the wild on low-end edge devices. This is\nfurther affirmed by our hardware implementation and evaluation on NVIDIA Jetson\nNano.\n","authors":["Raja Sunkara","Tie Luo"],"pdf_url":"https://arxiv.org/pdf/2307.05945v1.pdf","comment":"Published in Pattern Recognition (Elsevier), July 2023"},{"id":"http://arxiv.org/abs/2306.04934v2","updated":"2023-07-12T06:20:04Z","published":"2023-06-08T04:32:10Z","title":"On the Effectiveness of Out-of-Distribution Data in Self-Supervised\n  Long-Tail Learning","summary":"  Though Self-supervised learning (SSL) has been widely studied as a promising\ntechnique for representation learning, it doesn't generalize well on\nlong-tailed datasets due to the majority classes dominating the feature space.\nRecent work shows that the long-tailed learning performance could be boosted by\nsampling extra in-domain (ID) data for self-supervised training, however,\nlarge-scale ID data which can rebalance the minority classes are expensive to\ncollect. In this paper, we propose an alternative but easy-to-use and effective\nsolution, Contrastive with Out-of-distribution (OOD) data for Long-Tail\nlearning (COLT), which can effectively exploit OOD data to dynamically\nre-balance the feature space. We empirically identify the counter-intuitive\nusefulness of OOD samples in SSL long-tailed learning and principally design a\nnovel SSL method. Concretely, we first localize the `head' and `tail' samples\nby assigning a tailness score to each OOD sample based on its neighborhoods in\nthe feature space. Then, we propose an online OOD sampling strategy to\ndynamically re-balance the feature space. Finally, we enforce the model to be\ncapable of distinguishing ID and OOD samples by a distribution-level supervised\ncontrastive loss. Extensive experiments are conducted on various datasets and\nseveral state-of-the-art SSL frameworks to verify the effectiveness of the\nproposed method. The results show that our method significantly improves the\nperformance of SSL on long-tailed datasets by a large margin, and even\noutperforms previous work which uses external ID data. Our code is available at\nhttps://github.com/JianhongBai/COLT.\n","authors":["Jianhong Bai","Zuozhu Liu","Hualiang Wang","Jin Hao","Yang Feng","Huanpeng Chu","Haoji Hu"],"pdf_url":"https://arxiv.org/pdf/2306.04934v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05942v1","updated":"2023-07-12T06:14:36Z","published":"2023-07-12T06:14:36Z","title":"Prototypical Contrastive Transfer Learning for Multimodal Language\n  Understanding","summary":"  Although domestic service robots are expected to assist individuals who\nrequire support, they cannot currently interact smoothly with people through\nnatural language. For example, given the instruction \"Bring me a bottle from\nthe kitchen,\" it is difficult for such robots to specify the bottle in an\nindoor environment. Most conventional models have been trained on real-world\ndatasets that are labor-intensive to collect, and they have not fully leveraged\nsimulation data through a transfer learning framework. In this study, we\npropose a novel transfer learning approach for multimodal language\nunderstanding called Prototypical Contrastive Transfer Learning (PCTL), which\nuses a new contrastive loss called Dual ProtoNCE. We introduce PCTL to the task\nof identifying target objects in domestic environments according to free-form\nnatural language instructions. To validate PCTL, we built new real-world and\nsimulation datasets. Our experiment demonstrated that PCTL outperformed\nexisting methods. Specifically, PCTL achieved an accuracy of 78.1%, whereas\nsimple fine-tuning achieved an accuracy of 73.4%.\n","authors":["Seitaro Otsuki","Shintaro Ishikawa","Komei Sugiura"],"pdf_url":"https://arxiv.org/pdf/2307.05942v1.pdf","comment":"Accepted for presentation at IROS23"},{"id":"http://arxiv.org/abs/2306.00001v2","updated":"2023-07-12T06:10:52Z","published":"2023-05-22T12:57:38Z","title":"TinyissimoYOLO: A Quantized, Low-Memory Footprint, TinyML Object\n  Detection Network for Low Power Microcontrollers","summary":"  This paper introduces a highly flexible, quantized, memory-efficient, and\nultra-lightweight object detection network, called TinyissimoYOLO. It aims to\nenable object detection on microcontrollers in the power domain of milliwatts,\nwith less than 0.5MB memory available for storing convolutional neural network\n(CNN) weights. The proposed quantized network architecture with 422k\nparameters, enables real-time object detection on embedded microcontrollers,\nand it has been evaluated to exploit CNN accelerators. In particular, the\nproposed network has been deployed on the MAX78000 microcontroller achieving\nhigh frame-rate of up to 180fps and an ultra-low energy consumption of only\n196{\\mu}J per inference with an inference efficiency of more than 106\nMAC/Cycle. TinyissimoYOLO can be trained for any multi-object detection.\nHowever, considering the small network size, adding object detection classes\nwill increase the size and memory consumption of the network, thus object\ndetection with up to 3 classes is demonstrated. Furthermore, the network is\ntrained using quantization-aware training and deployed with 8-bit quantization\non different microcontrollers, such as STM32H7A3, STM32L4R9, Apollo4b and on\nthe MAX78000's CNN accelerator. Performance evaluations are presented in this\npaper.\n","authors":["Julian Moosmann","Marco Giordano","Christian Vogt","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2306.00001v2.pdf","comment":"Published In: 2023 IEEE 5th International Conference on Artificial\n  Intelligence Circuits and Systems (AICAS)"},{"id":"http://arxiv.org/abs/2307.05136v2","updated":"2023-07-12T06:00:33Z","published":"2023-07-11T09:27:00Z","title":"Unveiling the Invisible: Enhanced Detection and Analysis of Deteriorated\n  Areas in Solar PV Modules Using Unsupervised Sensing Algorithms and 3D\n  Augmented Reality","summary":"  Solar Photovoltaic (PV) is increasingly being used to address the global\nconcern of energy security. However, hot spot and snail trails in PV modules\ncaused mostly by crakes reduce their efficiency and power capacity. This\narticle presents a groundbreaking methodology for automatically identifying and\nanalyzing anomalies like hot spots and snail trails in Solar Photovoltaic (PV)\nmodules, leveraging unsupervised sensing algorithms and 3D Augmented Reality\n(AR) visualization. By transforming the traditional methods of diagnosis and\nrepair, our approach not only enhances efficiency but also substantially cuts\ndown the cost of PV system maintenance. Validated through computer simulations\nand real-world image datasets, the proposed framework accurately identifies\ndirty regions, emphasizing the critical role of regular maintenance in\noptimizing the power capacity of solar PV modules. Our immediate objective is\nto leverage drone technology for real-time, automatic solar panel detection,\nsignificantly boosting the efficacy of PV maintenance. The proposed methodology\ncould revolutionize solar PV maintenance, enabling swift, precise anomaly\ndetection without human intervention. This could result in significant cost\nsavings, heightened energy production, and improved overall performance of\nsolar PV systems. Moreover, the novel combination of unsupervised sensing\nalgorithms with 3D AR visualization heralds new opportunities for further\nresearch and development in solar PV maintenance.\n","authors":["Adel Oulefki","Yassine Himeur","Thaweesak Trongtiraku","Kahina Amara","Sos Agaian","Samir Benbelkacem","Mohamed Amine Guerroudji","Mohamed Zemmouri","Sahla Ferhat","Nadia Zenati","Shadi Atalla","Wathiq Mansoor"],"pdf_url":"https://arxiv.org/pdf/2307.05136v2.pdf","comment":"13 pages, 8 figures, 1 table"},{"id":"http://arxiv.org/abs/2307.05934v1","updated":"2023-07-12T05:59:42Z","published":"2023-07-12T05:59:42Z","title":"Sem-CS: Semantic CLIPStyler for Text-Based Image Style Transfer","summary":"  CLIPStyler demonstrated image style transfer with realistic textures using\nonly a style text description (instead of requiring a reference style image).\nHowever, the ground semantics of objects in the style transfer output is lost\ndue to style spill-over on salient and background objects (content mismatch) or\nover-stylization. To solve this, we propose Semantic CLIPStyler (Sem-CS), that\nperforms semantic style transfer. Sem-CS first segments the content image into\nsalient and non-salient objects and then transfers artistic style based on a\ngiven style text description. The semantic style transfer is achieved using\nglobal foreground loss (for salient objects) and global background loss (for\nnon-salient objects). Our empirical results, including DISTS, NIMA and user\nstudy scores, show that our proposed framework yields superior qualitative and\nquantitative performance. Our code is available at\ngithub.com/chandagrover/sem-cs.\n","authors":["Chanda Grover Kamra","Indra Deep Mastan","Debayan Gupta"],"pdf_url":"https://arxiv.org/pdf/2307.05934v1.pdf","comment":"5 pages, 4 Figures, 2 Tables. arXiv admin note: substantial text\n  overlap with arXiv:2303.06334"},{"id":"http://arxiv.org/abs/2307.05929v1","updated":"2023-07-12T05:49:21Z","published":"2023-07-12T05:49:21Z","title":"A New Dataset and Comparative Study for Aphid Cluster Detection","summary":"  Aphids are one of the main threats to crops, rural families, and global food\nsecurity. Chemical pest control is a necessary component of crop production for\nmaximizing yields, however, it is unnecessary to apply the chemical approaches\nto the entire fields in consideration of the environmental pollution and the\ncost. Thus, accurately localizing the aphid and estimating the infestation\nlevel is crucial to the precise local application of pesticides. Aphid\ndetection is very challenging as each individual aphid is really small and all\naphids are crowded together as clusters. In this paper, we propose to estimate\nthe infection level by detecting aphid clusters. We have taken millions of\nimages in the sorghum fields, manually selected 5,447 images that contain\naphids, and annotated each aphid cluster in the image. To use these images for\nmachine learning models, we crop the images into patches and created a labeled\ndataset with over 151,000 image patches. Then, we implement and compare the\nperformance of four state-of-the-art object detection models.\n","authors":["Tianxiao Zhang","Kaidong Li","Xiangyu Chen","Cuncong Zhong","Bo Luo","Ivan Grijalva Teran","Brian McCornack","Daniel Flippo","Ajay Sharda","Guanghui Wang"],"pdf_url":"https://arxiv.org/pdf/2307.05929v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05921v1","updated":"2023-07-12T05:36:47Z","published":"2023-07-12T05:36:47Z","title":"Reading Radiology Imaging Like The Radiologist","summary":"  Automated radiology report generation aims to generate radiology reports that\ncontain rich, fine-grained descriptions of radiology imaging. Compared with\nimage captioning in the natural image domain, medical images are very similar\nto each other, with only minor differences in the occurrence of diseases. Given\nthe importance of these minor differences in the radiology report, it is\ncrucial to encourage the model to focus more on the subtle regions of disease\noccurrence. Secondly, the problem of visual and textual data biases is serious.\nNot only do normal cases make up the majority of the dataset, but sentences\ndescribing areas with pathological changes also constitute only a small part of\nthe paragraph. Lastly, generating medical image reports involves the challenge\nof long text generation, which requires more expertise and empirical training\nin medical knowledge. As a result, the difficulty of generating such reports is\nincreased. To address these challenges, we propose a disease-oriented retrieval\nframework that utilizes similar reports as prior knowledge references. We\ndesign a factual consistency captioning generator to generate more accurate and\nfactually consistent disease descriptions. Our framework can find most similar\nreports for a given disease from the CXR database by retrieving a\ndisease-oriented mask consisting of the position and morphological\ncharacteristics. By referencing the disease-oriented similar report and the\nvisual features, the factual consistency model can generate a more accurate\nradiology report.\n","authors":["Yuhao Wang"],"pdf_url":"https://arxiv.org/pdf/2307.05921v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05920v1","updated":"2023-07-12T05:19:10Z","published":"2023-07-12T05:19:10Z","title":"Unified Medical Image-Text-Label Contrastive Learning With Continuous\n  Prompt","summary":"  Contrastive language-image Pre-training (CLIP) [13] can leverage large\ndatasets of unlabeled Image-Text pairs, which have demonstrated impressive\nperformance in various downstream tasks. Given that annotating medical data is\ntime-consuming and laborious, Image-Text Pre-training has promising\napplications in exploiting large-scale medical image and radiology report\ndatasets. However, medical Image-Text Pre-training faces several challenges, as\nfollows: (1) Due to privacy concerns, the amount of available medical data is\nrelatively small compared to natural data, leading to weaker generalization\nability of the model. (2) Medical images are highly similar with only\nfine-grained differences in subtleties, resulting in a large number of\nfalse-negative sample pairs in comparison learning. (3) The hand-crafted Prompt\nusually differs from the natural medical image report, Subtle changes in\nwording can lead to significant differences in performance. In this paper, we\npropose a unified Image-Text-Label contrastive learning framework based on\ncontinuous prompts, with three main contributions. First, We unified the data\nof images, text, and labels, which greatly expanded the training data that the\nmodel could utilize. Second, we address the issue of data diversity and the\nimpact of hand-crafted prompts on model performance by introducing continuous\nimplicit prompts. Lastly, we propose a ImageText-Label contrastive Training to\nmitigate the problem of too many false-negative samples. We demonstrate through\nsufficient experiments that the Unified Medical Contrastive Learning (UMCL)\nframework exhibits excellent performance on several downstream tasks.\n","authors":["Yuhao Wang"],"pdf_url":"https://arxiv.org/pdf/2307.05920v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05468v2","updated":"2023-07-12T05:11:23Z","published":"2023-07-11T17:53:43Z","title":"My3DGen: Building Lightweight Personalized 3D Generative Model","summary":"  Our paper presents My3DGen, a practical system for creating a personalized\nand lightweight 3D generative prior using as few as 10 images. My3DGen can\nreconstruct multi-view consistent images from an input test image, and generate\nnovel appearances by interpolating between any two images of the same\nindividual. While recent studies have demonstrated the effectiveness of\npersonalized generative priors in producing high-quality 2D portrait\nreconstructions and syntheses, to the best of our knowledge, we are the first\nto develop a personalized 3D generative prior. Instead of fine-tuning a large\npre-trained generative model with millions of parameters to achieve\npersonalization, we propose a parameter-efficient approach. Our method involves\nutilizing a pre-trained model with fixed weights as a generic prior, while\ntraining a separate personalized prior through low-rank decomposition of the\nweights in each convolution and fully connected layer. However,\nparameter-efficient few-shot fine-tuning on its own often leads to overfitting.\nTo address this, we introduce a regularization technique based on symmetry of\nhuman faces. This regularization enforces that novel view renderings of a\ntraining sample, rendered from symmetric poses, exhibit the same identity. By\nincorporating this symmetry prior, we enhance the quality of reconstruction and\nsynthesis, particularly for non-frontal (profile) faces. Our final system\ncombines low-rank fine-tuning with symmetry regularization and significantly\nsurpasses the performance of pre-trained models, e.g. EG3D. It introduces only\napproximately 0.6 million additional parameters per identity compared to 31\nmillion for full finetuning of the original model. As a result, our system\nachieves a 50-fold reduction in model size without sacrificing the quality of\nthe generated 3D faces. Code will be available at our project page:\nhttps://luchaoqi.github.io/my3dgen.\n","authors":["Luchao Qi","Jiaye Wu","Shengze Wang","Soumyadip Sengupta"],"pdf_url":"https://arxiv.org/pdf/2307.05468v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05916v1","updated":"2023-07-12T04:53:36Z","published":"2023-07-12T04:53:36Z","title":"SwiFT: Swin 4D fMRI Transformer","summary":"  The modeling of spatiotemporal brain dynamics from high-dimensional data,\nsuch as 4D functional MRI, is a formidable task in neuroscience. To address\nthis challenge, we present SwiFT (Swin 4D fMRI Transformer), a Swin Transformer\narchitecture that can learn brain dynamics directly from 4D functional brain\nMRI data in a memory and computation-efficient manner. SwiFT achieves this by\nimplementing a 4D window multi-head self-attention mechanism and absolute\npositional embeddings. We evaluate SwiFT using multiple largest-scale human\nfunctional brain imaging datasets in tasks such as predicting sex, age, and\ncognitive intelligence. Our experimental outcomes reveal that SwiFT\nconsistently outperforms recent state-of-the-art models. To the best of our\nknowledge, SwiFT is the first Swin Transformer architecture that can process\ndimensional spatiotemporal brain functional data in an end-to-end fashion.\nFurthermore, due to the end-to-end learning capability, we also show that\ncontrastive loss-based self-supervised pre-training of SwiFT is also feasible\nfor achieving improved performance on a downstream task. We believe that our\nwork holds substantial potential in facilitating scalable learning of\nfunctional brain imaging in neuroscience research by reducing the hurdles\nassociated with applying Transformer models to high-dimensional fMRI.\n","authors":["Peter Yongho Kim","Junbeom Kwon","Sunghwan Joo","Sangyoon Bae","Donggyu Lee","Yoonho Jung","Shinjae Yoo","Jiook Cha","Taesup Moon"],"pdf_url":"https://arxiv.org/pdf/2307.05916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05913v1","updated":"2023-07-12T04:40:00Z","published":"2023-07-12T04:40:00Z","title":"Close-up View synthesis by Interpolating Optical Flow","summary":"  The virtual viewpoint is perceived as a new technique in virtual navigation,\nas yet not supported due to the lack of depth information and obscure camera\nparameters. In this paper, a method for achieving close-up virtual view is\nproposed and it only uses optical flow to build parallax effects to realize\npseudo 3D projection without using depth sensor. We develop a bidirectional\noptical flow method to obtain any virtual viewpoint by proportional\ninterpolation of optical flow. Moreover, with the ingenious application of the\noptical-flow-value, we achieve clear and visual-fidelity magnified results\nthrough lens stretching in any corner, which overcomes the visual distortion\nand image blur through viewpoint magnification and transition in Google Street\nView system.\n","authors":["Xinyi Bai","Ze Wang","Lu Yang","Hong Cheng"],"pdf_url":"https://arxiv.org/pdf/2307.05913v1.pdf","comment":"4 pages, 5 figures"},{"id":"http://arxiv.org/abs/2307.05014v2","updated":"2023-07-12T04:19:48Z","published":"2023-07-11T05:17:42Z","title":"Test-Time Training on Video Streams","summary":"  Prior work has established test-time training (TTT) as a general framework to\nfurther improve a trained model at test time. Before making a prediction on\neach test instance, the model is trained on the same instance using a\nself-supervised task, such as image reconstruction with masked autoencoders. We\nextend TTT to the streaming setting, where multiple test instances - video\nframes in our case - arrive in temporal order. Our extension is online TTT: The\ncurrent model is initialized from the previous model, then trained on the\ncurrent frame and a small window of frames immediately before. Online TTT\nsignificantly outperforms the fixed-model baseline for four tasks, on three\nreal-world datasets. The relative improvement is 45% and 66% for instance and\npanoptic segmentation. Surprisingly, online TTT also outperforms its offline\nvariant that accesses more information, training on all frames from the entire\ntest video regardless of temporal order. This differs from previous findings\nusing synthetic videos. We conceptualize locality as the advantage of online\nover offline TTT. We analyze the role of locality with ablations and a theory\nbased on bias-variance trade-off.\n","authors":["Renhao Wang","Yu Sun","Yossi Gandelsman","Xinlei Chen","Alexei A. Efros","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2307.05014v2.pdf","comment":"Project website with videos, dataset and code:\n  https://video-ttt.github.io/"},{"id":"http://arxiv.org/abs/2307.05901v1","updated":"2023-07-12T04:15:36Z","published":"2023-07-12T04:15:36Z","title":"Single Domain Generalization via Normalised Cross-correlation Based\n  Convolutions","summary":"  Deep learning techniques often perform poorly in the presence of domain\nshift, where the test data follows a different distribution than the training\ndata. The most practically desirable approach to address this issue is Single\nDomain Generalization (S-DG), which aims to train robust models using data from\na single source. Prior work on S-DG has primarily focused on using data\naugmentation techniques to generate diverse training data. In this paper, we\nexplore an alternative approach by investigating the robustness of linear\noperators, such as convolution and dense layers commonly used in deep learning.\nWe propose a novel operator called XCNorm that computes the normalized\ncross-correlation between weights and an input feature patch. This approach is\ninvariant to both affine shifts and changes in energy within a local feature\npatch and eliminates the need for commonly used non-linear activation\nfunctions. We show that deep neural networks composed of this operator are\nrobust to common semantic distribution shifts. Furthermore, our empirical\nresults on single-domain generalization benchmarks demonstrate that our\nproposed technique performs comparably to the state-of-the-art methods.\n","authors":["WeiQin Chuah","Ruwan Tennakoon","Reza Hoseinnezhad","David Suter","Alireza Bab-Hadiashar"],"pdf_url":"https://arxiv.org/pdf/2307.05901v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2307.05899v1","updated":"2023-07-12T04:11:08Z","published":"2023-07-12T04:11:08Z","title":"DiffuseGAE: Controllable and High-fidelity Image Manipulation from\n  Disentangled Representation","summary":"  Diffusion probabilistic models (DPMs) have shown remarkable results on\nvarious image synthesis tasks such as text-to-image generation and image\ninpainting. However, compared to other generative methods like VAEs and GANs,\nDPMs lack a low-dimensional, interpretable, and well-decoupled latent code.\nRecently, diffusion autoencoders (Diff-AE) were proposed to explore the\npotential of DPMs for representation learning via autoencoding. Diff-AE\nprovides an accessible latent space that exhibits remarkable interpretability,\nallowing us to manipulate image attributes based on latent codes from the\nspace. However, previous works are not generic as they only operated on a few\nlimited attributes. To further explore the latent space of Diff-AE and achieve\na generic editing pipeline, we proposed a module called Group-supervised\nAutoEncoder(dubbed GAE) for Diff-AE to achieve better disentanglement on the\nlatent code. Our proposed GAE has trained via an attribute-swap strategy to\nacquire the latent codes for multi-attribute image manipulation based on\nexamples. We empirically demonstrate that our method enables\nmultiple-attributes manipulation and achieves convincing sample quality and\nattribute alignments, while significantly reducing computational requirements\ncompared to pixel-based approaches for representational decoupling. Code will\nbe released soon.\n","authors":["Yipeng Leng","Qiangjuan Huang","Zhiyuan Wang","Yangyang Liu","Haoyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2307.05899v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05898v1","updated":"2023-07-12T04:10:16Z","published":"2023-07-12T04:10:16Z","title":"Rectifying Noisy Labels with Sequential Prior: Multi-Scale Temporal\n  Feature Affinity Learning for Robust Video Segmentation","summary":"  Noisy label problems are inevitably in existence within medical image\nsegmentation causing severe performance degradation. Previous segmentation\nmethods for noisy label problems only utilize a single image while the\npotential of leveraging the correlation between images has been overlooked.\nEspecially for video segmentation, adjacent frames contain rich contextual\ninformation beneficial in cognizing noisy labels. Based on two insights, we\npropose a Multi-Scale Temporal Feature Affinity Learning (MS-TFAL) framework to\nresolve noisy-labeled medical video segmentation issues. First, we argue the\nsequential prior of videos is an effective reference, i.e., pixel-level\nfeatures from adjacent frames are close in distance for the same class and far\nin distance otherwise. Therefore, Temporal Feature Affinity Learning (TFAL) is\ndevised to indicate possible noisy labels by evaluating the affinity between\npixels in two adjacent frames. We also notice that the noise distribution\nexhibits considerable variations across video, image, and pixel levels. In this\nway, we introduce Multi-Scale Supervision (MSS) to supervise the network from\nthree different perspectives by re-weighting and refining the samples. This\ndesign enables the network to concentrate on clean samples in a coarse-to-fine\nmanner. Experiments with both synthetic and real-world label noise demonstrate\nthat our method outperforms recent state-of-the-art robust segmentation\napproaches. Code is available at https://github.com/BeileiCui/MS-TFAL.\n","authors":["Beilei Cui","Minqing Zhang","Mengya Xu","An Wang","Wu Yuan","Hongliang Ren"],"pdf_url":"https://arxiv.org/pdf/2307.05898v1.pdf","comment":"Accepted by MICCAI 2023"},{"id":"http://arxiv.org/abs/2307.05896v1","updated":"2023-07-12T03:51:57Z","published":"2023-07-12T03:51:57Z","title":"Deep learning-based estimation of whole-body kinematics from multi-view\n  images","summary":"  It is necessary to analyze the whole-body kinematics (including joint\nlocations and joint angles) to assess risks of fatal and musculoskeletal\ninjuries in occupational tasks. Human pose estimation has gotten more attention\nin recent years as a method to minimize the errors in determining joint\nlocations. However, the joint angles are not often estimated, nor is the\nquality of joint angle estimation assessed. In this paper, we presented an\nend-to-end approach on direct joint angle estimation from multi-view images.\nOur method leveraged the volumetric pose representation and mapped the rotation\nrepresentation to a continuous space where each rotation was uniquely\nrepresented. We also presented a new kinematic dataset in the domain of\nresidential roofing with a data processing pipeline to generate necessary\nannotations for the supervised training procedure on direct joint angle\nestimation. We achieved a mean angle error of $7.19^\\circ$ on the new Roofing\ndataset and $8.41^\\circ$ on the Human3.6M dataset, paving the way for\nemployment of on-site kinematic analysis using multi-view images.\n","authors":["Kien X. Nguyen","Liying Zheng","Ashley L. Hawke","Robert E. Carey","Scott P. Breloff","Kang Li","Xi Peng"],"pdf_url":"https://arxiv.org/pdf/2307.05896v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05892v1","updated":"2023-07-12T03:45:45Z","published":"2023-07-12T03:45:45Z","title":"SC-NeuS: Consistent Neural Surface Reconstruction from Sparse and Noisy\n  Views","summary":"  The recent neural surface reconstruction by volume rendering approaches have\nmade much progress by achieving impressive surface reconstruction quality, but\nare still limited to dense and highly accurate posed views. To overcome such\ndrawbacks, this paper pays special attention on the consistent surface\nreconstruction from sparse views with noisy camera poses. Unlike previous\napproaches, the key difference of this paper is to exploit the multi-view\nconstraints directly from the explicit geometry of the neural surface, which\ncan be used as effective regularization to jointly learn the neural surface and\nrefine the camera poses. To build effective multi-view constraints, we\nintroduce a fast differentiable on-surface intersection to generate on-surface\npoints, and propose view-consistent losses based on such differentiable points\nto regularize the neural surface learning. Based on this point, we propose a\njointly learning strategy for neural surface and camera poses, named SC-NeuS,\nto perform geometry-consistent surface reconstruction in an end-to-end manner.\nWith extensive evaluation on public datasets, our SC-NeuS can achieve\nconsistently better surface reconstruction results with fine-grained details\nthan previous state-of-the-art neural surface reconstruction approaches,\nespecially from sparse and noisy camera views.\n","authors":["Shi-Sheng Huang","Zi-Xin Zou","Yi-Chi Zhang","Hua Huang"],"pdf_url":"https://arxiv.org/pdf/2307.05892v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.01361v3","updated":"2023-07-12T03:44:59Z","published":"2022-10-04T04:06:44Z","title":"Uncertainty-Aware Lidar Place Recognition in Novel Environments","summary":"  State-of-the-art lidar place recognition models exhibit unreliable\nperformance when tested on environments different from their training dataset,\nwhich limits their use in complex and evolving environments. To address this\nissue, we investigate the task of uncertainty-aware lidar place recognition,\nwhere each predicted place must have an associated uncertainty that can be used\nto identify and reject incorrect predictions. We introduce a novel evaluation\nprotocol and present the first comprehensive benchmark for this task, testing\nacross five uncertainty estimation techniques and three large-scale datasets.\nOur results show that an Ensembles approach is the highest performing\ntechnique, consistently improving the performance of lidar place recognition\nand uncertainty estimation in novel environments, though it incurs a\ncomputational cost. Code is publicly available at\nhttps://github.com/csiro-robotics/Uncertainty-LPR.\n","authors":["Keita Mason","Joshua Knights","Milad Ramezani","Peyman Moghadam","Dimity Miller"],"pdf_url":"https://arxiv.org/pdf/2210.01361v3.pdf","comment":"8 pages, 4 figures. Accepted for publication at IEEE IROS 2023"},{"id":"http://arxiv.org/abs/2307.05890v1","updated":"2023-07-12T03:39:54Z","published":"2023-07-12T03:39:54Z","title":"FreeSeed: Frequency-band-aware and Self-guided Network for Sparse-view\n  CT Reconstruction","summary":"  Sparse-view computed tomography (CT) is a promising solution for expediting\nthe scanning process and mitigating radiation exposure to patients, the\nreconstructed images, however, contain severe streak artifacts, compromising\nsubsequent screening and diagnosis. Recently, deep learning-based image\npost-processing methods along with their dual-domain counterparts have shown\npromising results. However, existing methods usually produce over-smoothed\nimages with loss of details due to (1) the difficulty in accurately modeling\nthe artifact patterns in the image domain, and (2) the equal treatment of each\npixel in the loss function. To address these issues, we concentrate on the\nimage post-processing and propose a simple yet effective FREquency-band-awarE\nand SElf-guidED network, termed FreeSeed, which can effectively remove artifact\nand recover missing detail from the contaminated sparse-view CT images.\nSpecifically, we first propose a frequency-band-aware artifact modeling network\n(FreeNet), which learns artifact-related frequency-band attention in Fourier\ndomain for better modeling the globally distributed streak artifact on the\nsparse-view CT images. We then introduce a self-guided artifact refinement\nnetwork (SeedNet), which leverages the predicted artifact to assist FreeNet in\ncontinuing to refine the severely corrupted details. Extensive experiments\ndemonstrate the superior performance of FreeSeed and its dual-domain\ncounterpart over the state-of-the-art sparse-view CT reconstruction methods.\nSource code is made available at https://github.com/Masaaki-75/freeseed.\n","authors":["Chenglong Ma","Zilong Li","Junping Zhang","Yi Zhang","Hongming Shan"],"pdf_url":"https://arxiv.org/pdf/2307.05890v1.pdf","comment":"MICCAI 2023"},{"id":"http://arxiv.org/abs/2307.05889v1","updated":"2023-07-12T03:33:11Z","published":"2023-07-12T03:33:11Z","title":"Rethinking Mitosis Detection: Towards Diverse Data and Feature\n  Representation","summary":"  Mitosis detection is one of the fundamental tasks in computational pathology,\nwhich is extremely challenging due to the heterogeneity of mitotic cell. Most\nof the current studies solve the heterogeneity in the technical aspect by\nincreasing the model complexity. However, lacking consideration of the\nbiological knowledge and the complex model design may lead to the overfitting\nproblem while limited the generalizability of the detection model. In this\npaper, we systematically study the morphological appearances in different\nmitotic phases as well as the ambiguous non-mitotic cells and identify that\nbalancing the data and feature diversity can achieve better generalizability.\nBased on this observation, we propose a novel generalizable framework (MitDet)\nfor mitosis detection. The data diversity is considered by the proposed\ndiversity-guided sample balancing (DGSB). And the feature diversity is\npreserved by inter- and intra- class feature diversity-preserved module\n(InCDP). Stain enhancement (SE) module is introduced to enhance the\ndomain-relevant diversity of both data and features simultaneously. Extensive\nexperiments have demonstrated that our proposed model outperforms all the SOTA\napproaches in several popular mitosis detection datasets in both internal and\nexternal test sets using minimal annotation efforts with point annotations\nonly. Comprehensive ablation studies have also proven the effectiveness of the\nrethinking of data and feature diversity balancing. By analyzing the results\nquantitatively and qualitatively, we believe that our proposed model not only\nachieves SOTA performance but also might inspire the future studies in new\nperspectives. Source code is at https://github.com/Onehour0108/MitDet.\n","authors":["Hao Wang","Jiatai Lin","Danyi Li","Jing Wang","Bingchao Zhao","Zhenwei Shi","Xipeng Pan","Huadeng Wang","Bingbing Li","Changhong Liang","Guoqiang Han","Li Liang","Chu Han","Zaiyi Liu"],"pdf_url":"https://arxiv.org/pdf/2307.05889v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.12286v2","updated":"2023-07-12T03:30:00Z","published":"2023-05-20T21:38:05Z","title":"Low-Earth Satellite Orbit Determination Using Deep Convolutional\n  Networks with Satellite Imagery","summary":"  It is increasingly common for satellites to lose connection with the ground\nstations on Earth with which they communicate, due to signal interruptions from\nthe Earth's ionosphere and magnetosphere. Given the important roles that\nsatellites play in national defense, public safety, and worldwide\ncommunications, finding ways to determine satellite trajectories in such\nsituations is a crucially important task. In this paper, we demonstrate the\nefficacy of a novel computer vision based approach, which relies on earth\nimagery taken by the satellite itself, to determine the orbit of a satellite\nthat has lost contact with its ground stations. We empirically observe\nsignificant improvements by more than an order of magnitude, over the present\nstate of the art approach, namely, the Gibbs method for an initial orbit\nestimate with the Kalman filter for differential error correction. We further\ninvestigate the performance of the approach by comparing various neural\nnetworks, namely, ResNet50, ResNet101, VGG19, VGG16, AlexNet, and CoAtNet4.\n","authors":["Rohit Khorana"],"pdf_url":"https://arxiv.org/pdf/2305.12286v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05874v1","updated":"2023-07-12T02:02:18Z","published":"2023-07-12T02:02:18Z","title":"Multi-Object Tracking as Attention Mechanism","summary":"  We propose a conceptually simple and thus fast multi-object tracking (MOT)\nmodel that does not require any attached modules, such as the Kalman filter,\nHungarian algorithm, transformer blocks, or graph networks. Conventional MOT\nmodels are built upon the multi-step modules listed above, and thus the\ncomputational cost is high. Our proposed end-to-end MOT model,\n\\textit{TicrossNet}, is composed of a base detector and a cross-attention\nmodule only. As a result, the overhead of tracking does not increase\nsignificantly even when the number of instances ($N_t$) increases. We show that\nTicrossNet runs \\textit{in real-time}; specifically, it achieves 32.6 FPS on\nMOT17 and 31.0 FPS on MOT20 (Tesla V100), which includes as many as $>$100\ninstances per frame. We also demonstrate that TicrossNet is robust to $N_t$;\nthus, it does not have to change the size of the base detector, depending on\n$N_t$, as is often done by other models for real-time processing.\n","authors":["Hiroshi Fukui","Taiki Miyagawa","Yusuke Morishita"],"pdf_url":"https://arxiv.org/pdf/2307.05874v1.pdf","comment":"Accepted to IEEE International Conference on Image Processing (IEEE\n  ICIP) 2023"},{"id":"http://arxiv.org/abs/2307.05873v1","updated":"2023-07-12T01:59:26Z","published":"2023-07-12T01:59:26Z","title":"OG: Equip vision occupancy with instance segmentation and visual\n  grounding","summary":"  Occupancy prediction tasks focus on the inference of both geometry and\nsemantic labels for each voxel, which is an important perception mission.\nHowever, it is still a semantic segmentation task without distinguishing\nvarious instances. Further, although some existing works, such as\nOpen-Vocabulary Occupancy (OVO), have already solved the problem of open\nvocabulary detection, visual grounding in occupancy has not been solved to the\nbest of our knowledge. To tackle the above two limitations, this paper proposes\nOccupancy Grounding (OG), a novel method that equips vanilla occupancy instance\nsegmentation ability and could operate visual grounding in a voxel manner with\nthe help of grounded-SAM. Keys to our approach are (1) affinity field\nprediction for instance clustering and (2) association strategy for aligning 2D\ninstance masks and 3D occupancy instances. Extensive experiments have been\nconducted whose visualization results and analysis are shown below. Our code\nwill be publicly released soon.\n","authors":["Zichao Dong","Hang Ji","Weikun Zhang","Xufeng Huang","Junbo Chen"],"pdf_url":"https://arxiv.org/pdf/2307.05873v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.04104v3","updated":"2023-07-12T01:36:19Z","published":"2022-07-08T19:02:50Z","title":"Towards a More Rigorous Science of Blindspot Discovery in Image\n  Classification Models","summary":"  A growing body of work studies Blindspot Discovery Methods (\"BDM\"s): methods\nthat use an image embedding to find semantically meaningful (i.e., united by a\nhuman-understandable concept) subsets of the data where an image classifier\nperforms significantly worse. Motivated by observed gaps in prior work, we\nintroduce a new framework for evaluating BDMs, SpotCheck, that uses synthetic\nimage datasets to train models with known blindspots and a new BDM, PlaneSpot,\nthat uses a 2D image representation. We use SpotCheck to run controlled\nexperiments that identify factors that influence BDM performance (e.g., the\nnumber of blindspots in a model, or features used to define the blindspot) and\nshow that PlaneSpot is competitive with and in many cases outperforms existing\nBDMs. Importantly, we validate these findings by designing additional\nexperiments that use real image data from MS-COCO, a large image benchmark\ndataset. Our findings suggest several promising directions for future work on\nBDM design and evaluation. Overall, we hope that the methodology and analyses\npresented in this work will help facilitate a more rigorous science of\nblindspot discovery.\n","authors":["Gregory Plumb","Nari Johnson","Ángel Alexander Cabrera","Ameet Talwalkar"],"pdf_url":"https://arxiv.org/pdf/2207.04104v3.pdf","comment":"reviewed on OpenReview: https://openreview.net/forum?id=MaDvbLaBiF"},{"id":"http://arxiv.org/abs/2304.10701v4","updated":"2023-07-12T01:31:34Z","published":"2023-04-21T02:02:02Z","title":"Matching-based Data Valuation for Generative Model","summary":"  Data valuation is critical in machine learning, as it helps enhance model\ntransparency and protect data properties. Existing data valuation methods have\nprimarily focused on discriminative models, neglecting deep generative models\nthat have recently gained considerable attention. Similar to discriminative\nmodels, there is an urgent need to assess data contributions in deep generative\nmodels as well. However, previous data valuation approaches mainly relied on\ndiscriminative model performance metrics and required model retraining.\nConsequently, they cannot be applied directly and efficiently to recent deep\ngenerative models, such as generative adversarial networks and diffusion\nmodels, in practice. To bridge this gap, we formulate the data valuation\nproblem in generative models from a similarity-matching perspective.\nSpecifically, we introduce Generative Model Valuator (GMValuator), the first\nmodel-agnostic approach for any generative models, designed to provide data\nvaluation for generation tasks. We have conducted extensive experiments to\ndemonstrate the effectiveness of the proposed method. To the best of their\nknowledge, GMValuator is the first work that offers a training-free, post-hoc\ndata valuation strategy for deep generative models.\n","authors":["Jiaxi Yang","Wenglong Deng","Benlin Liu","Yangsibo Huang","Xiaoxiao Li"],"pdf_url":"https://arxiv.org/pdf/2304.10701v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05853v1","updated":"2023-07-12T00:13:04Z","published":"2023-07-12T00:13:04Z","title":"GLA-GCN: Global-local Adaptive Graph Convolutional Network for 3D Human","summary":"  3D human pose estimation has been researched for decades with promising\nfruits. 3D human pose lifting is one of the promising research directions\ntoward the task where both estimated pose and ground truth pose data are used\nfor training. Existing pose lifting works mainly focus on improving the\nperformance of estimated pose, but they usually underperform when testing on\nthe ground truth pose data. We observe that the performance of the estimated\npose can be easily improved by preparing good quality 2D pose, such as\nfine-tuning the 2D pose or using advanced 2D pose detectors. As such, we\nconcentrate on improving the 3D human pose lifting via ground truth data for\nthe future improvement of more quality estimated pose data. Towards this goal,\na simple yet effective model called Global-local Adaptive Graph Convolutional\nNetwork (GLA-GCN) is proposed in this work. Our GLA-GCN globally models the\nspatiotemporal structure via a graph representation and backtraces local joint\nfeatures for 3D human pose estimation via individually connected layers. To\nvalidate our model design, we conduct extensive experiments on three benchmark\ndatasets: Human3.6M, HumanEva-I, and MPI-INF-3DHP. Experimental results show\nthat our GLA-GCN implemented with ground truth 2D poses significantly\noutperforms state-of-the-art methods (e.g., up to around 3%, 17%, and 13% error\nreductions on Human3.6M, HumanEva-I, and MPI-INF-3DHP, respectively).\n","authors":["Bruce X. B. Yu","Zhi Zhang","Yongxu Liu","Sheng-hua Zhong","Yan Liu","Chang Wen Chen"],"pdf_url":"https://arxiv.org/pdf/2307.05853v1.pdf","comment":"12 pages"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2307.06213v1","updated":"2023-07-12T15:00:52Z","published":"2023-07-12T15:00:52Z","title":"Testing different Log Bases For Vector Model Weighting Technique","summary":"  Information retrieval systems retrieves relevant documents based on a query\nsubmitted by the user. The documents are initially indexed and the words in the\ndocuments are assigned weights using a weighting technique called TFIDF which\nis the product of Term Frequency (TF) and Inverse Document Frequency (IDF). TF\nrepresents the number of occurrences of a term in a document. IDF measures\nwhether the term is common or rare across all documents. It is computed by\ndividing the total number of documents in the system by the number of documents\ncontaining the term and then computing the logarithm of the quotient. By\ndefault, we use base 10 to calculate the logarithm. In this paper, we are going\nto test this weighting technique by using a range of log bases from 0.1 to\n100.0 to calculate the IDF. Testing different log bases for vector model\nweighting technique is to highlight the importance of understanding the\nperformance of the system at different weighting values. We use the documents\nof MED, CRAN, NPL, LISA, and CISI test collections that scientists assembled\nexplicitly for experiments in data information retrieval systems.\n","authors":["Kamel Assaf"],"pdf_url":"https://arxiv.org/pdf/2307.06213v1.pdf","comment":"15 pages, 7 figures, vector model, logarithms, tfidf"},{"id":"http://arxiv.org/abs/2305.02759v3","updated":"2023-07-12T12:29:04Z","published":"2023-05-04T11:53:38Z","title":"Disentangled Contrastive Collaborative Filtering","summary":"  Recent studies show that graph neural networks (GNNs) are prevalent to model\nhigh-order relationships for collaborative filtering (CF). Towards this\nresearch line, graph contrastive learning (GCL) has exhibited powerful\nperformance in addressing the supervision label shortage issue by learning\naugmented user and item representations. While many of them show their\neffectiveness, two key questions still remain unexplored: i) Most existing\nGCL-based CF models are still limited by ignoring the fact that user-item\ninteraction behaviors are often driven by diverse latent intent factors (e.g.,\nshopping for family party, preferred color or brand of products); ii) Their\nintroduced non-adaptive augmentation techniques are vulnerable to noisy\ninformation, which raises concerns about the model's robustness and the risk of\nincorporating misleading self-supervised signals. In light of these\nlimitations, we propose a Disentangled Contrastive Collaborative Filtering\nframework (DCCF) to realize intent disentanglement with self-supervised\naugmentation in an adaptive fashion. With the learned disentangled\nrepresentations with global context, our DCCF is able to not only distill\nfiner-grained latent factors from the entangled self-supervision signals but\nalso alleviate the augmentation-induced noise. Finally, the cross-view\ncontrastive learning task is introduced to enable adaptive augmentation with\nour parameterized interaction mask generator. Experiments on various public\ndatasets demonstrate the superiority of our method compared to existing\nsolutions. Our model implementation is released at the link\nhttps://github.com/HKUDS/DCCF.\n","authors":["Xubin Ren","Lianghao Xia","Jiashu Zhao","Dawei Yin","Chao Huang"],"pdf_url":"https://arxiv.org/pdf/2305.02759v3.pdf","comment":"Published as a SIGIR'23 full paper"},{"id":"http://arxiv.org/abs/2307.06005v1","updated":"2023-07-12T08:33:16Z","published":"2023-07-12T08:33:16Z","title":"DDNAS: Discretized Differentiable Neural Architecture Search for Text\n  Classification","summary":"  Neural Architecture Search (NAS) has shown promising capability in learning\ntext representation. However, existing text-based NAS neither performs a\nlearnable fusion of neural operations to optimize the architecture, nor encodes\nthe latent hierarchical categorization behind text input. This paper presents a\nnovel NAS method, Discretized Differentiable Neural Architecture Search\n(DDNAS), for text representation learning and classification. With the\ncontinuous relaxation of architecture representation, DDNAS can use gradient\ndescent to optimize the search. We also propose a novel discretization layer\nvia mutual information maximization, which is imposed on every search node to\nmodel the latent hierarchical categorization in text representation. Extensive\nexperiments conducted on eight diverse real datasets exhibit that DDNAS can\nconsistently outperform the state-of-the-art NAS methods. While DDNAS relies on\nonly three basic operations, i.e., convolution, pooling, and none, to be the\ncandidates of NAS building blocks, its promising performance is noticeable and\nextensible to obtain further improvement by adding more different operations.\n","authors":["Kuan-Chun Chen","Cheng-Te Li","Kuo-Jung Lee"],"pdf_url":"https://arxiv.org/pdf/2307.06005v1.pdf","comment":"ACM Trans. Intell. Syst. Technol. (TIST) 2023"},{"id":"http://arxiv.org/abs/2307.05974v1","updated":"2023-07-12T07:42:52Z","published":"2023-07-12T07:42:52Z","title":"Contrastive Learning for Conversion Rate Prediction","summary":"  Conversion rate (CVR) prediction plays an important role in advertising\nsystems. Recently, supervised deep neural network-based models have shown\npromising performance in CVR prediction. However, they are data hungry and\nrequire an enormous amount of training data. In online advertising systems,\nalthough there are millions to billions of ads, users tend to click only a\nsmall set of them and to convert on an even smaller set. This data sparsity\nissue restricts the power of these deep models. In this paper, we propose the\nContrastive Learning for CVR prediction (CL4CVR) framework. It associates the\nsupervised CVR prediction task with a contrastive learning task, which can\nlearn better data representations exploiting abundant unlabeled data and\nimprove the CVR prediction performance. To tailor the contrastive learning task\nto the CVR prediction problem, we propose embedding masking (EM), rather than\nfeature masking, to create two views of augmented samples. We also propose a\nfalse negative elimination (FNE) component to eliminate samples with the same\nfeature as the anchor sample, to account for the natural property in user\nbehavior data. We further propose a supervised positive inclusion (SPI)\ncomponent to include additional positive samples for each anchor sample, in\norder to make full use of sparse but precious user conversion events.\nExperimental results on two real-world conversion datasets demonstrate the\nsuperior performance of CL4CVR. The source code is available at\nhttps://github.com/DongRuiHust/CL4CVR.\n","authors":["Wentao Ouyang","Rui Dong","Xiuwu Zhang","Chaofeng Guo","Jinmei Luo","Xiangzheng Liu","Yanlong Du"],"pdf_url":"https://arxiv.org/pdf/2307.05974v1.pdf","comment":"SIGIR 2023"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2305.12529v2","updated":"2023-07-12T17:58:59Z","published":"2023-05-21T17:59:39Z","title":"DreamWaltz: Make a Scene with Complex 3D Animatable Avatars","summary":"  We present DreamWaltz, a novel framework for generating and animating complex\n3D avatars given text guidance and parametric human body prior. While recent\nmethods have shown encouraging results for text-to-3D generation of common\nobjects, creating high-quality and animatable 3D avatars remains challenging.\nTo create high-quality 3D avatars, DreamWaltz proposes 3D-consistent\nocclusion-aware Score Distillation Sampling (SDS) to optimize implicit neural\nrepresentations with canonical poses. It provides view-aligned supervision via\n3D-aware skeleton conditioning which enables complex avatar generation without\nartifacts and multiple faces. For animation, our method learns an animatable\nand generalizable avatar representation which could map arbitrary poses to the\ncanonical pose representation. Extensive evaluations demonstrate that\nDreamWaltz is an effective and robust approach for creating 3D avatars that can\ntake on complex shapes and appearances as well as novel poses for animation.\nThe proposed framework further enables the creation of complex scenes with\ndiverse compositions, including avatar-avatar, avatar-object and avatar-scene\ninteractions. See https://dreamwaltz3d.github.io/ for more vivid 3D avatar and\nanimation results.\n","authors":["Yukun Huang","Jianan Wang","Ailing Zeng","He Cao","Xianbiao Qi","Yukai Shi","Zheng-Jun Zha","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.12529v2.pdf","comment":"project page at https://dreamwaltz3d.github.io/"},{"id":"http://arxiv.org/abs/2307.06333v1","updated":"2023-07-12T17:55:08Z","published":"2023-07-12T17:55:08Z","title":"Diagnosis, Feedback, Adaptation: A Human-in-the-Loop Framework for\n  Test-Time Policy Adaptation","summary":"  Policies often fail due to distribution shift -- changes in the state and\nreward that occur when a policy is deployed in new environments. Data\naugmentation can increase robustness by making the model invariant to\ntask-irrelevant changes in the agent's observation. However, designers don't\nknow which concepts are irrelevant a priori, especially when different end\nusers have different preferences about how the task is performed. We propose an\ninteractive framework to leverage feedback directly from the user to identify\npersonalized task-irrelevant concepts. Our key idea is to generate\ncounterfactual demonstrations that allow users to quickly identify possible\ntask-relevant and irrelevant concepts. The knowledge of task-irrelevant\nconcepts is then used to perform data augmentation and thus obtain a policy\nadapted to personalized user objectives. We present experiments validating our\nframework on discrete and continuous control tasks with real human users. Our\nmethod (1) enables users to better understand agent failure, (2) reduces the\nnumber of demonstrations required for fine-tuning, and (3) aligns the agent to\nindividual user task preferences.\n","authors":["Andi Peng","Aviv Netanyahu","Mark Ho","Tianmin Shu","Andreea Bobu","Julie Shah","Pulkit Agrawal"],"pdf_url":"https://arxiv.org/pdf/2307.06333v1.pdf","comment":"International Conference on Machine Learning (ICML) 2023"},{"id":"http://arxiv.org/abs/2307.06328v1","updated":"2023-07-12T17:47:35Z","published":"2023-07-12T17:47:35Z","title":"Budgeting Counterfactual for Offline RL","summary":"  The main challenge of offline reinforcement learning, where data is limited,\narises from a sequence of counterfactual reasoning dilemmas within the realm of\npotential actions: What if we were to choose a different course of action?\nThese circumstances frequently give rise to extrapolation errors, which tend to\naccumulate exponentially with the problem horizon. Hence, it becomes crucial to\nacknowledge that not all decision steps are equally important to the final\noutcome, and to budget the number of counterfactual decisions a policy make in\norder to control the extrapolation. Contrary to existing approaches that use\nregularization on either the policy or value function, we propose an approach\nto explicitly bound the amount of out-of-distribution actions during training.\nSpecifically, our method utilizes dynamic programming to decide where to\nextrapolate and where not to, with an upper bound on the decisions different\nfrom behavior policy. It balances between the potential for improvement from\ntaking out-of-distribution actions and the risk of making errors due to\nextrapolation. Theoretically, we justify our method by the constrained\noptimality of the fixed point solution to our $Q$ updating rules. Empirically,\nwe show that the overall performance of our method is better than the\nstate-of-the-art offline RL methods on tasks in the widely-used D4RL\nbenchmarks.\n","authors":["Yao Liu","Pratik Chaudhari","Rasool Fakoor"],"pdf_url":"https://arxiv.org/pdf/2307.06328v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.03190v2","updated":"2023-07-12T17:45:01Z","published":"2023-07-06T17:59:31Z","title":"Synthesizing Artistic Cinemagraphs from Text","summary":"  We introduce Text2Cinemagraph, a fully automated method for creating\ncinemagraphs from text descriptions - an especially challenging task when\nprompts feature imaginary elements and artistic styles, given the complexity of\ninterpreting the semantics and motions of these images. Existing single-image\nanimation methods fall short on artistic inputs, and recent text-based video\nmethods frequently introduce temporal inconsistencies, struggling to keep\ncertain regions static. To address these challenges, we propose an idea of\nsynthesizing image twins from a single text prompt - a pair of an artistic\nimage and its pixel-aligned corresponding natural-looking twin. While the\nartistic image depicts the style and appearance detailed in our text prompt,\nthe realistic counterpart greatly simplifies layout and motion analysis.\nLeveraging existing natural image and video datasets, we can accurately segment\nthe realistic image and predict plausible motion given the semantic\ninformation. The predicted motion can then be transferred to the artistic image\nto create the final cinemagraph. Our method outperforms existing approaches in\ncreating cinemagraphs for natural landscapes as well as artistic and\nother-worldly scenes, as validated by automated metrics and user studies.\nFinally, we demonstrate two extensions: animating existing paintings and\ncontrolling motion directions using text.\n","authors":["Aniruddha Mahapatra","Aliaksandr Siarohin","Hsin-Ying Lee","Sergey Tulyakov","Jun-Yan Zhu"],"pdf_url":"https://arxiv.org/pdf/2307.03190v2.pdf","comment":"Project website: https://text2cinemagraph.github.io/website/"},{"id":"http://arxiv.org/abs/2307.06324v1","updated":"2023-07-12T17:41:07Z","published":"2023-07-12T17:41:07Z","title":"Provably Faster Gradient Descent via Long Steps","summary":"  This work establishes provably faster convergence rates for gradient descent\nvia a computer-assisted analysis technique. Our theory allows nonconstant\nstepsize policies with frequent long steps potentially violating descent by\nanalyzing the overall effect of many iterations at once rather than the typical\none-iteration inductions used in most first-order method analyses. We show that\nlong steps, which may increase the objective value in the short term, lead to\nprovably faster convergence in the long term. A conjecture towards proving a\nfaster $O(1/T\\log T)$ rate for gradient descent is also motivated along with\nsimple numerical validation.\n","authors":["Benjamin Grimmer"],"pdf_url":"https://arxiv.org/pdf/2307.06324v1.pdf","comment":"14pages plus references and appendix"},{"id":"http://arxiv.org/abs/2307.06307v1","updated":"2023-07-12T17:09:18Z","published":"2023-07-12T17:09:18Z","title":"Facial Reenactment Through a Personalized Generator","summary":"  In recent years, the role of image generative models in facial reenactment\nhas been steadily increasing. Such models are usually subject-agnostic and\ntrained on domain-wide datasets. The appearance of the reenacted individual is\nlearned from a single image, and hence, the entire breadth of the individual's\nappearance is not entirely captured, leading these methods to resort to\nunfaithful hallucination. Thanks to recent advancements, it is now possible to\ntrain a personalized generative model tailored specifically to a given\nindividual. In this paper, we propose a novel method for facial reenactment\nusing a personalized generator. We train the generator using frames from a\nshort, yet varied, self-scan video captured using a simple commodity camera.\nImages synthesized by the personalized generator are guaranteed to preserve\nidentity. The premise of our work is that the task of reenactment is thus\nreduced to accurately mimicking head poses and expressions. To this end, we\nlocate the desired frames in the latent space of the personalized generator\nusing carefully designed latent optimization. Through extensive evaluation, we\ndemonstrate state-of-the-art performance for facial reenactment. Furthermore,\nwe show that since our reenactment takes place in a semantic latent space, it\ncan be semantically edited and stylized in post-processing.\n","authors":["Ariel Elazary","Yotam Nitzan","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2307.06307v1.pdf","comment":"Project webpage: https://arielazary.github.io/PGR/"},{"id":"http://arxiv.org/abs/2307.06306v1","updated":"2023-07-12T17:02:32Z","published":"2023-07-12T17:02:32Z","title":"Locally Adaptive Federated Learning via Stochastic Polyak Stepsizes","summary":"  State-of-the-art federated learning algorithms such as FedAvg require\ncarefully tuned stepsizes to achieve their best performance. The improvements\nproposed by existing adaptive federated methods involve tuning of additional\nhyperparameters such as momentum parameters, and consider adaptivity only in\nthe server aggregation round, but not locally. These methods can be inefficient\nin many practical scenarios because they require excessive tuning of\nhyperparameters and do not capture local geometric information. In this work,\nwe extend the recently proposed stochastic Polyak stepsize (SPS) to the\nfederated learning setting, and propose new locally adaptive and nearly\nparameter-free distributed SPS variants (FedSPS and FedDecSPS). We prove that\nFedSPS converges linearly in strongly convex and sublinearly in convex settings\nwhen the interpolation condition (overparametrization) is satisfied, and\nconverges to a neighborhood of the solution in the general case. We extend our\nproposed method to a decreasing stepsize version FedDecSPS, that converges also\nwhen the interpolation condition does not hold. We validate our theoretical\nclaims by performing illustrative convex experiments. Our proposed algorithms\nmatch the optimization performance of FedAvg with the best tuned\nhyperparameters in the i.i.d. case, and outperform FedAvg in the non-i.i.d.\ncase.\n","authors":["Sohom Mukherjee","Nicolas Loizou","Sebastian U. Stich"],"pdf_url":"https://arxiv.org/pdf/2307.06306v1.pdf","comment":"33 pages, 6 figures"},{"id":"http://arxiv.org/abs/2307.06304v1","updated":"2023-07-12T17:01:03Z","published":"2023-07-12T17:01:03Z","title":"Patch n' Pack: NaViT, a Vision Transformer for any Aspect Ratio and\n  Resolution","summary":"  The ubiquitous and demonstrably suboptimal choice of resizing images to a\nfixed resolution before processing them with computer vision models has not yet\nbeen successfully challenged. However, models such as the Vision Transformer\n(ViT) offer flexible sequence-based modeling, and hence varying input sequence\nlengths. We take advantage of this with NaViT (Native Resolution ViT) which\nuses sequence packing during training to process inputs of arbitrary\nresolutions and aspect ratios. Alongside flexible model usage, we demonstrate\nimproved training efficiency for large-scale supervised and contrastive\nimage-text pretraining. NaViT can be efficiently transferred to standard tasks\nsuch as image and video classification, object detection, and semantic\nsegmentation and leads to improved results on robustness and fairness\nbenchmarks. At inference time, the input resolution flexibility can be used to\nsmoothly navigate the test-time cost-performance trade-off. We believe that\nNaViT marks a departure from the standard, CNN-designed, input and modelling\npipeline used by most computer vision models, and represents a promising\ndirection for ViTs.\n","authors":["Mostafa Dehghani","Basil Mustafa","Josip Djolonga","Jonathan Heek","Matthias Minderer","Mathilde Caron","Andreas Steiner","Joan Puigcerver","Robert Geirhos","Ibrahim Alabdulmohsin","Avital Oliver","Piotr Padlewski","Alexey Gritsenko","Mario Lučić","Neil Houlsby"],"pdf_url":"https://arxiv.org/pdf/2307.06304v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06299v1","updated":"2023-07-12T16:53:32Z","published":"2023-07-12T16:53:32Z","title":"Towards a Certified Proof Checker for Deep Neural Network Verification","summary":"  Recent developments in deep neural networks (DNNs) have led to their adoption\nin safety-critical systems, which in turn has heightened the need for\nguaranteeing their safety. These safety properties of DNNs can be proven using\ntools developed by the verification community. However, these tools are\nthemselves prone to implementation bugs and numerical stability problems, which\nmake their reliability questionable. To overcome this, some verifiers produce\nproofs of their results which can be checked by a trusted checker. In this\nwork, we present a novel implementation of a proof checker for DNN\nverification. It improves on existing implementations by offering numerical\nstability and greater verifiability. To achieve this, we leverage two key\ncapabilities of Imandra, an industrial theorem prover: its support of infinite\nprecision real arithmetic and its formal verification infrastructure. So far,\nwe have implemented a proof checker in Imandra, specified its correctness\nproperties and started to verify the checker's compliance with them. Our\nongoing work focuses on completing the formal verification of the checker and\nfurther optimizing its performance.\n","authors":["Remi Desmartin","Omri Isac","Grant Passmore","Kathrin Stark","Guy Katz","Ekaterina Komendantskaya"],"pdf_url":"https://arxiv.org/pdf/2307.06299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.04927v2","updated":"2023-07-12T16:39:35Z","published":"2023-07-10T22:28:33Z","title":"Probabilistic Counterexample Guidance for Safer Reinforcement Learning\n  (Extended Version)","summary":"  Safe exploration aims at addressing the limitations of Reinforcement Learning\n(RL) in safety-critical scenarios, where failures during trial-and-error\nlearning may incur high costs. Several methods exist to incorporate external\nknowledge or to use proximal sensor data to limit the exploration of unsafe\nstates. However, reducing exploration risks in unknown environments, where an\nagent must discover safety threats during exploration, remains challenging. In\nthis paper, we target the problem of safe exploration by guiding the training\nwith counterexamples of the safety requirement. Our method abstracts both\ncontinuous and discrete state-space systems into compact abstract models\nrepresenting the safety-relevant knowledge acquired by the agent during\nexploration. We then exploit probabilistic counterexample generation to\nconstruct minimal simulation submodels eliciting safety requirement violations,\nwhere the agent can efficiently train offline to refine its policy towards\nminimising the risk of safety violations during the subsequent online\nexploration. We demonstrate our method's effectiveness in reducing safety\nviolations during online exploration in preliminary experiments by an average\nof 40.3% compared with QL and DQN standard algorithms and 29.1% compared with\nprevious related work, while achieving comparable cumulative rewards with\nrespect to unrestricted exploration and alternative approaches.\n","authors":["Xiaotong Ji","Antonio Filieri"],"pdf_url":"https://arxiv.org/pdf/2307.04927v2.pdf","comment":"Accepted and Evaluated by the 20th International Conference on\n  Quantitative Evaluation of Systems 2023"},{"id":"http://arxiv.org/abs/2307.06290v1","updated":"2023-07-12T16:37:31Z","published":"2023-07-12T16:37:31Z","title":"Instruction Mining: High-Quality Instruction Data Selection for Large\n  Language Models","summary":"  Large language models typically undergo two training stages, pretraining and\nfinetuning. Despite that large-scale pretraining endows the model with strong\ncapabilities to generate natural language responses, these pretrained models\ncan still fail to understand human instructions at times. To enhance language\nmodels' ability of interpreting and responding to instructions, instruction\nfinetuning has emerged as a critical method in this area. Recent studies found\nthat large language models can be finetuned to perform well even with a small\namount of high-quality instruction-following data. However, the selection of\nhigh-quality datasets for finetuning language models still lacks clear\nguidelines to follow. In this paper, we propose InstructMining, a linear rule\nfor evaluating instruction-following data quality. We formulate InstructMining\nusing specific natural language indicators. To investigate the relationship\nbetween data quality and these indicators, we further conduct extensive\nfinetuning experiments. The experiment results are then applied to estimating\nparameters in InstructMining. To further investigate its performance, we use\nInstructMining to select high-quality data from unseen datasets. Results\ndemonstrate that InstructMining can help select relatively high-quality samples\nfrom various instruction-following datasets. Compared to models finetuned on\nunfiltered datasets, models finetuned on InstructMining selected datasets\nperform better on 42.5% cases.\n","authors":["Yihan Cao","Yanbin Kang","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2307.06290v1.pdf","comment":"Work in progress. 12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2307.06287v1","updated":"2023-07-12T16:35:41Z","published":"2023-07-12T16:35:41Z","title":"Rational Neural Network Controllers","summary":"  Neural networks have shown great success in many machine learning related\ntasks, due to their ability to act as general function approximators. Recent\nwork has demonstrated the effectiveness of neural networks in control systems\n(known as neural feedback loops), most notably by using a neural network as a\ncontroller. However, one of the big challenges of this approach is that neural\nnetworks have been shown to be sensitive to adversarial attacks. This means\nthat, unless they are designed properly, they are not an ideal candidate for\ncontrollers due to issues with robustness and uncertainty, which are pivotal\naspects of control systems. There has been initial work on robustness to both\nanalyse and design dynamical systems with neural network controllers. However,\none prominent issue with these methods is that they use existing neural network\narchitectures tailored for traditional machine learning tasks. These structures\nmay not be appropriate for neural network controllers and it is important to\nconsider alternative architectures. This paper considers rational neural\nnetworks and presents novel rational activation functions, which can be used\neffectively in robustness problems for neural feedback loops. Rational\nactivation functions are replaced by a general rational neural network\nstructure, which is convex in the neural network's parameters. A method is\nproposed to recover a stabilising controller from a Sum of Squares feasibility\ntest. This approach is then applied to a refined rational neural network which\nis more compatible with Sum of Squares programming. Numerical examples show\nthat this method can successfully recover stabilising rational neural network\ncontrollers for neural feedback loops with non-linear plants with noise and\nparametric uncertainty.\n","authors":["Matthew Newton","Antonis Papachristodoulou"],"pdf_url":"https://arxiv.org/pdf/2307.06287v1.pdf","comment":"20 Pages, 12 Figures"},{"id":"http://arxiv.org/abs/2304.06848v2","updated":"2023-07-12T16:34:08Z","published":"2023-04-13T22:32:21Z","title":"CAR-DESPOT: Causally-Informed Online POMDP Planning for Robots in\n  Confounded Environments","summary":"  Robots operating in real-world environments must reason about possible\noutcomes of stochastic actions and make decisions based on partial observations\nof the true world state. A major challenge for making accurate and robust\naction predictions is the problem of confounding, which if left untreated can\nlead to prediction errors. The partially observable Markov decision process\n(POMDP) is a widely-used framework to model these stochastic and\npartially-observable decision-making problems. However, due to a lack of\nexplicit causal semantics, POMDP planning methods are prone to confounding bias\nand thus in the presence of unobserved confounders may produce underperforming\npolicies. This paper presents a novel causally-informed extension of \"anytime\nregularized determinized sparse partially observable tree\" (AR-DESPOT), a\nmodern anytime online POMDP planner, using causal modelling and inference to\neliminate errors caused by unmeasured confounder variables. We further propose\na method to learn offline the partial parameterisation of the causal model for\nplanning, from ground truth model data. We evaluate our methods on a toy\nproblem with an unobserved confounder and show that the learned causal model is\nhighly accurate, while our planning method is more robust to confounding and\nproduces overall higher performing policies than AR-DESPOT.\n","authors":["Ricardo Cannizzaro","Lars Kunze"],"pdf_url":"https://arxiv.org/pdf/2304.06848v2.pdf","comment":"8 pages, 3 figures, accepted to 2023 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS)"},{"id":"http://arxiv.org/abs/2307.06283v1","updated":"2023-07-12T16:28:21Z","published":"2023-07-12T16:28:21Z","title":"Tackling Computational Heterogeneity in FL: A Few Theoretical Insights","summary":"  The future of machine learning lies in moving data collection along with\ntraining to the edge. Federated Learning, for short FL, has been recently\nproposed to achieve this goal. The principle of this approach is to aggregate\nmodels learned over a large number of distributed clients, i.e.,\nresource-constrained mobile devices that collect data from their environment,\nto obtain a new more general model. The latter is subsequently redistributed to\nclients for further training. A key feature that distinguishes federated\nlearning from data-center-based distributed training is the inherent\nheterogeneity. In this work, we introduce and analyse a novel aggregation\nframework that allows for formalizing and tackling computational heterogeneity\nin federated optimization, in terms of both heterogeneous data and local\nupdates. Proposed aggregation algorithms are extensively analyzed from a\ntheoretical, and an experimental prospective.\n","authors":["Adnan Ben Mansour","Gaia Carenini","Alexandre Duplessis"],"pdf_url":"https://arxiv.org/pdf/2307.06283v1.pdf","comment":"22 pages, 7 figures. Extended version of the paper \"Federated\n  Learning Aggregation: New Robust Algorithms with Guarantees\"\n  (arXiv:2205.10864)"},{"id":"http://arxiv.org/abs/2307.06272v1","updated":"2023-07-12T16:16:37Z","published":"2023-07-12T16:16:37Z","title":"Exposing the Fake: Effective Diffusion-Generated Images Detection","summary":"  Image synthesis has seen significant advancements with the advent of\ndiffusion-based generative models like Denoising Diffusion Probabilistic Models\n(DDPM) and text-to-image diffusion models. Despite their efficacy, there is a\ndearth of research dedicated to detecting diffusion-generated images, which\ncould pose potential security and privacy risks. This paper addresses this gap\nby proposing a novel detection method called Stepwise Error for\nDiffusion-generated Image Detection (SeDID). Comprising statistical-based\n$\\text{SeDID}_{\\text{Stat}}$ and neural network-based\n$\\text{SeDID}_{\\text{NNs}}$, SeDID exploits the unique attributes of diffusion\nmodels, namely deterministic reverse and deterministic denoising computation\nerrors. Our evaluations demonstrate SeDID's superior performance over existing\nmethods when applied to diffusion models. Thus, our work makes a pivotal\ncontribution to distinguishing diffusion model-generated images, marking a\nsignificant step in the domain of artificial intelligence security.\n","authors":["Ruipeng Ma","Jinhao Duan","Fei Kong","Xiaoshuang Shi","Kaidi Xu"],"pdf_url":"https://arxiv.org/pdf/2307.06272v1.pdf","comment":"AdvML-Frontiers@ICML 2023"},{"id":"http://arxiv.org/abs/2305.19779v2","updated":"2023-07-12T16:13:19Z","published":"2023-05-31T12:12:40Z","title":"Deep learning and MCMC with aggVAE for shifting administrative\n  boundaries: mapping malaria prevalence in Kenya","summary":"  Model-based disease mapping remains a fundamental policy-informing tool in\npublic health and disease surveillance. Hierarchical Bayesian models have\nbecome the state-of-the-art approach for disease mapping since they are able to\ncapture structure in the data, as well as to characterise uncertainty. When\nworking with areal data, e.g.~aggregates at the administrative unit level such\nas district or province, routinely used models rely on the adjacency structure\nof areal units to account for spatial correlations. The goal of disease\nsurveillance systems is to track disease outcomes over time. This task provides\nchallenging in situations of crises, such as political changes, leading to\nchanges of administrative boundaries. Kenya is an example of a country where\nchange of boundaries took place in 2010. Moreover, the adjacency-based approach\nignores the continuous nature of spatial processes and cannot solve the\nchange-of-support problem, i.e.~when administrative boundaries change or when\nestimates must be produced at a different administrative level. We present a\nnovel, practical, and easy to implement solution relying on a methodology\ncombining deep generative modelling and fully Bayesian inference: we build on\nthe recently proposed PriorVAE method able to encode spatial priors over small\nareas with variational autoencoders, to map malaria prevalence in Kenya.\n","authors":["Elizaveta Semenova","Swapnil Mishra","Samir Bhatt","Seth Flaxman","H Juliette T Unwin"],"pdf_url":"https://arxiv.org/pdf/2305.19779v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06267v1","updated":"2023-07-12T16:11:57Z","published":"2023-07-12T16:11:57Z","title":"Physics-informed Machine Learning for Calibrating Macroscopic Traffic\n  Flow Models","summary":"  Well-calibrated traffic flow models are fundamental to understanding traffic\nphenomena and designing control strategies. Traditional calibration has been\ndeveloped base on optimization methods. In this paper, we propose a novel\nphysics-informed, learning-based calibration approach that achieves\nperformances comparable to and even better than those of optimization-based\nmethods. To this end, we combine the classical deep autoencoder, an\nunsupervised machine learning model consisting of one encoder and one decoder,\nwith traffic flow models. Our approach informs the decoder of the physical\ntraffic flow models and thus induces the encoder to yield reasonable traffic\nparameters given flow and speed measurements. We also introduce the denoising\nautoencoder into our method so that it can handles not only with normal data\nbut also with corrupted data with missing values. We verified our approach with\na case study of I-210 E in California.\n","authors":["Yu Tang","Li Jin","Kaan Ozbay"],"pdf_url":"https://arxiv.org/pdf/2307.06267v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.12559v3","updated":"2023-07-12T16:09:56Z","published":"2023-02-24T10:24:03Z","title":"From Noisy Fixed-Point Iterations to Private ADMM for Centralized and\n  Federated Learning","summary":"  We study differentially private (DP) machine learning algorithms as instances\nof noisy fixed-point iterations, in order to derive privacy and utility results\nfrom this well-studied framework. We show that this new perspective recovers\npopular private gradient-based methods like DP-SGD and provides a principled\nway to design and analyze new private optimization algorithms in a flexible\nmanner. Focusing on the widely-used Alternating Directions Method of\nMultipliers (ADMM) method, we use our general framework to derive novel private\nADMM algorithms for centralized, federated and fully decentralized learning.\nFor these three algorithms, we establish strong privacy guarantees leveraging\nprivacy amplification by iteration and by subsampling. Finally, we provide\nutility guarantees using a unified analysis that exploits a recent linear\nconvergence result for noisy fixed-point iterations.\n","authors":["Edwige Cyffers","Aurélien Bellet","Debabrota Basu"],"pdf_url":"https://arxiv.org/pdf/2302.12559v3.pdf","comment":"Accepted to ICML 2023. v3: added references to the first papers that\n  introduced block-wise fixed-point iterations (Iutzeler et al., 2013; Bianchi\n  et al., 2016)"},{"id":"http://arxiv.org/abs/2302.02988v2","updated":"2023-07-12T16:06:58Z","published":"2023-02-06T18:27:11Z","title":"Asymptotically Optimal Fixed-Budget Best Arm Identification with\n  Variance-Dependent Bounds","summary":"  We investigate the problem of fixed-budget best arm identification (BAI) for\nminimizing expected simple regret. In an adaptive experiment, a decision maker\ndraws one of multiple treatment arms based on past observations and observes\nthe outcome of the drawn arm. After the experiment, the decision maker\nrecommends the treatment arm with the highest expected outcome. We evaluate the\ndecision based on the expected simple regret, which is the difference between\nthe expected outcomes of the best arm and the recommended arm. Due to inherent\nuncertainty, we evaluate the regret using the minimax criterion. First, we\nderive asymptotic lower bounds for the worst-case expected simple regret, which\nare characterized by the variances of potential outcomes (leading factor).\nBased on the lower bounds, we propose the Two-Stage (TS)-Hirano-Imbens-Ridder\n(HIR) strategy, which utilizes the HIR estimator (Hirano et al., 2003) in\nrecommending the best arm. Our theoretical analysis shows that the TS-HIR\nstrategy is asymptotically minimax optimal, meaning that the leading factor of\nits worst-case expected simple regret matches our derived worst-case lower\nbound. Additionally, we consider extensions of our method, such as the\nasymptotic optimality for the probability of misidentification. Finally, we\nvalidate the proposed method's effectiveness through simulations.\n","authors":["Masahiro Kato","Masaaki Imaizumi","Takuya Ishihara","Toru Kitagawa"],"pdf_url":"https://arxiv.org/pdf/2302.02988v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06263v1","updated":"2023-07-12T16:03:34Z","published":"2023-07-12T16:03:34Z","title":"On the hierarchical Bayesian modelling of frequency response functions","summary":"  Population-based structural health monitoring (PBSHM) aims to share valuable\ninformation among members of a population, such as normal- and damage-condition\ndata, to improve inferences regarding the health states of the members. Even\nwhen the population is comprised of nominally-identical structures, benign\nvariations among the members will exist as a result of slight differences in\nmaterial properties, geometry, boundary conditions, or environmental effects\n(e.g., temperature changes). These discrepancies can affect modal properties\nand present as changes in the characteristics of the resonance peaks of the\nfrequency response function (FRF). Many SHM strategies depend on monitoring the\ndynamic properties of structures, so benign variations can be challenging for\nthe practical implementation of these systems. Another common challenge with\nvibration-based SHM is data loss, which may result from transmission issues,\nsensor failure, a sample-rate mismatch between sensors, and other causes.\nMissing data in the time domain will result in decreased resolution in the\nfrequency domain, which can impair dynamic characterisation. The hierarchical\nBayesian approach provides a useful modelling structure for PBSHM, because\nstatistical distributions at the population and individual (or domain) level\nare learnt simultaneously to bolster statistical strength among the parameters.\nAs a result, variance is reduced among the parameter estimates, particularly\nwhen data are limited. In this paper, combined probabilistic FRF models are\ndeveloped for a small population of nominally-identical helicopter blades under\nvarying temperature conditions, using a hierarchical Bayesian structure. These\nmodels address critical challenges in SHM, by accommodating benign variations\nthat present as differences in the underlying dynamics, while also considering\n(and utilising), the similarities among the blades.\n","authors":["T. A. Dardeno","R. S. Mills","N. Dervilis","K. Worden","L. A. Bull"],"pdf_url":"https://arxiv.org/pdf/2307.06263v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04878v2","updated":"2023-07-12T15:57:25Z","published":"2023-03-08T20:33:09Z","title":"DeepGD: A Multi-Objective Black-Box Test Selection Approach for Deep\n  Neural Networks","summary":"  Deep neural networks (DNNs) are widely used in various application domains\nsuch as image processing, speech recognition, and natural language processing.\nHowever, testing DNN models may be challenging due to the complexity and size\nof their input domain. Particularly, testing DNN models often requires\ngenerating or exploring large unlabeled datasets. In practice, DNN test\noracles, which identify the correct outputs for inputs, often require expensive\nmanual effort to label test data, possibly involving multiple experts to ensure\nlabeling correctness. In this paper, we propose DeepGD, a black-box\nmulti-objective test selection approach for DNN models. It reduces the cost of\nlabeling by prioritizing the selection of test inputs with high fault revealing\npower from large unlabeled datasets. DeepGD not only selects test inputs with\nhigh uncertainty scores to trigger as many mispredicted inputs as possible but\nalso maximizes the probability of revealing distinct faults in the DNN model by\nselecting diverse mispredicted inputs. The experimental results conducted on\nfour widely used datasets and five DNN models show that in terms of\nfault-revealing ability: (1) White-box, coverage-based approaches fare poorly,\n(2) DeepGD outperforms existing black-box test selection approaches in terms of\nfault detection, and (3) DeepGD also leads to better guidance for DNN model\nretraining when using selected inputs to augment the training set.\n","authors":["Zohreh Aghababaeyan","Manel Abdellatif","Mahboubeh Dadkhah","Lionel Briand"],"pdf_url":"https://arxiv.org/pdf/2303.04878v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05350v2","updated":"2023-07-12T15:56:15Z","published":"2023-07-07T01:10:18Z","title":"Dividing and Conquering a BlackBox to a Mixture of Interpretable Models:\n  Route, Interpret, Repeat","summary":"  ML model design either starts with an interpretable model or a Blackbox and\nexplains it post hoc. Blackbox models are flexible but difficult to explain,\nwhile interpretable models are inherently explainable. Yet, interpretable\nmodels require extensive ML knowledge and tend to be less flexible and\nunderperforming than their Blackbox variants. This paper aims to blur the\ndistinction between a post hoc explanation of a Blackbox and constructing\ninterpretable models. Beginning with a Blackbox, we iteratively carve out a\nmixture of interpretable experts (MoIE) and a residual network. Each\ninterpretable model specializes in a subset of samples and explains them using\nFirst Order Logic (FOL), providing basic reasoning on concepts from the\nBlackbox. We route the remaining samples through a flexible residual. We repeat\nthe method on the residual network until all the interpretable models explain\nthe desired proportion of data. Our extensive experiments show that our route,\ninterpret, and repeat approach (1) identifies a diverse set of\ninstance-specific concepts with high concept completeness via MoIE without\ncompromising in performance, (2) identifies the relatively ``harder'' samples\nto explain via residuals, (3) outperforms the interpretable by-design models by\nsignificant margins during test-time interventions, and (4) fixes the shortcut\nlearned by the original Blackbox. The code for MoIE is publicly available at:\n\\url{https://github.com/batmanlab/ICML-2023-Route-interpret-repeat}\n","authors":["Shantanu Ghosh","Ke Yu","Forough Arabshahi","Kayhan Batmanghelich"],"pdf_url":"https://arxiv.org/pdf/2307.05350v2.pdf","comment":"appeared as v5 of arXiv:2302.10289 which was replaced in error, which\n  drifted into a different work, accepted in ICML 2023"},{"id":"http://arxiv.org/abs/2307.06255v1","updated":"2023-07-12T15:50:38Z","published":"2023-07-12T15:50:38Z","title":"Machine learning and Topological data analysis identify unique features\n  of human papillae in 3D scans","summary":"  The tongue surface houses a range of papillae that are integral to the\nmechanics and chemistry of taste and textural sensation. Although gustatory\nfunction of papillae is well investigated, the uniqueness of papillae within\nand across individuals remains elusive. Here, we present the first machine\nlearning framework on 3D microscopic scans of human papillae (n = 2092),\nuncovering the uniqueness of geometric and topological features of papillae.\nThe finer differences in shapes of papillae are investigated computationally\nbased on a number of features derived from discrete differential geometry and\ncomputational topology. Interpretable machine learning techniques show that\npersistent homology features of the papillae shape are the most effective in\npredicting the biological variables. Models trained on these features with\nsmall volumes of data samples predict the type of papillae with an accuracy of\n85%. The papillae type classification models can map the spatial arrangement of\nfiliform and fungiform papillae on a surface. Remarkably, the papillae are\nfound to be distinctive across individuals and an individual can be identified\nwith an accuracy of 48% among the 15 participants from a single papillae.\nCollectively, this is the first unprecedented evidence demonstrating that\ntongue papillae can serve as a unique identifier inspiring new research\ndirection for food preferences and oral diagnostics.\n","authors":["Rayna Andreeva","Anwesha Sarkar","Rik Sarkar"],"pdf_url":"https://arxiv.org/pdf/2307.06255v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.04149v2","updated":"2023-07-12T15:49:41Z","published":"2023-07-09T10:56:44Z","title":"Latent Graph Attention for Enhanced Spatial Context","summary":"  Global contexts in images are quite valuable in image-to-image translation\nproblems. Conventional attention-based and graph-based models capture the\nglobal context to a large extent, however, these are computationally expensive.\nMoreover, the existing approaches are limited to only learning the pairwise\nsemantic relation between any two points on the image. In this paper, we\npresent Latent Graph Attention (LGA) a computationally inexpensive (linear to\nthe number of nodes) and stable, modular framework for incorporating the global\ncontext in the existing architectures, especially empowering small-scale\narchitectures to give performance closer to large size architectures, thus\nmaking the light-weight architectures more useful for edge devices with lower\ncompute power and lower energy needs. LGA propagates information spatially\nusing a network of locally connected graphs, thereby facilitating to construct\na semantically coherent relation between any two spatially distant points that\nalso takes into account the influence of the intermediate pixels. Moreover, the\ndepth of the graph network can be used to adapt the extent of contextual spread\nto the target dataset, thereby being able to explicitly control the added\ncomputational cost. To enhance the learning mechanism of LGA, we also introduce\na novel contrastive loss term that helps our LGA module to couple well with the\noriginal architecture at the expense of minimal additional computational load.\nWe show that incorporating LGA improves the performance on three challenging\napplications, namely transparent object segmentation, image restoration for\ndehazing and optical flow estimation.\n","authors":["Ayush Singh","Yash Bhambhu","Himanshu Buckchash","Deepak K. Gupta","Dilip K. Prasad"],"pdf_url":"https://arxiv.org/pdf/2307.04149v2.pdf","comment":"20 pages, 7 figures"},{"id":"http://arxiv.org/abs/2306.11941v2","updated":"2023-07-12T15:44:22Z","published":"2023-06-20T23:38:24Z","title":"Efficient Dynamics Modeling in Interactive Environments with Koopman\n  Theory","summary":"  The accurate modeling of dynamics in interactive environments is critical for\nsuccessful long-range prediction. Such a capability could advance Reinforcement\nLearning (RL) and Planning algorithms, but achieving it is challenging.\nInaccuracies in model estimates can compound, resulting in increased errors\nover long horizons. We approach this problem from the lens of Koopman theory,\nwhere the nonlinear dynamics of the environment can be linearized in a\nhigh-dimensional latent space. This allows us to efficiently parallelize the\nsequential problem of long-range prediction using convolution, while accounting\nfor the agent's action at every time step. Our approach also enables stability\nanalysis and better control over gradients through time. Taken together, these\nadvantages result in significant improvement over the existing approaches, both\nin the efficiency and the accuracy of modeling dynamics over extended horizons.\nWe also report promising experimental results in dynamics modeling for the\nscenarios of both model-based planning and model-free RL.\n","authors":["Arnab Kumar Mondal","Siba Smarak Panigrahi","Sai Rajeswar","Kaleem Siddiqi","Siamak Ravanbakhsh"],"pdf_url":"https://arxiv.org/pdf/2306.11941v2.pdf","comment":"18 pages, 3 figures"},{"id":"http://arxiv.org/abs/2306.10548v3","updated":"2023-07-12T15:40:14Z","published":"2023-06-18T12:56:46Z","title":"MARBLE: Music Audio Representation Benchmark for Universal Evaluation","summary":"  In the era of extensive intersection between art and Artificial Intelligence\n(AI), such as image generation and fiction co-creation, AI for music remains\nrelatively nascent, particularly in music understanding. This is evident in the\nlimited work on deep music representations, the scarcity of large-scale\ndatasets, and the absence of a universal and community-driven benchmark. To\naddress this issue, we introduce the Music Audio Representation Benchmark for\nuniversaL Evaluation, termed MARBLE. It aims to provide a benchmark for various\nMusic Information Retrieval (MIR) tasks by defining a comprehensive taxonomy\nwith four hierarchy levels, including acoustic, performance, score, and\nhigh-level description. We then establish a unified protocol based on 14 tasks\non 8 public-available datasets, providing a fair and standard assessment of\nrepresentations of all open-sourced pre-trained models developed on music\nrecordings as baselines. Besides, MARBLE offers an easy-to-use, extendable, and\nreproducible suite for the community, with a clear statement on copyright\nissues on datasets. Results suggest recently proposed large-scale pre-trained\nmusical language models perform the best in most tasks, with room for further\nimprovement. The leaderboard and toolkit repository are published at\nhttps://marble-bm.shef.ac.uk to promote future music AI research.\n","authors":["Ruibin Yuan","Yinghao Ma","Yizhi Li","Ge Zhang","Xingran Chen","Hanzhi Yin","Le Zhuo","Yiqi Liu","Jiawen Huang","Zeyue Tian","Binyue Deng","Ningzhi Wang","Chenghua Lin","Emmanouil Benetos","Anton Ragni","Norbert Gyenge","Roger Dannenberg","Wenhu Chen","Gus Xia","Wei Xue","Si Liu","Shi Wang","Ruibo Liu","Yike Guo","Jie Fu"],"pdf_url":"https://arxiv.org/pdf/2306.10548v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06250v1","updated":"2023-07-12T15:39:39Z","published":"2023-07-12T15:39:39Z","title":"Identifiability Guarantees for Causal Disentanglement from Soft\n  Interventions","summary":"  Causal disentanglement aims to uncover a representation of data using latent\nvariables that are interrelated through a causal model. Such a representation\nis identifiable if the latent model that explains the data is unique. In this\npaper, we focus on the scenario where unpaired observational and interventional\ndata are available, with each intervention changing the mechanism of a latent\nvariable. When the causal variables are fully observed, statistically\nconsistent algorithms have been developed to identify the causal model under\nfaithfulness assumptions. We here show that identifiability can still be\nachieved with unobserved causal variables, given a generalized notion of\nfaithfulness. Our results guarantee that we can recover the latent causal model\nup to an equivalence class and predict the effect of unseen combinations of\ninterventions, in the limit of infinite data. We implement our causal\ndisentanglement framework by developing an autoencoding variational Bayes\nalgorithm and apply it to the problem of predicting combinatorial perturbation\neffects in genomics.\n","authors":["Jiaqi Zhang","Chandler Squires","Kristjan Greenewald","Akash Srivastava","Karthikeyan Shanmugam","Caroline Uhler"],"pdf_url":"https://arxiv.org/pdf/2307.06250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06244v1","updated":"2023-07-12T15:34:39Z","published":"2023-07-12T15:34:39Z","title":"Diffusion Based Multi-Agent Adversarial Tracking","summary":"  Target tracking plays a crucial role in real-world scenarios, particularly in\ndrug-trafficking interdiction, where the knowledge of an adversarial target's\nlocation is often limited. Improving autonomous tracking systems will enable\nunmanned aerial, surface, and underwater vehicles to better assist in\ninterdicting smugglers that use manned surface, semi-submersible, and aerial\nvessels. As unmanned drones proliferate, accurate autonomous target estimation\nis even more crucial for security and safety. This paper presents Constrained\nAgent-based Diffusion for Enhanced Multi-Agent Tracking (CADENCE), an approach\naimed at generating comprehensive predictions of adversary locations by\nleveraging past sparse state information. To assess the effectiveness of this\napproach, we evaluate predictions on single-target and multi-target pursuit\nenvironments, employing Monte-Carlo sampling of the diffusion model to estimate\nthe probability associated with each generated trajectory. We propose a novel\ncross-attention based diffusion model that utilizes constraint-based sampling\nto generate multimodal track hypotheses. Our single-target model surpasses the\nperformance of all baseline methods on Average Displacement Error (ADE) for\npredictions across all time horizons.\n","authors":["Sean Ye","Manisha Natarajan","Zixuan Wu","Matthew Gombolay"],"pdf_url":"https://arxiv.org/pdf/2307.06244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06243v1","updated":"2023-07-12T15:34:10Z","published":"2023-07-12T15:34:10Z","title":"Reconstructing Spatiotemporal Data with C-VAEs","summary":"  The continuous representation of spatiotemporal data commonly relies on using\nabstract data types, such as \\textit{moving regions}, to represent entities\nwhose shape and position continuously change over time. Creating this\nrepresentation from discrete snapshots of real-world entities requires using\ninterpolation methods to compute in-between data representations and estimate\nthe position and shape of the object of interest at arbitrary temporal points.\nExisting region interpolation methods often fail to generate smooth and\nrealistic representations of a region's evolution. However, recent advancements\nin deep learning techniques have revealed the potential of deep models trained\non discrete observations to capture spatiotemporal dependencies through\nimplicit feature learning.\n  In this work, we explore the capabilities of Conditional Variational\nAutoencoder (C-VAE) models to generate smooth and realistic representations of\nthe spatiotemporal evolution of moving regions. We evaluate our proposed\napproach on a sparsely annotated dataset on the burnt area of a forest fire. We\napply compression operations to sample from the dataset and use the C-VAE model\nand other commonly used interpolation algorithms to generate in-between region\nrepresentations. To evaluate the performance of the methods, we compare their\ninterpolation results with manually annotated data and regions generated by a\nU-Net model. We also assess the quality of generated data considering temporal\nconsistency metrics.\n  The proposed C-VAE-based approach demonstrates competitive results in\ngeometric similarity metrics. It also exhibits superior temporal consistency,\nsuggesting that C-VAE models may be a viable alternative to modelling the\nspatiotemporal evolution of 2D moving regions.\n","authors":["Tiago F. R. Ribeiro","Fernando Silva","Rogério Luís de C. Costa"],"pdf_url":"https://arxiv.org/pdf/2307.06243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06240v1","updated":"2023-07-12T15:28:26Z","published":"2023-07-12T15:28:26Z","title":"DSSE: a drone swarm search environment","summary":"  The Drone Swarm Search project is an environment, based on PettingZoo, that\nis to be used in conjunction with multi-agent (or single-agent) reinforcement\nlearning algorithms. It is an environment in which the agents (drones), have to\nfind the targets (shipwrecked people). The agents do not know the position of\nthe target and do not receive rewards related to their own distance to the\ntarget(s). However, the agents receive the probabilities of the target(s) being\nin a certain cell of the map. The aim of this project is to aid in the study of\nreinforcement learning algorithms that require dynamic probabilities as inputs.\n","authors":["Manuel Castanares","Luis F. S. Carrete","Enrico F. Damiani","Leonardo D. M. de Abreu","José Fernando B. Brancalion","Fabrício J. Barth"],"pdf_url":"https://arxiv.org/pdf/2307.06240v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2307.06235v1","updated":"2023-07-12T15:27:06Z","published":"2023-07-12T15:27:06Z","title":"Unified Molecular Modeling via Modality Blending","summary":"  Self-supervised molecular representation learning is critical for\nmolecule-based tasks such as AI-assisted drug discovery. Recent studies\nconsider leveraging both 2D and 3D information for representation learning,\nwith straightforward alignment strategies that treat each modality separately.\nIn this work, we introduce a novel \"blend-then-predict\" self-supervised\nlearning method (MoleBLEND), which blends atom relations from different\nmodalities into one unified relation matrix for encoding, then recovers\nmodality-specific information for both 2D and 3D structures. By treating atom\nrelationships as anchors, seemingly dissimilar 2D and 3D manifolds are aligned\nand integrated at fine-grained relation-level organically. Extensive\nexperiments show that MoleBLEND achieves state-of-the-art performance across\nmajor 2D/3D benchmarks. We further provide theoretical insights from the\nperspective of mutual-information maximization, demonstrating that our method\nunifies contrastive, generative (inter-modal prediction) and mask-then-predict\n(intra-modal prediction) objectives into a single cohesive blend-then-predict\nframework.\n","authors":["Qiying Yu","Yudi Zhang","Yuyan Ni","Shikun Feng","Yanyan Lan","Hao Zhou","Jingjing Liu"],"pdf_url":"https://arxiv.org/pdf/2307.06235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.01212v5","updated":"2023-07-12T15:09:24Z","published":"2022-10-03T20:07:51Z","title":"spred: Solving $L_1$ Penalty with SGD","summary":"  We propose to minimize a generic differentiable objective with $L_1$\nconstraint using a simple reparametrization and straightforward stochastic\ngradient descent. Our proposal is the direct generalization of previous ideas\nthat the $L_1$ penalty may be equivalent to a differentiable reparametrization\nwith weight decay. We prove that the proposed method, \\textit{spred}, is an\nexact differentiable solver of $L_1$ and that the reparametrization trick is\ncompletely ``benign\" for a generic nonconvex function. Practically, we\ndemonstrate the usefulness of the method in (1) training sparse neural networks\nto perform gene selection tasks, which involves finding relevant features in a\nvery high dimensional space, and (2) neural network compression task, to which\nprevious attempts at applying the $L_1$-penalty have been unsuccessful.\nConceptually, our result bridges the gap between the sparsity in deep learning\nand conventional statistical learning.\n","authors":["Liu Ziyin","Zihao Wang"],"pdf_url":"https://arxiv.org/pdf/2210.01212v5.pdf","comment":"ICML 2023, 16 pages, 10 figures, and 2 tables"},{"id":"http://arxiv.org/abs/2307.04617v2","updated":"2023-07-12T15:04:16Z","published":"2023-07-10T15:02:13Z","title":"Weakly-supervised positional contrastive learning: application to\n  cirrhosis classification","summary":"  Large medical imaging datasets can be cheaply and quickly annotated with\nlow-confidence, weak labels (e.g., radiological scores). Access to\nhigh-confidence labels, such as histology-based diagnoses, is rare and costly.\nPretraining strategies, like contrastive learning (CL) methods, can leverage\nunlabeled or weakly-annotated datasets. These methods typically require large\nbatch sizes, which poses a difficulty in the case of large 3D images at full\nresolution, due to limited GPU memory. Nevertheless, volumetric positional\ninformation about the spatial context of each 2D slice can be very important\nfor some medical applications. In this work, we propose an efficient\nweakly-supervised positional (WSP) contrastive learning strategy where we\nintegrate both the spatial context of each 2D slice and a weak label via a\ngeneric kernel-based loss function. We illustrate our method on cirrhosis\nprediction using a large volume of weakly-labeled images, namely radiological\nlow-confidence annotations, and small strongly-labeled (i.e., high-confidence)\ndatasets. The proposed model improves the classification AUC by 5% with respect\nto a baseline model on our internal dataset, and by 26% on the public LIHC\ndataset from the Cancer Genome Atlas. The code is available at:\nhttps://github.com/Guerbet-AI/wsp-contrastive.\n","authors":["Emma Sarfati","Alexandre Bône","Marc-Michel Rohé","Pietro Gori","Isabelle Bloch"],"pdf_url":"https://arxiv.org/pdf/2307.04617v2.pdf","comment":"MICCAI 2023"},{"id":"http://arxiv.org/abs/2211.16237v2","updated":"2023-07-12T15:03:37Z","published":"2022-11-29T14:21:34Z","title":"Closing the gap between SVRG and TD-SVRG with Gradient Splitting","summary":"  Temporal difference (TD) learning is a policy evaluation in reinforcement\nlearning whose performance can be enhanced by variance reduction techniques.\nRecently, multiple works have sought to fuse TD learning with SVRG to obtain a\npolicy evaluation method with a geometric rate of convergence. However, the\nresulting convergence rate is significantly weaker than what is achieved by\nSVRG in the setting of convex optimization. In this work we utilize a recent\ninterpretation of TD-learning as the splitting of the gradient of an\nappropriately chosen function, thus simplifying the algorithm and fusing TD\nwith SVRG. Our main result is a geometric convergence bound with predetermined\nlearning rate of $1/8$, which is identical to the convergence bound available\nfor SVRG in the convex setting. Our theoretical findings are supported by a set\nof experiments.\n","authors":["Arsenii Mustafin","Alex Olshevsky","Ioannis Ch. Paschalidis"],"pdf_url":"https://arxiv.org/pdf/2211.16237v2.pdf","comment":"30 pages, 9 figures, 5 tables"},{"id":"http://arxiv.org/abs/2307.06207v1","updated":"2023-07-12T14:52:31Z","published":"2023-07-12T14:52:31Z","title":"Local Conditional Neural Fields for Versatile and Generalizable\n  Large-Scale Reconstructions in Computational Imaging","summary":"  Deep learning has transformed computational imaging, but traditional\npixel-based representations limit their ability to capture continuous,\nmultiscale details of objects. Here we introduce a novel Local Conditional\nNeural Fields (LCNF) framework, leveraging a continuous implicit neural\nrepresentation to address this limitation. LCNF enables flexible object\nrepresentation and facilitates the reconstruction of multiscale information. We\ndemonstrate the capabilities of LCNF in solving the highly ill-posed inverse\nproblem in Fourier ptychographic microscopy (FPM) with multiplexed\nmeasurements, achieving robust, scalable, and generalizable large-scale phase\nretrieval. Unlike traditional neural fields frameworks, LCNF incorporates a\nlocal conditional representation that promotes model generalization, learning\nmultiscale information, and efficient processing of large-scale imaging data.\nBy combining an encoder and a decoder conditioned on a learned latent vector,\nLCNF achieves versatile continuous-domain super-resolution image\nreconstruction. We demonstrate accurate reconstruction of wide field-of-view,\nhigh-resolution phase images using only a few multiplexed measurements. LCNF\nrobustly captures the continuous object priors and eliminates various phase\nartifacts, even when it is trained on imperfect datasets. The framework\nexhibits strong generalization, reconstructing diverse objects even with\nlimited training data. Furthermore, LCNF can be trained on a physics simulator\nusing natural images and successfully applied to experimental measurements on\nbiological samples. Our results highlight the potential of LCNF for solving\nlarge-scale inverse problems in computational imaging, with broad applicability\nin various deep-learning-based techniques.\n","authors":["Hao Wang","Lei Tian"],"pdf_url":"https://arxiv.org/pdf/2307.06207v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2205.01179v2","updated":"2023-07-12T14:23:51Z","published":"2022-05-02T19:49:53Z","title":"VAE-Loco: Versatile Quadruped Locomotion by Learning a Disentangled Gait\n  Representation","summary":"  Quadruped locomotion is rapidly maturing to a degree where robots are able to\nrealise highly dynamic manoeuvres. However, current planners are unable to vary\nkey gait parameters of the in-swing feet midair. In this work we address this\nlimitation and show that it is pivotal in increasing controller robustness by\nlearning a latent space capturing the key stance phases constituting a\nparticular gait. This is achieved via a generative model trained on a single\ntrot style, which encourages disentanglement such that application of a drive\nsignal to a single dimension of the latent state induces holistic plans\nsynthesising a continuous variety of trot styles. We demonstrate that specific\nproperties of the drive signal map directly to gait parameters such as cadence,\nfootstep height and full stance duration. Due to the nature of our approach\nthese synthesised gaits are continuously variable online during robot\noperation. The use of a generative model facilitates the detection and\nmitigation of disturbances to provide a versatile and robust planning\nframework. We evaluate our approach on two versions of the real ANYmal\nquadruped robots and demonstrate that our method achieves a continuous blend of\ndynamic trot styles whilst being robust and reactive to external perturbations.\n","authors":["Alexander L. Mitchell","Wolfgang Merkt","Mathieu Geisert","Siddhant Gangapurwala","Martin Engelcke","Oiwi Parker Jones","Ioannis Havoutis","Ingmar Posner"],"pdf_url":"https://arxiv.org/pdf/2205.01179v2.pdf","comment":"16 pages, 13 figures, 1 table, accepted by IEEE Transactions on\n  Robotics (T-RO) as an extended paper. arXiv admin note: substantial text\n  overlap with arXiv:2112.04809"},{"id":"http://arxiv.org/abs/2110.06672v3","updated":"2023-07-12T14:13:24Z","published":"2021-10-13T12:17:46Z","title":"The Deep Generative Decoder: MAP estimation of representations improves\n  modeling of single-cell RNA data","summary":"  Learning low-dimensional representations of single-cell transcriptomics has\nbecome instrumental to its downstream analysis. The state of the art is\ncurrently represented by neural network models such as variational autoencoders\n(VAEs) which use a variational approximation of the likelihood for inference.\nWe here present the Deep Generative Decoder (DGD), a simple generative model\nthat computes model parameters and representations directly via maximum a\nposteriori (MAP) estimation. The DGD handles complex parameterized latent\ndistributions naturally unlike VAEs which typically use a fixed Gaussian\ndistribution, because of the complexity of adding other types. We first show\nits general functionality on a commonly used benchmark set, Fashion-MNIST.\nSecondly, we apply the model to multiple single-cell data sets. Here the DGD\nlearns low-dimensional, meaningful and well-structured latent representations\nwith sub-clustering beyond the provided labels. The advantages of this approach\nare its simplicity and its capability to provide representations of much\nsmaller dimensionality than a comparable VAE.\n","authors":["Viktoria Schuster","Anders Krogh"],"pdf_url":"https://arxiv.org/pdf/2110.06672v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.12808v2","updated":"2023-07-12T14:07:17Z","published":"2023-02-24T18:41:48Z","title":"Linearization Algorithms for Fully Composite Optimization","summary":"  This paper studies first-order algorithms for solving fully composite\noptimization problems over convex and compact sets. We leverage the structure\nof the objective by handling its differentiable and non-differentiable\ncomponents separately, linearizing only the smooth parts. This provides us with\nnew generalizations of the classical Frank-Wolfe method and the Conditional\nGradient Sliding algorithm, that cater to a subclass of non-differentiable\nproblems. Our algorithms rely on a stronger version of the linear minimization\noracle, which can be efficiently implemented in several practical applications.\nWe provide the basic version of our method with an affine-invariant analysis\nand prove global convergence rates for both convex and non-convex objectives.\nFurthermore, in the convex case, we propose an accelerated method with\ncorrespondingly improved complexity. Finally, we provide illustrative\nexperiments to support our theoretical results.\n","authors":["Maria-Luiza Vladarean","Nikita Doikov","Martin Jaggi","Nicolas Flammarion"],"pdf_url":"https://arxiv.org/pdf/2302.12808v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06175v1","updated":"2023-07-12T14:02:03Z","published":"2023-07-12T14:02:03Z","title":"Learning Decentralized Partially Observable Mean Field Control for\n  Artificial Collective Behavior","summary":"  Recent reinforcement learning (RL) methods have achieved success in various\ndomains. However, multi-agent RL (MARL) remains a challenge in terms of\ndecentralization, partial observability and scalability to many agents.\nMeanwhile, collective behavior requires resolution of the aforementioned\nchallenges, and remains of importance to many state-of-the-art applications\nsuch as active matter physics, self-organizing systems, opinion dynamics, and\nbiological or robotic swarms. Here, MARL via mean field control (MFC) offers a\npotential solution to scalability, but fails to consider decentralized and\npartially observable systems. In this paper, we enable decentralized behavior\nof agents under partial information by proposing novel models for decentralized\npartially observable MFC (Dec-POMFC), a broad class of problems with\npermutation-invariant agents allowing for reduction to tractable single-agent\nMarkov decision processes (MDP) with single-agent RL solution. We provide\nrigorous theoretical results, including a dynamic programming principle,\ntogether with optimality guarantees for Dec-POMFC solutions applied to finite\nswarms of interest. Algorithmically, we propose Dec-POMFC-based policy gradient\nmethods for MARL via centralized training and decentralized execution, together\nwith policy gradient approximation guarantees. In addition, we improve upon\nstate-of-the-art histogram-based MFC by kernel methods, which is of separate\ninterest also for fully observable MFC. We evaluate numerically on\nrepresentative collective behavior tasks such as adapted Kuramoto and Vicsek\nswarming models, being on par with state-of-the-art MARL. Overall, our\nframework takes a step towards RL-based engineering of artificial collective\nbehavior via MFC.\n","authors":["Kai Cui","Sascha Hauck","Christian Fabian","Heinz Koeppl"],"pdf_url":"https://arxiv.org/pdf/2307.06175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.03042v2","updated":"2023-07-12T13:57:41Z","published":"2023-07-06T15:06:41Z","title":"Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain","summary":"  Adapting pretrained language models to novel domains, such as clinical\napplications, traditionally involves retraining their entire set of parameters.\nHowever, this approach is increasingly proven to be impractical owing to the\nsubstantial computational requirements associated with training such large\nlanguage models. To address this issue, Parameter-Efficient Fine-Tuning (PEFT)\ntechniques offer a viable solution by selectively fine-tuning a small subset of\nadditional parameters, significantly reducing the computational requirements\nfor domain adaptation. In this study, we propose Clinical LLaMA-LoRA, a PEFT\nadapter layer built upon the open-sourced LLaMA model. Clinical LLaMA-LoRA is\ntrained using clinical notes obtained from the MIMIC-IV database, thereby\ncreating a specialised adapter designed for the clinical domain. Additionally,\nwe propose a two-step PEFT framework which fuses Clinical LLaMA-LoRA with\nDownstream LLaMA-LoRA, another PEFT adapter specialised for downstream tasks.\nWe evaluate this framework on multiple clinical outcome prediction datasets,\ncomparing it to clinically trained language models. Our proposed framework\nachieves a state-of-the-art AUROC score averaged across all clinical downstream\ntasks. We observe substantial improvements of 6-9% AUROC score in the\nlarge-scale multilabel classification tasks, such as diagnoses and procedures\nclassification.\n","authors":["Aryo Pradipta Gema","Luke Daines","Pasquale Minervini","Beatrice Alex"],"pdf_url":"https://arxiv.org/pdf/2307.03042v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06167v1","updated":"2023-07-12T13:46:40Z","published":"2023-07-12T13:46:40Z","title":"Auxiliary-Tasks Learning for Physics-Informed Neural Network-Based\n  Partial Differential Equations Solving","summary":"  Physics-informed neural networks (PINNs) have emerged as promising surrogate\nmodes for solving partial differential equations (PDEs). Their effectiveness\nlies in the ability to capture solution-related features through neural\nnetworks. However, original PINNs often suffer from bottlenecks, such as low\naccuracy and non-convergence, limiting their applicability in complex physical\ncontexts. To alleviate these issues, we proposed auxiliary-task learning-based\nphysics-informed neural networks (ATL-PINNs), which provide four different\nauxiliary-task learning modes and investigate their performance compared with\noriginal PINNs. We also employ the gradient cosine similarity algorithm to\nintegrate auxiliary problem loss with the primary problem loss in ATL-PINNs,\nwhich aims to enhance the effectiveness of the auxiliary-task learning modes.\nTo the best of our knowledge, this is the first study to introduce\nauxiliary-task learning modes in the context of physics-informed learning. We\nconduct experiments on three PDE problems across different fields and\nscenarios. Our findings demonstrate that the proposed auxiliary-task learning\nmodes can significantly improve solution accuracy, achieving a maximum\nperformance boost of 96.62% (averaging 28.23%) compared to the original\nsingle-task PINNs. The code and dataset are open source at\nhttps://github.com/junjun-yan/ATL-PINN.\n","authors":["Junjun Yan","Xinhai Chen","Zhichao Wang","Enqiang Zhou","Jie Liu"],"pdf_url":"https://arxiv.org/pdf/2307.06167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06162v1","updated":"2023-07-12T13:42:09Z","published":"2023-07-12T13:42:09Z","title":"Deep Generative Models for Physiological Signals: A Systematic\n  Literature Review","summary":"  In this paper, we present a systematic literature review on deep generative\nmodels for physiological signals, particularly electrocardiogram,\nelectroencephalogram, photoplethysmogram and electromyogram. Compared to the\nexisting review papers, we present the first review that summarizes the recent\nstate-of-the-art deep generative models. By analysing the state-of-the-art\nresearch related to deep generative models along with their main applications\nand challenges, this review contributes to the overall understanding of these\nmodels applied to physiological signals. Additionally, by highlighting the\nemployed evaluation protocol and the most used physiological databases, this\nreview facilitates the assessment and benchmarking of deep generative models.\n","authors":["Nour Neifar","Afef Mdhaffar","Achraf Ben-Hamadou","Mohamed Jmaiel"],"pdf_url":"https://arxiv.org/pdf/2307.06162v1.pdf","comment":"paper under review, 34 pages"},{"id":"http://arxiv.org/abs/2302.06375v2","updated":"2023-07-12T13:40:00Z","published":"2023-02-13T14:08:40Z","title":"One Transformer for All Time Series: Representing and Training with\n  Time-Dependent Heterogeneous Tabular Data","summary":"  There is a recent growing interest in applying Deep Learning techniques to\ntabular data, in order to replicate the success of other Artificial\nIntelligence areas in this structured domain. Specifically interesting is the\ncase in which tabular data have a time dependence, such as, for instance\nfinancial transactions. However, the heterogeneity of the tabular values, in\nwhich categorical elements are mixed with numerical items, makes this\nadaptation difficult. In this paper we propose a Transformer architecture to\nrepresent heterogeneous time-dependent tabular data, in which numerical\nfeatures are represented using a set of frequency functions and the whole\nnetwork is uniformly trained with a unique loss function.\n","authors":["Simone Luetto","Fabrizio Garuti","Enver Sangineto","Lorenzo Forni","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2302.06375v2.pdf","comment":"9 pages, 2 figures, 7 tables"},{"id":"http://arxiv.org/abs/2303.01388v2","updated":"2023-07-12T13:31:58Z","published":"2023-03-02T16:18:00Z","title":"Reinforced Labels: Multi-Agent Deep Reinforcement Learning for\n  Point-Feature Label Placement","summary":"  Over recent years, Reinforcement Learning combined with Deep Learning\ntechniques has successfully proven to solve complex problems in various\ndomains, including robotics, self-driving cars, and finance. In this paper, we\nare introducing Reinforcement Learning (RL) to label placement, a complex task\nin data visualization that seeks optimal positioning for labels to avoid\noverlap and ensure legibility. Our novel point-feature label placement method\nutilizes Multi-Agent Deep Reinforcement Learning (MADRL) to learn the label\nplacement strategy, which is the first machine-learning-driven labeling method\nin contrast to existing hand-crafted algorithms designed by human experts. To\nfacilitate RL learning, we developed an environment where an agent acts as a\nproxy for a label, a short textual annotation that augments visualization. Our\nresults show that the strategy trained by our method significantly outperforms\nthe random strategy of an untrained agent and compared methods designed by\nhuman experts in terms of completeness (i.e., the number of placed labels). The\ntrade-off is increased computation time, making the proposed method slower than\ncompared methods. Nevertheless, our method is ideal for scenarios where the\nlabeling can be computed in advance, and completeness is essential, such as\ncartographic maps, technical drawings, and medical atlases. Additionally, we\nconducted a user study to assess the perceived performance. The outcomes\nrevealed that the participants considered the proposed method to be\nsignificantly better than the other examined methods. This indicates that the\nimproved completeness is not just reflected in the quantitative metrics but\nalso in the subjective evaluation of the participants.\n","authors":["Petr Bobák","Ladislav Čmolík","Martin Čadík"],"pdf_url":"https://arxiv.org/pdf/2303.01388v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06152v1","updated":"2023-07-12T13:20:18Z","published":"2023-07-12T13:20:18Z","title":"Maneuver Decision-Making Through Automatic Curriculum Reinforcement\n  Learning Without Handcrafted Reward functions","summary":"  Maneuver decision-making is the core of unmanned combat aerial vehicle for\nautonomous air combat. To solve this problem, we propose an automatic\ncurriculum reinforcement learning method, which enables agents to learn\neffective decisions in air combat from scratch. The range of initial states are\nused for distinguishing curricula of different difficulty levels, thereby\nmaneuver decision is divided into a series of sub-tasks from easy to difficult,\nand test results are used to change sub-tasks. As sub-tasks change, agents\ngradually learn to complete a series of sub-tasks from easy to difficult,\nenabling them to make effective maneuvering decisions to cope with various\nstates without the need to spend effort designing reward functions. The\nablation studied show that the automatic curriculum learning proposed in this\narticle is an essential component for training through reinforcement learning,\nnamely, agents cannot complete effective decisions without curriculum learning.\nSimulation experiments show that, after training, agents are able to make\neffective decisions given different states, including tracking, attacking and\nescaping, which are both rational and interpretable.\n","authors":["Zhang Hong-Peng"],"pdf_url":"https://arxiv.org/pdf/2307.06152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14961v2","updated":"2023-07-12T13:17:11Z","published":"2023-05-24T09:56:20Z","title":"Deep Learning for Survival Analysis: A Review","summary":"  The influx of deep learning (DL) techniques into the field of survival\nanalysis in recent years has led to substantial methodological progress; for\ninstance, learning from unstructured or high-dimensional data such as images,\ntext or omics data. In this work, we conduct a comprehensive systematic review\nof DL-based methods for time-to-event analysis, characterizing them according\nto both survival- and DL-related attributes. In summary, the reviewed methods\noften address only a small subset of tasks relevant to time-to-event data -\ne.g., single-risk right-censored data - and neglect to incorporate more complex\nsettings. Our findings are summarized in an editable, open-source, interactive\ntable: https://survival-org.github.io/DL4Survival. As this research area is\nadvancing rapidly, we encourage community contribution in order to keep this\ndatabase up to date.\n","authors":["Simon Wiegrebe","Philipp Kopper","Raphael Sonabend","Bernd Bischl","Andreas Bender"],"pdf_url":"https://arxiv.org/pdf/2305.14961v2.pdf","comment":"17 pages, 7 figures, 2 tables, 1 interactive table"},{"id":"http://arxiv.org/abs/2307.06148v1","updated":"2023-07-12T13:10:08Z","published":"2023-07-12T13:10:08Z","title":"NetGPT: A Native-AI Network Architecture Beyond Provisioning\n  Personalized Generative Services","summary":"  Large language models (LLMs) have triggered tremendous success to empower\ndaily life by generative information, and the personalization of LLMs could\nfurther contribute to their applications due to better alignment with human\nintents. Towards personalized generative services, a collaborative cloud-edge\nmethodology sounds promising, as it facilitates the effective orchestration of\nheterogeneous distributed communication and computing resources. In this\narticle, after discussing the pros and cons of several candidate cloud-edge\ncollaboration techniques, we put forward NetGPT to capably deploy appropriate\nLLMs at the edge and the cloud in accordance with their computing capacity. In\naddition, edge LLMs could efficiently leverage location-based information for\npersonalized prompt completion, thus benefiting the interaction with cloud\nLLMs. After deploying representative open-source LLMs (e.g., GPT-2-base and\nLLaMA model) at the edge and the cloud, we present the feasibility of NetGPT on\nthe basis of low-rank adaptation-based light-weight fine-tuning. Subsequently,\nwe highlight substantial essential changes required for a native artificial\nintelligence (AI) network architecture towards NetGPT, with special emphasis on\ndeeper integration of communications and computing resources and careful\ncalibration of logical AI workflow. Furthermore, we demonstrate several\nby-product benefits of NetGPT, given edge LLM's astonishing capability to\npredict trends and infer intents, which possibly leads to a unified solution\nfor intelligent network management \\& orchestration. In a nutshell, we argue\nthat NetGPT is a promising native-AI network architecture beyond provisioning\npersonalized generative services.\n","authors":["Yuxuan Chen","Rongpeng Li","Zhifeng Zhao","Chenghui Peng","Jianjun Wu","Ekram Hossain","Honggang Zhang"],"pdf_url":"https://arxiv.org/pdf/2307.06148v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.03529v3","updated":"2023-07-12T13:07:37Z","published":"2022-12-07T09:22:33Z","title":"Towards Fleet-wide Sharing of Wind Turbine Condition Information through\n  Privacy-preserving Federated Learning","summary":"  Terabytes of data are collected by wind turbine manufacturers from their\nfleets every day. And yet, a lack of data access and sharing impedes exploiting\nthe full potential of the data. We present a distributed machine learning\napproach that preserves the data privacy by leaving the data on the wind\nturbines while still enabling fleet-wide learning on those local data. We show\nthat through federated fleet-wide learning, turbines with little or no\nrepresentative training data can benefit from more accurate normal behavior\nmodels. Customizing the global federated model to individual turbines yields\nthe highest fault detection accuracy in cases where the monitored target\nvariable is distributed heterogeneously across the fleet. We demonstrate this\nfor bearing temperatures, a target variable whose normal behavior can vary\nwidely depending on the turbine. We show that no turbine experiences a loss in\nmodel performance from participating in the federated learning process,\nresulting in superior performance of the federated learning strategy in our\ncase studies. The distributed learning increases the normal behavior model\ntraining times by about a factor of ten due to increased communication overhead\nand slower model convergence.\n","authors":["Lorin Jenkel","Stefan Jonas","Angela Meyer"],"pdf_url":"https://arxiv.org/pdf/2212.03529v3.pdf","comment":"Added: case study results for data from a different fleet;\n  distribution shift discussion; formatting and presentation changes. Original\n  results remain unchanged"},{"id":"http://arxiv.org/abs/2307.01719v2","updated":"2023-07-12T12:58:02Z","published":"2023-07-04T13:45:52Z","title":"MOPO-LSI: A User Guide","summary":"  MOPO-LSI is an open-source Multi-Objective Portfolio Optimization Library for\nSustainable Investments. This document provides a user guide for MOPO-LSI\nversion 1.0, including problem setup, workflow and the hyper-parameters in\nconfigurations.\n","authors":["Yong Zheng","Kumar Neelotpal Shukla","Jasmine Xu"," David"," Wang","Michael O'Leary"],"pdf_url":"https://arxiv.org/pdf/2307.01719v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06125v1","updated":"2023-07-12T12:25:33Z","published":"2023-07-12T12:25:33Z","title":"Learning Hierarchical Interactive Multi-Object Search for Mobile\n  Manipulation","summary":"  Existing object-search approaches enable robots to search through free\npathways, however, robots operating in unstructured human-centered environments\nfrequently also have to manipulate the environment to their needs. In this\nwork, we introduce a novel interactive multi-object search task in which a\nrobot has to open doors to navigate rooms and search inside cabinets and\ndrawers to find target objects. These new challenges require combining\nmanipulation and navigation skills in unexplored environments. We present\nHIMOS, a hierarchical reinforcement learning approach that learns to compose\nexploration, navigation, and manipulation skills. To achieve this, we design an\nabstract high-level action space around a semantic map memory and leverage the\nexplored environment as instance navigation points. We perform extensive\nexperiments in simulation and the real-world that demonstrate that HIMOS\neffectively transfers to new environments in a zero-shot manner. It shows\nrobustness to unseen subpolicies, failures in their execution, and different\nrobot kinematics. These capabilities open the door to a wide range of\ndownstream tasks across embodied AI and real-world use cases.\n","authors":["Fabian Schmalstieg","Daniel Honerkamp","Tim Welschehold","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2307.06125v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2307.06123v1","updated":"2023-07-12T12:23:47Z","published":"2023-07-12T12:23:47Z","title":"SoK: Comparing Different Membership Inference Attacks with a\n  Comprehensive Benchmark","summary":"  Membership inference (MI) attacks threaten user privacy through determining\nif a given data example has been used to train a target model. However, it has\nbeen increasingly recognized that the \"comparing different MI attacks\"\nmethodology used in the existing works has serious limitations. Due to these\nlimitations, we found (through the experiments in this work) that some\ncomparison results reported in the literature are quite misleading. In this\npaper, we seek to develop a comprehensive benchmark for comparing different MI\nattacks, called MIBench, which consists not only the evaluation metrics, but\nalso the evaluation scenarios. And we design the evaluation scenarios from four\nperspectives: the distance distribution of data samples in the target dataset,\nthe distance between data samples of the target dataset, the differential\ndistance between two datasets (i.e., the target dataset and a generated dataset\nwith only nonmembers), and the ratio of the samples that are made no inferences\nby an MI attack. The evaluation metrics consist of ten typical evaluation\nmetrics. We have identified three principles for the proposed \"comparing\ndifferent MI attacks\" methodology, and we have designed and implemented the\nMIBench benchmark with 84 evaluation scenarios for each dataset. In total, we\nhave used our benchmark to fairly and systematically compare 15\nstate-of-the-art MI attack algorithms across 588 evaluation scenarios, and\nthese evaluation scenarios cover 7 widely used datasets and 7 representative\ntypes of models. All codes and evaluations of MIBench are publicly available at\nhttps://github.com/MIBench/MIBench.github.io/blob/main/README.md.\n","authors":["Jun Niu","Xiaoyan Zhu","Moxuan Zeng","Ge Zhang","Qingyang Zhao","Chunhui Huang","Yangming Zhang","Suyu An","Yangzhong Wang","Xinghui Yue","Zhipeng He","Weihao Guo","Kuo Shen","Peng Liu","Yulong Shen","Xiaohong Jiang","Jianfeng Ma","Yuqing Zhang"],"pdf_url":"https://arxiv.org/pdf/2307.06123v1.pdf","comment":"21 pages,15 figures"},{"id":"http://arxiv.org/abs/2302.13948v2","updated":"2023-07-12T12:19:10Z","published":"2023-02-27T16:49:35Z","title":"Supervised topological data analysis for MALDI mass spectrometry imaging\n  applications","summary":"  Background: Matrix-assisted laser desorption/ionization mass spectrometry\nimaging (MALDI MSI) displays significant potential for applications in cancer\nresearch, especially in tumor typing and subtyping. Lung cancer is the primary\ncause of tumor-related deaths, where the most lethal entities are\nadenocarcinoma (ADC) and squamous cell carcinoma (SqCC). Distinguishing between\nthese two common subtypes is crucial for therapy decisions and successful\npatient management.\n  Results: We propose a new algebraic topological framework, which obtains\nintrinsic information from MALDI data and transforms it to reflect topological\npersistence. Our framework offers two main advantages. Firstly, topological\npersistence aids in distinguishing the signal from noise. Secondly, it\ncompresses the MALDI data, saving storage space and optimizes computational\ntime for subsequent classification tasks. We present an algorithm that\nefficiently implements our topological framework, relying on a single tuning\nparameter. Afterwards, logistic regression and random forest classifiers are\nemployed on the extracted persistence features, thereby accomplishing an\nautomated tumor (sub-)typing process. To demonstrate the competitiveness of our\nproposed framework, we conduct experiments on a real-world MALDI dataset using\ncross-validation. Furthermore, we showcase the effectiveness of the single\ndenoising parameter by evaluating its performance on synthetic MALDI images\nwith varying levels of noise.\n  Conclusion: Our empirical experiments demonstrate that the proposed algebraic\ntopological framework successfully captures and leverages the intrinsic\nspectral information from MALDI data, leading to competitive results in\nclassifying lung cancer subtypes. Moreover, the frameworks ability to be\nfine-tuned for denoising highlights its versatility and potential for enhancing\ndata analysis in MALDI applications.\n","authors":["Gideon Klaila","Vladimir Vutov","Anastasios Stefanou"],"pdf_url":"https://arxiv.org/pdf/2302.13948v2.pdf","comment":"22 pages, 10 figures"},{"id":"http://arxiv.org/abs/2307.03544v2","updated":"2023-07-12T12:15:32Z","published":"2023-07-07T12:20:56Z","title":"Roman Numeral Analysis with Graph Neural Networks: Onset-wise\n  Predictions from Note-wise Features","summary":"  Roman Numeral analysis is the important task of identifying chords and their\nfunctional context in pieces of tonal music. This paper presents a new approach\nto automatic Roman Numeral analysis in symbolic music. While existing\ntechniques rely on an intermediate lossy representation of the score, we\npropose a new method based on Graph Neural Networks (GNNs) that enable the\ndirect description and processing of each individual note in the score. The\nproposed architecture can leverage notewise features and interdependencies\nbetween notes but yield onset-wise representation by virtue of our novel edge\ncontraction algorithm. Our results demonstrate that ChordGNN outperforms\nexisting state-of-the-art models, achieving higher accuracy in Roman Numeral\nanalysis on the reference datasets. In addition, we investigate variants of our\nmodel using proposed techniques such as NADE, and post-processing of the chord\npredictions. The full source code for this work is available at\nhttps://github.com/manoskary/chordgnn\n","authors":["Emmanouil Karystinaios","Gerhard Widmer"],"pdf_url":"https://arxiv.org/pdf/2307.03544v2.pdf","comment":"In Proceedings of the 24th Conference of the International Society\n  for Music Information Retrieval (ISMIR 2023), Milan, Italy"},{"id":"http://arxiv.org/abs/2307.06104v1","updated":"2023-07-12T12:02:36Z","published":"2023-07-12T12:02:36Z","title":"Deep learning for dynamic graphs: models and benchmarks","summary":"  Recent progress in research on Deep Graph Networks (DGNs) has led to a\nmaturation of the domain of learning on graphs. Despite the growth of this\nresearch field, there are still important challenges that are yet unsolved.\nSpecifically, there is an urge of making DGNs suitable for predictive tasks on\nrealworld systems of interconnected entities, which evolve over time. With the\naim of fostering research in the domain of dynamic graphs, at first, we survey\nrecent advantages in learning both temporal and spatial information, providing\na comprehensive overview of the current state-of-the-art in the domain of\nrepresentation learning for dynamic graphs. Secondly, we conduct a fair\nperformance comparison among the most popular proposed approaches, leveraging\nrigorous model selection and assessment for all the methods, thus establishing\na sound baseline for evaluating new architectures and approaches\n","authors":["Alessio Gravina","Davide Bacciu"],"pdf_url":"https://arxiv.org/pdf/2307.06104v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.11873v2","updated":"2023-07-12T11:57:13Z","published":"2023-01-27T17:27:07Z","title":"A Deep Learning Method for Comparing Bayesian Hierarchical Models","summary":"  Bayesian model comparison (BMC) offers a principled approach for assessing\nthe relative merits of competing computational models and propagating\nuncertainty into model selection decisions. However, BMC is often intractable\nfor the popular class of hierarchical models due to their high-dimensional\nnested parameter structure. To address this intractability, we propose a deep\nlearning method for performing BMC on any set of hierarchical models which can\nbe instantiated as probabilistic programs. Since our method enables amortized\ninference, it allows efficient re-estimation of posterior model probabilities\nand fast performance validation prior to any real-data application. In a series\nof extensive validation studies, we benchmark the performance of our method\nagainst the state-of-the-art bridge sampling method and demonstrate excellent\namortized inference across all BMC settings. We then showcase our method by\ncomparing four hierarchical evidence accumulation models that have previously\nbeen deemed intractable for BMC due to partly implicit likelihoods. In this\napplication, we corroborate evidence for the recently proposed L\\'evy flight\nmodel of decision-making and show how transfer learning can be leveraged to\nenhance training efficiency. We provide reproducible code for all analyses and\nan open-source implementation of our method.\n","authors":["Lasse Elsemüller","Martin Schnuerch","Paul-Christian Bürkner","Stefan T. Radev"],"pdf_url":"https://arxiv.org/pdf/2301.11873v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.01241v2","updated":"2023-07-12T11:42:33Z","published":"2023-02-02T17:23:28Z","title":"Diagrammatization: Rationalizing with diagrammatic AI explanations for\n  abductive-deductive reasoning on hypotheses","summary":"  Many visualizations have been developed for explainable AI (XAI), but they\noften require further reasoning by users to interpret. We argue that XAI should\nsupport diagrammatic and abductive reasoning for the AI to perform hypothesis\ngeneration and evaluation to reduce the interpretability gap. We propose\nDiagrammatization to i) perform Peircean abductive-deductive reasoning, ii)\nfollow domain conventions, and iii) explain with diagrams visually or verbally.\nWe implemented DiagramNet for a clinical application to predict cardiac\ndiagnoses from heart auscultation, and explain with shape-based murmur\ndiagrams. In modeling studies, we found that DiagramNet not only provides\nfaithful murmur shape explanations, but also has better prediction performance\nthan baseline models. We further demonstrate the interpretability and\ntrustworthiness of diagrammatic explanations in a qualitative user study with\nmedical students, showing that clinically-relevant, diagrammatic explanations\nare preferred over technical saliency map explanations. This work contributes\ninsights into providing domain-conventional abductive explanations for\nuser-centric XAI.\n","authors":["Brian Y. Lim","Joseph P. Cahaly","Chester Y. F. Sng","Adam Chew"],"pdf_url":"https://arxiv.org/pdf/2302.01241v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06097v1","updated":"2023-07-12T11:38:34Z","published":"2023-07-12T11:38:34Z","title":"Learning Stochastic Dynamical Systems as an Implicit Regularization with\n  Graph Neural Networks","summary":"  Stochastic Gumbel graph networks are proposed to learn high-dimensional time\nseries, where the observed dimensions are often spatially correlated. To that\nend, the observed randomness and spatial-correlations are captured by learning\nthe drift and diffusion terms of the stochastic differential equation with a\nGumble matrix embedding, respectively. In particular, this novel framework\nenables us to investigate the implicit regularization effect of the noise terms\nin S-GGNs. We provide a theoretical guarantee for the proposed S-GGNs by\nderiving the difference between the two corresponding loss functions in a small\nneighborhood of weight. Then, we employ Kuramoto's model to generate data for\ncomparing the spectral density from the Hessian Matrix of the two loss\nfunctions. Experimental results on real-world data, demonstrate that S-GGNs\nexhibit superior convergence, robustness, and generalization, compared with\nstate-of-the-arts.\n","authors":["Jin Guo","Ting Gao","Yufu Lan","Peng Zhang","Sikun Yang","Jinqiao Duan"],"pdf_url":"https://arxiv.org/pdf/2307.06097v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2307.06093v1","updated":"2023-07-12T11:36:27Z","published":"2023-07-12T11:36:27Z","title":"Online Laplace Model Selection Revisited","summary":"  The Laplace approximation provides a closed-form model selection objective\nfor neural networks (NN). Online variants, which optimise NN parameters jointly\nwith hyperparameters, like weight decay strength, have seen renewed interest in\nthe Bayesian deep learning community. However, these methods violate Laplace's\nmethod's critical assumption that the approximation is performed around a mode\nof the loss, calling into question their soundness. This work re-derives online\nLaplace methods, showing them to target a variational bound on a mode-corrected\nvariant of the Laplace evidence which does not make stationarity assumptions.\nOnline Laplace and its mode-corrected counterpart share stationary points where\n1. the NN parameters are a maximum a posteriori, satisfying the Laplace\nmethod's assumption, and 2. the hyperparameters maximise the Laplace evidence,\nmotivating online methods. We demonstrate that these optima are roughly\nattained in practise by online algorithms using full-batch gradient descent on\nUCI regression datasets. The optimised hyperparameters prevent overfitting and\noutperform validation-based early stopping.\n","authors":["Jihao Andreas Lin","Javier Antorán","José Miguel Hernández-Lobato"],"pdf_url":"https://arxiv.org/pdf/2307.06093v1.pdf","comment":"Advances in Approximate Bayesian Inference 2023"},{"id":"http://arxiv.org/abs/2307.06092v1","updated":"2023-07-12T11:35:37Z","published":"2023-07-12T11:35:37Z","title":"Quantitative CLTs in Deep Neural Networks","summary":"  We study the distribution of a fully connected neural network with random\nGaussian weights and biases in which the hidden layer widths are proportional\nto a large constant $n$. Under mild assumptions on the non-linearity, we obtain\nquantitative bounds on normal approximations valid at large but finite $n$ and\nany fixed network depth. Our theorems show, both for the finite-dimensional\ndistributions and the entire process, that the distance between a random fully\nconnected network (and its derivatives) to the corresponding infinite width\nGaussian process scales like $n^{-\\gamma}$ for $\\gamma>0,$ with the exponent\ndepending on the metric used to measure discrepancy. Our bounds are stronger in\nterms of their dependence on network width than any previously available in the\nliterature.\n","authors":["Stefano Favaro","Boris Hanin","Domenico Marinucci","Ivan Nourdin","Giovanni Peccati"],"pdf_url":"https://arxiv.org/pdf/2307.06092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.13152v3","updated":"2023-07-12T11:11:05Z","published":"2023-02-25T20:36:41Z","title":"On Bellman's principle of optimality and Reinforcement learning for\n  safety-constrained Markov decision process","summary":"  We study optimality for the safety-constrained Markov decision process which\nis the underlying framework for safe reinforcement learning. Specifically, we\nconsider a constrained Markov decision process (with finite states and finite\nactions) where the goal of the decision maker is to reach a target set while\navoiding an unsafe set(s) with certain probabilistic guarantees. Therefore the\nunderlying Markov chain for any control policy will be multichain since by\ndefinition there exists a target set and an unsafe set. The decision maker also\nhas to be optimal (with respect to a cost function) while navigating to the\ntarget set. This gives rise to a multi-objective optimization problem. We\nhighlight the fact that Bellman's principle of optimality may not hold for\nconstrained Markov decision problems with an underlying multichain structure\n(as shown by the counterexample due to Haviv. We resolve the counterexample by\nformulating the aforementioned multi-objective optimization problem as a\nzero-sum game and thereafter construct an asynchronous value iteration scheme\nfor the Lagrangian (similar to Shapley's algorithm). Finally, we consider the\nreinforcement learning problem for the same and construct a modified\n$Q$-learning algorithm for learning the Lagrangian from data. We also provide a\nlower bound on the number of iterations required for learning the Lagrangian\nand corresponding error bounds.\n","authors":["Rahul Misra","Rafał Wisniewski","Carsten Skovmose Kallesøe"],"pdf_url":"https://arxiv.org/pdf/2302.13152v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06060v1","updated":"2023-07-12T10:22:28Z","published":"2023-07-12T10:22:28Z","title":"Interpreting deep embeddings for disease progression clustering","summary":"  We propose a novel approach for interpreting deep embeddings in the context\nof patient clustering. We evaluate our approach on a dataset of participants\nwith type 2 diabetes from the UK Biobank, and demonstrate clinically meaningful\ninsights into disease progression patterns.\n","authors":["Anna Munoz-Farre","Antonios Poulakakis-Daktylidis","Dilini Mahesha Kothalawala","Andrea Rodriguez-Martinez"],"pdf_url":"https://arxiv.org/pdf/2307.06060v1.pdf","comment":"Workshop on Interpretable ML in Healthcare at International\n  Conference on Machine Learning (ICML), Honolulu, Hawaii, USA. 2023"},{"id":"http://arxiv.org/abs/2307.06055v1","updated":"2023-07-12T10:17:54Z","published":"2023-07-12T10:17:54Z","title":"Function-Space Regularization for Deep Bayesian Classification","summary":"  Bayesian deep learning approaches assume model parameters to be latent random\nvariables and infer posterior distributions to quantify uncertainty, increase\nsafety and trust, and prevent overconfident and unpredictable behavior.\nHowever, weight-space priors are model-specific, can be difficult to interpret\nand are hard to specify. Instead, we apply a Dirichlet prior in predictive\nspace and perform approximate function-space variational inference. To this\nend, we interpret conventional categorical predictions from stochastic neural\nnetwork classifiers as samples from an implicit Dirichlet distribution. By\nadapting the inference, the same function-space prior can be combined with\ndifferent models without affecting model architecture or size. We illustrate\nthe flexibility and efficacy of such a prior with toy experiments and\ndemonstrate scalability, improved uncertainty quantification and adversarial\nrobustness with large-scale image classification experiments.\n","authors":["Jihao Andreas Lin","Joe Watson","Pascal Klink","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2307.06055v1.pdf","comment":"Advances in Approximate Bayesian Inference 2023"},{"id":"http://arxiv.org/abs/2010.02613v3","updated":"2023-07-12T10:00:51Z","published":"2020-10-06T10:46:27Z","title":"Deep Learning based Uncertainty Decomposition for Real-time Control","summary":"  Data-driven control in unknown environments requires a clear understanding of\nthe involved uncertainties for ensuring safety and efficient exploration. While\naleatoric uncertainty that arises from measurement noise can often be\nexplicitly modeled given a parametric description, it can be harder to model\nepistemic uncertainty, which describes the presence or absence of training\ndata. The latter can be particularly useful for implementing exploratory\ncontrol strategies when system dynamics are unknown. We propose a novel method\nfor detecting the absence of training data using deep learning, which gives a\ncontinuous valued scalar output between $0$ (indicating low uncertainty) and\n$1$ (indicating high uncertainty). We utilize this detector as a proxy for\nepistemic uncertainty and show its advantages over existing approaches on\nsynthetic and real-world datasets. Our approach can be directly combined with\naleatoric uncertainty estimates and allows for uncertainty estimation in\nreal-time as the inference is sample-free unlike existing approaches for\nuncertainty modeling. We further demonstrate the practicality of this\nuncertainty estimate in deploying online data-efficient control on a simulated\nquadcopter acted upon by an unknown disturbance model.\n","authors":["Neha Das","Jonas Umlauft","Armin Lederer","Thomas Beckers","Sandra Hirche"],"pdf_url":"https://arxiv.org/pdf/2010.02613v3.pdf","comment":"Accepted at IFAC World Congress 2023"},{"id":"http://arxiv.org/abs/2307.06048v1","updated":"2023-07-12T10:00:22Z","published":"2023-07-12T10:00:22Z","title":"Online Inventory Problems: Beyond the i.i.d. Setting with Online Convex\n  Optimization","summary":"  We study multi-product inventory control problems where a manager makes\nsequential replenishment decisions based on partial historical information in\norder to minimize its cumulative losses. Our motivation is to consider general\ndemands, losses and dynamics to go beyond standard models which usually rely on\nnewsvendor-type losses, fixed dynamics, and unrealistic i.i.d. demand\nassumptions. We propose MaxCOSD, an online algorithm that has provable\nguarantees even for problems with non-i.i.d. demands and stateful dynamics,\nincluding for instance perishability. We consider what we call non-degeneracy\nassumptions on the demand process, and argue that they are necessary to allow\nlearning.\n","authors":["Massil Hihat","Stéphane Gaïffas","Guillaume Garrigos","Simon Bussy"],"pdf_url":"https://arxiv.org/pdf/2307.06048v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06046v1","updated":"2023-07-12T09:49:15Z","published":"2023-07-12T09:49:15Z","title":"An OOD Multi-Task Perspective for Link Prediction with New Relation\n  Types and Nodes","summary":"  The task of inductive link prediction in (discrete) attributed multigraphs\ninfers missing attributed links (relations) between nodes in new test\nmultigraphs. Traditional relational learning methods face the challenge of\nlimited generalization to OOD test multigraphs containing both novel nodes and\nnovel relation types not seen in training. Recently, under the only assumption\nthat all relation types share the same structural predictive patterns (single\ntask), Gao et al. (2023) proposed an OOD link prediction method using the\ntheoretical concept of double exchangeability (for nodes & relation types), in\ncontrast to the (single) exchangeability (only for nodes) used to design Graph\nNeural Networks (GNNs). In this work we further extend the double\nexchangeability concept to multi-task double exchangeability, where we define\nlink prediction in attributed multigraphs that can have distinct and\npotentially conflicting predictive patterns for different sets of relation\ntypes (multiple tasks). Our empirical results on real-world datasets\ndemonstrate that our approach can effectively generalize to entirely new\nrelation types in test, without access to additional information, yielding\nsignificant performance improvements over existing methods.\n","authors":["Jincheng Zhou","Beatrice Bevilacqua","Bruno Ribeiro"],"pdf_url":"https://arxiv.org/pdf/2307.06046v1.pdf","comment":"23 pages, 3 figures"},{"id":"http://arxiv.org/abs/2307.06040v1","updated":"2023-07-12T09:35:16Z","published":"2023-07-12T09:35:16Z","title":"Rhythm Modeling for Voice Conversion","summary":"  Voice conversion aims to transform source speech into a different target\nvoice. However, typical voice conversion systems do not account for rhythm,\nwhich is an important factor in the perception of speaker identity. To bridge\nthis gap, we introduce Urhythmic-an unsupervised method for rhythm conversion\nthat does not require parallel data or text transcriptions. Using\nself-supervised representations, we first divide source audio into segments\napproximating sonorants, obstruents, and silences. Then we model rhythm by\nestimating speaking rate or the duration distribution of each segment type.\nFinally, we match the target speaking rate or rhythm by time-stretching the\nspeech segments. Experiments show that Urhythmic outperforms existing\nunsupervised methods in terms of quality and prosody. Code and checkpoints:\nhttps://github.com/bshall/urhythmic. Audio demo page:\nhttps://ubisoft-laforge.github.io/speech/urhythmic.\n","authors":["Benjamin van Niekerk","Marc-André Carbonneau","Herman Kamper"],"pdf_url":"https://arxiv.org/pdf/2307.06040v1.pdf","comment":"5 pages, 4 figures, 4 tables, submitted to IEEE Signal Processing\n  Letters"},{"id":"http://arxiv.org/abs/2102.00479v2","updated":"2023-07-12T09:33:34Z","published":"2021-01-31T16:17:56Z","title":"Fast Rates for the Regret of Offline Reinforcement Learning","summary":"  We study the regret of reinforcement learning from offline data generated by\na fixed behavior policy in an infinite-horizon discounted Markov decision\nprocess (MDP). While existing analyses of common approaches, such as fitted\n$Q$-iteration (FQI), suggest a $O(1/\\sqrt{n})$ convergence for regret,\nempirical behavior exhibits \\emph{much} faster convergence. In this paper, we\npresent a finer regret analysis that exactly characterizes this phenomenon by\nproviding fast rates for the regret convergence. First, we show that given any\nestimate for the optimal quality function $Q^*$, the regret of the policy it\ndefines converges at a rate given by the exponentiation of the $Q^*$-estimate's\npointwise convergence rate, thus speeding it up. The level of exponentiation\ndepends on the level of noise in the \\emph{decision-making} problem, rather\nthan the estimation problem. We establish such noise levels for linear and\ntabular MDPs as examples. Second, we provide new analyses of FQI and Bellman\nresidual minimization to establish the correct pointwise convergence\nguarantees. As specific cases, our results imply $O(1/n)$ regret rates in\nlinear cases and $\\exp(-\\Omega(n))$ regret rates in tabular cases. We extend\nour findings to general function approximation by extending our results to\nregret guarantees based on $L_p$-convergence rates for estimating $Q^*$ rather\nthan pointwise rates, where $L_2$ guarantees for nonparametric $Q^*$-estimation\ncan be ensured under mild conditions.\n","authors":["Yichun Hu","Nathan Kallus","Masatoshi Uehara"],"pdf_url":"https://arxiv.org/pdf/2102.00479v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.04026v3","updated":"2023-07-12T09:26:38Z","published":"2023-06-06T21:41:31Z","title":"Value Functions are Control Barrier Functions: Verification of Safe\n  Policies using Control Theory","summary":"  Guaranteeing safe behaviour of reinforcement learning (RL) policies poses\nsignificant challenges for safety-critical applications, despite RL's\ngenerality and scalability. To address this, we propose a new approach to apply\nverification methods from control theory to learned value functions. By\nanalyzing task structures for safety preservation, we formalize original\ntheorems that establish links between value functions and control barrier\nfunctions. Further, we propose novel metrics for verifying value functions in\nsafe control tasks and practical implementation details to improve learning.\nOur work presents a novel method for certificate learning, which unlocks a\ndiversity of verification techniques from control theory for RL policies, and\nmarks a significant step towards a formal framework for the general, scalable,\nand verifiable design of RL-based control systems. Code and videos are\navailable at this https url: https://rl-cbf.github.io/\n","authors":["Daniel C. H. Tan","Fernando Acero","Robert McCarthy","Dimitrios Kanoulas","Zhibin Li"],"pdf_url":"https://arxiv.org/pdf/2306.04026v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.04003v2","updated":"2023-07-12T09:24:34Z","published":"2023-05-06T10:36:39Z","title":"ANTONIO: Towards a Systematic Method of Generating NLP Benchmarks for\n  Verification","summary":"  Verification of machine learning models used in Natural Language Processing\n(NLP) is known to be a hard problem. In particular, many known neural network\nverification methods that work for computer vision and other numeric datasets\ndo not work for NLP. Here, we study technical reasons that underlie this\nproblem. Based on this analysis, we propose practical methods and heuristics\nfor preparing NLP datasets and models in a way that renders them amenable to\nknown verification methods based on abstract interpretation. We implement these\nmethods as a Python library called ANTONIO that links to the neural network\nverifiers ERAN and Marabou. We perform evaluation of the tool using an NLP\ndataset R-U-A-Robot suggested as a benchmark for verifying legally critical NLP\napplications. We hope that, thanks to its general applicability, this work will\nopen novel possibilities for including NLP verification problems into neural\nnetwork verification competitions, and will popularise NLP problems within this\ncommunity.\n","authors":["Marco Casadio","Luca Arnaboldi","Matthew L. Daggitt","Omri Isac","Tanvi Dinkar","Daniel Kienitz","Verena Rieser","Ekaterina Komendantskaya"],"pdf_url":"https://arxiv.org/pdf/2305.04003v2.pdf","comment":"To appear in proceedings of 6th Workshop on Formal Methods for\n  ML-Enabled Autonomous Systems (Affiliated with CAV 2023)"},{"id":"http://arxiv.org/abs/2306.07129v2","updated":"2023-07-12T09:16:47Z","published":"2023-06-12T14:07:53Z","title":"Collaborative Robotic Biopsy with Trajectory Guidance and Needle Tip\n  Force Feedback","summary":"  The diagnostic value of biopsies is highly dependent on the placement of\nneedles. Robotic trajectory guidance has been shown to improve needle\npositioning, but feedback for real-time navigation is limited. Haptic display\nof needle tip forces can provide rich feedback for needle navigation by\nenabling localization of tissue structures along the insertion path. We present\na collaborative robotic biopsy system that combines trajectory guidance with\nkinesthetic feedback to assist the physician in needle placement. The robot\naligns the needle while the insertion is performed in collaboration with a\nmedical expert who controls the needle position on site. We present a needle\ndesign that senses forces at the needle tip based on optical coherence\ntomography and machine learning for real-time data processing. Our robotic\nsetup allows operators to sense deep tissue interfaces independent of\nfrictional forces to improve needle placement relative to a desired target\nstructure. We first evaluate needle tip force sensing in ex-vivo tissue in a\nphantom study. We characterize the tip forces during insertions with constant\nvelocity and demonstrate the ability to detect tissue interfaces in a\ncollaborative user study. Participants are able to detect 91% of ex-vivo tissue\ninterfaces based on needle tip force feedback alone. Finally, we demonstrate\nthat even smaller, deep target structures can be accurately sampled by\nperforming post-mortem in situ biopsies of the pancreas.\n","authors":["Robin Mieling","Maximilian Neidhardt","Sarah Latus","Carolin Stapper","Stefan Gerlach","Inga Kniep","Axel Heinemann","Benjamin Ondruschka","Alexander Schlaefer"],"pdf_url":"https://arxiv.org/pdf/2306.07129v2.pdf","comment":"Presented at ICRA 2023"},{"id":"http://arxiv.org/abs/2307.01227v2","updated":"2023-07-12T09:14:53Z","published":"2023-07-03T04:47:42Z","title":"ESGCN: Edge Squeeze Attention Graph Convolutional Network for Traffic\n  Flow Forecasting","summary":"  Traffic forecasting is a highly challenging task owing to the dynamical\nspatio-temporal dependencies of traffic flows. To handle this, we focus on\nmodeling the spatio-temporal dynamics and propose a network termed Edge Squeeze\nGraph Convolutional Network (ESGCN) to forecast traffic flow in multiple\nregions. ESGCN consists of two modules: W-module and ES module. W-module is a\nfully node-wise convolutional network. It encodes the time-series of each\ntraffic region separately and decomposes the time-series at various scales to\ncapture fine and coarse features. The ES module models the spatio-temporal\ndynamics using Graph Convolutional Network (GCN) and generates an Adaptive\nAdjacency Matrix (AAM) with temporal features. To improve the accuracy of AAM,\nwe introduce three key concepts. 1) Using edge features to directly capture the\nspatiotemporal flow representation among regions. 2) Applying an edge attention\nmechanism to GCN to extract the AAM from the edge features. Here, the attention\nmechanism can effectively determine important spatio-temporal adjacency\nrelations. 3) Proposing a novel node contrastive loss to suppress obstructed\nconnections and emphasize related connections. Experimental results show that\nESGCN achieves state-of-the-art performance by a large margin on four\nreal-world datasets (PEMS03, 04, 07, and 08) with a low computational cost.\n","authors":["Sangrok Lee","Ha Young Kim"],"pdf_url":"https://arxiv.org/pdf/2307.01227v2.pdf","comment":"7 Pages, 3 figures"},{"id":"http://arxiv.org/abs/2307.06026v1","updated":"2023-07-12T09:14:35Z","published":"2023-07-12T09:14:35Z","title":"Learning from Exemplary Explanations","summary":"  eXplanation Based Learning (XBL) is a form of Interactive Machine Learning\n(IML) that provides a model refining approach via user feedback collected on\nmodel explanations. Although the interactivity of XBL promotes model\ntransparency, XBL requires a huge amount of user interaction and can become\nexpensive as feedback is in the form of detailed annotation rather than simple\ncategory labelling which is more common in IML. This expense is exacerbated in\nhigh stakes domains such as medical image classification. To reduce the effort\nand expense of XBL we introduce a new approach that uses two input instances\nand their corresponding Gradient Weighted Class Activation Mapping (GradCAM)\nmodel explanations as exemplary explanations to implement XBL. Using a medical\nimage classification task, we demonstrate that, using minimal human input, our\napproach produces improved explanations (+0.02, +3%) and achieves reduced\nclassification performance (-0.04, -4%) when compared against a model trained\nwithout interactions.\n","authors":["Misgina Tsighe Hagos","Kathleen M. Curran","Brian Mac Namee"],"pdf_url":"https://arxiv.org/pdf/2307.06026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06013v1","updated":"2023-07-12T08:51:20Z","published":"2023-07-12T08:51:20Z","title":"An Effective and Efficient Time-aware Entity Alignment Framework via\n  Two-aspect Three-view Label Propagation","summary":"  Entity alignment (EA) aims to find the equivalent entity pairs between\ndifferent knowledge graphs (KGs), which is crucial to promote knowledge fusion.\nWith the wide use of temporal knowledge graphs (TKGs), time-aware EA (TEA)\nmethods appear to enhance EA. Existing TEA models are based on Graph Neural\nNetworks (GNN) and achieve state-of-the-art (SOTA) performance, but it is\ndifficult to transfer them to large-scale TKGs due to the scalability issue of\nGNN. In this paper, we propose an effective and efficient non-neural EA\nframework between TKGs, namely LightTEA, which consists of four essential\ncomponents: (1) Two-aspect Three-view Label Propagation, (2) Sparse Similarity\nwith Temporal Constraints, (3) Sinkhorn Operator, and (4) Temporal Iterative\nLearning. All of these modules work together to improve the performance of EA\nwhile reducing the time consumption of the model. Extensive experiments on\npublic datasets indicate that our proposed model significantly outperforms the\nSOTA methods for EA between TKGs, and the time consumed by LightTEA is only\ndozens of seconds at most, no more than 10% of the most efficient TEA method.\n","authors":["Li Cai","Xin Mao","Youshao Xiao","Changxu Wu","Man Lan"],"pdf_url":"https://arxiv.org/pdf/2307.06013v1.pdf","comment":"Accepted by IJCAI 2023"},{"id":"http://arxiv.org/abs/2110.11337v4","updated":"2023-07-12T08:48:42Z","published":"2021-10-20T08:24:44Z","title":"Empowering General-purpose User Representation with Full-life Cycle\n  Behavior Modeling","summary":"  User Modeling plays an essential role in industry. In this field,\ntask-agnostic approaches, which generate general-purpose representation\napplicable to diverse downstream user cognition tasks, is a promising direction\nbeing more valuable and economical than task-specific representation learning.\nWith the rapid development of Internet service platforms, user behaviors have\nbeen accumulated continuously. However, existing general-purpose user\nrepresentation researches have little ability for full-life cycle modeling on\nextremely long behavior sequences since user registration. In this study, we\npropose a novel framework called full- Life cycle User Representation Model\n(LURM) to tackle this challenge. Specifically, LURM consists of two cascaded\nsub-models: (I) Bag-of-Interests (BoI) encodes user behaviors in any time\nperiod into a sparse vector with super-high dimension (e.g., 10^5); (II)\nSelf-supervised Multi-anchor Encoder Network (SMEN) maps sequences of BoI\nfeatures to multiple low-dimensional user representations. Specially, SMEN\nachieves almost lossless dimensionality reduction, benefiting from a novel\nmulti-anchor module which can learn different aspects of user interests.\nExperiments on several benchmark datasets show that our approach outperforms\nstate-of-the-art general-purpose representation methods.\n","authors":["Bei Yang","Jie Gu","Ke Liu","Xiaoxiao Xu","Renjun Xu","Qinghui Sun","Hong Liu"],"pdf_url":"https://arxiv.org/pdf/2110.11337v4.pdf","comment":"Accepted by KDD 2023"},{"id":"http://arxiv.org/abs/2305.06802v2","updated":"2023-07-12T08:38:52Z","published":"2023-05-11T13:51:21Z","title":"Physics-Informed Neural Networks for Discovering Localised Eigenstates\n  in Disordered Media","summary":"  The Schr\\\"{o}dinger equation with random potentials is a fundamental model\nfor understanding the behaviour of particles in disordered systems. Disordered\nmedia are characterised by complex potentials that lead to the localisation of\nwavefunctions, also called Anderson localisation. These wavefunctions may have\nsimilar scales of eigenenergies which poses difficulty in their discovery. It\nhas been a longstanding challenge due to the high computational cost and\ncomplexity of solving the Schr\\\"{o}dinger equation. Recently, machine-learning\ntools have been adopted to tackle these challenges. In this paper, based upon\nrecent advances in machine learning, we present a novel approach for\ndiscovering localised eigenstates in disordered media using physics-informed\nneural networks (PINNs). We focus on the spectral approximation of Hamiltonians\nin one dimension with potentials that are randomly generated according to the\nBernoulli, normal, and uniform distributions. We introduce a novel feature to\nthe loss function that exploits known physical phenomena occurring in these\nregions to scan across the domain and successfully discover these eigenstates,\nregardless of the similarity of their eigenenergies. We present various\nexamples to demonstrate the performance of the proposed approach and compare it\nwith isogeometric analysis.\n","authors":["Liam Harcombe","Quanling Deng"],"pdf_url":"https://arxiv.org/pdf/2305.06802v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.06006v1","updated":"2023-07-12T08:35:24Z","published":"2023-07-12T08:35:24Z","title":"What Happens During Finetuning of Vision Transformers: An Invariance\n  Based Investigation","summary":"  The pretrain-finetune paradigm usually improves downstream performance over\ntraining a model from scratch on the same task, becoming commonplace across\nmany areas of machine learning. While pretraining is empirically observed to be\nbeneficial for a range of tasks, there is not a clear understanding yet of the\nreasons for this effect. In this work, we examine the relationship between\npretrained vision transformers and the corresponding finetuned versions on\nseveral benchmark datasets and tasks. We present new metrics that specifically\ninvestigate the degree to which invariances learned by a pretrained model are\nretained or forgotten during finetuning. Using these metrics, we present a\nsuite of empirical findings, including that pretraining induces transferable\ninvariances in shallow layers and that invariances from deeper pretrained\nlayers are compressed towards shallower layers during finetuning. Together,\nthese findings contribute to understanding some of the reasons for the\nsuccesses of pretrained models and the changes that a pretrained model\nundergoes when finetuned on a downstream task.\n","authors":["Gabriele Merlin","Vedant Nanda","Ruchit Rawal","Mariya Toneva"],"pdf_url":"https://arxiv.org/pdf/2307.06006v1.pdf","comment":"Accepted to CoLLAs 2023"},{"id":"http://arxiv.org/abs/2307.06005v1","updated":"2023-07-12T08:33:16Z","published":"2023-07-12T08:33:16Z","title":"DDNAS: Discretized Differentiable Neural Architecture Search for Text\n  Classification","summary":"  Neural Architecture Search (NAS) has shown promising capability in learning\ntext representation. However, existing text-based NAS neither performs a\nlearnable fusion of neural operations to optimize the architecture, nor encodes\nthe latent hierarchical categorization behind text input. This paper presents a\nnovel NAS method, Discretized Differentiable Neural Architecture Search\n(DDNAS), for text representation learning and classification. With the\ncontinuous relaxation of architecture representation, DDNAS can use gradient\ndescent to optimize the search. We also propose a novel discretization layer\nvia mutual information maximization, which is imposed on every search node to\nmodel the latent hierarchical categorization in text representation. Extensive\nexperiments conducted on eight diverse real datasets exhibit that DDNAS can\nconsistently outperform the state-of-the-art NAS methods. While DDNAS relies on\nonly three basic operations, i.e., convolution, pooling, and none, to be the\ncandidates of NAS building blocks, its promising performance is noticeable and\nextensible to obtain further improvement by adding more different operations.\n","authors":["Kuan-Chun Chen","Cheng-Te Li","Kuo-Jung Lee"],"pdf_url":"https://arxiv.org/pdf/2307.06005v1.pdf","comment":"ACM Trans. Intell. Syst. Technol. (TIST) 2023"},{"id":"http://arxiv.org/abs/2203.13739v4","updated":"2023-07-12T08:31:17Z","published":"2022-03-25T16:12:45Z","title":"High Dimensional Quantum Machine Learning With Small Quantum Computers","summary":"  Quantum computers hold great promise to enhance machine learning, but their\ncurrent qubit counts restrict the realisation of this promise. In an attempt to\nplacate this limitation techniques can be applied for evaluating a quantum\ncircuit using a machine with fewer qubits than the circuit naively requires.\nThese techniques work by evaluating many smaller circuits on the smaller\nmachine, that are then combined in a polynomial to replicate the output of the\nlarger machine. This scheme requires more circuit evaluations than are\npractical for general circuits. However, we investigate the possibility that\nfor certain applications many of these subcircuits are superfluous, and that a\nmuch smaller sum is sufficient to estimate the full circuit. We construct a\nmachine learning model that may be capable of approximating the outputs of the\nlarger circuit with much fewer circuit evaluations. We successfully apply our\nmodel to the task of digit recognition, using simulated quantum computers much\nsmaller than the data dimension. The model is also applied to the task of\napproximating a random 10 qubit PQC with simulated access to a 5 qubit\ncomputer, even with only relatively modest number of circuits our model\nprovides an accurate approximation of the 10 qubit PQCs output, superior to a\nneural network attempt. The developed method might be useful for implementing\nquantum models on larger data throughout the NISQ era.\n","authors":["Simon C. Marshall","Casper Gyurik","Vedran Dunjko"],"pdf_url":"https://arxiv.org/pdf/2203.13739v4.pdf","comment":"13 pages, 7 figures"},{"id":"http://arxiv.org/abs/2306.15951v2","updated":"2023-07-12T08:18:30Z","published":"2023-06-28T06:21:22Z","title":"Reduce Computational Complexity for Convolutional Layers by Skipping\n  Zeros","summary":"  Deep neural networks rely on parallel processors for acceleration. To design\noperators for them, it requires not only good algorithm to reduce complexity,\nbut also sufficient utilization of hardwares. Convolutional layers mainly\ncontain 3 kinds of operators: convolution in forward propagation, deconvolution\nand dilated-convolution in backward propagation. When executing these\noperators, 0s are always added to tensors, causing redundant calculations. This\npaper gives C-K-S algorithm (ConvV2, KS-deconv, Sk-dilated), which skips these\n0s in two ways: trim the filters to exclude padded 0s; transform sparse tensors\nto dense tensors, to avoid inserted 0s in deconvolution and\ndilated-convolution. In contrast to regular convolution, deconvolution is hard\nto accelerate due to its complicacy. This paper provides high-performance GPU\nimplementations of C-K-S, and verifies their effectiveness with comparison to\nPyTorch. According to the experiments, C-K-S has advantages over PyTorch in\ncertain cases, especially in deconvolution on small feature-maps. Further\nenhancement of C-K-S can be done by making full optimizations oriented at\nspecific GPU architectures.\n","authors":["Zhiyi Zhang","Pengfei Zhang","Zhuopin Xu","Qi Wang"],"pdf_url":"https://arxiv.org/pdf/2306.15951v2.pdf","comment":"To download the code of Dragon-Alpha and experimental datas, please\n  go to https://github.com/GilgameshXYZ123/Dragon-Alpha"},{"id":"http://arxiv.org/abs/2101.10870v2","updated":"2023-07-12T08:11:19Z","published":"2021-01-23T12:42:41Z","title":"B-HAR: an open-source baseline framework for in depth study of human\n  activity recognition datasets and workflows","summary":"  Human Activity Recognition (HAR), based on machine and deep learning\nalgorithms is considered one of the most promising technologies to monitor\nprofessional and daily life activities for different categories of people\n(e.g., athletes, elderly, kids, employers) in order to provide a variety of\nservices related, for example to well-being, empowering of technical\nperformances, prevention of risky situation, and educational purposes. However,\nthe analysis of the effectiveness and the efficiency of HAR methodologies\nsuffers from the lack of a standard workflow, which might represent the\nbaseline for the estimation of the quality of the developed pattern recognition\nmodels. This makes the comparison among different approaches a challenging\ntask. In addition, researchers can make mistakes that, when not detected,\ndefinitely affect the achieved results. To mitigate such issues, this paper\nproposes an open-source automatic and highly configurable framework, named\nB-HAR, for the definition, standardization, and development of a baseline\nframework in order to evaluate and compare HAR methodologies. It implements the\nmost popular data processing methods for data preparation and the most commonly\nused machine and deep learning pattern recognition models.\n","authors":["Florenc Demrozi","Cristian Turetta","Graziano Pravadelli"],"pdf_url":"https://arxiv.org/pdf/2101.10870v2.pdf","comment":"9 Pages, 3 Figures, 3 Tables, Link to B-HAR Library:\n  https://github.com/B-HAR-HumanActivityRecognition/B-HAR"},{"id":"http://arxiv.org/abs/2306.11380v3","updated":"2023-07-12T08:07:49Z","published":"2023-06-20T08:38:31Z","title":"A Bayesian Take on Gaussian Process Networks","summary":"  Gaussian Process Networks (GPNs) are a class of directed graphical models\nwhich employ Gaussian processes as priors for the conditional expectation of\neach variable given its parents in the network. The model allows describing\ncontinuous joint distributions in a compact but flexible manner with minimal\nparametric assumptions on the dependencies between variables. Bayesian\nstructure learning of GPNs requires computing the posterior over graphs of the\nnetwork and is computationally infeasible even in low dimensions. This work\nimplements Monte Carlo and Markov Chain Monte Carlo methods to sample from the\nposterior distribution of network structures. As such, the approach follows the\nBayesian paradigm, comparing models via their marginal likelihood and computing\nthe posterior probability of the GPN features. Simulation studies show that our\nmethod outperforms state-of-the-art algorithms in recovering the graphical\nstructure of the network and provides an accurate approximation of its\nposterior distribution.\n","authors":["Enrico Giudice","Jack Kuipers","Giusi Moffa"],"pdf_url":"https://arxiv.org/pdf/2306.11380v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05988v1","updated":"2023-07-12T08:04:29Z","published":"2023-07-12T08:04:29Z","title":"A Comprehensive Review of Automated Data Annotation Techniques in Human\n  Activity Recognition","summary":"  Human Activity Recognition (HAR) has become one of the leading research\ntopics of the last decade. As sensing technologies have matured and their\neconomic costs have declined, a host of novel applications, e.g., in\nhealthcare, industry, sports, and daily life activities have become popular.\nThe design of HAR systems requires different time-consuming processing steps,\nsuch as data collection, annotation, and model training and optimization. In\nparticular, data annotation represents the most labor-intensive and cumbersome\nstep in HAR, since it requires extensive and detailed manual work from human\nannotators. Therefore, different methodologies concerning the automation of the\nannotation procedure in HAR have been proposed. The annotation problem occurs\nin different notions and scenarios, which all require individual solutions. In\nthis paper, we provide the first systematic review on data annotation\ntechniques for HAR. By grouping existing approaches into classes and providing\na taxonomy, our goal is to support the decision on which techniques can be\nbeneficially used in a given scenario.\n","authors":["Florenc Demrozi","Cristian Turetta","Fadi Al Machot","Graziano Pravadelli","Philipp H. Kindt"],"pdf_url":"https://arxiv.org/pdf/2307.05988v1.pdf","comment":"37 pages, 5 figures, 20 tables"},{"id":"http://arxiv.org/abs/2307.05979v1","updated":"2023-07-12T07:51:12Z","published":"2023-07-12T07:51:12Z","title":"Transformers in Reinforcement Learning: A Survey","summary":"  Transformers have significantly impacted domains like natural language\nprocessing, computer vision, and robotics, where they improve performance\ncompared to other neural networks. This survey explores how transformers are\nused in reinforcement learning (RL), where they are seen as a promising\nsolution for addressing challenges such as unstable training, credit\nassignment, lack of interpretability, and partial observability. We begin by\nproviding a brief domain overview of RL, followed by a discussion on the\nchallenges of classical RL algorithms. Next, we delve into the properties of\nthe transformer and its variants and discuss the characteristics that make them\nwell-suited to address the challenges inherent in RL. We examine the\napplication of transformers to various aspects of RL, including representation\nlearning, transition and reward function modeling, and policy optimization. We\nalso discuss recent research that aims to enhance the interpretability and\nefficiency of transformers in RL, using visualization techniques and efficient\ntraining strategies. Often, the transformer architecture must be tailored to\nthe specific needs of a given application. We present a broad overview of how\ntransformers have been adapted for several applications, including robotics,\nmedicine, language modeling, cloud computing, and combinatorial optimization.\nWe conclude by discussing the limitations of using transformers in RL and\nassess their potential for catalyzing future breakthroughs in this field.\n","authors":["Pranav Agarwal","Aamer Abdul Rahman","Pierre-Luc St-Charles","Simon J. D. Prince","Samira Ebrahimi Kahou"],"pdf_url":"https://arxiv.org/pdf/2307.05979v1.pdf","comment":"35 pages, 11 figures"},{"id":"http://arxiv.org/abs/2307.05977v1","updated":"2023-07-12T07:48:29Z","published":"2023-07-12T07:48:29Z","title":"Towards Safe Self-Distillation of Internet-Scale Text-to-Image Diffusion\n  Models","summary":"  Large-scale image generation models, with impressive quality made possible by\nthe vast amount of data available on the Internet, raise social concerns that\nthese models may generate harmful or copyrighted content. The biases and\nharmfulness arise throughout the entire training process and are hard to\ncompletely remove, which have become significant hurdles to the safe deployment\nof these models. In this paper, we propose a method called SDD to prevent\nproblematic content generation in text-to-image diffusion models. We\nself-distill the diffusion model to guide the noise estimate conditioned on the\ntarget removal concept to match the unconditional one. Compared to the previous\nmethods, our method eliminates a much greater proportion of harmful content\nfrom the generated images without degrading the overall image quality.\nFurthermore, our method allows the removal of multiple concepts at once,\nwhereas previous works are limited to removing a single concept at a time.\n","authors":["Sanghyun Kim","Seohyeon Jung","Balhae Kim","Moonseok Choi","Jinwoo Shin","Juho Lee"],"pdf_url":"https://arxiv.org/pdf/2307.05977v1.pdf","comment":"17 pages, 13 figures, ICML 2023 Workshop on Challenges in Deployable\n  Generative AI"},{"id":"http://arxiv.org/abs/2307.05975v1","updated":"2023-07-12T07:44:13Z","published":"2023-07-12T07:44:13Z","title":"Outlier detection in regression: conic quadratic formulations","summary":"  In many applications, when building linear regression models, it is important\nto account for the presence of outliers, i.e., corrupted input data points.\nSuch problems can be formulated as mixed-integer optimization problems\ninvolving cubic terms, each given by the product of a binary variable and a\nquadratic term of the continuous variables. Existing approaches in the\nliterature, typically relying on the linearization of the cubic terms using\nbig-M constraints, suffer from weak relaxation and poor performance in\npractice. In this work we derive stronger second-order conic relaxations that\ndo not involve big-M constraints. Our computational experiments indicate that\nthe proposed formulations are several orders-of-magnitude faster than existing\nbig-M formulations in the literature for this problem.\n","authors":["Andrés Gómez","José Neto"],"pdf_url":"https://arxiv.org/pdf/2307.05975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05974v1","updated":"2023-07-12T07:42:52Z","published":"2023-07-12T07:42:52Z","title":"Contrastive Learning for Conversion Rate Prediction","summary":"  Conversion rate (CVR) prediction plays an important role in advertising\nsystems. Recently, supervised deep neural network-based models have shown\npromising performance in CVR prediction. However, they are data hungry and\nrequire an enormous amount of training data. In online advertising systems,\nalthough there are millions to billions of ads, users tend to click only a\nsmall set of them and to convert on an even smaller set. This data sparsity\nissue restricts the power of these deep models. In this paper, we propose the\nContrastive Learning for CVR prediction (CL4CVR) framework. It associates the\nsupervised CVR prediction task with a contrastive learning task, which can\nlearn better data representations exploiting abundant unlabeled data and\nimprove the CVR prediction performance. To tailor the contrastive learning task\nto the CVR prediction problem, we propose embedding masking (EM), rather than\nfeature masking, to create two views of augmented samples. We also propose a\nfalse negative elimination (FNE) component to eliminate samples with the same\nfeature as the anchor sample, to account for the natural property in user\nbehavior data. We further propose a supervised positive inclusion (SPI)\ncomponent to include additional positive samples for each anchor sample, in\norder to make full use of sparse but precious user conversion events.\nExperimental results on two real-world conversion datasets demonstrate the\nsuperior performance of CL4CVR. The source code is available at\nhttps://github.com/DongRuiHust/CL4CVR.\n","authors":["Wentao Ouyang","Rui Dong","Xiuwu Zhang","Chaofeng Guo","Jinmei Luo","Xiangzheng Liu","Yanlong Du"],"pdf_url":"https://arxiv.org/pdf/2307.05974v1.pdf","comment":"SIGIR 2023"},{"id":"http://arxiv.org/abs/2307.05973v1","updated":"2023-07-12T07:40:48Z","published":"2023-07-12T07:40:48Z","title":"VoxPoser: Composable 3D Value Maps for Robotic Manipulation with\n  Language Models","summary":"  Large language models (LLMs) are shown to possess a wealth of actionable\nknowledge that can be extracted for robot manipulation in the form of reasoning\nand planning. Despite the progress, most still rely on pre-defined motion\nprimitives to carry out the physical interactions with the environment, which\nremains a major bottleneck. In this work, we aim to synthesize robot\ntrajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a\nlarge variety of manipulation tasks given an open-set of instructions and an\nopen-set of objects. We achieve this by first observing that LLMs excel at\ninferring affordances and constraints given a free-form language instruction.\nMore importantly, by leveraging their code-writing capabilities, they can\ninteract with a visual-language model (VLM) to compose 3D value maps to ground\nthe knowledge into the observation space of the agent. The composed value maps\nare then used in a model-based planning framework to zero-shot synthesize\nclosed-loop robot trajectories with robustness to dynamic perturbations. We\nfurther demonstrate how the proposed framework can benefit from online\nexperiences by efficiently learning a dynamics model for scenes that involve\ncontact-rich interactions. We present a large-scale study of the proposed\nmethod in both simulated and real-robot environments, showcasing the ability to\nperform a large variety of everyday manipulation tasks specified in free-form\nnatural language. Project website: https://voxposer.github.io\n","authors":["Wenlong Huang","Chen Wang","Ruohan Zhang","Yunzhu Li","Jiajun Wu","Li Fei-Fei"],"pdf_url":"https://arxiv.org/pdf/2307.05973v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.10975v2","updated":"2023-07-12T07:39:28Z","published":"2023-02-21T20:23:56Z","title":"Improved uncertainty quantification for neural networks with Bayesian\n  last layer","summary":"  Uncertainty quantification is an essential task in machine learning - a task\nin which neural networks (NNs) have traditionally not excelled. This can be a\nlimitation for safety-critical applications, where uncertainty-aware methods\nlike Gaussian processes or Bayesian linear regression are often preferred.\nBayesian neural networks are an approach to address this limitation. They\nassume probability distributions for all parameters and yield distributed\npredictions. However, training and inference are typically intractable and\napproximations must be employed. A promising approximation is NNs with Bayesian\nlast layer (BLL). They assume distributed weights only in the last linear layer\nand yield a normally distributed prediction. NNs with BLL can be seen as a\nBayesian linear regression model with learned nonlinear features. To\napproximate the intractable Bayesian neural network, point estimates of the\ndistributed weights in all but the last layer should be obtained by maximizing\nthe marginal likelihood. This has previously been challenging, as the marginal\nlikelihood is expensive to evaluate in this setting and prohibits direct\ntraining through backpropagation. We present a reformulation of the\nlog-marginal likelihood of a NN with BLL which allows for efficient training\nusing backpropagation. Furthermore, we address the challenge of quantifying\nuncertainty for extrapolation points. We provide a metric to quantify the\ndegree of extrapolation and derive a method to improve the uncertainty\nquantification for these points. Our methods are derived for the multivariate\ncase and demonstrated in a simulation study, where we compare Bayesian linear\nregression applied to a previously trained neural network with our proposed\nalgorithm\n","authors":["Felix Fiedler","Sergio Lucia"],"pdf_url":"https://arxiv.org/pdf/2302.10975v2.pdf","comment":"10 pages, 4 figures, 1 table. This work has been submitted to the\n  IEEE for possible publication. Copyright may be transferred without notice,\n  after which this version may no longer be accessible"},{"id":"http://arxiv.org/abs/2307.05972v1","updated":"2023-07-12T07:38:24Z","published":"2023-07-12T07:38:24Z","title":"Self-Distilled Quantization: Achieving High Compression Rates in\n  Transformer-Based Language Models","summary":"  We investigate the effects of post-training quantization and\nquantization-aware training on the generalization of Transformer language\nmodels. We present a new method called self-distilled quantization (SDQ) that\nminimizes accumulative quantization errors and outperforms baselines. We apply\nSDQ to multilingual models XLM-R-Base and InfoXLM-Base and demonstrate that\nboth models can be reduced from 32-bit floating point weights to 8-bit integer\nweights while maintaining a high level of performance on the XGLUE benchmark.\nOur results also highlight the challenges of quantizing multilingual models,\nwhich must generalize to languages they were not fine-tuned on.\n","authors":["James O' Neill","Sourav Dutta"],"pdf_url":"https://arxiv.org/pdf/2307.05972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.00736v2","updated":"2023-07-12T07:27:45Z","published":"2022-10-03T06:54:33Z","title":"A large sample theory for infinitesimal gradient boosting","summary":"  Infinitesimal gradient boosting (Dombry and Duchamps, 2021) is defined as the\nvanishing-learning-rate limit of the popular tree-based gradient boosting\nalgorithm from machine learning. It is characterized as the solution of a\nnonlinear ordinary differential equation in a infinite-dimensional function\nspace where the infinitesimal boosting operator driving the dynamics depends on\nthe training sample. We consider the asymptotic behavior of the model in the\nlarge sample limit and prove its convergence to a deterministic process. This\npopulation limit is again characterized by a differential equation that depends\non the population distribution. We explore some properties of this population\nlimit: we prove that the dynamics makes the test error decrease and we consider\nits long time behavior.\n","authors":["Clement Dombry","Jean-Jil Duchamps"],"pdf_url":"https://arxiv.org/pdf/2210.00736v2.pdf","comment":"37 pages"},{"id":"http://arxiv.org/abs/2307.05959v1","updated":"2023-07-12T07:04:53Z","published":"2023-07-12T07:04:53Z","title":"Giving Robots a Hand: Learning Generalizable Manipulation with\n  Eye-in-Hand Human Video Demonstrations","summary":"  Eye-in-hand cameras have shown promise in enabling greater sample efficiency\nand generalization in vision-based robotic manipulation. However, for robotic\nimitation, it is still expensive to have a human teleoperator collect large\namounts of expert demonstrations with a real robot. Videos of humans performing\ntasks, on the other hand, are much cheaper to collect since they eliminate the\nneed for expertise in robotic teleoperation and can be quickly captured in a\nwide range of scenarios. Therefore, human video demonstrations are a promising\ndata source for learning generalizable robotic manipulation policies at scale.\nIn this work, we augment narrow robotic imitation datasets with broad unlabeled\nhuman video demonstrations to greatly enhance the generalization of eye-in-hand\nvisuomotor policies. Although a clear visual domain gap exists between human\nand robot data, our framework does not need to employ any explicit domain\nadaptation method, as we leverage the partial observability of eye-in-hand\ncameras as well as a simple fixed image masking scheme. On a suite of eight\nreal-world tasks involving both 3-DoF and 6-DoF robot arm control, our method\nimproves the success rates of eye-in-hand manipulation policies by 58%\n(absolute) on average, enabling robots to generalize to both new environment\nconfigurations and new tasks that are unseen in the robot demonstration data.\nSee video results at https://giving-robots-a-hand.github.io/ .\n","authors":["Moo Jin Kim","Jiajun Wu","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2307.05959v1.pdf","comment":"21 pages, 7 figures, project webpage at\n  https://giving-robots-a-hand.github.io/"},{"id":"http://arxiv.org/abs/2307.05949v1","updated":"2023-07-12T06:31:43Z","published":"2023-07-12T06:31:43Z","title":"Newell's theory based feature transformations for spatio-temporal\n  traffic prediction","summary":"  Deep learning (DL) models for spatio-temporal traffic flow forecasting employ\nconvolutional or graph-convolutional filters along with recurrent neural\nnetworks to capture spatial and temporal dependencies in traffic data. These\nmodels, such as CNN-LSTM, utilize traffic flows from neighboring detector\nstations to predict flows at a specific location of interest. However, these\nmodels are limited in their ability to capture the broader dynamics of the\ntraffic system, as they primarily learn features specific to the detector\nconfiguration and traffic characteristics at the target location. Hence, the\ntransferability of these models to different locations becomes challenging,\nparticularly when data is unavailable at the new location for model training.\nTo address this limitation, we propose a traffic flow physics-based feature\ntransformation for spatio-temporal DL models. This transformation incorporates\nNewell's uncongested and congested-state estimators of traffic flows at the\ntarget locations, enabling the models to learn broader dynamics of the system.\nOur methodology is empirically validated using traffic data from two different\nlocations. The results demonstrate that the proposed feature transformation\nimproves the models' performance in predicting traffic flows over different\nprediction horizons, as indicated by better goodness-of-fit statistics. An\nimportant advantage of our framework is its ability to be transferred to new\nlocations where data is unavailable. This is achieved by appropriately\naccounting for spatial dependencies based on station distances and various\ntraffic parameters. In contrast, regular DL models are not easily transferable\nas their inputs remain fixed. It should be noted that due to data limitations,\nwe were unable to perform spatial sensitivity analysis, which calls for further\nresearch using simulated data.\n","authors":["Agnimitra Sengupta","S. Ilgin Guler"],"pdf_url":"https://arxiv.org/pdf/2307.05949v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05948v1","updated":"2023-07-12T06:29:02Z","published":"2023-07-12T06:29:02Z","title":"Diversity-enhancing Generative Network for Few-shot Hypothesis\n  Adaptation","summary":"  Generating unlabeled data has been recently shown to help address the\nfew-shot hypothesis adaptation (FHA) problem, where we aim to train a\nclassifier for the target domain with a few labeled target-domain data and a\nwell-trained source-domain classifier (i.e., a source hypothesis), for the\nadditional information of the highly-compatible unlabeled data. However, the\ngenerated data of the existing methods are extremely similar or even the same.\nThe strong dependency among the generated data will lead the learning to fail.\nIn this paper, we propose a diversity-enhancing generative network (DEG-Net)\nfor the FHA problem, which can generate diverse unlabeled data with the help of\na kernel independence measure: the Hilbert-Schmidt independence criterion\n(HSIC). Specifically, DEG-Net will generate data via minimizing the HSIC value\n(i.e., maximizing the independence) among the semantic features of the\ngenerated data. By DEG-Net, the generated unlabeled data are more diverse and\nmore effective for addressing the FHA problem. Experimental results show that\nthe DEG-Net outperforms existing FHA baselines and further verifies that\ngenerating diverse data plays a vital role in addressing the FHA problem\n","authors":["Ruijiang Dong","Feng Liu","Haoang Chi","Tongliang Liu","Mingming Gong","Gang Niu","Masashi Sugiyama","Bo Han"],"pdf_url":"https://arxiv.org/pdf/2307.05948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05946v1","updated":"2023-07-12T06:23:31Z","published":"2023-07-12T06:23:31Z","title":"A Bayesian approach to quantifying uncertainties and improving\n  generalizability in traffic prediction models","summary":"  Deep-learning models for traffic data prediction can have superior\nperformance in modeling complex functions using a multi-layer architecture.\nHowever, a major drawback of these approaches is that most of these approaches\ndo not offer forecasts with uncertainty estimates, which are essential for\ntraffic operations and control. Without uncertainty estimates, it is difficult\nto place any level of trust to the model predictions, and operational\nstrategies relying on overconfident predictions can lead to worsening traffic\nconditions. In this study, we propose a Bayesian recurrent neural network\nframework for uncertainty quantification in traffic prediction with higher\ngeneralizability by introducing spectral normalization to its hidden layers. In\nour paper, we have shown that normalization alters the training process of deep\nneural networks by controlling the model's complexity and reducing the risk of\noverfitting to the training data. This, in turn, helps improve the\ngeneralization performance of the model on out-of-distribution datasets.\nResults demonstrate that spectral normalization improves uncertainty estimates\nand significantly outperforms both the layer normalization and model without\nnormalization in single-step prediction horizons. This improved performance can\nbe attributed to the ability of spectral normalization to better localize the\nfeature space of the data under perturbations. Our findings are especially\nrelevant to traffic management applications, where predicting traffic\nconditions across multiple locations is the goal, but the availability of\ntraining data from multiple locations is limited. Spectral normalization,\ntherefore, provides a more generalizable approach that can effectively capture\nthe underlying patterns in traffic data without requiring location-specific\nmodels.\n","authors":["Agnimitra Sengupta","Sudeepta Mondal","Adway Das","S. Ilgin Guler"],"pdf_url":"https://arxiv.org/pdf/2307.05946v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05945v1","updated":"2023-07-12T06:22:51Z","published":"2023-07-12T06:22:51Z","title":"YOGA: Deep Object Detection in the Wild with Lightweight Feature\n  Learning and Multiscale Attention","summary":"  We introduce YOGA, a deep learning based yet lightweight object detection\nmodel that can operate on low-end edge devices while still achieving\ncompetitive accuracy. The YOGA architecture consists of a two-phase feature\nlearning pipeline with a cheap linear transformation, which learns feature maps\nusing only half of the convolution filters required by conventional\nconvolutional neural networks. In addition, it performs multi-scale feature\nfusion in its neck using an attention mechanism instead of the naive\nconcatenation used by conventional detectors. YOGA is a flexible model that can\nbe easily scaled up or down by several orders of magnitude to fit a broad range\nof hardware constraints. We evaluate YOGA on COCO-val and COCO-testdev datasets\nwith other over 10 state-of-the-art object detectors. The results show that\nYOGA strikes the best trade-off between model size and accuracy (up to 22%\nincrease of AP and 23-34% reduction of parameters and FLOPs), making it an\nideal choice for deployment in the wild on low-end edge devices. This is\nfurther affirmed by our hardware implementation and evaluation on NVIDIA Jetson\nNano.\n","authors":["Raja Sunkara","Tie Luo"],"pdf_url":"https://arxiv.org/pdf/2307.05945v1.pdf","comment":"Published in Pattern Recognition (Elsevier), July 2023"},{"id":"http://arxiv.org/abs/2206.05200v2","updated":"2023-07-12T05:57:42Z","published":"2022-06-10T15:57:23Z","title":"Dynamic mean field programming","summary":"  A dynamic mean field theory is developed for finite state and action Bayesian\nreinforcement learning in the large state space limit. In an analogy with\nstatistical physics, the Bellman equation is studied as a disordered dynamical\nsystem; the Markov decision process transition probabilities are interpreted as\ncouplings and the value functions as deterministic spins that evolve\ndynamically. Thus, the mean-rewards and transition probabilities are considered\nto be quenched random variables. The theory reveals that, under certain\nassumptions, the state-action values are statistically independent across\nstate-action pairs in the asymptotic state space limit, and provides the form\nof the distribution exactly. The results hold in the finite and discounted\ninfinite horizon settings, for both value iteration and policy evaluation. The\nstate-action value statistics can be computed from a set of mean field\nequations, which we call dynamic mean field programming (DMFP). For policy\nevaluation the equations are exact. For value iteration, approximate equations\nare obtained by appealing to extreme value theory or bounds. The result\nprovides analytic insight into the statistical structure of tabular\nreinforcement learning, for example revealing the conditions under which\nreinforcement learning is equivalent to a set of independent multi-armed bandit\nproblems.\n","authors":["George Stamatescu"],"pdf_url":"https://arxiv.org/pdf/2206.05200v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05926v1","updated":"2023-07-12T05:46:37Z","published":"2023-07-12T05:46:37Z","title":"Filling time-series gaps using image techniques: Multidimensional\n  context autoencoder approach for building energy data imputation","summary":"  Building energy prediction and management has become increasingly important\nin recent decades, driven by the growth of Internet of Things (IoT) devices and\nthe availability of more energy data. However, energy data is often collected\nfrom multiple sources and can be incomplete or inconsistent, which can hinder\naccurate predictions and management of energy systems and limit the usefulness\nof the data for decision-making and research. To address this issue, past\nstudies have focused on imputing missing gaps in energy data, including random\nand continuous gaps. One of the main challenges in this area is the lack of\nvalidation on a benchmark dataset with various building and meter types, making\nit difficult to accurately evaluate the performance of different imputation\nmethods. Another challenge is the lack of application of state-of-the-art\nimputation methods for missing gaps in energy data. Contemporary\nimage-inpainting methods, such as Partial Convolution (PConv), have been widely\nused in the computer vision domain and have demonstrated their effectiveness in\ndealing with complex missing patterns. To study whether energy data imputation\ncan benefit from the image-based deep learning method, this study compared\nPConv, Convolutional neural networks (CNNs), and weekly persistence method\nusing one of the biggest publicly available whole building energy datasets,\nconsisting of 1479 power meters worldwide, as the benchmark. The results show\nthat, compared to the CNN with the raw time series (1D-CNN) and the weekly\npersistence method, neural network models with reshaped energy data with two\ndimensions reduced the Mean Squared Error (MSE) by 10% to 30%. The advanced\ndeep learning method, Partial convolution (PConv), has further reduced the MSE\nby 20-30% than 2D-CNN and stands out among all models.\n","authors":["Chun Fu","Matias Quintana","Zoltan Nagy","Clayton Miller"],"pdf_url":"https://arxiv.org/pdf/2307.05926v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05920v1","updated":"2023-07-12T05:19:10Z","published":"2023-07-12T05:19:10Z","title":"Unified Medical Image-Text-Label Contrastive Learning With Continuous\n  Prompt","summary":"  Contrastive language-image Pre-training (CLIP) [13] can leverage large\ndatasets of unlabeled Image-Text pairs, which have demonstrated impressive\nperformance in various downstream tasks. Given that annotating medical data is\ntime-consuming and laborious, Image-Text Pre-training has promising\napplications in exploiting large-scale medical image and radiology report\ndatasets. However, medical Image-Text Pre-training faces several challenges, as\nfollows: (1) Due to privacy concerns, the amount of available medical data is\nrelatively small compared to natural data, leading to weaker generalization\nability of the model. (2) Medical images are highly similar with only\nfine-grained differences in subtleties, resulting in a large number of\nfalse-negative sample pairs in comparison learning. (3) The hand-crafted Prompt\nusually differs from the natural medical image report, Subtle changes in\nwording can lead to significant differences in performance. In this paper, we\npropose a unified Image-Text-Label contrastive learning framework based on\ncontinuous prompts, with three main contributions. First, We unified the data\nof images, text, and labels, which greatly expanded the training data that the\nmodel could utilize. Second, we address the issue of data diversity and the\nimpact of hand-crafted prompts on model performance by introducing continuous\nimplicit prompts. Lastly, we propose a ImageText-Label contrastive Training to\nmitigate the problem of too many false-negative samples. We demonstrate through\nsufficient experiments that the Unified Medical Contrastive Learning (UMCL)\nframework exhibits excellent performance on several downstream tasks.\n","authors":["Yuhao Wang"],"pdf_url":"https://arxiv.org/pdf/2307.05920v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.04603v2","updated":"2023-07-12T05:18:51Z","published":"2023-07-07T09:01:42Z","title":"Solvent: A Framework for Protein Folding","summary":"  Consistency and reliability are crucial for conducting AI research. Many\nfamous research fields, such as object detection, have been compared and\nvalidated with solid benchmark frameworks. After AlphaFold2, the protein\nfolding task has entered a new phase, and many methods are proposed based on\nthe component of AlphaFold2. The importance of a unified research framework in\nprotein folding contains implementations and benchmarks to consistently and\nfairly compare various approaches. To achieve this, we present Solvent, an\nprotein folding framework that supports significant components of\nstate-of-th-arts models in the manner of off-the-shelf interface Solvent\ncontains different models implemented in a unified codebase and supports\ntraining and evaluation for defined models on the same dataset. We benchmark\nwell-known algorithms and their components and provide experiments that give\nhelpful insights into the protein structure modeling field. We hope that\nSolvent will increase the reliability and consistency of proposed models and\ngives efficiency in both speed and costs, resulting in acceleration on protein\nfolding modeling research. The code is available at\nhttps://github.com/kakaobrain/solvent, and the project will continue to be\ndeveloped.\n","authors":["Jaemyung Lee","Kyeongtak Han","Jaehoon Kim","Hasun Yu","Youhan Lee"],"pdf_url":"https://arxiv.org/pdf/2307.04603v2.pdf","comment":"preprint, 8pages"},{"id":"http://arxiv.org/abs/2307.05915v1","updated":"2023-07-12T04:44:31Z","published":"2023-07-12T04:44:31Z","title":"Prompt Generate Train (PGT): A framework for few-shot domain adaptation,\n  alignment, and uncertainty calibration of a retriever augmented generation\n  (RAG) model for domain specific open book question-answering","summary":"  We present a framework - Prompt, Generate, Train (PGT) - to efficiently\ndevelop a generative question-answering model for open-book question-answering\nover a proprietary collection of text documents. The framework adapts a\nretriever augmented generation model to the target domain using supervised\nfinetuning and reinforcement learning with synthetic feedback in a few-shot\nsetting. This yields an aligned, uncertainty calibrated model that is\ncompetitive with GPT-4 based in-context retrieval augmented generation in\ngenerating relevant answers at lower serving costs. The synthetic generation\npipeline generates high quality synthetic training data musing a medium sized\nLLM, Flan-T5 XXL, and a novel consistency filtering scheme. The pipeline is\ndesigned to generate both abstractive and extractive questions that span the\nentire corpus. Using samples from this dataset, the framework fine-tunes a\nsmaller RAG model comprising a dense retriever and a smaller sized LLM on\nsamples from the dataset. In parallel, the framework trains a Reward model to\nscore domain grounded answers higher than hallucinated answers. In the next\nphase, the framework aligns to the RAG model with the target domain using\nreinforcement learning. This step improves the RAG model's ability to generate\ngrounded answers and ignore out of domain questions. In the final phase, the\nframework calibrates the model uncertainty for extractive question-answers.\nThis is a desirable feature since the model can be integrated into a cascading\nsystem where the RAG model's answer is surfaced only when the model is\nconfident of its answer.\n","authors":["C. S. Krishna"],"pdf_url":"https://arxiv.org/pdf/2307.05915v1.pdf","comment":"10"},{"id":"http://arxiv.org/abs/2307.05914v1","updated":"2023-07-12T04:43:59Z","published":"2023-07-12T04:43:59Z","title":"FIS-ONE: Floor Identification System with One Label for Crowdsourced RF\n  Signals","summary":"  Floor labels of crowdsourced RF signals are crucial for many smart-city\napplications, such as multi-floor indoor localization, geofencing, and robot\nsurveillance. To build a prediction model to identify the floor number of a new\nRF signal upon its measurement, conventional approaches using the crowdsourced\nRF signals assume that at least few labeled signal samples are available on\neach floor. In this work, we push the envelope further and demonstrate that it\nis technically feasible to enable such floor identification with only one\nfloor-labeled signal sample on the bottom floor while having the rest of signal\nsamples unlabeled.\n  We propose FIS-ONE, a novel floor identification system with only one labeled\nsample. FIS-ONE consists of two steps, namely signal clustering and cluster\nindexing. We first build a bipartite graph to model the RF signal samples and\nobtain a latent representation of each node (each signal sample) using our\nattention-based graph neural network model so that the RF signal samples can be\nclustered more accurately. Then, we tackle the problem of indexing the clusters\nwith proper floor labels, by leveraging the observation that signals from an\naccess point can be detected on different floors, i.e., signal spillover.\nSpecifically, we formulate a cluster indexing problem as a combinatorial\noptimization problem and show that it is equivalent to solving a traveling\nsalesman problem, whose (near-)optimal solution can be found efficiently. We\nhave implemented FIS-ONE and validated its effectiveness on the Microsoft\ndataset and in three large shopping malls. Our results show that FIS-ONE\noutperforms other baseline algorithms significantly, with up to 23% improvement\nin adjusted rand index and 25% improvement in normalized mutual information\nusing only one floor-labeled signal sample.\n","authors":["Weipeng Zhuo","Ka Ho Chiu","Jierun Chen","Ziqi Zhao","S. -H. Gary Chan","Sangtae Ha","Chul-Ho Lee"],"pdf_url":"https://arxiv.org/pdf/2307.05914v1.pdf","comment":"Accepted by IEEE ICDCS 2023"},{"id":"http://arxiv.org/abs/2307.05911v1","updated":"2023-07-12T04:38:44Z","published":"2023-07-12T04:38:44Z","title":"Grain and Grain Boundary Segmentation using Machine Learning with Real\n  and Generated Datasets","summary":"  We report significantly improved accuracy of grain boundary segmentation\nusing Convolutional Neural Networks (CNN) trained on a combination of real and\ngenerated data. Manual segmentation is accurate but time-consuming, and\nexisting computational methods are faster but often inaccurate. To combat this\ndilemma, machine learning models can be used to achieve the accuracy of manual\nsegmentation and have the efficiency of a computational method. An extensive\ndataset of from 316L stainless steel samples is additively manufactured,\nprepared, polished, etched, and then microstructure grain images were\nsystematically collected. Grain segmentation via existing computational methods\nand manual (by-hand) were conducted, to create \"real\" training data. A Voronoi\ntessellation pattern combined with random synthetic noise and simulated\ndefects, is developed to create a novel artificial grain image fabrication\nmethod. This provided training data supplementation for data-intensive machine\nlearning methods. The accuracy of the grain measurements from microstructure\nimages segmented via computational methods and machine learning methods\nproposed in this work are calculated and compared to provide much benchmarks in\ngrain segmentation. Over 400 images of the microstructure of stainless steel\nsamples were manually segmented for machine learning training applications.\nThis data and the artificial data is available on Kaggle.\n","authors":["Peter Warren","Nandhini Raju","Abhilash Prasad","Shajahan Hossain","Ramesh Subramanian","Jayanta Kapat","Navin Manjooran","Ranajay Ghosh"],"pdf_url":"https://arxiv.org/pdf/2307.05911v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.10590v4","updated":"2023-07-12T04:34:36Z","published":"2022-11-19T05:16:08Z","title":"Bidirectional Generation of Structure and Properties Through a Single\n  Molecular Foundation Model","summary":"  The recent success of large foundation models in artificial intelligence has\nprompted the emergence of chemical pre-trained models. Despite the growing\ninterest in large molecular pre-trained models that provide informative\nrepresentations for downstream tasks, attempts for multimodal pre-training\napproaches on the molecule domain were limited. To address this, we present a\nnovel multimodal molecular pre-trained model that incorporates the modalities\nof structure and biochemical properties, drawing inspiration from recent\nadvances in multimodal learning techniques. Our proposed model pipeline of data\nhandling and training objectives aligns the structure/property features in a\ncommon embedding space, which enables the model to regard bidirectional\ninformation between the molecules' structure and properties. These\ncontributions emerge synergistic knowledge, allowing us to tackle both\nmultimodal and unimodal downstream tasks through a single model. Through\nextensive experiments, we demonstrate that our model shows remarkable\ncapabilities in solving various meaningful chemical challenges, including\nconditional molecule generation, property prediction, molecule classification,\nand reaction prediction.\n","authors":["Jinho Chang","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2211.10590v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05908v1","updated":"2023-07-12T04:28:41Z","published":"2023-07-12T04:28:41Z","title":"Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM\n  Decoding","summary":"  This paper presents \"Predictive Pipelined Decoding (PPD),\" an approach that\nspeeds up greedy decoding in Large Language Models (LLMs) while maintaining the\nexact same output as the original decoding. Unlike conventional strategies, PPD\nemploys additional compute resources to parallelize the initiation of\nsubsequent token decoding during the current token decoding. This innovative\nmethod reduces decoding latency and reshapes the understanding of trade-offs in\nLLM decoding strategies. We have developed a theoretical framework that allows\nus to analyze the trade-off between computation and latency. Using this\nframework, we can analytically estimate the potential reduction in latency\nassociated with our proposed method, achieved through the assessment of the\nmatch rate, represented as p_correct. The results demonstrate that the use of\nextra computational resources has the potential to accelerate LLM greedy\ndecoding.\n","authors":["Seongjun Yang","Gibbeum Lee","Jaewoong Cho","Dimitris Papailiopoulos","Kangwook Lee"],"pdf_url":"https://arxiv.org/pdf/2307.05908v1.pdf","comment":"ES-FoMo Workshop at ICML 2023"},{"id":"http://arxiv.org/abs/2307.05906v1","updated":"2023-07-12T04:23:26Z","published":"2023-07-12T04:23:26Z","title":"Mini-Batch Optimization of Contrastive Loss","summary":"  Contrastive learning has gained significant attention as a method for\nself-supervised learning. The contrastive loss function ensures that embeddings\nof positive sample pairs (e.g., different samples from the same class or\ndifferent views of the same object) are similar, while embeddings of negative\npairs are dissimilar. Practical constraints such as large memory requirements\nmake it challenging to consider all possible positive and negative pairs,\nleading to the use of mini-batch optimization. In this paper, we investigate\nthe theoretical aspects of mini-batch optimization in contrastive learning. We\nshow that mini-batch optimization is equivalent to full-batch optimization if\nand only if all $\\binom{N}{B}$ mini-batches are selected, while sub-optimality\nmay arise when examining only a subset. We then demonstrate that utilizing\nhigh-loss mini-batches can speed up SGD convergence and propose a spectral\nclustering-based approach for identifying these high-loss mini-batches. Our\nexperimental results validate our theoretical findings and demonstrate that our\nproposed algorithm outperforms vanilla SGD in practically relevant settings,\nproviding a better understanding of mini-batch optimization in contrastive\nlearning.\n","authors":["Jaewoong Cho","Kartik Sreenivasan","Keon Lee","Kyunghoo Mun","Soheun Yi","Jeong-Gwan Lee","Anna Lee","Jy-yong Sohn","Dimitris Papailiopoulos","Kangwook Lee"],"pdf_url":"https://arxiv.org/pdf/2307.05906v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05014v2","updated":"2023-07-12T04:19:48Z","published":"2023-07-11T05:17:42Z","title":"Test-Time Training on Video Streams","summary":"  Prior work has established test-time training (TTT) as a general framework to\nfurther improve a trained model at test time. Before making a prediction on\neach test instance, the model is trained on the same instance using a\nself-supervised task, such as image reconstruction with masked autoencoders. We\nextend TTT to the streaming setting, where multiple test instances - video\nframes in our case - arrive in temporal order. Our extension is online TTT: The\ncurrent model is initialized from the previous model, then trained on the\ncurrent frame and a small window of frames immediately before. Online TTT\nsignificantly outperforms the fixed-model baseline for four tasks, on three\nreal-world datasets. The relative improvement is 45% and 66% for instance and\npanoptic segmentation. Surprisingly, online TTT also outperforms its offline\nvariant that accesses more information, training on all frames from the entire\ntest video regardless of temporal order. This differs from previous findings\nusing synthetic videos. We conceptualize locality as the advantage of online\nover offline TTT. We analyze the role of locality with ablations and a theory\nbased on bias-variance trade-off.\n","authors":["Renhao Wang","Yu Sun","Yossi Gandelsman","Xinlei Chen","Alexei A. Efros","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2307.05014v2.pdf","comment":"Project website with videos, dataset and code:\n  https://video-ttt.github.io/"},{"id":"http://arxiv.org/abs/2307.05902v1","updated":"2023-07-12T04:19:47Z","published":"2023-07-12T04:19:47Z","title":"Stability Guarantees for Feature Attributions with Multiplicative\n  Smoothing","summary":"  Explanation methods for machine learning models tend to not provide any\nformal guarantees and may not reflect the underlying decision-making process.\nIn this work, we analyze stability as a property for reliable feature\nattribution methods. We prove that relaxed variants of stability are guaranteed\nif the model is sufficiently Lipschitz with respect to the masking of features.\nTo achieve such a model, we develop a smoothing method called Multiplicative\nSmoothing (MuS). We show that MuS overcomes theoretical limitations of standard\nsmoothing techniques and can be integrated with any classifier and feature\nattribution method. We evaluate MuS on vision and language models with a\nvariety of feature attribution methods, such as LIME and SHAP, and demonstrate\nthat MuS endows feature attributions with non-trivial stability guarantees.\n","authors":["Anton Xue","Rajeev Alur","Eric Wong"],"pdf_url":"https://arxiv.org/pdf/2307.05902v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2104.05942v3","updated":"2023-07-12T04:06:53Z","published":"2021-04-13T05:09:41Z","title":"Recurrent Equilibrium Networks: Flexible Dynamic Models with Guaranteed\n  Stability and Robustness","summary":"  This paper introduces recurrent equilibrium networks (RENs), a new class of\nnonlinear dynamical models} for applications in machine learning, system\nidentification and control. The new model class admits ``built in'' behavioural\nguarantees of stability and robustness. All models in the proposed class are\ncontracting -- a strong form of nonlinear stability -- and models can satisfy\nprescribed incremental integral quadratic constraints (IQC), including\nLipschitz bounds and incremental passivity. RENs are otherwise very flexible:\nthey can represent all stable linear systems, all previously-known sets of\ncontracting recurrent neural networks and echo state networks, all deep\nfeedforward neural networks, and all stable Wiener/Hammerstein models, and can\napproximate all fading-memory and contracting nonlinear systems. RENs are\nparameterized directly by a vector in R^N, i.e. stability and robustness are\nensured without parameter constraints, which simplifies learning since\n\\HL{generic methods for unconstrained optimization such as stochastic gradient\ndescent and its variants can be used}. The performance and robustness of the\nnew model set is evaluated on benchmark nonlinear system identification\nproblems, and the paper also presents applications in data-driven nonlinear\nobserver design and control with stability guarantees.\n","authors":["Max Revay","Ruigang Wang","Ian R. Manchester"],"pdf_url":"https://arxiv.org/pdf/2104.05942v3.pdf","comment":"To appear in IEEE Transactions on Automatic Control"},{"id":"http://arxiv.org/abs/2307.05893v1","updated":"2023-07-12T03:48:26Z","published":"2023-07-12T03:48:26Z","title":"Deep Unrolling for Nonconvex Robust Principal Component Analysis","summary":"  We design algorithms for Robust Principal Component Analysis (RPCA) which\nconsists in decomposing a matrix into the sum of a low rank matrix and a sparse\nmatrix. We propose a deep unrolled algorithm based on an accelerated\nalternating projection algorithm which aims to solve RPCA in its nonconvex\nform. The proposed procedure combines benefits of deep neural networks and the\ninterpretability of the original algorithm and it automatically learns\nhyperparameters. We demonstrate the unrolled algorithm's effectiveness on\nsynthetic datasets and also on a face modeling problem, where it leads to both\nbetter numerical and visual performances.\n","authors":["Elizabeth Z. C. Tan","Caroline Chaux","Emmanuel Soubies","Vincent Y. F. Tan"],"pdf_url":"https://arxiv.org/pdf/2307.05893v1.pdf","comment":"7 pages, 3 figures; Accepted to the 2023 IEEE International Workshop\n  on Machine Learning for Signal Processing"},{"id":"http://arxiv.org/abs/2307.05891v1","updated":"2023-07-12T03:42:24Z","published":"2023-07-12T03:42:24Z","title":"PID-Inspired Inductive Biases for Deep Reinforcement Learning in\n  Partially Observable Control Tasks","summary":"  Deep reinforcement learning (RL) has shown immense potential for learning to\ncontrol systems through data alone. However, one challenge deep RL faces is\nthat the full state of the system is often not observable. When this is the\ncase, the policy needs to leverage the history of observations to infer the\ncurrent state. At the same time, differences between the training and testing\nenvironments makes it critical for the policy not to overfit to the sequence of\nobservations it sees at training time. As such, there is an important balancing\nact between having the history encoder be flexible enough to extract relevant\ninformation, yet be robust to changes in the environment. To strike this\nbalance, we look to the PID controller for inspiration. We assert the PID\ncontroller's success shows that only summing and differencing are needed to\naccumulate information over time for many control tasks. Following this\nprinciple, we propose two architectures for encoding history: one that directly\nuses PID features and another that extends these core ideas and can be used in\narbitrary control tasks. When compared with prior approaches, our encoders\nproduce policies that are often more robust and achieve better performance on a\nvariety of tracking tasks. Going beyond tracking tasks, our policies achieve\n1.7x better performance on average over previous state-of-the-art methods on a\nsuite of high dimensional control tasks.\n","authors":["Ian Char","Jeff Schneider"],"pdf_url":"https://arxiv.org/pdf/2307.05891v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05888v1","updated":"2023-07-12T03:31:34Z","published":"2023-07-12T03:31:34Z","title":"Efficient Task Offloading Algorithm for Digital Twin in Edge/Cloud\n  Computing Environment","summary":"  In the era of Internet of Things (IoT), Digital Twin (DT) is envisioned to\nempower various areas as a bridge between physical objects and the digital\nworld. Through virtualization and simulation techniques, multiple functions can\nbe achieved by leveraging computing resources. In this process, Mobile Cloud\nComputing (MCC) and Mobile Edge Computing (MEC) have become two of the key\nfactors to achieve real-time feedback. However, current works only considered\nedge servers or cloud servers in the DT system models. Besides, The models\nignore the DT with not only one data resource. In this paper, we propose a new\nDT system model considering a heterogeneous MEC/MCC environment. Each DT in the\nmodel is maintained in one of the servers via multiple data collection devices.\nThe offloading decision-making problem is also considered and a new offloading\nscheme is proposed based on Distributed Deep Learning (DDL). Simulation results\ndemonstrate that our proposed algorithm can effectively and efficiently\ndecrease the system's average latency and energy consumption. Significant\nimprovement is achieved compared with the baselines under the dynamic\nenvironment of DTs.\n","authors":["Ziru Zhang","Xuling Zhang","Guangzhi Zhu","Yuyang Wang","Pan Hui"],"pdf_url":"https://arxiv.org/pdf/2307.05888v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05881v1","updated":"2023-07-12T03:03:40Z","published":"2023-07-12T03:03:40Z","title":"Dynamic Prediction using Time-Dependent Cox Survival Neural Network","summary":"  The target of dynamic prediction is to provide individualized risk\npredictions over time which can be updated as new data become available.\nMotivated by establishing a dynamic prediction model for the progressive eye\ndisease, age-related macular degeneration (AMD), we proposed a time-dependent\nCox model-based survival neural network (tdCoxSNN) to predict its progression\non a continuous time scale using longitudinal fundus images. tdCoxSNN extends\nthe time-dependent Cox model by utilizing a neural network to model the\nnon-linear effect of the time-dependent covariates on the survival outcome.\nAdditionally, by incorporating the convolutional neural network (CNN), tdCoxSNN\ncan take the longitudinal raw images as input. We evaluate and compare our\nproposed method with joint modeling and landmarking approaches through\ncomprehensive simulations using two time-dependent accuracy metrics, the Brier\nScore and dynamic AUC. We applied the proposed approach to two real datasets.\nOne is a large AMD study, the Age-Related Eye Disease Study (AREDS), in which\nmore than 50,000 fundus images were captured over a period of 12 years for more\nthan 4,000 participants. Another is a public dataset of the primary biliary\ncirrhosis (PBC) disease, in which multiple lab tests were longitudinally\ncollected to predict the time-to-liver transplant. Our approach achieves\nsatisfactory prediction performance in both simulation studies and the two real\ndata analyses. tdCoxSNN was implemented in PyTorch, Tensorflow, and\nR-Tensorflow.\n","authors":["Lang Zeng","Jipeng Zhang","Wei Chen","Ying Ding"],"pdf_url":"https://arxiv.org/pdf/2307.05881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.00129v5","updated":"2023-07-12T02:51:55Z","published":"2022-01-31T22:39:42Z","title":"Fundamental Limits for Sensor-Based Robot Control","summary":"  Our goal is to develop theory and algorithms for establishing fundamental\nlimits on performance imposed by a robot's sensors for a given task. In order\nto achieve this, we define a quantity that captures the amount of task-relevant\ninformation provided by a sensor. Using a novel version of the generalized Fano\ninequality from information theory, we demonstrate that this quantity provides\nan upper bound on the highest achievable expected reward for one-step decision\nmaking tasks. We then extend this bound to multi-step problems via a dynamic\nprogramming approach. We present algorithms for numerically computing the\nresulting bounds, and demonstrate our approach on three examples: (i) the lava\nproblem from the literature on partially observable Markov decision processes,\n(ii) an example with continuous state and observation spaces corresponding to a\nrobot catching a freely-falling object, and (iii) obstacle avoidance using a\ndepth sensor with non-Gaussian noise. We demonstrate the ability of our\napproach to establish strong limits on achievable performance for these\nproblems by comparing our upper bounds with achievable lower bounds (computed\nby synthesizing or learning concrete control policies).\n","authors":["Anirudha Majumdar","Zhiting Mei","Vincent Pacelli"],"pdf_url":"https://arxiv.org/pdf/2202.00129v5.pdf","comment":"Extended version of paper presented at the 2022 Robotics: Science and\n  Systems (RSS) conference"},{"id":"http://arxiv.org/abs/2306.06331v2","updated":"2023-07-12T01:56:52Z","published":"2023-06-10T02:01:02Z","title":"Investigating the Effectiveness of ChatGPT in Mathematical Reasoning and\n  Problem Solving: Evidence from the Vietnamese National High School Graduation\n  Examination","summary":"  This study offers a complete analysis of ChatGPT's mathematics abilities in\nresponding to multiple-choice questions for the Vietnamese National High School\nGraduation Examination (VNHSGE) on a range of subjects and difficulty levels.\nThe dataset included 250 questions divided into four levels: knowledge (K),\ncomprehension (C), application (A), and high application (H), and it included\nten themes that covered diverse mathematical concepts. The outcomes demonstrate\nthat ChatGPT's performance varies depending on the difficulty level and\nsubject. It performed best on questions at Level (K), with an accuracy rate of\n$83\\%$; but, as the difficulty level rose, it scored poorly, with an accuracy\nrate of $10\\%$. The study has also shown that ChatGPT significantly succeeds in\nproviding responses to questions on subjects including exponential and\nlogarithmic functions, geometric progression, and arithmetic progression. The\nstudy found that ChatGPT had difficulty correctly answering questions on topics\nincluding derivatives and applications, spatial geometry, and Oxyz spatial\ncalculus. Additionally, this study contrasted ChatGPT outcomes with Vietnamese\nstudents in VNHSGE and in other math competitions. ChatGPT dominated in the SAT\nMath competition with a success rate of $70\\%$, followed by VNHSGE mathematics\n($58.8\\%)$. However, its success rates were lower on other exams, such as AP\nStatistics, the GRE Quantitative, AMC 10, AMC 12, and AP Calculus BC. These\nresults suggest that ChatGPT has the potential to be an effective teaching tool\nfor mathematics, but more work is needed to enhance its handling of graphical\ndata and address the challenges presented by questions that are getting more\nchallenging.\n","authors":["Xuan-Quy Dao","Ngoc-Bich Le"],"pdf_url":"https://arxiv.org/pdf/2306.06331v2.pdf","comment":"17 pages, 13 images"},{"id":"http://arxiv.org/abs/2207.04104v3","updated":"2023-07-12T01:36:19Z","published":"2022-07-08T19:02:50Z","title":"Towards a More Rigorous Science of Blindspot Discovery in Image\n  Classification Models","summary":"  A growing body of work studies Blindspot Discovery Methods (\"BDM\"s): methods\nthat use an image embedding to find semantically meaningful (i.e., united by a\nhuman-understandable concept) subsets of the data where an image classifier\nperforms significantly worse. Motivated by observed gaps in prior work, we\nintroduce a new framework for evaluating BDMs, SpotCheck, that uses synthetic\nimage datasets to train models with known blindspots and a new BDM, PlaneSpot,\nthat uses a 2D image representation. We use SpotCheck to run controlled\nexperiments that identify factors that influence BDM performance (e.g., the\nnumber of blindspots in a model, or features used to define the blindspot) and\nshow that PlaneSpot is competitive with and in many cases outperforms existing\nBDMs. Importantly, we validate these findings by designing additional\nexperiments that use real image data from MS-COCO, a large image benchmark\ndataset. Our findings suggest several promising directions for future work on\nBDM design and evaluation. Overall, we hope that the methodology and analyses\npresented in this work will help facilitate a more rigorous science of\nblindspot discovery.\n","authors":["Gregory Plumb","Nari Johnson","Ángel Alexander Cabrera","Ameet Talwalkar"],"pdf_url":"https://arxiv.org/pdf/2207.04104v3.pdf","comment":"reviewed on OpenReview: https://openreview.net/forum?id=MaDvbLaBiF"},{"id":"http://arxiv.org/abs/2304.10701v4","updated":"2023-07-12T01:31:34Z","published":"2023-04-21T02:02:02Z","title":"Matching-based Data Valuation for Generative Model","summary":"  Data valuation is critical in machine learning, as it helps enhance model\ntransparency and protect data properties. Existing data valuation methods have\nprimarily focused on discriminative models, neglecting deep generative models\nthat have recently gained considerable attention. Similar to discriminative\nmodels, there is an urgent need to assess data contributions in deep generative\nmodels as well. However, previous data valuation approaches mainly relied on\ndiscriminative model performance metrics and required model retraining.\nConsequently, they cannot be applied directly and efficiently to recent deep\ngenerative models, such as generative adversarial networks and diffusion\nmodels, in practice. To bridge this gap, we formulate the data valuation\nproblem in generative models from a similarity-matching perspective.\nSpecifically, we introduce Generative Model Valuator (GMValuator), the first\nmodel-agnostic approach for any generative models, designed to provide data\nvaluation for generation tasks. We have conducted extensive experiments to\ndemonstrate the effectiveness of the proposed method. To the best of their\nknowledge, GMValuator is the first work that offers a training-free, post-hoc\ndata valuation strategy for deep generative models.\n","authors":["Jiaxi Yang","Wenglong Deng","Benlin Liu","Yangsibo Huang","Xiaoxiao Li"],"pdf_url":"https://arxiv.org/pdf/2304.10701v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05862v1","updated":"2023-07-12T01:11:52Z","published":"2023-07-12T01:11:52Z","title":"Ecosystem-level Analysis of Deployed Machine Learning Reveals\n  Homogeneous Outcomes","summary":"  Machine learning is traditionally studied at the model level: researchers\nmeasure and improve the accuracy, robustness, bias, efficiency, and other\ndimensions of specific models. In practice, the societal impact of machine\nlearning is determined by the surrounding context of machine learning\ndeployments. To capture this, we introduce ecosystem-level analysis: rather\nthan analyzing a single model, we consider the collection of models that are\ndeployed in a given context. For example, ecosystem-level analysis in hiring\nrecognizes that a job candidate's outcomes are not only determined by a single\nhiring algorithm or firm but instead by the collective decisions of all the\nfirms they applied to. Across three modalities (text, images, speech) and 11\ndatasets, we establish a clear trend: deployed machine learning is prone to\nsystemic failure, meaning some users are exclusively misclassified by all\nmodels available. Even when individual models improve at the population level\nover time, we find these improvements rarely reduce the prevalence of systemic\nfailure. Instead, the benefits of these improvements predominantly accrue to\nindividuals who are already correctly classified by other models. In light of\nthese trends, we consider medical imaging for dermatology where the costs of\nsystemic failure are especially high. While traditional analyses reveal racial\nperformance disparities for both models and humans, ecosystem-level analysis\nreveals new forms of racial disparity in model predictions that do not present\nin human predictions. These examples demonstrate ecosystem-level analysis has\nunique strengths for characterizing the societal impact of machine learning.\n","authors":["Connor Toups","Rishi Bommasani","Kathleen A. Creel","Sarah H. Bana","Dan Jurafsky","Percy Liang"],"pdf_url":"https://arxiv.org/pdf/2307.05862v1.pdf","comment":"All code is available at\n  https://github.com/rishibommasani/EcosystemLevelAnalysis"},{"id":"http://arxiv.org/abs/2305.04492v7","updated":"2023-07-12T01:04:28Z","published":"2023-05-08T06:36:46Z","title":"MGR: Multi-generator Based Rationalization","summary":"  Rationalization is to employ a generator and a predictor to construct a\nself-explaining NLP model in which the generator selects a subset of\nhuman-intelligible pieces of the input text to the following predictor.\nHowever, rationalization suffers from two key challenges, i.e., spurious\ncorrelation and degeneration, where the predictor overfits the spurious or\nmeaningless pieces solely selected by the not-yet well-trained generator and in\nturn deteriorates the generator. Although many studies have been proposed to\naddress the two challenges, they are usually designed separately and do not\ntake both of them into account. In this paper, we propose a simple yet\neffective method named MGR to simultaneously solve the two problems. The key\nidea of MGR is to employ multiple generators such that the occurrence stability\nof real pieces is improved and more meaningful pieces are delivered to the\npredictor. Empirically, we show that MGR improves the F1 score by up to 20.9%\nas compared to state-of-the-art methods. Codes are available at\nhttps://github.com/jugechengzi/Rationalization-MGR .\n","authors":["Wei Liu","Haozhao Wang","Jun Wang","Ruixuan Li","Xinyang Li","Yuankai Zhang","Yang Qiu"],"pdf_url":"https://arxiv.org/pdf/2305.04492v7.pdf","comment":"ACL 2023, oral presentation. arXiv admin note: text overlap with\n  arXiv:2209.08285"},{"id":"http://arxiv.org/abs/2210.01892v3","updated":"2023-07-12T01:02:19Z","published":"2022-10-04T20:28:43Z","title":"Polysemanticity and Capacity in Neural Networks","summary":"  Individual neurons in neural networks often represent a mixture of unrelated\nfeatures. This phenomenon, called polysemanticity, can make interpreting neural\nnetworks more difficult and so we aim to understand its causes. We propose\ndoing so through the lens of feature \\emph{capacity}, which is the fractional\ndimension each feature consumes in the embedding space. We show that in a toy\nmodel the optimal capacity allocation tends to monosemantically represent the\nmost important features, polysemantically represent less important features (in\nproportion to their impact on the loss), and entirely ignore the least\nimportant features. Polysemanticity is more prevalent when the inputs have\nhigher kurtosis or sparsity and more prevalent in some architectures than\nothers. Given an optimal allocation of capacity, we go on to study the geometry\nof the embedding space. We find a block-semi-orthogonal structure, with\ndiffering block sizes in different models, highlighting the impact of model\narchitecture on the interpretability of its neurons.\n","authors":["Adam Scherlis","Kshitij Sachan","Adam S. Jermyn","Joe Benton","Buck Shlegeris"],"pdf_url":"https://arxiv.org/pdf/2210.01892v3.pdf","comment":"22 pages, 7 figures. Corrected typos in Figure 7, improved notation\n  to distinguish column and row vectors, corrected proof in Appendix A, and\n  other misc changes"},{"id":"http://arxiv.org/abs/2307.05857v1","updated":"2023-07-12T00:35:19Z","published":"2023-07-12T00:35:19Z","title":"FAIRO: Fairness-aware Adaptation in Sequential-Decision Making for\n  Human-in-the-Loop Systems","summary":"  Achieving fairness in sequential-decision making systems within\nHuman-in-the-Loop (HITL) environments is a critical concern, especially when\nmultiple humans with different behavior and expectations are affected by the\nsame adaptation decisions in the system. This human variability factor adds\nmore complexity since policies deemed fair at one point in time may become\ndiscriminatory over time due to variations in human preferences resulting from\ninter- and intra-human variability. This paper addresses the fairness problem\nfrom an equity lens, considering human behavior variability, and the changes in\nhuman preferences over time. We propose FAIRO, a novel algorithm for\nfairness-aware sequential-decision making in HITL adaptation, which\nincorporates these notions into the decision-making process. In particular,\nFAIRO decomposes this complex fairness task into adaptive sub-tasks based on\nindividual human preferences through leveraging the Options reinforcement\nlearning framework. We design FAIRO to generalize to three types of HITL\napplication setups that have the shared adaptation decision problem.\nFurthermore, we recognize that fairness-aware policies can sometimes conflict\nwith the application's utility. To address this challenge, we provide a\nfairness-utility tradeoff in FAIRO, allowing system designers to balance the\nobjectives of fairness and utility based on specific application requirements.\nExtensive evaluations of FAIRO on the three HITL applications demonstrate its\ngeneralizability and effectiveness in promoting fairness while accounting for\nhuman variability. On average, FAIRO can improve fairness compared with other\nmethods across all three applications by 35.36%.\n","authors":["Tianyu Zhao","Mojtaba Taherisadr","Salma Elmalaki"],"pdf_url":"https://arxiv.org/pdf/2307.05857v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.08172v2","updated":"2023-07-12T00:32:53Z","published":"2023-04-17T11:38:22Z","title":"Pointwise convergence theorem of gradient descent in sparse deep neural\n  network","summary":"  The theoretical structure of deep neural network (DNN) has been clarified\ngradually. Imaizumi-Fukumizu (2019) and Suzuki (2019) clarified that the\nlearning ability of DNN is superior to the previous theories when the target\nfunction is non-smooth functions. However, as far as the author is aware, none\nof the numerous works to date attempted to mathematically investigate what kind\nof DNN architectures really induce pointwise convergence of gradient descent\n(without any statistical argument), and this attempt seems to be closer to the\npractical DNNs. In this paper we restrict target functions to non-smooth\nindicator functions, and construct a deep neural network inducing pointwise\nconvergence provided by gradient descent process in ReLU-DNN. The DNN has a\nsparse and a special shape, with certain variable transformations.\n","authors":["Tsuyoshi Yoneda"],"pdf_url":"https://arxiv.org/pdf/2304.08172v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2307.06027v1","updated":"2023-07-12T09:16:33Z","published":"2023-07-12T09:16:33Z","title":"Semantic Communications System with Model Division Multiple Access and\n  Controllable Coding Rate for Point Cloud","summary":"  Point cloud, as a 3D representation, is widely used in autonomous driving,\nvirtual reality (VR), and augmented reality (AR). However, traditional\ncommunication systems think that the point cloud's semantic information is\nirrelevant to communication, which hinders the efficient transmission of point\nclouds in the era of artificial intelligence (AI). This paper proposes a point\ncloud based semantic communication system (PCSC), which uses AI-based encoding\ntechniques to extract the semantic information of the point cloud and joint\nsource-channel coding (JSCC) technology to overcome the distortion caused by\nnoise channels and solve the \"cliff effect\" in traditional communication. In\naddition, the system realizes the controllable coding rate without fine-tuning\nthe network. The method analyzes the coded semantic vector's importance and\ndiscards semantically-unimportant information, thereby improving the\ntransmission efficiency. Besides, PCSC and the recently proposed non-orthogonal\nmodel division multiple access (MDMA) technology are combined to design a point\ncloud MDMA transmission system (M-PCSC) for multi-user transmission. Relevant\nexperimental results show that the proposed method outperforms the traditional\nmethod 10dB in the same channel bandwidth ratio under the PSNR D1 and PSNR D2\nmetrics. In terms of transmission, the proposed method can effectively solve\nthe \"cliff effect\" in the traditional methods.\n","authors":["Xiaoyi Liu","Haotai Liang","Zhicheng Bao","Chen Dong","Xiaodong Xu"],"pdf_url":"https://arxiv.org/pdf/2307.06027v1.pdf","comment":null}]},"2023-07-11T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2307.05827v1","updated":"2023-07-11T22:36:47Z","published":"2023-07-11T22:36:47Z","title":"Relational Extraction on Wikipedia Tables using Convolutional and Memory\n  Networks","summary":"  Relation extraction (RE) is the task of extracting relations between entities\nin text. Most RE methods extract relations from free-form running text and\nleave out other rich data sources, such as tables. We explore RE from the\nperspective of applying neural methods on tabularly organized data. We\nintroduce a new model consisting of Convolutional Neural Network (CNN) and\nBidirectional-Long Short Term Memory (BiLSTM) network to encode entities and\nlearn dependencies among them, respectively. We evaluate our model on a large\nand recent dataset and compare results with previous neural methods.\nExperimental results show that our model consistently outperforms the previous\nmodel for the task of relation extraction on tabular data. We perform\ncomprehensive error analyses and ablation study to show the contribution of\nvarious components of our model. Finally, we discuss the usefulness and\ntrade-offs of our approach, and provide suggestions for fostering further\nresearch.\n","authors":["Arif Shahriar","Rohan Saha","Denilson Barbosa"],"pdf_url":"https://arxiv.org/pdf/2307.05827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05796v1","updated":"2023-07-11T20:42:06Z","published":"2023-07-11T20:42:06Z","title":"Improved POS tagging for spontaneous, clinical speech using data\n  augmentation","summary":"  This paper addresses the problem of improving POS tagging of transcripts of\nspeech from clinical populations. In contrast to prior work on parsing and POS\ntagging of transcribed speech, we do not make use of an in domain treebank for\ntraining. Instead, we train on an out of domain treebank of newswire using data\naugmentation techniques to make these structures resemble natural, spontaneous\nspeech. We trained a parser with and without the augmented data and tested its\nperformance using manually validated POS tags in clinical speech produced by\npatients with various types of neurodegenerative conditions.\n","authors":["Seth Kulick","Neville Ryant","David J. Irwin","Naomi Nevler","Sunghye Cho"],"pdf_url":"https://arxiv.org/pdf/2307.05796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05782v1","updated":"2023-07-11T20:21:02Z","published":"2023-07-11T20:21:02Z","title":"Large Language Models","summary":"  Artificial intelligence is making spectacular progress, and one of the best\nexamples is the development of large language models (LLMs) such as OpenAI's\nGPT series. In these lectures, written for readers with a background in\nmathematics or physics, we give a brief history and survey of the state of the\nart, and describe the underlying transformer architecture in detail. We then\nexplore some current ideas on how LLMs work and how models trained to predict\nthe next word in a text are able to perform other tasks displaying\nintelligence.\n","authors":["Michael R. Douglas"],"pdf_url":"https://arxiv.org/pdf/2307.05782v1.pdf","comment":"46 pages"},{"id":"http://arxiv.org/abs/2307.05779v1","updated":"2023-07-11T20:15:47Z","published":"2023-07-11T20:15:47Z","title":"Neural Machine Translation Data Generation and Augmentation using\n  ChatGPT","summary":"  Neural models have revolutionized the field of machine translation, but\ncreating parallel corpora is expensive and time-consuming. We investigate an\nalternative to manual parallel corpora - hallucinated parallel corpora created\nby generative language models. Although these models are themselves trained on\nparallel data, they can leverage a multilingual vector space to create data,\nand may be able to supplement small manually-procured corpora. Our experiments\nhighlight two key findings - despite a lack of diversity in their output, the\nhallucinated data improves the translation signal, even when the domain clashes\nwith the original dataset.\n","authors":["Wayne Yang","Garrett Nicolai"],"pdf_url":"https://arxiv.org/pdf/2307.05779v1.pdf","comment":"8 Pages, 4 Figures, 4 Tables"},{"id":"http://arxiv.org/abs/2306.11197v3","updated":"2023-07-11T19:51:21Z","published":"2023-06-19T23:10:02Z","title":"Sparse Modular Activation for Efficient Sequence Modeling","summary":"  Linear State Space Models (SSMs) have demonstrated strong performance in a\nvariety of sequence modeling tasks due to their efficient encoding of the\nrecurrent structure. However, in more comprehensive tasks like language\nmodeling and machine translation, self-attention-based models still outperform\nSSMs. Hybrid models employing both SSM and self-attention generally show\npromising performance, but current approaches apply attention modules\nstatically and uniformly to all elements in the input sequences, leading to\nsub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse\nModular Activation (SMA), a general mechanism enabling neural networks to\nsparsely and dynamically activate sub-modules for sequence elements in a\ndifferentiable manner. Through allowing each element to skip non-activated\nsub-modules, SMA reduces computation and memory consumption at both training\nand inference stages of sequence modeling. As a specific instantiation of SMA,\nwe design a novel neural architecture, SeqBoat, which employs SMA to sparsely\nactivate a Gated Attention Unit (GAU) based on the state representations\nlearned from an SSM. By constraining the GAU to only conduct local attention on\nthe activated inputs, SeqBoat can achieve linear inference complexity with\ntheoretically infinite attention span, and provide substantially better\nquality-efficiency trade-off than the chunking-based models. With experiments\non a wide range of tasks, including language modeling, speech classification\nand long-range arena, SeqBoat brings new state-of-the-art results among hybrid\nmodels with linear complexity and reveals the amount of attention needed for\neach task through the learned sparse activation patterns.\n","authors":["Liliang Ren","Yang Liu","Shuohang Wang","Yichong Xu","Chenguang Zhu","ChengXiang Zhai"],"pdf_url":"https://arxiv.org/pdf/2306.11197v3.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2212.10258v2","updated":"2023-07-11T19:33:44Z","published":"2022-12-20T14:06:50Z","title":"In and Out-of-Domain Text Adversarial Robustness via Label Smoothing","summary":"  Recently it has been shown that state-of-the-art NLP models are vulnerable to\nadversarial attacks, where the predictions of a model can be drastically\naltered by slight modifications to the input (such as synonym substitutions).\nWhile several defense techniques have been proposed, and adapted, to the\ndiscrete nature of text adversarial attacks, the benefits of general-purpose\nregularization methods such as label smoothing for language models, have not\nbeen studied. In this paper, we study the adversarial robustness provided by\nvarious label smoothing strategies in foundational models for diverse NLP tasks\nin both in-domain and out-of-domain settings. Our experiments show that label\nsmoothing significantly improves adversarial robustness in pre-trained models\nlike BERT, against various popular attacks. We also analyze the relationship\nbetween prediction confidence and robustness, showing that label smoothing\nreduces over-confident errors on adversarial examples.\n","authors":["Yahan Yang","Soham Dan","Dan Roth","Insup Lee"],"pdf_url":"https://arxiv.org/pdf/2212.10258v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05741v1","updated":"2023-07-11T19:08:31Z","published":"2023-07-11T19:08:31Z","title":"Towards Robust and Efficient Continual Language Learning","summary":"  As the application space of language models continues to evolve, a natural\nquestion to ask is how we can quickly adapt models to new tasks. We approach\nthis classic question from a continual learning perspective, in which we aim to\ncontinue fine-tuning models trained on past tasks on new tasks, with the goal\nof \"transferring\" relevant knowledge. However, this strategy also runs the risk\nof doing more harm than good, i.e., negative transfer. In this paper, we\nconstruct a new benchmark of task sequences that target different possible\ntransfer scenarios one might face, such as a sequence of tasks with high\npotential of positive transfer, high potential for negative transfer, no\nexpected effect, or a mixture of each. An ideal learner should be able to\nmaximally exploit information from all tasks that have any potential for\npositive transfer, while also avoiding the negative effects of any distracting\ntasks that may confuse it. We then propose a simple, yet effective, learner\nthat satisfies many of our desiderata simply by leveraging a selective strategy\nfor initializing new models from past task checkpoints. Still, limitations\nremain, and we hope this benchmark can help the community to further build and\nanalyze such learners.\n","authors":["Adam Fisch","Amal Rannen-Triki","Razvan Pascanu","Jörg Bornschein","Angeliki Lazaridou","Elena Gribovskaya","Marc'Aurelio Ranzato"],"pdf_url":"https://arxiv.org/pdf/2307.05741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18703v4","updated":"2023-07-11T18:34:08Z","published":"2023-05-30T03:00:30Z","title":"Domain Specialization as the Key to Make Large Language Models\n  Disruptive: A Comprehensive Survey","summary":"  Large language models (LLMs) have significantly advanced the field of natural\nlanguage processing (NLP), providing a highly useful, task-agnostic foundation\nfor a wide range of applications. However, directly applying LLMs to solve\nsophisticated problems in specific domains meets many hurdles, caused by the\nheterogeneity of domain data, the sophistication of domain knowledge, the\nuniqueness of domain objectives, and the diversity of the constraints (e.g.,\nvarious social norms, cultural conformity, religious beliefs, and ethical\nstandards in the domain applications). Domain specification techniques are key\nto make large language models disruptive in many applications. Specifically, to\nsolve these hurdles, there has been a notable increase in research and\npractices conducted in recent years on the domain specialization of LLMs. This\nemerging field of study, with its substantial potential for impact,\nnecessitates a comprehensive and systematic review to better summarize and\nguide ongoing work in this area. In this article, we present a comprehensive\nsurvey on domain specification techniques for large language models, an\nemerging direction critical for large language model applications. First, we\npropose a systematic taxonomy that categorizes the LLM domain-specialization\ntechniques based on the accessibility to LLMs and summarizes the framework for\nall the subcategories as well as their relations and differences to each other.\nSecond, we present an extensive taxonomy of critical application domains that\ncan benefit dramatically from specialized LLMs, discussing their practical\nsignificance and open challenges. Last, we offer our insights into the current\nresearch status and future trends in this area.\n","authors":["Chen Ling","Xujiang Zhao","Jiaying Lu","Chengyuan Deng","Can Zheng","Junxiang Wang","Tanmoy Chowdhury","Yun Li","Hejie Cui","Xuchao Zhang","Tianjiao Zhao","Amit Panalkar","Wei Cheng","Haoyu Wang","Yanchi Liu","Zhengzhang Chen","Haifeng Chen","Chris White","Quanquan Gu","Jian Pei","Carl Yang","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2305.18703v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.08844v2","updated":"2023-07-11T18:29:12Z","published":"2023-05-15T17:57:16Z","title":"RL4F: Generating Natural Language Feedback with Reinforcement Learning\n  for Repairing Model Outputs","summary":"  Despite their unprecedented success, even the largest language models make\nmistakes. Similar to how humans learn and improve using feedback, previous work\nproposed providing language models with natural language feedback to guide them\nin repairing their outputs. Because human-generated critiques are expensive to\nobtain, researchers have devised learned critique generators in lieu of human\ncritics while assuming one can train downstream models to utilize generated\nfeedback. However, this approach does not apply to black-box or limited access\nmodels such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of\nlarge general-purpose language agents, fine-tuning is neither computationally\nnor spatially efficient as it results in multiple copies of the network. In\nthis work, we introduce RL4F (Reinforcement Learning for Feedback), a\nmulti-agent collaborative framework where the critique generator is trained to\nmaximize end-task performance of GPT-3, a fixed model more than 200 times its\nsize. RL4F produces critiques that help GPT-3 revise its outputs. We study\nthree datasets for action planning, summarization and alphabetization and show\nrelative improvements up to 10% in multiple text similarity metrics over other\nlearned, retrieval-augmented or prompting-based critique generators.\n","authors":["Afra Feyza Akyürek","Ekin Akyürek","Aman Madaan","Ashwin Kalyan","Peter Clark","Derry Wijaya","Niket Tandon"],"pdf_url":"https://arxiv.org/pdf/2305.08844v2.pdf","comment":"ACL 2023"},{"id":"http://arxiv.org/abs/2307.05695v1","updated":"2023-07-11T18:02:09Z","published":"2023-07-11T18:02:09Z","title":"Stack More Layers Differently: High-Rank Training Through Low-Rank\n  Updates","summary":"  Despite the dominance and effectiveness of scaling, resulting in large\nnetworks with hundreds of billions of parameters, the necessity to train\noverparametrized models remains poorly understood, and alternative approaches\ndo not necessarily make it cheaper to train high-performance models. In this\npaper, we explore low-rank training techniques as an alternative approach to\ntraining large neural networks. We introduce a novel method called ReLoRA,\nwhich utilizes low-rank updates to train high-rank networks. We apply ReLoRA to\npre-training transformer language models with up to 350M parameters and\ndemonstrate comparable performance to regular neural network training.\nFurthermore, we observe that the efficiency of ReLoRA increases with model\nsize, making it a promising approach for training multi-billion-parameter\nnetworks efficiently. Our findings shed light on the potential of low-rank\ntraining techniques and their implications for scaling laws.\n","authors":["Vladislav Lialin","Namrata Shivagunde","Sherin Muckatira","Anna Rumshisky"],"pdf_url":"https://arxiv.org/pdf/2307.05695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.15542v2","updated":"2023-07-11T17:57:06Z","published":"2023-05-24T20:03:04Z","title":"TOAST: Transfer Learning via Attention Steering","summary":"  Transfer learning involves adapting a pre-trained model to novel downstream\ntasks. However, we observe that current transfer learning methods often fail to\nfocus on task-relevant features. In this work, we explore refocusing model\nattention for transfer learning. We introduce Top-Down Attention Steering\n(TOAST), a novel transfer learning algorithm that keeps the pre-trained\nbackbone frozen, selects task-relevant features in the output, and feeds those\nfeatures back to the model to steer the attention to the task-specific\nfeatures. By refocusing the attention only, TOAST achieves state-of-the-art\nresults on a number of transfer learning benchmarks, while having a small\nnumber of tunable parameters. Compared to fully fine-tuning, LoRA, and prompt\ntuning, TOAST substantially improves performance across a range of fine-grained\nvisual classification datasets (e.g., 81.1% -> 86.2% on FGVC). TOAST also\noutperforms the fully fine-tuned Alpaca and Vicuna models on\ninstruction-following language generation. Code is available at\nhttps://github.com/bfshi/TOAST.\n","authors":["Baifeng Shi","Siyu Gai","Trevor Darrell","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2305.15542v2.pdf","comment":"Code is available at https://github.com/bfshi/TOAST"},{"id":"http://arxiv.org/abs/2206.03318v2","updated":"2023-07-11T17:43:57Z","published":"2022-06-07T14:08:07Z","title":"LegoNN: Building Modular Encoder-Decoder Models","summary":"  State-of-the-art encoder-decoder models (e.g. for machine translation (MT) or\nautomatic speech recognition (ASR)) are constructed and trained end-to-end as\nan atomic unit. No component of the model can be (re-)used without the others,\nmaking it impossible to share parts, e.g. a high resourced decoder, across\ntasks. We describe LegoNN, a procedure for building encoder-decoder\narchitectures in a way so that its parts can be applied to other tasks without\nthe need for any fine-tuning. To achieve this reusability, the interface\nbetween encoder and decoder modules is grounded to a sequence of marginal\ndistributions over a pre-defined discrete vocabulary. We present two approaches\nfor ingesting these marginals; one is differentiable, allowing the flow of\ngradients across the entire network, and the other is gradient-isolating. To\nenable the portability of decoder modules between MT tasks for different source\nlanguages and across other tasks like ASR, we introduce a modality agnostic\nencoder which consists of a length control mechanism to dynamically adapt\nencoders' output lengths in order to match the expected input length range of\npre-trained decoders. We present several experiments to demonstrate the\neffectiveness of LegoNN models: a trained language generation LegoNN decoder\nmodule from German-English (De-En) MT task can be reused without any\nfine-tuning for the Europarl English ASR and the Romanian-English (Ro-En) MT\ntasks, matching or beating the performance of baseline. After fine-tuning,\nLegoNN models improve the Ro-En MT task by 1.5 BLEU points and achieve 12.5%\nrelative WER reduction on the Europarl ASR task. To show how the approach\ngeneralizes, we compose a LegoNN ASR model from three modules -- each has been\nlearned within different end-to-end trained models on three different datasets\n-- achieving an overall WER reduction of 19.5%.\n","authors":["Siddharth Dalmia","Dmytro Okhonko","Mike Lewis","Sergey Edunov","Shinji Watanabe","Florian Metze","Luke Zettlemoyer","Abdelrahman Mohamed"],"pdf_url":"https://arxiv.org/pdf/2206.03318v2.pdf","comment":"IEEE/ACM Transactions on Audio, Speech, and Language Processing\n  (TASLP)"},{"id":"http://arxiv.org/abs/2307.05454v1","updated":"2023-07-11T17:33:03Z","published":"2023-07-11T17:33:03Z","title":"Empowering Cross-lingual Behavioral Testing of NLP Models with\n  Typological Features","summary":"  A challenge towards developing NLP systems for the world's languages is\nunderstanding how they generalize to typological differences relevant for\nreal-world applications. To this end, we propose M2C, a morphologically-aware\nframework for behavioral testing of NLP models. We use M2C to generate tests\nthat probe models' behavior in light of specific linguistic features in 12\ntypologically diverse languages. We evaluate state-of-the-art language models\non the generated tests. While models excel at most tests in English, we\nhighlight generalization failures to specific typological characteristics such\nas temporal expressions in Swahili and compounding possessives in Finish. Our\nfindings motivate the development of models that address these blind spots.\n","authors":["Ester Hlavnova","Sebastian Ruder"],"pdf_url":"https://arxiv.org/pdf/2307.05454v1.pdf","comment":"In Proceedings of the 61st Annual Meeting of the Association for\n  Computational Linguistics(ACL). July 2023"},{"id":"http://arxiv.org/abs/2307.05440v1","updated":"2023-07-11T17:06:52Z","published":"2023-07-11T17:06:52Z","title":"ISLTranslate: Dataset for Translating Indian Sign Language","summary":"  Sign languages are the primary means of communication for many\nhard-of-hearing people worldwide. Recently, to bridge the communication gap\nbetween the hard-of-hearing community and the rest of the population, several\nsign language translation datasets have been proposed to enable the development\nof statistical sign language translation systems. However, there is a dearth of\nsign language resources for the Indian sign language. This resource paper\nintroduces ISLTranslate, a translation dataset for continuous Indian Sign\nLanguage (ISL) consisting of 31k ISL-English sentence/phrase pairs. To the best\nof our knowledge, it is the largest translation dataset for continuous Indian\nSign Language. We provide a detailed analysis of the dataset. To validate the\nperformance of existing end-to-end Sign language to spoken language translation\nsystems, we benchmark the created dataset with a transformer-based model for\nISL translation.\n","authors":["Abhinav Joshi","Susmit Agrawal","Ashutosh Modi"],"pdf_url":"https://arxiv.org/pdf/2307.05440v1.pdf","comment":"Accepted at ACL 2023 Findings, 8 Pages"},{"id":"http://arxiv.org/abs/2212.10722v2","updated":"2023-07-11T17:06:19Z","published":"2022-12-21T02:28:07Z","title":"Contrastive Error Attribution for Finetuned Language Models","summary":"  Recent work has identified noisy and misannotated data as a core cause of\nhallucinations and unfaithful outputs in Natural Language Generation (NLG)\ntasks. Consequently, identifying and removing these examples is a key open\nchallenge in creating reliable NLG systems. In this work, we introduce a\nframework to identify and remove low-quality training instances that lead to\nundesirable outputs, such as faithfulness errors in text summarization. We show\nthat existing approaches for error tracing, such as gradient-based influence\nmeasures, do not perform reliably for detecting faithfulness errors in NLG\ndatasets. We overcome the drawbacks of existing error tracing methods through a\nnew, contrast-based estimate that compares undesired generations to\nhuman-corrected outputs. Our proposed method can achieve a mean average\nprecision of 0.93 at detecting known data errors across synthetic tasks with\nknown ground truth, substantially outperforming existing approaches. Using this\napproach and re-training models on cleaned data leads to a 70% reduction in\nentity hallucinations on the NYT dataset and a 55% reduction in semantic errors\non the E2E dataset.\n","authors":["Faisal Ladhak","Esin Durmus","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2212.10722v2.pdf","comment":"ACL 2023"},{"id":"http://arxiv.org/abs/2307.05414v1","updated":"2023-07-11T16:30:45Z","published":"2023-07-11T16:30:45Z","title":"Duncode Characters Shorter","summary":"  This paper investigates the employment of various encoders in text\ntransformation, converting characters into bytes. It discusses local encoders\nsuch as ASCII and GB-2312, which encode specific characters into shorter bytes,\nand universal encoders like UTF-8 and UTF-16, which can encode the complete\nUnicode set with greater space requirements and are gaining widespread\nacceptance. Other encoders, including SCSU, BOCU-1, and binary encoders,\nhowever, lack self-synchronizing capabilities. Duncode is introduced as an\ninnovative encoding method that aims to encode the entire Unicode character set\nwith high space efficiency, akin to local encoders. It has the potential to\ncompress multiple characters of a string into a Duncode unit using fewer bytes.\nDespite offering less self-synchronizing identification information, Duncode\nsurpasses UTF8 in terms of space efficiency. The application is available at\n\\url{https://github.com/laohur/duncode}. Additionally, we have developed a\nbenchmark for evaluating character encoders across different languages. It\nencompasses 179 languages and can be accessed at\n\\url{https://github.com/laohur/wiki2txt}.\n","authors":["Changshang Xue"],"pdf_url":"https://arxiv.org/pdf/2307.05414v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05410v1","updated":"2023-07-11T16:25:09Z","published":"2023-07-11T16:25:09Z","title":"BLUEX: A benchmark based on Brazilian Leading Universities Entrance\n  eXams","summary":"  One common trend in recent studies of language models (LMs) is the use of\nstandardized tests for evaluation. However, despite being the fifth most spoken\nlanguage worldwide, few such evaluations have been conducted in Portuguese.\nThis is mainly due to the lack of high-quality datasets available to the\ncommunity for carrying out evaluations in Portuguese. To address this gap, we\nintroduce the Brazilian Leading Universities Entrance eXams (BLUEX), a dataset\nof entrance exams from the two leading universities in Brazil: UNICAMP and USP.\nThe dataset includes annotated metadata for evaluating the performance of NLP\nmodels on a variety of subjects. Furthermore, BLUEX includes a collection of\nrecently administered exams that are unlikely to be included in the training\ndata of many popular LMs as of 2023. The dataset is also annotated to indicate\nthe position of images in each question, providing a valuable resource for\nadvancing the state-of-the-art in multimodal language understanding and\nreasoning. We describe the creation and characteristics of BLUEX and establish\na benchmark through experiments with state-of-the-art LMs, demonstrating its\npotential for advancing the state-of-the-art in natural language understanding\nand reasoning in Portuguese. The data and relevant code can be found at\nhttps://github.com/Portuguese-Benchmark-Datasets/BLUEX\n","authors":["Thales Sales Almeida","Thiago Laitz","Giovana K. Bonás","Rodrigo Nogueira"],"pdf_url":"https://arxiv.org/pdf/2307.05410v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14793v2","updated":"2023-07-11T16:15:00Z","published":"2023-05-24T06:44:42Z","title":"Faithful Low-Resource Data-to-Text Generation through Cycle Training","summary":"  Methods to generate text from structured data have advanced significantly in\nrecent years, primarily due to fine-tuning of pre-trained language models on\nlarge datasets. However, such models can fail to produce output faithful to the\ninput data, particularly on out-of-domain data. Sufficient annotated data is\noften not available for specific domains, leading us to seek an unsupervised\napproach to improve the faithfulness of output text. Since the problem is\nfundamentally one of consistency between the representations of the structured\ndata and text, we evaluate the effectiveness of cycle training in this work.\nCycle training uses two models which are inverses of each other: one that\ngenerates text from structured data, and one which generates the structured\ndata from natural language text. We show that cycle training, when initialized\nwith a small amount of supervised data (100 samples in our case), achieves\nnearly the same performance as fully supervised approaches for the data-to-text\ngeneration task on the WebNLG, E2E, WTQ, and WSQL datasets. We perform\nextensive empirical analysis with automated evaluation metrics and a newly\ndesigned human evaluation schema to reveal different cycle training strategies'\neffectiveness of reducing various types of generation errors. Our code is\npublicly available at https://github.com/Edillower/CycleNLG.\n","authors":["Zhuoer Wang","Marcus Collins","Nikhita Vedula","Simone Filice","Shervin Malmasi","Oleg Rokhlenko"],"pdf_url":"https://arxiv.org/pdf/2305.14793v2.pdf","comment":"19 pages, 4 figures, ACL 2023"},{"id":"http://arxiv.org/abs/2307.05354v1","updated":"2023-07-11T15:44:01Z","published":"2023-07-11T15:44:01Z","title":"GujiBERT and GujiGPT: Construction of Intelligent Information Processing\n  Foundation Language Models for Ancient Texts","summary":"  In the context of the rapid development of large language models, we have\nmeticulously trained and introduced the GujiBERT and GujiGPT language models,\nwhich are foundational models specifically designed for intelligent information\nprocessing of ancient texts. These models have been trained on an extensive\ndataset that encompasses both simplified and traditional Chinese characters,\nallowing them to effectively handle various natural language processing tasks\nrelated to ancient books, including but not limited to automatic sentence\nsegmentation, punctuation, word segmentation, part-of-speech tagging, entity\nrecognition, and automatic translation. Notably, these models have exhibited\nexceptional performance across a range of validation tasks using publicly\navailable datasets. Our research findings highlight the efficacy of employing\nself-supervised methods to further train the models using classical text\ncorpora, thus enhancing their capability to tackle downstream tasks. Moreover,\nit is worth emphasizing that the choice of font, the scale of the corpus, and\nthe initial model selection all exert significant influence over the ultimate\nexperimental outcomes. To cater to the diverse text processing preferences of\nresearchers in digital humanities and linguistics, we have developed three\ndistinct categories comprising a total of nine model variations. We believe\nthat by sharing these foundational language models specialized in the domain of\nancient texts, we can facilitate the intelligent processing and scholarly\nexploration of ancient literary works and, consequently, contribute to the\nglobal dissemination of China's rich and esteemed traditional culture in this\nnew era.\n","authors":["Dongbo Wang","Chang Liu","Zhixiao Zhao","Si Shen","Liu Liu","Bin Li","Haotian Hu","Mengcheng Wu","Litao Lin","Xue Zhao","Xiyu Wang"],"pdf_url":"https://arxiv.org/pdf/2307.05354v1.pdf","comment":"22pages,0 figure"},{"id":"http://arxiv.org/abs/2307.05337v1","updated":"2023-07-11T15:26:49Z","published":"2023-07-11T15:26:49Z","title":"Explaining Competitive-Level Programming Solutions using LLMs","summary":"  In this paper, we approach competitive-level programming problem-solving as a\ncomposite task of reasoning and code generation. We propose a novel method to\nautomatically annotate natural language explanations to \\textit{<problem,\nsolution>} pairs. We show that despite poor performance in solving\ncompetitive-level programming problems, state-of-the-art LLMs exhibit a strong\ncapacity in describing and explaining solutions. Our explanation generation\nmethodology can generate a structured solution explanation for the problem\ncontaining descriptions and analysis. To evaluate the quality of the annotated\nexplanations, we examine their effectiveness in two aspects: 1) satisfying the\nhuman programming expert who authored the oracle solution, and 2) aiding LLMs\nin solving problems more effectively. The experimental results on the\nCodeContests dataset demonstrate that while LLM GPT3.5's and GPT-4's abilities\nin describing the solution are comparable, GPT-4 shows a better understanding\nof the key idea behind the solution.\n","authors":["Jierui Li","Szymon Tworkowski","Yingying Wu","Raymond Mooney"],"pdf_url":"https://arxiv.org/pdf/2307.05337v1.pdf","comment":"14 pages, presented at the 1st NLRSE workshop"},{"id":"http://arxiv.org/abs/2305.15066v2","updated":"2023-07-11T15:08:00Z","published":"2023-05-24T11:53:19Z","title":"GPT4Graph: Can Large Language Models Understand Graph Structured Data ?\n  An Empirical Evaluation and Benchmarking","summary":"  Large language models~(LLM) like ChatGPT have become indispensable to\nartificial general intelligence~(AGI), demonstrating excellent performance in\nvarious natural language processing tasks. In the real world, graph data is\nubiquitous and an essential part of AGI and prevails in domains like social\nnetwork analysis, bioinformatics and recommender systems. The training corpus\nof large language models often includes some algorithmic components, which\nallows them to achieve certain effects on some graph data-related problems.\nHowever, there is still little research on their performance on a broader range\nof graph-structured data. In this study, we conduct an extensive investigation\nto assess the proficiency of LLMs in comprehending graph data, employing a\ndiverse range of structural and semantic-related tasks. Our analysis\nencompasses 10 distinct tasks that evaluate the LLMs' capabilities in graph\nunderstanding. Through our study, we not only uncover the current limitations\nof language models in comprehending graph structures and performing associated\nreasoning tasks but also emphasize the necessity for further advancements and\nnovel approaches to enhance their graph processing capabilities. Our findings\ncontribute valuable insights towards bridging the gap between language models\nand graph understanding, paving the way for more effective graph mining and\nknowledge extraction.\n","authors":["Jiayan Guo","Lun Du","Hengyu Liu","Mengyu Zhou","Xinyi He","Shi Han"],"pdf_url":"https://arxiv.org/pdf/2305.15066v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05300v1","updated":"2023-07-11T14:45:19Z","published":"2023-07-11T14:45:19Z","title":"Unleashing Cognitive Synergy in Large Language Models: A Task-Solving\n  Agent through Multi-Persona Self-Collaboration","summary":"  Human intelligence thrives on the concept of cognitive synergy, where\ncollaboration and information integration among different cognitive processes\nyield superior outcomes compared to individual cognitive processes in\nisolation. Although Large Language Models (LLMs) have demonstrated promising\nperformance as general task-solving agents, they still struggle with tasks that\nrequire intensive domain knowledge and complex reasoning. In this work, we\npropose Solo Performance Prompting (SPP), which transforms a single LLM into a\ncognitive synergist by engaging in multi-turn self-collaboration with multiple\npersonas. A cognitive synergist refers to an intelligent agent that\ncollaborates with multiple minds, combining their individual strengths and\nknowledge, to enhance problem-solving and overall performance in complex tasks.\nBy dynamically identifying and simulating different personas based on task\ninputs, SPP unleashes the potential of cognitive synergy in LLMs. We have\ndiscovered that assigning multiple, fine-grained personas in LLMs elicits\nbetter problem-solving abilities compared to using a single or fixed number of\npersonas. We evaluate SPP on three challenging tasks: Trivia Creative Writing,\nCodenames Collaborative, and Logic Grid Puzzle, encompassing both\nknowledge-intensive and reasoning-intensive types. Unlike previous works, such\nas Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, SPP\neffectively elicits internal knowledge acquisition abilities, reduces\nhallucination, and maintains strong reasoning capabilities. Code, data, and\nprompts can be found at:\nhttps://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.\n","authors":["Zhenhailong Wang","Shaoguang Mao","Wenshan Wu","Tao Ge","Furu Wei","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2307.05300v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2301.06024v3","updated":"2023-07-11T14:20:54Z","published":"2023-01-15T06:25:50Z","title":"A data science and machine learning approach to continuous analysis of\n  Shakespeare's plays","summary":"  The availability of quantitative text analysis methods has provided new ways\nof analyzing literature in a manner that was not available in the\npre-information era. Here we apply comprehensive machine learning analysis to\nthe work of William Shakespeare. The analysis shows clear changes in the style\nof writing over time, with the most significant changes in the sentence length,\nfrequency of adjectives and adverbs, and the sentiments expressed in the text.\nApplying machine learning to make a stylometric prediction of the year of the\nplay shows a Pearson correlation of 0.71 between the actual and predicted year,\nindicating that Shakespeare's writing style as reflected by the quantitative\nmeasurements changed over time. Additionally, it shows that the stylometrics of\nsome of the plays is more similar to plays written either before or after the\nyear they were written. For instance, Romeo and Juliet is dated 1596, but is\nmore similar in stylometrics to plays written by Shakespeare after 1600. The\nsource code for the analysis is available for free download.\n","authors":["Charles Swisher","Lior Shamir"],"pdf_url":"https://arxiv.org/pdf/2301.06024v3.pdf","comment":"Journal of Data Mining and Digital Humanities, accepted"},{"id":"http://arxiv.org/abs/2307.05260v1","updated":"2023-07-11T13:51:12Z","published":"2023-07-11T13:51:12Z","title":"U-CREAT: Unsupervised Case Retrieval using Events extrAcTion","summary":"  The task of Prior Case Retrieval (PCR) in the legal domain is about\nautomatically citing relevant (based on facts and precedence) prior legal cases\nin a given query case. To further promote research in PCR, in this paper, we\npropose a new large benchmark (in English) for the PCR task: IL-PCR (Indian\nLegal Prior Case Retrieval) corpus. Given the complex nature of case relevance\nand the long size of legal documents, BM25 remains a strong baseline for\nranking the cited prior documents. In this work, we explore the role of events\nin legal case retrieval and propose an unsupervised retrieval method-based\npipeline U-CREAT (Unsupervised Case Retrieval using Events Extraction). We find\nthat the proposed unsupervised retrieval method significantly increases\nperformance compared to BM25 and makes retrieval faster by a considerable\nmargin, making it applicable to real-time case retrieval systems. Our proposed\nsystem is generic, we show that it generalizes across two different legal\nsystems (Indian and Canadian), and it shows state-of-the-art performance on the\nbenchmarks for both the legal systems (IL-PCR and COLIEE corpora).\n","authors":["Abhinav Joshi","Akshat Sharma","Sai Kiran Tanikella","Ashutosh Modi"],"pdf_url":"https://arxiv.org/pdf/2307.05260v1.pdf","comment":"Accepted at ACL 2023, 15 pages (12 main + 3 Appendix)"},{"id":"http://arxiv.org/abs/2306.09841v2","updated":"2023-07-11T13:41:20Z","published":"2023-06-16T13:39:35Z","title":"Are Large Language Models Really Good Logical Reasoners? A Comprehensive\n  Evaluation and Beyond","summary":"  Logical reasoning consistently plays a fundamental and significant role in\nthe domains of knowledge engineering and artificial intelligence. Recently,\nLarge Language Models (LLMs) have emerged as a noteworthy innovation in natural\nlanguage processing (NLP), exhibiting impressive achievements across various\nclassic NLP tasks. However, the question of whether LLMs can effectively\naddress the task of logical reasoning, which requires gradual cognitive\ninference similar to human intelligence, remains unanswered. To this end, we\naim to bridge this gap and provide comprehensive evaluations in this paper.\nFirstly, to offer systematic evaluations, we select fifteen typical logical\nreasoning datasets and organize them into deductive, inductive, abductive and\nmixed-form reasoning settings. Considering the comprehensiveness of\nevaluations, we include three representative LLMs (i.e., text-davinci-003,\nChatGPT and BARD) and evaluate them on all selected datasets under zero-shot,\none-shot and three-shot settings. Secondly, different from previous evaluations\nrelying only on simple metrics (e.g., accuracy), we propose fine-level\nevaluations from objective and subjective manners, covering both answers and\nexplanations. Additionally, to uncover the logical flaws of LLMs, problematic\ncases will be attributed to five error types from two dimensions, i.e.,\nevidence selection process and reasoning process. Thirdly, to avoid the\ninfluences of knowledge bias and purely focus on benchmarking the logical\nreasoning capability of LLMs, we propose a new dataset with neutral content. It\ncontains 3,000 samples and covers deductive, inductive and abductive settings.\nBased on the in-depth evaluations, this paper finally forms a general\nevaluation scheme of logical reasoning capability from six dimensions. It\nreflects the pros and cons of LLMs and gives guiding directions for future\nworks.\n","authors":["Fangzhi Xu","Qika Lin","Jiawei Han","Tianzhe Zhao","Jun Liu","Erik Cambria"],"pdf_url":"https://arxiv.org/pdf/2306.09841v2.pdf","comment":"14 pages, 11 figures"},{"id":"http://arxiv.org/abs/2306.08702v2","updated":"2023-07-11T13:10:19Z","published":"2023-06-14T19:00:12Z","title":"Does mBERT understand Romansh? Evaluating word embeddings using word\n  alignment","summary":"  We test similarity-based word alignment models (SimAlign and awesome-align)\nin combination with word embeddings from mBERT and XLM-R on parallel sentences\nin German and Romansh. Since Romansh is an unseen language, we are dealing with\na zero-shot setting. Using embeddings from mBERT, both models reach an\nalignment error rate of 0.22, which outperforms fast_align, a statistical\nmodel, and is on par with similarity-based word alignment for seen languages.\nWe interpret these results as evidence that mBERT contains information that can\nbe meaningful and applicable to Romansh.\n  To evaluate performance, we also present a new trilingual corpus, which we\ncall the DERMIT (DE-RM-IT) corpus, containing press releases made by the Canton\nof Grisons in German, Romansh and Italian in the past 25 years. The corpus\ncontains 4 547 parallel documents and approximately 100 000 sentence pairs in\neach language combination. We additionally present a gold standard for\nGerman-Romansh word alignment. The data is available at\nhttps://github.com/eyldlv/DERMIT-Corpus.\n","authors":["Eyal Liron Dolev"],"pdf_url":"https://arxiv.org/pdf/2306.08702v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05228v1","updated":"2023-07-11T12:48:55Z","published":"2023-07-11T12:48:55Z","title":"Attribute Controlled Dialogue Prompting","summary":"  Prompt-tuning has become an increasingly popular parameter-efficient method\nfor adapting large pretrained language models to downstream tasks. However,\nboth discrete prompting and continuous prompting assume fixed prompts for all\ndata samples within a task, neglecting the fact that inputs vary greatly in\nsome tasks such as open-domain dialogue generation. In this paper, we present a\nnovel, instance-specific prompt-tuning algorithm for dialogue generation.\nSpecifically, we generate prompts based on instance-level control code, rather\nthan the conversation history, to explore their impact on controlled dialogue\ngeneration. Experiments on popular open-domain dialogue datasets, evaluated on\nboth automated metrics and human evaluation, demonstrate that our method is\nsuperior to prompting baselines and comparable to fine-tuning with only 5%-6%\nof total parameters.\n","authors":["Runcheng Liu","Ahmad Rashid","Ivan Kobyzev","Mehdi Rezagholizadeh","Pascal Poupart"],"pdf_url":"https://arxiv.org/pdf/2307.05228v1.pdf","comment":"Accepted at ACL 2023 In Findings"},{"id":"http://arxiv.org/abs/2206.02892v2","updated":"2023-07-11T12:44:20Z","published":"2022-06-06T20:32:58Z","title":"Discriminative Models Can Still Outperform Generative Models in Aspect\n  Based Sentiment Analysis","summary":"  Aspect-based Sentiment Analysis (ABSA) helps to explain customers' opinions\ntowards products and services. In the past, ABSA models were discriminative,\nbut more recently generative models have been used to generate aspects and\npolarities directly from text. In contrast, discriminative models commonly\nfirst select aspects from the text, and then classify the aspect's polarity.\nPrevious results showed that generative models outperform discriminative models\non several English ABSA datasets. Here, we evaluate and contrast two\nstate-of-the-art discriminative and generative models in several settings:\ncross-lingual, cross-domain, and cross-lingual and domain, to understand\ngeneralizability in settings other than English mono-lingual in-domain. Our\nmore thorough evaluation shows that, contrary to previous studies,\ndiscriminative models can still outperform generative models in almost all\nsettings.\n","authors":["Dhruv Mullick","Alona Fyshe","Bilal Ghanem"],"pdf_url":"https://arxiv.org/pdf/2206.02892v2.pdf","comment":"Submission for work done up till December 2022"},{"id":"http://arxiv.org/abs/2307.05646v1","updated":"2023-07-11T12:43:28Z","published":"2023-07-11T12:43:28Z","title":"Better Handling Coreference Resolution in Aspect Level Sentiment\n  Classification by Fine-Tuning Language Models","summary":"  Customer feedback is invaluable to companies as they refine their products.\nMonitoring customer feedback can be automated with Aspect Level Sentiment\nClassification (ALSC) which allows us to analyse specific aspects of the\nproducts in reviews. Large Language Models (LLMs) are the heart of many\nstate-of-the-art ALSC solutions, but they perform poorly in some scenarios\nrequiring Coreference Resolution (CR). In this work, we propose a framework to\nimprove an LLM's performance on CR-containing reviews by fine tuning on highly\ninferential tasks. We show that the performance improvement is likely\nattributed to the improved model CR ability. We also release a new dataset that\nfocuses on CR in ALSC.\n","authors":["Dhruv Mullick","Bilal Ghanem","Alona Fyshe"],"pdf_url":"https://arxiv.org/pdf/2307.05646v1.pdf","comment":"Work done up till December 2022"},{"id":"http://arxiv.org/abs/2307.05174v1","updated":"2023-07-11T11:12:06Z","published":"2023-07-11T11:12:06Z","title":"Mao-Zedong At SemEval-2023 Task 4: Label Represention Multi-Head\n  Attention Model With Contrastive Learning-Enhanced Nearest Neighbor Mechanism\n  For Multi-Label Text Classification","summary":"  The study of human values is essential in both practical and theoretical\ndomains. With the development of computational linguistics, the creation of\nlarge-scale datasets has made it possible to automatically recognize human\nvalues accurately. SemEval 2023 Task 4\\cite{kiesel:2023} provides a set of\narguments and 20 types of human values that are implicitly expressed in each\nargument. In this paper, we present our team's solution. We use the\nRoberta\\cite{liu_roberta_2019} model to obtain the word vector encoding of the\ndocument and propose a multi-head attention mechanism to establish connections\nbetween specific labels and semantic components. Furthermore, we use a\ncontrastive learning-enhanced K-nearest neighbor\nmechanism\\cite{su_contrastive_2022} to leverage existing instance information\nfor prediction. Our approach achieved an F1 score of 0.533 on the test set and\nranked fourth on the leaderboard.\n","authors":["Che Zhang","Ping'an Liu","Zhenyang Xiao","Haojun Fei"],"pdf_url":"https://arxiv.org/pdf/2307.05174v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05162v1","updated":"2023-07-11T10:38:58Z","published":"2023-07-11T10:38:58Z","title":"SuryaKiran at MEDIQA-Sum 2023: Leveraging LoRA for Clinical Dialogue\n  Summarization","summary":"  Finetuning Large Language Models helps improve the results for\ndomain-specific use cases. End-to-end finetuning of large language models is\ntime and resource intensive and has high storage requirements to store the\nfinetuned version of the large language model. Parameter Efficient Fine Tuning\n(PEFT) methods address the time and resource challenges by keeping the large\nlanguage model as a fixed base and add additional layers, which the PEFT\nmethods finetune. This paper demonstrates the evaluation results for one such\nPEFT method Low Rank Adaptation (LoRA), for Clinical Dialogue Summarization.\nThe evaluation results show that LoRA works at par with end-to-end finetuning\nfor a large language model. The paper presents the evaluations done for solving\nboth the Subtask A and B from ImageCLEFmedical\n{https://www.imageclef.org/2023/medical}\n","authors":["Kunal Suri","Prakhar Mishra","Saumajit Saha","Atul Singh"],"pdf_url":"https://arxiv.org/pdf/2307.05162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.06385v2","updated":"2023-07-11T09:49:50Z","published":"2022-12-13T05:46:40Z","title":"TencentPretrain: A Scalable and Flexible Toolkit for Pre-training Models\n  of Different Modalities","summary":"  Recently, the success of pre-training in text domain has been fully extended\nto vision, audio, and cross-modal scenarios. The proposed pre-training models\nof different modalities are showing a rising trend of homogeneity in their\nmodel structures, which brings the opportunity to implement different\npre-training models within a uniform framework. In this paper, we present\nTencentPretrain, a toolkit supporting pre-training models of different\nmodalities. The core feature of TencentPretrain is the modular design. The\ntoolkit uniformly divides pre-training models into 5 components: embedding,\nencoder, target embedding, decoder, and target. As almost all of common modules\nare provided in each component, users can choose the desired modules from\ndifferent components to build a complete pre-training model. The modular design\nenables users to efficiently reproduce existing pre-training models or build\nbrand-new one. We test the toolkit on text, vision, and audio benchmarks and\nshow that it can match the performance of the original implementations.\n","authors":["Zhe Zhao","Yudong Li","Cheng Hou","Jing Zhao","Rong Tian","Weijie Liu","Yiren Chen","Ningyuan Sun","Haoyan Liu","Weiquan Mao","Han Guo","Weigang Guo","Taiqiang Wu","Tao Zhu","Wenhang Shi","Chen Chen","Shan Huang","Sihong Chen","Liqun Liu","Feifei Li","Xiaoshuai Chen","Xingwu Sun","Zhanhui Kang","Xiaoyong Du","Linlin Shen","Kimmo Yan"],"pdf_url":"https://arxiv.org/pdf/2212.06385v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05134v1","updated":"2023-07-11T09:23:05Z","published":"2023-07-11T09:23:05Z","title":"TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation","summary":"  The progress in the generation of synthetic images has made it crucial to\nassess their quality. While several metrics have been proposed to assess the\nrendering of images, it is crucial for Text-to-Image (T2I) models, which\ngenerate images based on a prompt, to consider additional aspects such as to\nwhich extent the generated image matches the important content of the prompt.\nMoreover, although the generated images usually result from a random starting\npoint, the influence of this one is generally not considered. In this article,\nwe propose a new metric based on prompt templates to study the alignment\nbetween the content specified in the prompt and the corresponding generated\nimages. It allows us to better characterize the alignment in terms of the type\nof the specified objects, their number, and their color. We conducted a study\non several recent T2I models about various aspects. An additional interesting\nresult we obtained with our approach is that image quality can vary drastically\ndepending on the latent noise used as a seed for the images. We also quantify\nthe influence of the number of concepts in the prompt, their order as well as\ntheir (color) attributes. Finally, our method allows us to identify some latent\nseeds that produce better images than others, opening novel directions of\nresearch on this understudied topic.\n","authors":["Paul Grimal","Hervé Le Borgne","Olivier Ferret","Julien Tourille"],"pdf_url":"https://arxiv.org/pdf/2307.05134v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05131v1","updated":"2023-07-11T09:20:33Z","published":"2023-07-11T09:20:33Z","title":"Overview of BioASQ 2023: The eleventh BioASQ challenge on Large-Scale\n  Biomedical Semantic Indexing and Question Answering","summary":"  This is an overview of the eleventh edition of the BioASQ challenge in the\ncontext of the Conference and Labs of the Evaluation Forum (CLEF) 2023. BioASQ\nis a series of international challenges promoting advances in large-scale\nbiomedical semantic indexing and question answering. This year, BioASQ\nconsisted of new editions of the two established tasks b and Synergy, and a new\ntask (MedProcNER) on semantic annotation of clinical content in Spanish with\nmedical procedures, which have a critical role in medical practice. In this\nedition of BioASQ, 28 competing teams submitted the results of more than 150\ndistinct systems in total for the three different shared tasks of the\nchallenge. Similarly to previous editions, most of the participating systems\nachieved competitive performance, suggesting the continuous advancement of the\nstate-of-the-art in the field.\n","authors":["Anastasios Nentidis","Georgios Katsimpras","Anastasia Krithara","Salvador Lima López","Eulália Farré-Maduell","Luis Gasco","Martin Krallinger","Georgios Paliouras"],"pdf_url":"https://arxiv.org/pdf/2307.05131v1.pdf","comment":"24 pages, 12 tables, 3 figures. CLEF2023. arXiv admin note: text\n  overlap with arXiv:2210.06852"},{"id":"http://arxiv.org/abs/2301.06862v2","updated":"2023-07-11T09:08:33Z","published":"2023-01-17T13:15:44Z","title":"Algorithms for Acyclic Weighted Finite-State Automata with Failure Arcs","summary":"  Weighted finite-state automata (WSFAs) are commonly used in NLP. Failure\ntransitions are a useful extension for compactly representing backoffs or\ninterpolation in $n$-gram models and CRFs, which are special cases of WFSAs.\nThe pathsum in ordinary acyclic WFSAs is efficiently computed by the backward\nalgorithm in time $O(|E|)$, where $E$ is the set of transitions. However, this\ndoes not allow failure transitions, and preprocessing the WFSA to eliminate\nfailure transitions could greatly increase $|E|$. We extend the backward\nalgorithm to handle failure transitions directly. Our approach is efficient\nwhen the average state has outgoing arcs for only a small fraction $s \\ll 1$ of\nthe alphabet $\\Sigma$. We propose an algorithm for general acyclic WFSAs which\nruns in $O{\\left(|E| + s |\\Sigma| |Q| T_\\text{max} \\log{|\\Sigma|}\\right)}$,\nwhere $Q$ is the set of states and $T_\\text{max}$ is the size of the largest\nconnected component of failure transitions. When the failure transition\ntopology satisfies a condition exemplified by CRFs, the $T_\\text{max}$ factor\ncan be dropped, and when the weight semiring is a ring, the $\\log{|\\Sigma|}$\nfactor can be dropped. In the latter case (ring-weighted acyclic WFSAs), we\nalso give an alternative algorithm with complexity $\\displaystyle O{\\left(|E| +\n|\\Sigma| |Q| \\min(1,s\\pi_\\text{max}) \\right)}$, where $\\pi_\\text{max}$ is the\nsize of the longest failure path.\n","authors":["Anej Svete","Benjamin Dayan","Tim Vieira","Ryan Cotterell","Jason Eisner"],"pdf_url":"https://arxiv.org/pdf/2301.06862v2.pdf","comment":"9 pages, Proceedings of EMNLP 2022"},{"id":"http://arxiv.org/abs/2307.05113v1","updated":"2023-07-11T08:45:46Z","published":"2023-07-11T08:45:46Z","title":"Beyond the Obvious: Evaluating the Reasoning Ability In Real-life\n  Scenarios of Language Models on Life Scapes Reasoning\n  Benchmark~(LSR-Benchmark)","summary":"  This paper introduces the Life Scapes Reasoning Benchmark (LSR-Benchmark), a\nnovel dataset targeting real-life scenario reasoning, aiming to close the gap\nin artificial neural networks' ability to reason in everyday contexts. In\ncontrast to domain knowledge reasoning datasets, LSR-Benchmark comprises\nfree-text formatted questions with rich information on real-life scenarios,\nhuman behaviors, and character roles. The dataset consists of 2,162 questions\ncollected from open-source online sources and is manually annotated to improve\nits quality. Experiments are conducted using state-of-the-art language models,\nsuch as gpt3.5-turbo and instruction fine-tuned llama models, to test the\nperformance in LSR-Benchmark. The results reveal that humans outperform these\nmodels significantly, indicating a persisting challenge for machine learning\nmodels in comprehending daily human life.\n","authors":["Zhouhong Gu","Zihan Li","Lin Zhang","Zhuozhi Xiong","Sihang Jiang","Xiaoxuan Zhu","Shusen Wang","Zili Wang","Jianchen Wang","Haoning Ye","Wenhao Huang","Yikai Zhang","Hongwei Feng","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2307.05113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05083v1","updated":"2023-07-11T07:32:12Z","published":"2023-07-11T07:32:12Z","title":"Vacaspati: A Diverse Corpus of Bangla Literature","summary":"  Bangla (or Bengali) is the fifth most spoken language globally; yet, the\nstate-of-the-art NLP in Bangla is lagging for even simple tasks such as\nlemmatization, POS tagging, etc. This is partly due to lack of a varied quality\ncorpus. To alleviate this need, we build Vacaspati, a diverse corpus of Bangla\nliterature. The literary works are collected from various websites; only those\nworks that are publicly available without copyright violations or restrictions\nare collected. We believe that published literature captures the features of a\nlanguage much better than newspapers, blogs or social media posts which tend to\nfollow only a certain literary pattern and, therefore, miss out on language\nvariety. Our corpus Vacaspati is varied from multiple aspects, including type\nof composition, topic, author, time, space, etc. It contains more than 11\nmillion sentences and 115 million words. We also built a word embedding model,\nVac-FT, using FastText from Vacaspati as well as trained an Electra model,\nVac-BERT, using the corpus. Vac-BERT has far fewer parameters and requires only\na fraction of resources compared to other state-of-the-art transformer models\nand yet performs either better or similar on various downstream tasks. On\nmultiple downstream tasks, Vac-FT outperforms other FastText-based models. We\nalso demonstrate the efficacy of Vacaspati as a corpus by showing that similar\nmodels built from other corpora are not as effective. The models are available\nat https://bangla.iitk.ac.in/.\n","authors":["Pramit Bhattacharyya","Joydeep Mondal","Subhadip Maji","Arnab Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2307.05083v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05082v1","updated":"2023-07-11T07:31:58Z","published":"2023-07-11T07:31:58Z","title":"OntoChatGPT Information System: Ontology-Driven Structured Prompts for\n  ChatGPT Meta-Learning","summary":"  This research presents a comprehensive methodology for utilizing an\nontology-driven structured prompts system in interplay with ChatGPT, a widely\nused large language model (LLM). The study develops formal models, both\ninformation and functional, and establishes the methodological foundations for\nintegrating ontology-driven prompts with ChatGPT's meta-learning capabilities.\nThe resulting productive triad comprises the methodological foundations,\nadvanced information technology, and the OntoChatGPT system, which collectively\nenhance the effectiveness and performance of chatbot systems. The\nimplementation of this technology is demonstrated using the Ukrainian language\nwithin the domain of rehabilitation. By applying the proposed methodology, the\nOntoChatGPT system effectively extracts entities from contexts, classifies\nthem, and generates relevant responses. The study highlights the versatility of\nthe methodology, emphasizing its applicability not only to ChatGPT but also to\nother chatbot systems based on LLMs, such as Google's Bard utilizing the PaLM 2\nLLM. The underlying principles of meta-learning, structured prompts, and\nontology-driven information retrieval form the core of the proposed\nmethodology, enabling their adaptation and utilization in various LLM-based\nsystems. This versatile approach opens up new possibilities for NLP and\ndialogue systems, empowering developers to enhance the performance and\nfunctionality of chatbot systems across different domains and languages.\n","authors":["Oleksandr Palagin","Vladislav Kaverinskiy","Anna Litvin","Kyrylo Malakhov"],"pdf_url":"https://arxiv.org/pdf/2307.05082v1.pdf","comment":"14 pages, 1 figure. Published. International Journal of Computing,\n  22(2), 170-183. https://doi.org/10.47839/ijc.22.2.3086"},{"id":"http://arxiv.org/abs/2307.05081v1","updated":"2023-07-11T07:29:18Z","published":"2023-07-11T07:29:18Z","title":"Argumentative Segmentation Enhancement for Legal Summarization","summary":"  We use the combination of argumentative zoning [1] and a legal argumentative\nscheme to create legal argumentative segments. Based on the argumentative\nsegmentation, we propose a novel task of classifying argumentative segments of\nlegal case decisions. GPT-3.5 is used to generate summaries based on\nargumentative segments. In terms of automatic evaluation metrics, our method\ngenerates higher quality argumentative summaries while leaving out less\nrelevant context as compared to GPT-4 and non-GPT models.\n","authors":["Huihui Xu","Kevin Ashley"],"pdf_url":"https://arxiv.org/pdf/2307.05081v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05052v1","updated":"2023-07-11T07:03:29Z","published":"2023-07-11T07:03:29Z","title":"Towards Understanding In-Context Learning with Contrastive\n  Demonstrations and Saliency Maps","summary":"  We investigate the role of various demonstration components in the in-context\nlearning (ICL) performance of large language models (LLMs). Specifically, we\nexplore the impacts of ground-truth labels, input distribution, and\ncomplementary explanations, particularly when these are altered or perturbed.\nWe build on previous work, which offers mixed findings on how these elements\ninfluence ICL. To probe these questions, we employ explainable NLP (XNLP)\nmethods and utilize saliency maps of contrastive demonstrations for both\nqualitative and quantitative analysis. Our findings reveal that flipping\nground-truth labels significantly affects the saliency, though it's more\nnoticeable in larger LLMs. Our analysis of the input distribution at a granular\nlevel reveals that changing sentiment-indicative terms in a sentiment analysis\ntask to neutral ones does not have as substantial an impact as altering\nground-truth labels. Finally, we find that the effectiveness of complementary\nexplanations in boosting ICL performance is task-dependent, with limited\nbenefits seen in sentiment analysis tasks compared to symbolic reasoning tasks.\nThese insights are critical for understanding the functionality of LLMs and\nguiding the development of effective demonstrations, which is increasingly\nrelevant in light of the growing use of LLMs in applications such as ChatGPT.\nOur research code is publicly available at https://github.com/paihengxu/XICL.\n","authors":["Zongxia Li","Paiheng Xu","Fuxiao Liu","Hyemi Song"],"pdf_url":"https://arxiv.org/pdf/2307.05052v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2307.05627v1","updated":"2023-07-11T06:27:13Z","published":"2023-07-11T06:27:13Z","title":"Separate-and-Aggregate: A Transformer-based Patch Refinement Model for\n  Knowledge Graph Completion","summary":"  Knowledge graph completion (KGC) is the task of inferencing missing facts\nfrom any given knowledge graphs (KG). Previous KGC methods typically represent\nknowledge graph entities and relations as trainable continuous embeddings and\nfuse the embeddings of the entity $h$ (or $t$) and relation $r$ into hidden\nrepresentations of query $(h, r, ?)$ (or $(?, r, t$)) to approximate the\nmissing entities. To achieve this, they either use shallow linear\ntransformations or deep convolutional modules. However, the linear\ntransformations suffer from the expressiveness issue while the deep\nconvolutional modules introduce unnecessary inductive bias, which could\npotentially degrade the model performance. Thus, we propose a novel\nTransformer-based Patch Refinement Model (PatReFormer) for KGC. PatReFormer\nfirst segments the embedding into a sequence of patches and then employs\ncross-attention modules to allow bi-directional embedding feature interaction\nbetween the entities and relations, leading to a better understanding of the\nunderlying KG. We conduct experiments on four popular KGC benchmarks, WN18RR,\nFB15k-237, YAGO37 and DB100K. The experimental results show significant\nperformance improvement from existing KGC methods on standard KGC evaluation\nmetrics, e.g., MRR and H@n. Our analysis first verifies the effectiveness of\nour model design choices in PatReFormer. We then find that PatReFormer can\nbetter capture KG information from a large relation embedding dimension.\nFinally, we demonstrate that the strength of PatReFormer is at complex relation\ntypes, compared to other KGC models\n","authors":["Chen Chen","Yufei Wang","Yang Zhang","Quan Z. Sheng","Kwok-Yan Lam"],"pdf_url":"https://arxiv.org/pdf/2307.05627v1.pdf","comment":"Accepted by ADMA 2023, oral"},{"id":"http://arxiv.org/abs/2212.09603v2","updated":"2023-07-11T05:17:19Z","published":"2022-12-19T16:41:19Z","title":"Explanation Regeneration via Information Bottleneck","summary":"  Explaining the black-box predictions of NLP models naturally and accurately\nis an important open problem in natural language generation. These free-text\nexplanations are expected to contain sufficient and carefully-selected evidence\nto form supportive arguments for predictions. Due to the superior generative\ncapacity of large pretrained language models, recent work built on prompt\nengineering enables explanation generation without specific training. However,\nexplanation generated through single-pass prompting often lacks sufficiency and\nconciseness. To address this problem, we develop an information bottleneck\nmethod EIB to produce refined explanations that are sufficient and concise. Our\napproach regenerates the free-text explanation by polishing the single-pass\noutput from the pretrained language model but retaining the information that\nsupports the contents being explained. Experiments on two out-of-domain tasks\nverify the effectiveness of EIB through automatic evaluation and\nthoroughly-conducted human evaluation.\n","authors":["Qintong Li","Zhiyong Wu","Lingpeng Kong","Wei Bi"],"pdf_url":"https://arxiv.org/pdf/2212.09603v2.pdf","comment":"Accepted in ACL2023 Findings"},{"id":"http://arxiv.org/abs/2307.02682v2","updated":"2023-07-11T04:10:49Z","published":"2023-07-05T23:01:26Z","title":"Zero-Shot Dense Video Captioning by Jointly Optimizing Text and Moment","summary":"  Dense video captioning, a task of localizing meaningful moments and\ngenerating relevant captions for videos, often requires a large, expensive\ncorpus of annotated video segments paired with text. In an effort to minimize\nthe annotation cost, we propose ZeroTA, a novel method for dense video\ncaptioning in a zero-shot manner. Our method does not require any videos or\nannotations for training; instead, it localizes and describes events within\neach input video at test time by optimizing solely on the input. This is\naccomplished by introducing a soft moment mask that represents a temporal\nsegment in the video and jointly optimizing it with the prefix parameters of a\nlanguage model. This joint optimization aligns a frozen language generation\nmodel (i.e., GPT-2) with a frozen vision-language contrastive model (i.e.,\nCLIP) by maximizing the matching score between the generated text and a moment\nwithin the video. We also introduce a pairwise temporal IoU loss to let a set\nof soft moment masks capture multiple distinct events within the video. Our\nmethod effectively discovers diverse significant events within the video, with\nthe resulting captions appropriately describing these events. The empirical\nresults demonstrate that ZeroTA surpasses zero-shot baselines and even\noutperforms the state-of-the-art few-shot method on the widely-used benchmark\nActivityNet Captions. Moreover, our method shows greater robustness compared to\nsupervised methods when evaluated in out-of-domain scenarios. This research\nprovides insight into the potential of aligning widely-used models, such as\nlanguage generation models and vision-language models, to unlock a new\ncapability: understanding temporal aspects of videos.\n","authors":["Yongrae Jo","Seongyun Lee","Aiden SJ Lee","Hyunji Lee","Hanseok Oh","Minjoon Seo"],"pdf_url":"https://arxiv.org/pdf/2307.02682v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05006v1","updated":"2023-07-11T03:57:00Z","published":"2023-07-11T03:57:00Z","title":"Improving RNN-Transducers with Acoustic LookAhead","summary":"  RNN-Transducers (RNN-Ts) have gained widespread acceptance as an end-to-end\nmodel for speech to text conversion because of their high accuracy and\nstreaming capabilities. A typical RNN-T independently encodes the input audio\nand the text context, and combines the two encodings by a thin joint network.\nWhile this architecture provides SOTA streaming accuracy, it also makes the\nmodel vulnerable to strong LM biasing which manifests as multi-step\nhallucination of text without acoustic evidence. In this paper we propose\nLookAhead that makes text representations more acoustically grounded by looking\nahead into the future within the audio input. This technique yields a\nsignificant 5%-20% relative reduction in word error rate on both in-domain and\nout-of-domain evaluation sets.\n","authors":["Vinit S. Unni","Ashish Mittal","Preethi Jyothi","Sunita Sarawagi"],"pdf_url":"https://arxiv.org/pdf/2307.05006v1.pdf","comment":"5 pages, 1 fig, 7 tables, Proceedings of Interspeech 2023"},{"id":"http://arxiv.org/abs/2201.09523v2","updated":"2023-07-11T03:26:54Z","published":"2022-01-24T08:34:41Z","title":"BTPK-based interpretable method for NER tasks based on Talmudic Public\n  Announcement Logic","summary":"  As one of the basic tasks in natural language processing (NLP), named entity\nrecognition (NER) is an important basic tool for downstream tasks of NLP, such\nas information extraction, syntactic analysis, machine translation and so on.\nThe internal operation logic of current name entity recognition model is\nblack-box to the user, so the user has no basis to determine which name entity\nmakes more sense. Therefore, a user-friendly explainable recognition process\nwould be very useful for many people. In this paper, we propose a novel\ninterpretable method, BTPK (Binary Talmudic Public Announcement Logic model),\nto help users understand the internal recognition logic of the name entity\nrecognition tasks based on Talmudic Public Announcement Logic. BTPK model can\nalso capture the semantic information in the input sentences, that is, the\ncontext dependency of the sentence. We observed the public announcement of BTPK\npresents the inner decision logic of BRNNs, and the explanations obtained from\na BTPK model show us how BRNNs essentially handle NER tasks.\n","authors":["Yulin Chen","Beishui Liao","Bruno Bentzen","Bo Yuan","Zelai Yao","Haixiao Chi","Dov Gabbay"],"pdf_url":"https://arxiv.org/pdf/2201.09523v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2307.04964v1","updated":"2023-07-11T01:55:24Z","published":"2023-07-11T01:55:24Z","title":"Secrets of RLHF in Large Language Models Part I: PPO","summary":"  Large language models (LLMs) have formulated a blueprint for the advancement\nof artificial general intelligence. Its primary objective is to function as a\nhuman-centric (helpful, honest, and harmless) assistant. Alignment with humans\nassumes paramount significance, and reinforcement learning with human feedback\n(RLHF) emerges as the pivotal technological paradigm underpinning this pursuit.\nCurrent technical routes usually include \\textbf{reward models} to measure\nhuman preferences, \\textbf{Proximal Policy Optimization} (PPO) to optimize\npolicy model outputs, and \\textbf{process supervision} to improve step-by-step\nreasoning capabilities. However, due to the challenges of reward design,\nenvironment interaction, and agent training, coupled with huge trial and error\ncost of large language models, there is a significant barrier for AI\nresearchers to motivate the development of technical alignment and safe landing\nof LLMs. The stable training of RLHF has still been a puzzle. In the first\nreport, we dissect the framework of RLHF, re-evaluate the inner workings of\nPPO, and explore how the parts comprising PPO algorithms impact policy agent\ntraining. We identify policy constraints being the key factor for the effective\nimplementation of the PPO algorithm. Therefore, we explore the PPO-max, an\nadvanced version of PPO algorithm, to efficiently improve the training\nstability of the policy model. Based on our main results, we perform a\ncomprehensive analysis of RLHF abilities compared with SFT models and ChatGPT.\nThe absence of open-source implementations has posed significant challenges to\nthe investigation of LLMs alignment. Therefore, we are eager to release\ntechnical reports, reward models and PPO codes\n","authors":["Rui Zheng","Shihan Dou","Songyang Gao","Wei Shen","Binghai Wang","Yan Liu","Senjie Jin","Qin Liu","Limao Xiong","Lu Chen","Zhiheng Xi","Yuhao Zhou","Nuo Xu","Wenbin Lai","Minghao Zhu","Rongxiang Weng","Wensen Cheng","Cheng Chang","Zhangyue Yin","Yuan Hua","Haoran Huang","Tianxiang Sun","Hang Yan","Tao Gui","Qi Zhang","Xipeng Qiu","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2307.04964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.04963v1","updated":"2023-07-11T01:53:19Z","published":"2023-07-11T01:53:19Z","title":"DyCL: Dynamic Neural Network Compilation Via Program Rewriting and Graph\n  Optimization","summary":"  DL compiler's primary function is to translate DNN programs written in\nhigh-level DL frameworks such as PyTorch and TensorFlow into portable\nexecutables. These executables can then be flexibly executed by the deployed\nhost programs. However, existing DL compilers rely on a tracing mechanism,\nwhich involves feeding a runtime input to a neural network program and tracing\nthe program execution paths to generate the computational graph necessary for\ncompilation. Unfortunately, this mechanism falls short when dealing with modern\ndynamic neural networks (DyNNs) that possess varying computational graphs\ndepending on the inputs. Consequently, conventional DL compilers struggle to\naccurately compile DyNNs into executable code. To address this limitation, we\npropose \\tool, a general approach that enables any existing DL compiler to\nsuccessfully compile DyNNs. \\tool tackles the dynamic nature of DyNNs by\nintroducing a compilation mechanism that redistributes the control and data\nflow of the original DNN programs during the compilation process. Specifically,\n\\tool develops program analysis and program transformation techniques to\nconvert a dynamic neural network into multiple sub-neural networks. Each\nsub-neural network is devoid of conditional statements and is compiled\nindependently. Furthermore, \\tool synthesizes a host module that models the\ncontrol flow of the DyNNs and facilitates the invocation of the sub-neural\nnetworks. Our evaluation demonstrates the effectiveness of \\tool, achieving a\n100\\% success rate in compiling all dynamic neural networks. Moreover, the\ncompiled executables generated by \\tool exhibit significantly improved\nperformance, running between $1.12\\times$ and $20.21\\times$ faster than the\noriginal DyNNs executed on general-purpose DL frameworks.\n","authors":["Simin Chen","Shiyi Wei","Cong Liu","Wei Yang"],"pdf_url":"https://arxiv.org/pdf/2307.04963v1.pdf","comment":"This paper has been accepted to ISSTA 2023"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2307.04106v2","updated":"2023-07-11T23:55:53Z","published":"2023-07-09T06:07:22Z","title":"Parametric Depth Based Feature Representation Learning for Object\n  Detection and Segmentation in Bird's Eye View","summary":"  Recent vision-only perception models for autonomous driving achieved\npromising results by encoding multi-view image features into Bird's-Eye-View\n(BEV) space. A critical step and the main bottleneck of these methods is\ntransforming image features into the BEV coordinate frame. This paper focuses\non leveraging geometry information, such as depth, to model such feature\ntransformation. Existing works rely on non-parametric depth distribution\nmodeling leading to significant memory consumption, or ignore the geometry\ninformation to address this problem. In contrast, we propose to use parametric\ndepth distribution modeling for feature transformation. We first lift the 2D\nimage features to the 3D space defined for the ego vehicle via a predicted\nparametric depth distribution for each pixel in each view. Then, we aggregate\nthe 3D feature volume based on the 3D space occupancy derived from depth to the\nBEV frame. Finally, we use the transformed features for downstream tasks such\nas object detection and semantic segmentation. Existing semantic segmentation\nmethods do also suffer from an hallucination problem as they do not take\nvisibility information into account. This hallucination can be particularly\nproblematic for subsequent modules such as control and planning. To mitigate\nthe issue, our method provides depth uncertainty and reliable visibility-aware\nestimations. We further leverage our parametric depth modeling to present a\nnovel visibility-aware evaluation metric that, when taken into account, can\nmitigate the hallucination problem. Extensive experiments on object detection\nand semantic segmentation on the nuScenes datasets demonstrate that our method\noutperforms existing methods on both tasks.\n","authors":["Jiayu Yang","Enze Xie","Miaomiao Liu","Jose M. Alvarez"],"pdf_url":"https://arxiv.org/pdf/2307.04106v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05845v1","updated":"2023-07-11T23:36:49Z","published":"2023-07-11T23:36:49Z","title":"PIGEON: Predicting Image Geolocations","summary":"  We introduce PIGEON, a multi-task end-to-end system for planet-scale image\ngeolocalization that achieves state-of-the-art performance on both external\nbenchmarks and in human evaluation. Our work incorporates semantic geocell\ncreation with label smoothing, conducts pretraining of a vision transformer on\nimages with geographic information, and refines location predictions with\nProtoNets across a candidate set of geocells. The contributions of PIGEON are\nthree-fold: first, we design a semantic geocells creation and splitting\nalgorithm based on open-source data which can be adapted to any geospatial\ndataset. Second, we show the effectiveness of intra-geocell refinement and the\napplicability of unsupervised clustering and ProtNets to the task. Finally, we\nmake our pre-trained CLIP transformer model, StreetCLIP, publicly available for\nuse in adjacent domains with applications to fighting climate change and urban\nand rural scene understanding.\n","authors":["Lukas Haas","Silas Alberti","Michal Skreta"],"pdf_url":"https://arxiv.org/pdf/2307.05845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05832v1","updated":"2023-07-11T22:56:55Z","published":"2023-07-11T22:56:55Z","title":"Bag of Views: An Appearance-based Approach to Next-Best-View Planning\n  for 3D Reconstruction","summary":"  UAV-based intelligent data acquisition for 3D reconstruction and monitoring\nof infrastructure has been experiencing an increasing surge of interest due to\nthe recent advancements in image processing and deep learning-based techniques.\nView planning is an essential part of this task that dictates the information\ncapture strategy and heavily impacts the quality of the 3D model generated from\nthe captured data. Recent methods have used prior knowledge or partial\nreconstruction of the target to accomplish view planning for active\nreconstruction; the former approach poses a challenge for complex or newly\nidentified targets while the latter is computationally expensive. In this work,\nwe present Bag-of-Views (BoV), a fully appearance-based model used to assign\nutility to the captured views for both offline dataset refinement and online\nnext-best-view (NBV) planning applications targeting the task of 3D\nreconstruction. With this contribution, we also developed the View Planning\nToolbox (VPT), a lightweight package for training and testing machine\nlearning-based view planning frameworks, custom view dataset generation of\narbitrary 3D scenes, and 3D reconstruction. Through experiments which pair a\nBoV-based reinforcement learning model with VPT, we demonstrate the efficacy of\nour model in reducing the number of required views for high-quality\nreconstructions in dataset refinement and NBV planning.\n","authors":["Sara Hatami Gazani","Matthew Tucsok","Iraj Mantegh","Homayoun Najjaran"],"pdf_url":"https://arxiv.org/pdf/2307.05832v1.pdf","comment":"Submitted to IEEE Robotics and Automation Letters (RA-L)"},{"id":"http://arxiv.org/abs/2304.04319v2","updated":"2023-07-11T21:11:39Z","published":"2023-04-09T21:25:07Z","title":"On the dice loss gradient and the ways to mimic it","summary":"  In the past few years, in the context of fully-supervised semantic\nsegmentation, several losses -- such as cross-entropy and dice -- have emerged\nas de facto standards to supervise neural networks. The Dice loss is an\ninteresting case, as it comes from the relaxation of the popular Dice\ncoefficient; one of the main evaluation metric in medical imaging applications.\nIn this paper, we first study theoretically the gradient of the dice loss,\nshowing that concretely it is a weighted negative of the ground truth, with a\nvery small dynamic range. This enables us, in the second part of this paper, to\nmimic the supervision of the dice loss, through a simple element-wise\nmultiplication of the network output with a negative of the ground truth. This\nrather surprising result sheds light on the practical supervision performed by\nthe dice loss during gradient descent. This can help the practitioner to\nunderstand and interpret results while guiding researchers when designing new\nlosses.\n","authors":["Hoel Kervadec","Marleen de Bruijne"],"pdf_url":"https://arxiv.org/pdf/2304.04319v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05804v1","updated":"2023-07-11T21:00:47Z","published":"2023-07-11T21:00:47Z","title":"Improving Segmentation and Detection of Lesions in CT Scans Using\n  Intensity Distribution Supervision","summary":"  We propose a method to incorporate the intensity information of a target\nlesion on CT scans in training segmentation and detection networks. We first\nbuild an intensity-based lesion probability (ILP) function from an intensity\nhistogram of the target lesion. It is used to compute the probability of being\nthe lesion for each voxel based on its intensity. Finally, the computed ILP map\nof each input CT scan is provided as additional supervision for network\ntraining, which aims to inform the network about possible lesion locations in\nterms of intensity values at no additional labeling cost. The method was\napplied to improve the segmentation of three different lesion types, namely,\nsmall bowel carcinoid tumor, kidney tumor, and lung nodule. The effectiveness\nof the proposed method on a detection task was also investigated. We observed\nimprovements of 41.3% -> 47.8%, 74.2% -> 76.0%, and 26.4% -> 32.7% in\nsegmenting small bowel carcinoid tumor, kidney tumor, and lung nodule,\nrespectively, in terms of per case Dice scores. An improvement of 64.6% ->\n75.5% was achieved in detecting kidney tumors in terms of average precision.\nThe results of different usages of the ILP map and the effect of varied amount\nof training data are also presented.\n","authors":["Seung Yeon Shin","Thomas C. Shen","Ronald M. Summers"],"pdf_url":"https://arxiv.org/pdf/2307.05804v1.pdf","comment":"Computerized Medical Imaging and Graphics 2023"},{"id":"http://arxiv.org/abs/2307.05801v1","updated":"2023-07-11T20:52:46Z","published":"2023-07-11T20:52:46Z","title":"Differentiable Forward Projector for X-ray Computed Tomography","summary":"  Data-driven deep learning has been successfully applied to various computed\ntomographic reconstruction problems. The deep inference models may outperform\nexisting analytical and iterative algorithms, especially in ill-posed CT\nreconstruction. However, those methods often predict images that do not agree\nwith the measured projection data. This paper presents an accurate\ndifferentiable forward and back projection software library to ensure the\nconsistency between the predicted images and the original measurements. The\nsoftware library efficiently supports various projection geometry types while\nminimizing the GPU memory footprint requirement, which facilitates seamless\nintegration with existing deep learning training and inference pipelines. The\nproposed software is available as open source: https://github.com/LLNL/LEAP.\n","authors":["Hyojin Kim","Kyle Champley"],"pdf_url":"https://arxiv.org/pdf/2307.05801v1.pdf","comment":"ICML 2023 Workshop: Differentiable Almost Everything: Differentiable\n  Relaxations, Algorithms, Operators, and Simulators"},{"id":"http://arxiv.org/abs/2307.05800v1","updated":"2023-07-11T20:50:40Z","published":"2023-07-11T20:50:40Z","title":"A Hierarchical Transformer Encoder to Improve Entire Neoplasm\n  Segmentation on Whole Slide Image of Hepatocellular Carcinoma","summary":"  In digital histopathology, entire neoplasm segmentation on Whole Slide Image\n(WSI) of Hepatocellular Carcinoma (HCC) plays an important role, especially as\na preprocessing filter to automatically exclude healthy tissue, in histological\nmolecular correlations mining and other downstream histopathological tasks. The\nsegmentation task remains challenging due to HCC's inherent high-heterogeneity\nand the lack of dependency learning in large field of view. In this article, we\npropose a novel deep learning architecture with a hierarchical Transformer\nencoder, HiTrans, to learn the global dependencies within expanded\n4096$\\times$4096 WSI patches. HiTrans is designed to encode and decode the\npatches with larger reception fields and the learned global dependencies,\ncompared to the state-of-the-art Fully Convolutional Neural networks (FCNN).\nEmpirical evaluations verified that HiTrans leads to better segmentation\nperformance by taking into account regional and global dependency information.\n","authors":["Zhuxian Guo","Qitong Wang","Henning Müller","Themis Palpanas","Nicolas Loménie","Camille Kurtz"],"pdf_url":"https://arxiv.org/pdf/2307.05800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05799v1","updated":"2023-07-11T20:46:19Z","published":"2023-07-11T20:46:19Z","title":"3D Medical Image Segmentation based on multi-scale MPU-Net","summary":"  The high cure rate of cancer is inextricably linked to physicians' accuracy\nin diagnosis and treatment, therefore a model that can accomplish\nhigh-precision tumor segmentation has become a necessity in many applications\nof the medical industry. It can effectively lower the rate of misdiagnosis\nwhile considerably lessening the burden on clinicians. However, fully automated\ntarget organ segmentation is problematic due to the irregular stereo structure\nof 3D volume organs. As a basic model for this class of real applications,\nU-Net excels. It can learn certain global and local features, but still lacks\nthe capacity to grasp spatial long-range relationships and contextual\ninformation at multiple scales. This paper proposes a tumor segmentation model\nMPU-Net for patient volume CT images, which is inspired by Transformer with a\nglobal attention mechanism. By combining image serialization with the Position\nAttention Module, the model attempts to comprehend deeper contextual\ndependencies and accomplish precise positioning. Each layer of the decoder is\nalso equipped with a multi-scale module and a cross-attention mechanism. The\ncapability of feature extraction and integration at different levels has been\nenhanced, and the hybrid loss function developed in this study can better\nexploit high-resolution characteristic information. Moreover, the suggested\narchitecture is tested and evaluated on the Liver Tumor Segmentation Challenge\n2017 (LiTS 2017) dataset. Compared with the benchmark model U-Net, MPU-Net\nshows excellent segmentation results. The dice, accuracy, precision,\nspecificity, IOU, and MCC metrics for the best model segmentation results are\n92.17%, 99.08%, 91.91%, 99.52%, 85.91%, and 91.74%, respectively. Outstanding\nindicators in various aspects illustrate the exceptional performance of this\nframework in automatic medical image segmentation.\n","authors":["Zeqiu. Yu","Shuo. Han"],"pdf_url":"https://arxiv.org/pdf/2307.05799v1.pdf","comment":"37 pages"},{"id":"http://arxiv.org/abs/2307.05786v1","updated":"2023-07-11T20:27:12Z","published":"2023-07-11T20:27:12Z","title":"Merging multiple input descriptors and supervisors in a deep neural\n  network for tractogram filtering","summary":"  One of the main issues of the current tractography methods is their high\nfalse-positive rate. Tractogram filtering is an option to remove false-positive\nstreamlines from tractography data in a post-processing step. In this paper, we\ntrain a deep neural network for filtering tractography data in which every\nstreamline of a tractogram is classified as {\\em plausible, implausible}, or\n{\\em inconclusive}. For this, we use four different tractogram filtering\nstrategies as supervisors: TractQuerier, RecobundlesX, TractSeg, and an\nanatomy-inspired filter. Their outputs are combined to obtain the\nclassification labels for the streamlines. We assessed the importance of\ndifferent types of information along the streamlines for performing this\nclassification task, including the coordinates of the streamlines, diffusion\ndata, landmarks, T1-weighted information, and a brain parcellation. We found\nthat the streamline coordinates are the most relevant followed by the diffusion\ndata in this particular classification task.\n","authors":["Daniel Jörgens","Pierre-Marc Jodoin","Maxime Descoteaux","Rodrigo Moreno"],"pdf_url":"https://arxiv.org/pdf/2307.05786v1.pdf","comment":"21 pages, 8 figures"},{"id":"http://arxiv.org/abs/2301.03198v4","updated":"2023-07-11T20:27:04Z","published":"2023-01-09T08:27:36Z","title":"The Algonauts Project 2023 Challenge: How the Human Brain Makes Sense of\n  Natural Scenes","summary":"  The sciences of biological and artificial intelligence are ever more\nintertwined. Neural computational principles inspire new intelligent machines,\nwhich are in turn used to advance theoretical understanding of the brain. To\npromote further exchange of ideas and collaboration between biological and\nartificial intelligence researchers, we introduce the 2023 installment of the\nAlgonauts Project challenge: How the Human Brain Makes Sense of Natural Scenes\n(http://algonauts.csail.mit.edu). This installment prompts the fields of\nartificial and biological intelligence to come together towards building\ncomputational models of the visual brain using the largest and richest dataset\nof fMRI responses to visual scenes, the Natural Scenes Dataset (NSD). NSD\nprovides high-quality fMRI responses to ~73,000 different naturalistic colored\nscenes, making it the ideal candidate for data-driven model building approaches\npromoted by the 2023 challenge. The challenge is open to all and makes results\ndirectly comparable and transparent through a public leaderboard automatically\nupdated after each submission, thus allowing for rapid model development. We\nbelieve that the 2023 installment will spark symbiotic collaborations between\nbiological and artificial intelligence scientists, leading to a deeper\nunderstanding of the brain through cutting-edge computational models and to\nnovel ways of engineering artificial intelligent agents through inductive\nbiases from biological systems.\n","authors":["A. T. Gifford","B. Lahner","S. Saba-Sadiya","M. G. Vilas","A. Lascelles","A. Oliva","K. Kay","G. Roig","R. M. Cichy"],"pdf_url":"https://arxiv.org/pdf/2301.03198v4.pdf","comment":"5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2307.05784v1","updated":"2023-07-11T20:23:23Z","published":"2023-07-11T20:23:23Z","title":"EgoAdapt: A multi-stream evaluation study of adaptation to real-world\n  egocentric user video","summary":"  In egocentric action recognition a single population model is typically\ntrained and subsequently embodied on a head-mounted device, such as an\naugmented reality headset. While this model remains static for new users and\nenvironments, we introduce an adaptive paradigm of two phases, where after\npretraining a population model, the model adapts on-device and online to the\nuser's experience. This setting is highly challenging due to the change from\npopulation to user domain and the distribution shifts in the user's data\nstream. Coping with the latter in-stream distribution shifts is the focus of\ncontinual learning, where progress has been rooted in controlled benchmarks but\nchallenges faced in real-world applications often remain unaddressed. We\nintroduce EgoAdapt, a benchmark for real-world egocentric action recognition\nthat facilitates our two-phased adaptive paradigm, and real-world challenges\nnaturally occur in the egocentric video streams from Ego4d, such as long-tailed\naction distributions and large-scale classification over 2740 actions. We\nintroduce an evaluation framework that directly exploits the user's data stream\nwith new metrics to measure the adaptation gain over the population model,\nonline generalization, and hindsight performance. In contrast to single-stream\nevaluation in existing works, our framework proposes a meta-evaluation that\naggregates the results from 50 independent user streams. We provide an\nextensive empirical study for finetuning and experience replay.\n","authors":["Matthias De Lange","Hamid Eghbalzadeh","Reuben Tan","Michael Iuzzolino","Franziska Meier","Karl Ridgeway"],"pdf_url":"https://arxiv.org/pdf/2307.05784v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2307.05780v1","updated":"2023-07-11T20:17:47Z","published":"2023-07-11T20:17:47Z","title":"Automated Artifact Detection in Ultra-widefield Fundus Photography of\n  Patients with Sickle Cell Disease","summary":"  Importance: Ultra-widefield fundus photography (UWF-FP) has shown utility in\nsickle cell retinopathy screening; however, image artifact may diminish quality\nand gradeability of images. Objective: To create an automated algorithm for\nUWF-FP artifact classification. Design: A neural network based automated\nartifact detection algorithm was designed to identify commonly encountered\nUWF-FP artifacts in a cross section of patient UWF-FP. A pre-trained ResNet-50\nneural network was trained on a subset of the images and the classification\naccuracy, sensitivity, and specificity were quantified on the hold out test\nset. Setting: The study is based on patients from a tertiary care hospital\nsite. Participants: There were 243 UWF-FP acquired from patients with sickle\ncell disease (SCD), and artifact labelling in the following categories was\nperformed: Eyelash Present, Lower Eyelid Obstructing, Upper Eyelid Obstructing,\nImage Too Dark, Dark Artifact, and Image Not Centered. Results: Overall, the\naccuracy for each class was Eyelash Present at 83.7%, Lower Eyelid Obstructing\nat 83.7%, Upper Eyelid Obstructing at 98.0%, Image Too Dark at 77.6%, Dark\nArtifact at 93.9%, and Image Not Centered at 91.8%. Conclusions and Relevance:\nThis automated algorithm shows promise in identifying common imaging artifacts\non a subset of Optos UWF-FP in SCD patients. Further refinement is ongoing with\nthe goal of improving efficiency of tele-retinal screening in sickle cell\nretinopathy (SCR) by providing a photographer real-time feedback as to the\ntypes of artifacts present, and the need for image re-acquisition. This\nalgorithm also may have potential future applicability in other retinal\ndiseases by improving quality and efficiency of image acquisition of UWF-FP.\n","authors":["Anqi Feng","Dimitri Johnson","Grace R. Reilly","Loka Thangamathesvaran","Ann Nampomba","Mathias Unberath","Adrienne W. Scott","Craig Jones"],"pdf_url":"https://arxiv.org/pdf/2307.05780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05766v1","updated":"2023-07-11T19:47:05Z","published":"2023-07-11T19:47:05Z","title":"Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology\n  Reporting","summary":"  Radiology reporting is a crucial part of the communication between\nradiologists and other medical professionals, but it can be time-consuming and\nerror-prone. One approach to alleviate this is structured reporting, which\nsaves time and enables a more accurate evaluation than free-text reports.\nHowever, there is limited research on automating structured reporting, and no\npublic benchmark is available for evaluating and comparing different methods.\nTo close this gap, we introduce Rad-ReStruct, a new benchmark dataset that\nprovides fine-grained, hierarchically ordered annotations in the form of\nstructured reports for X-Ray images. We model the structured reporting task as\nhierarchical visual question answering (VQA) and propose hi-VQA, a novel method\nthat considers prior context in the form of previously asked questions and\nanswers for populating a structured radiology report. Our experiments show that\nhi-VQA achieves competitive performance to the state-of-the-art on the medical\nVQA benchmark VQARad while performing best among methods without\ndomain-specific vision-language pretraining and provides a strong baseline on\nRad-ReStruct. Our work represents a significant step towards the automated\npopulation of structured radiology reports and provides a valuable first\nbenchmark for future research in this area. We will make all annotations and\nour code for annotation generation, model evaluation, and training publicly\navailable upon acceptance. Our dataset and code is available at\nhttps://github.com/ChantalMP/Rad-ReStruct.\n","authors":["Chantal Pellegrini","Matthias Keicher","Ege Özsoy","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2307.05766v1.pdf","comment":"accepted at MICCAI 2023"},{"id":"http://arxiv.org/abs/2307.05760v1","updated":"2023-07-11T19:30:09Z","published":"2023-07-11T19:30:09Z","title":"Line Art Colorization of Fakemon using Generative Adversarial Neural\n  Networks","summary":"  This work proposes a complete methodology to colorize images of Fakemon,\nanime-style monster-like creatures. In addition, we propose algorithms to\nextract the line art from colorized images as well as to extract color hints.\nOur work is the first in the literature to use automatic color hint extraction,\nto train the networks specifically with anime-styled creatures and to combine\nthe Pix2Pix and CycleGAN approaches, two different generative adversarial\nnetworks that create a single final result. Visual results of the colorizations\nare feasible but there is still room for improvement.\n","authors":["Erick Oliveira Rodrigues","Esteban Clua","Giovani Bernardes Vitor"],"pdf_url":"https://arxiv.org/pdf/2307.05760v1.pdf","comment":"art generation, lineart colorization, image colorization, generative\n  adversarial networks, stable diffusion, pokemon, fakemon, digimon"},{"id":"http://arxiv.org/abs/2211.02760v2","updated":"2023-07-11T19:04:44Z","published":"2022-11-04T21:51:53Z","title":"Development and evaluation of automated localisation and reconstruction\n  of all fruits on tomato plants in a greenhouse based on multi-view perception\n  and 3D multi-object tracking","summary":"  The ability to accurately represent and localise relevant objects is\nessential for robots to carry out tasks effectively. Traditional approaches,\nwhere robots simply capture an image, process that image to take an action, and\nthen forget the information, have proven to struggle in the presence of\nocclusions. Methods using multi-view perception, which have the potential to\naddress some of these problems, require a world model that guides the\ncollection, integration and extraction of information from multiple viewpoints.\nFurthermore, constructing a generic representation that can be applied in\nvarious environments and tasks is a difficult challenge. In this paper, a novel\napproach for building generic representations in occluded agro-food\nenvironments using multi-view perception and 3D multi-object tracking is\nintroduced. The method is based on a detection algorithm that generates partial\npoint clouds for each detected object, followed by a 3D multi-object tracking\nalgorithm that updates the representation over time. The accuracy of the\nrepresentation was evaluated in a real-world environment, where successful\nrepresentation and localisation of tomatoes in tomato plants were achieved,\ndespite high levels of occlusion, with the total count of tomatoes estimated\nwith a maximum error of 5.08% and the tomatoes tracked with an accuracy up to\n71.47%. Novel tracking metrics were introduced, demonstrating that valuable\ninsight into the errors in localising and representing the fruits can be\nprovided by their use. This approach presents a novel solution for building\nrepresentations in occluded agro-food environments, demonstrating potential to\nenable robots to perform tasks effectively in these challenging environments.\n","authors":["David Rapado Rincon","Eldert J. van Henten","Gert Kootstra"],"pdf_url":"https://arxiv.org/pdf/2211.02760v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.09452v2","updated":"2023-07-11T18:46:45Z","published":"2022-10-17T21:43:32Z","title":"Multiple Instance Learning via Iterative Self-Paced Supervised\n  Contrastive Learning","summary":"  Learning representations for individual instances when only bag-level labels\nare available is a fundamental challenge in multiple instance learning (MIL).\nRecent works have shown promising results using contrastive self-supervised\nlearning (CSSL), which learns to push apart representations corresponding to\ntwo different randomly-selected instances. Unfortunately, in real-world\napplications such as medical image classification, there is often class\nimbalance, so randomly-selected instances mostly belong to the same majority\nclass, which precludes CSSL from learning inter-class differences. To address\nthis issue, we propose a novel framework, Iterative Self-paced Supervised\nContrastive Learning for MIL Representations (ItS2CLR), which improves the\nlearned representation by exploiting instance-level pseudo labels derived from\nthe bag-level labels. The framework employs a novel self-paced sampling\nstrategy to ensure the accuracy of pseudo labels. We evaluate ItS2CLR on three\nmedical datasets, showing that it improves the quality of instance-level pseudo\nlabels and representations, and outperforms existing MIL methods in terms of\nboth bag and instance level accuracy. Code is available at\nhttps://github.com/Kangningthu/ItS2CLR\n","authors":["Kangning Liu","Weicheng Zhu","Yiqiu Shen","Sheng Liu","Narges Razavian","Krzysztof J. Geras","Carlos Fernandez-Granda"],"pdf_url":"https://arxiv.org/pdf/2210.09452v2.pdf","comment":"CVPR 2023 camera-ready version. The first two authors contribute\n  equally. The last two authors are joint last authors"},{"id":"http://arxiv.org/abs/2307.05707v1","updated":"2023-07-11T18:17:50Z","published":"2023-07-11T18:17:50Z","title":"MoP-CLIP: A Mixture of Prompt-Tuned CLIP Models for Domain Incremental\n  Learning","summary":"  Despite the recent progress in incremental learning, addressing catastrophic\nforgetting under distributional drift is still an open and important problem.\nIndeed, while state-of-the-art domain incremental learning (DIL) methods\nperform satisfactorily within known domains, their performance largely degrades\nin the presence of novel domains. This limitation hampers their\ngeneralizability, and restricts their scalability to more realistic settings\nwhere train and test data are drawn from different distributions. To address\nthese limitations, we present a novel DIL approach based on a mixture of\nprompt-tuned CLIP models (MoP-CLIP), which generalizes the paradigm of\nS-Prompting to handle both in-distribution and out-of-distribution data at\ninference. In particular, at the training stage we model the features\ndistribution of every class in each domain, learning individual text and visual\nprompts to adapt to a given domain. At inference, the learned distributions\nallow us to identify whether a given test sample belongs to a known domain,\nselecting the correct prompt for the classification task, or from an unseen\ndomain, leveraging a mixture of the prompt-tuned CLIP models. Our empirical\nevaluation reveals the poor performance of existing DIL methods under domain\nshift, and suggests that the proposed MoP-CLIP performs competitively in the\nstandard DIL settings while outperforming state-of-the-art methods in OOD\nscenarios. These results demonstrate the superiority of MoP-CLIP, offering a\nrobust and general solution to the problem of domain incremental learning.\n","authors":["Julien Nicolas","Florent Chiaroni","Imtiaz Ziko","Ola Ahmad","Christian Desrosiers","Jose Dolz"],"pdf_url":"https://arxiv.org/pdf/2307.05707v1.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2302.10873v3","updated":"2023-07-11T18:15:18Z","published":"2023-02-21T18:42:24Z","title":"Context-Aware Timewise VAEs for Real-Time Vehicle Trajectory Prediction","summary":"  Real-time, accurate prediction of human steering behaviors has wide\napplications, from developing intelligent traffic systems to deploying\nautonomous driving systems in both real and simulated worlds. In this paper, we\npresent ContextVAE, a context-aware approach for multi-modal vehicle trajectory\nprediction. Built upon the backbone architecture of a timewise variational\nautoencoder, ContextVAE observation encoding employs a dual attention mechanism\nthat accounts for the environmental context and the dynamic agents' states, in\na unified way. By utilizing features extracted from semantic maps during agent\nstate encoding, our approach takes into account both the social features\nexhibited by agents on the scene and the physical environment constraints to\ngenerate map-compliant and socially-aware trajectories. We perform extensive\ntesting on the nuScenes prediction challenge, Lyft Level 5 dataset and Waymo\nOpen Motion Dataset to show the effectiveness of our approach and its\nstate-of-the-art performance. In all tested datasets, ContextVAE models are\nfast to train and provide high-quality multi-modal predictions in real-time.\nOur code is available at: https://github.com/xupei0610/ContextVAE.\n","authors":["Pei Xu","Jean-Bernard Hayet","Ioannis Karamouzas"],"pdf_url":"https://arxiv.org/pdf/2302.10873v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.06718v4","updated":"2023-07-11T18:13:14Z","published":"2023-04-13T17:59:40Z","title":"Segment Everything Everywhere All at Once","summary":"  In this work, we present SEEM, a promptable and interactive model for\nsegmenting everything everywhere all at once in an image, as shown in Fig.1. In\nSEEM, we propose a novel decoding mechanism that enables diverse prompting for\nall types of segmentation tasks, aiming at a universal segmentation interface\nthat behaves like large language models (LLMs). More specifically, SEEM is\ndesigned with four desiderata: i) Versatility. We introduce a new visual prompt\nto unify different spatial queries including points, boxes, scribbles and\nmasks, which can further generalize to a different referring image; ii)\nCompositionality. We learn a joint visual-semantic space between text and\nvisual prompts, which facilitates the dynamic composition of two prompt types\nrequired for various segmentation tasks; iii) Interactivity. We further\nincorporate learnable memory prompts into the decoder to retain segmentation\nhistory through mask-guided cross-attention from decoder to image features; and\niv) Semantic-awareness. We use a text encoder to encode text queries and mask\nlabels into the same semantic space for open-vocabulary segmentation. We\nconduct a comprehensive empirical study to validate the effectiveness of SEEM\nacross diverse segmentation tasks. Notably, our single SEEM model achieves\ncompetitive performance across interactive segmentation, generic segmentation,\nreferring segmentation, and video object segmentation on 9 datasets with\nminimum 1/100 supervision. Furthermore, SEEM showcases a remarkable capacity\nfor generalization to novel prompts or their combinations, rendering it a\nreadily universal image segmentation interface.\n","authors":["Xueyan Zou","Jianwei Yang","Hao Zhang","Feng Li","Linjie Li","Jianfeng Wang","Lijuan Wang","Jianfeng Gao","Yong Jae Lee"],"pdf_url":"https://arxiv.org/pdf/2304.06718v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05704v1","updated":"2023-07-11T18:12:05Z","published":"2023-07-11T18:12:05Z","title":"A Causal Ordering Prior for Unsupervised Representation Learning","summary":"  Unsupervised representation learning with variational inference relies\nheavily on independence assumptions over latent variables. Causal\nrepresentation learning (CRL), however, argues that factors of variation in a\ndataset are, in fact, causally related. Allowing latent variables to be\ncorrelated, as a consequence of causal relationships, is more realistic and\ngeneralisable. So far, provably identifiable methods rely on: auxiliary\ninformation, weak labels, and interventional or even counterfactual data.\nInspired by causal discovery with functional causal models, we propose a fully\nunsupervised representation learning method that considers a data generation\nprocess with a latent additive noise model (ANM). We encourage the latent space\nto follow a causal ordering via loss function based on the Hessian of the\nlatent distribution.\n","authors":["Avinash Kori","Pedro Sanchez","Konstantinos Vilouras","Ben Glocker","Sotirios A. Tsaftaris"],"pdf_url":"https://arxiv.org/pdf/2307.05704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05700v1","updated":"2023-07-11T18:07:25Z","published":"2023-07-11T18:07:25Z","title":"SepHRNet: Generating High-Resolution Crop Maps from Remote Sensing\n  imagery using HRNet with Separable Convolution","summary":"  The accurate mapping of crop production is crucial for ensuring food\nsecurity, effective resource management, and sustainable agricultural\npractices. One way to achieve this is by analyzing high-resolution satellite\nimagery. Deep Learning has been successful in analyzing images, including\nremote sensing imagery. However, capturing intricate crop patterns is\nchallenging due to their complexity and variability. In this paper, we propose\na novel Deep learning approach that integrates HRNet with Separable\nConvolutional layers to capture spatial patterns and Self-attention to capture\ntemporal patterns of the data. The HRNet model acts as a backbone and extracts\nhigh-resolution features from crop images. Spatially separable convolution in\nthe shallow layers of the HRNet model captures intricate crop patterns more\neffectively while reducing the computational cost. The multi-head attention\nmechanism captures long-term temporal dependencies from the encoded vector\nrepresentation of the images. Finally, a CNN decoder generates a crop map from\nthe aggregated representation. Adaboost is used on top of this to further\nimprove accuracy. The proposed algorithm achieves a high classification\naccuracy of 97.5\\% and IoU of 55.2\\% in generating crop maps. We evaluate the\nperformance of our pipeline on the Zuericrop dataset and demonstrate that our\nresults outperform state-of-the-art models such as U-Net++, ResNet50, VGG19,\nInceptionV3, DenseNet, and EfficientNet. This research showcases the potential\nof Deep Learning for Earth Observation Systems.\n","authors":["Priyanka Goyal","Sohan Patnaik","Adway Mitra","Manjira Sinha"],"pdf_url":"https://arxiv.org/pdf/2307.05700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05473v1","updated":"2023-07-11T17:58:31Z","published":"2023-07-11T17:58:31Z","title":"Differentiable Blocks World: Qualitative 3D Decomposition by Rendering\n  Primitives","summary":"  Given a set of calibrated images of a scene, we present an approach that\nproduces a simple, compact, and actionable 3D world representation by means of\n3D primitives. While many approaches focus on recovering high-fidelity 3D\nscenes, we focus on parsing a scene into mid-level 3D representations made of a\nsmall set of textured primitives. Such representations are interpretable, easy\nto manipulate and suited for physics-based simulations. Moreover, unlike\nexisting primitive decomposition methods that rely on 3D input data, our\napproach operates directly on images through differentiable rendering.\nSpecifically, we model primitives as textured superquadric meshes and optimize\ntheir parameters from scratch with an image rendering loss. We highlight the\nimportance of modeling transparency for each primitive, which is critical for\noptimization and also enables handling varying numbers of primitives. We show\nthat the resulting textured primitives faithfully reconstruct the input images\nand accurately model the visible 3D points, while providing amodal shape\ncompletions of unseen object regions. We compare our approach to the state of\nthe art on diverse scenes from DTU, and demonstrate its robustness on real-life\ncaptures from BlendedMVS and Nerfstudio. We also showcase how our results can\nbe used to effortlessly edit a scene or perform physical simulations. Code and\nvideo results are available at https://www.tmonnier.com/DBW .\n","authors":["Tom Monnier","Jake Austin","Angjoo Kanazawa","Alexei A. Efros","Mathieu Aubry"],"pdf_url":"https://arxiv.org/pdf/2307.05473v1.pdf","comment":"Project webpage with code and videos: https://www.tmonnier.com/DBW"},{"id":"http://arxiv.org/abs/2307.05663v1","updated":"2023-07-11T17:57:40Z","published":"2023-07-11T17:57:40Z","title":"Objaverse-XL: A Universe of 10M+ 3D Objects","summary":"  Natural language processing and 2D vision models have attained remarkable\nproficiency on many tasks primarily by escalating the scale of training data.\nHowever, 3D vision tasks have not seen the same progress, in part due to the\nchallenges of acquiring high-quality 3D data. In this work, we present\nObjaverse-XL, a dataset of over 10 million 3D objects. Our dataset comprises\ndeduplicated 3D objects from a diverse set of sources, including manually\ndesigned objects, photogrammetry scans of landmarks and everyday items, and\nprofessional scans of historic and antique artifacts. Representing the largest\nscale and diversity in the realm of 3D datasets, Objaverse-XL enables\nsignificant new possibilities for 3D vision. Our experiments demonstrate the\nimprovements enabled with the scale provided by Objaverse-XL. We show that by\ntraining Zero123 on novel view synthesis, utilizing over 100 million multi-view\nrendered images, we achieve strong zero-shot generalization abilities. We hope\nthat releasing Objaverse-XL will enable further innovations in the field of 3D\nvision at scale.\n","authors":["Matt Deitke","Ruoshi Liu","Matthew Wallingford","Huong Ngo","Oscar Michel","Aditya Kusupati","Alan Fan","Christian Laforte","Vikram Voleti","Samir Yitzhak Gadre","Eli VanderBilt","Aniruddha Kembhavi","Carl Vondrick","Georgia Gkioxari","Kiana Ehsani","Ludwig Schmidt","Ali Farhadi"],"pdf_url":"https://arxiv.org/pdf/2307.05663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.15542v2","updated":"2023-07-11T17:57:06Z","published":"2023-05-24T20:03:04Z","title":"TOAST: Transfer Learning via Attention Steering","summary":"  Transfer learning involves adapting a pre-trained model to novel downstream\ntasks. However, we observe that current transfer learning methods often fail to\nfocus on task-relevant features. In this work, we explore refocusing model\nattention for transfer learning. We introduce Top-Down Attention Steering\n(TOAST), a novel transfer learning algorithm that keeps the pre-trained\nbackbone frozen, selects task-relevant features in the output, and feeds those\nfeatures back to the model to steer the attention to the task-specific\nfeatures. By refocusing the attention only, TOAST achieves state-of-the-art\nresults on a number of transfer learning benchmarks, while having a small\nnumber of tunable parameters. Compared to fully fine-tuning, LoRA, and prompt\ntuning, TOAST substantially improves performance across a range of fine-grained\nvisual classification datasets (e.g., 81.1% -> 86.2% on FGVC). TOAST also\noutperforms the fully fine-tuned Alpaca and Vicuna models on\ninstruction-following language generation. Code is available at\nhttps://github.com/bfshi/TOAST.\n","authors":["Baifeng Shi","Siyu Gai","Trevor Darrell","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2305.15542v2.pdf","comment":"Code is available at https://github.com/bfshi/TOAST"},{"id":"http://arxiv.org/abs/2307.05471v1","updated":"2023-07-11T17:56:22Z","published":"2023-07-11T17:56:22Z","title":"Scale Alone Does not Improve Mechanistic Interpretability in Vision\n  Models","summary":"  In light of the recent widespread adoption of AI systems, understanding the\ninternal information processing of neural networks has become increasingly\ncritical. Most recently, machine vision has seen remarkable progress by scaling\nneural networks to unprecedented levels in dataset and model size. We here ask\nwhether this extraordinary increase in scale also positively impacts the field\nof mechanistic interpretability. In other words, has our understanding of the\ninner workings of scaled neural networks improved as well? We here use a\npsychophysical paradigm to quantify mechanistic interpretability for a diverse\nsuite of models and find no scaling effect for interpretability - neither for\nmodel nor dataset size. Specifically, none of the nine investigated\nstate-of-the-art models are easier to interpret than the GoogLeNet model from\nalmost a decade ago. Latest-generation vision models appear even less\ninterpretable than older architectures, hinting at a regression rather than\nimprovement, with modern models sacrificing interpretability for accuracy.\nThese results highlight the need for models explicitly designed to be\nmechanistically interpretable and the need for more helpful interpretability\nmethods to increase our understanding of networks at an atomic level. We\nrelease a dataset containing more than 120'000 human responses from our\npsychophysical evaluation of 767 units across nine models. This dataset is\nmeant to facilitate research on automated instead of human-based\ninterpretability evaluations that can ultimately be leveraged to directly\noptimize the mechanistic interpretability of models.\n","authors":["Roland S. Zimmermann","Thomas Klein","Wieland Brendel"],"pdf_url":"https://arxiv.org/pdf/2307.05471v1.pdf","comment":"The first two authors contributed equally. Code available at\n  https://brendel-group.github.io/imi/"},{"id":"http://arxiv.org/abs/2307.05463v1","updated":"2023-07-11T17:50:15Z","published":"2023-07-11T17:50:15Z","title":"EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the\n  Backbone","summary":"  Video-language pre-training (VLP) has become increasingly important due to\nits ability to generalize to various vision and language tasks. However,\nexisting egocentric VLP frameworks utilize separate video and language encoders\nand learn task-specific cross-modal information only during fine-tuning,\nlimiting the development of a unified system. In this work, we introduce the\nsecond generation of egocentric video-language pre-training (EgoVLPv2), a\nsignificant improvement from the previous generation, by incorporating\ncross-modal fusion directly into the video and language backbones. EgoVLPv2\nlearns strong video-text representation during pre-training and reuses the\ncross-modal attention modules to support different downstream tasks in a\nflexible and efficient manner, reducing fine-tuning costs. Moreover, our\nproposed fusion in the backbone strategy is more lightweight and\ncompute-efficient than stacking additional fusion-specific layers. Extensive\nexperiments on a wide range of VL tasks demonstrate the effectiveness of\nEgoVLPv2 by achieving consistent state-of-the-art performance over strong\nbaselines across all downstream. Our project page can be found at\nhttps://shramanpramanick.github.io/EgoVLPv2/.\n","authors":["Shraman Pramanick","Yale Song","Sayan Nag","Kevin Qinghong Lin","Hardik Shah","Mike Zheng Shou","Rama Chellappa","Pengchuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2307.05463v1.pdf","comment":"22 pages, 11 figures"},{"id":"http://arxiv.org/abs/2307.05462v1","updated":"2023-07-11T17:50:02Z","published":"2023-07-11T17:50:02Z","title":"Efficient 3D Articulated Human Generation with Layered Surface Volumes","summary":"  Access to high-quality and diverse 3D articulated digital human assets is\ncrucial in various applications, ranging from virtual reality to social\nplatforms. Generative approaches, such as 3D generative adversarial networks\n(GANs), are rapidly replacing laborious manual content creation tools. However,\nexisting 3D GAN frameworks typically rely on scene representations that\nleverage either template meshes, which are fast but offer limited quality, or\nvolumes, which offer high capacity but are slow to render, thereby limiting the\n3D fidelity in GAN settings. In this work, we introduce layered surface volumes\n(LSVs) as a new 3D object representation for articulated digital humans. LSVs\nrepresent a human body using multiple textured mesh layers around a\nconventional template. These layers are rendered using alpha compositing with\nfast differentiable rasterization, and they can be interpreted as a volumetric\nrepresentation that allocates its capacity to a manifold of finite thickness\naround the template. Unlike conventional single-layer templates that struggle\nwith representing fine off-surface details like hair or accessories, our\nsurface volumes naturally capture such details. LSVs can be articulated, and\nthey exhibit exceptional efficiency in GAN settings, where a 2D generator\nlearns to synthesize the RGBA textures for the individual layers. Trained on\nunstructured, single-view 2D image datasets, our LSV-GAN generates high-quality\nand view-consistent 3D articulated digital humans without the need for\nview-inconsistent 2D upsampling networks.\n","authors":["Yinghao Xu","Wang Yifan","Alexander W. Bergman","Menglei Chai","Bolei Zhou","Gordon Wetzstein"],"pdf_url":"https://arxiv.org/pdf/2307.05462v1.pdf","comment":"Project page: https://www.computationalimaging.org/publications/lsv/\n  Demo: https://www.youtube.com/watch?v=vahgMFCM3j4"},{"id":"http://arxiv.org/abs/2307.05447v1","updated":"2023-07-11T17:22:22Z","published":"2023-07-11T17:22:22Z","title":"Bio-Inspired Night Image Enhancement Based on Contrast Enhancement and\n  Denoising","summary":"  Due to the low accuracy of object detection and recognition in many\nintelligent surveillance systems at nighttime, the quality of night images is\ncrucial. Compared with the corresponding daytime image, nighttime image is\ncharacterized as low brightness, low contrast and high noise. In this paper, a\nbio-inspired image enhancement algorithm is proposed to convert a low\nilluminance image to a brighter and clear one. Different from existing\nbio-inspired algorithm, the proposed method doesn't use any training sequences,\nwe depend on a novel chain of contrast enhancement and denoising algorithms\nwithout using any forms of recursive functions. Our method can largely improve\nthe brightness and contrast of night images, besides, suppress noise. Then we\nimplement on real experiment, and simulation experiment to test our algorithms.\nBoth results show the advantages of proposed algorithm over contrast pair,\nMeylan and Retinex.\n","authors":["Xinyi Bai","Steffi Agino Priyanka","Hsiao-Jung Tung","Yuankai Wang"],"pdf_url":"https://arxiv.org/pdf/2307.05447v1.pdf","comment":"International Conference on Cognitive Systems and Signal Processing\n  (2016)"},{"id":"http://arxiv.org/abs/2307.05409v1","updated":"2023-07-11T16:23:19Z","published":"2023-07-11T16:23:19Z","title":"3D detection of roof sections from a single satellite image and\n  application to LOD2-building reconstruction","summary":"  Reconstructing urban areas in 3D out of satellite raster images has been a\nlong-standing and challenging goal of both academical and industrial research.\nThe rare methods today achieving this objective at a Level Of Details $2$ rely\non procedural approaches based on geometry, and need stereo images and/or LIDAR\ndata as input. We here propose a method for urban 3D reconstruction named\nKIBS(\\textit{Keypoints Inference By Segmentation}), which comprises two novel\nfeatures: i) a full deep learning approach for the 3D detection of the roof\nsections, and ii) only one single (non-orthogonal) satellite raster image as\nmodel input. This is achieved in two steps: i) by a Mask R-CNN model performing\na 2D segmentation of the buildings' roof sections, and after blending these\nlatter segmented pixels within the RGB satellite raster image, ii) by another\nidentical Mask R-CNN model inferring the heights-to-ground of the roof\nsections' corners via panoptic segmentation, unto full 3D reconstruction of the\nbuildings and city. We demonstrate the potential of the KIBS method by\nreconstructing different urban areas in a few minutes, with a Jaccard index for\nthe 2D segmentation of individual roof sections of $88.55\\%$ and $75.21\\%$ on\nour two data sets resp., and a height's mean error of such correctly segmented\npixels for the 3D reconstruction of $1.60$ m and $2.06$ m on our two data sets\nresp., hence within the LOD2 precision range.\n","authors":["Johann Lussange","Mulin Yu","Yuliya Tarabalka","Florent Lafarge"],"pdf_url":"https://arxiv.org/pdf/2307.05409v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05397v1","updated":"2023-07-11T15:57:51Z","published":"2023-07-11T15:57:51Z","title":"On the Vulnerability of DeepFake Detectors to Attacks Generated by\n  Denoising Diffusion Models","summary":"  The detection of malicious Deepfakes is a constantly evolving problem, that\nrequires continuous monitoring of detectors, to ensure they are able to detect\nimage manipulations generated by the latest emerging models. In this paper, we\npresent a preliminary study that investigates the vulnerability of single-image\nDeepfake detectors to attacks created by a representative of the newest\ngeneration of generative methods, i.e. Denoising Diffusion Models (DDMs). Our\nexperiments are run on FaceForensics++, a commonly used benchmark dataset,\nconsisting of Deepfakes generated with various techniques for face swapping and\nface reenactment. The analysis shows, that reconstructing existing Deepfakes\nwith only one denoising diffusion step significantly decreases the accuracy of\nall tested detectors, without introducing visually perceptible image changes.\n","authors":["Marija Ivanovska","Vitomir Štruc"],"pdf_url":"https://arxiv.org/pdf/2307.05397v1.pdf","comment":"Submitted for review"},{"id":"http://arxiv.org/abs/2307.05396v1","updated":"2023-07-11T15:57:15Z","published":"2023-07-11T15:57:15Z","title":"Handwritten Text Recognition Using Convolutional Neural Network","summary":"  OCR (Optical Character Recognition) is a technology that offers comprehensive\nalphanumeric recognition of handwritten and printed characters at electronic\nspeed by merely scanning the document. Recently, the understanding of visual\ndata has been termed Intelligent Character Recognition (ICR). Intelligent\nCharacter Recognition (ICR) is the OCR module that can convert scans of\nhandwritten or printed characters into ASCII text. ASCII data is the standard\nformat for data encoding in electronic communication. ASCII assigns standard\nnumeric values to letters, numeral, symbols, white-spaces and other characters.\nIn more technical terms, OCR is the process of using an electronic device to\ntransform 2-Dimensional textual information into machine-encoded text. Anything\nthat contains text both machine written or handwritten can be scanned either\nthrough a scanner or just simply a picture of the text is enough for the\nrecognition system to distinguish the text. The goal of this papers is to show\nthe results of a Convolutional Neural Network model which has been trained on\nNational Institute of Science and Technology (NIST) dataset containing over a\n100,000 images. The network learns from the features extracted from the images\nand use it to generate the probability of each class to which the picture\nbelongs to. We have achieved an accuracy of 90.54% with a loss of 2.53%.\n","authors":["Atman Mishra","A. Sharath Ram","Kavyashree C"],"pdf_url":"https://arxiv.org/pdf/2307.05396v1.pdf","comment":"6 pages, 15 figures"},{"id":"http://arxiv.org/abs/2303.10703v3","updated":"2023-07-11T15:33:09Z","published":"2023-03-19T16:17:35Z","title":"CCTV-Gun: Benchmarking Handgun Detection in CCTV Images","summary":"  Gun violence is a critical security problem, and it is imperative for the\ncomputer vision community to develop effective gun detection algorithms for\nreal-world scenarios, particularly in Closed Circuit Television (CCTV)\nsurveillance data. Despite significant progress in visual object detection,\ndetecting guns in real-world CCTV images remains a challenging and\nunder-explored task. Firearms, especially handguns, are typically very small in\nsize, non-salient in appearance, and often severely occluded or\nindistinguishable from other small objects. Additionally, the lack of\nprincipled benchmarks and difficulty collecting relevant datasets further\nhinder algorithmic development. In this paper, we present a meticulously\ncrafted and annotated benchmark, called \\textbf{CCTV-Gun}, which addresses the\nchallenges of detecting handguns in real-world CCTV images. Our contribution is\nthree-fold. Firstly, we carefully select and analyze real-world CCTV images\nfrom three datasets, manually annotate handguns and their holders, and assign\neach image with relevant challenge factors such as blur and occlusion.\nSecondly, we propose a new cross-dataset evaluation protocol in addition to the\nstandard intra-dataset protocol, which is vital for gun detection in practical\nsettings. Finally, we comprehensively evaluate both classical and\nstate-of-the-art object detection algorithms, providing an in-depth analysis of\ntheir generalizing abilities. The benchmark will facilitate further research\nand development on this topic and ultimately enhance security. Code,\nannotations, and trained models are available at\nhttps://github.com/srikarym/CCTV-Gun.\n","authors":["Srikar Yellapragada","Zhenghong Li","Kevin Bhadresh Doshi","Purva Makarand Mhasakar","Heng Fan","Jie Wei","Erik Blasch","Bin Zhang","Haibin Ling"],"pdf_url":"https://arxiv.org/pdf/2303.10703v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05325v1","updated":"2023-07-11T15:11:06Z","published":"2023-07-11T15:11:06Z","title":"Self-supervised adversarial masking for 3D point cloud representation\n  learning","summary":"  Self-supervised methods have been proven effective for learning deep\nrepresentations of 3D point cloud data. Although recent methods in this domain\noften rely on random masking of inputs, the results of this approach can be\nimproved. We introduce PointCAM, a novel adversarial method for learning a\nmasking function for point clouds. Our model utilizes a self-distillation\nframework with an online tokenizer for 3D point clouds. Compared to previous\ntechniques that optimize patch-level and object-level objectives, we postulate\napplying an auxiliary network that learns how to select masks instead of\nchoosing them randomly. Our results show that the learned masking function\nachieves state-of-the-art or competitive performance on various downstream\ntasks. The source code is available at https://github.com/szacho/pointcam.\n","authors":["Michał Szachniewicz","Wojciech Kozłowski","Michał Stypułkowski","Maciej Zięba"],"pdf_url":"https://arxiv.org/pdf/2307.05325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05322v1","updated":"2023-07-11T15:09:10Z","published":"2023-07-11T15:09:10Z","title":"Class Instance Balanced Learning for Long-Tailed Classification","summary":"  The long-tailed image classification task remains important in the\ndevelopment of deep neural networks as it explicitly deals with large\nimbalances in the class frequencies of the training data. While uncommon in\nengineered datasets, this imbalance is almost always present in real-world\ndata. Previous approaches have shown that combining cross-entropy and\ncontrastive learning can improve performance on the long-tailed task, but they\ndo not explore the tradeoff between head and tail classes. We propose a novel\nclass instance balanced loss (CIBL), which reweights the relative contributions\nof a cross-entropy and a contrastive loss as a function of the frequency of\nclass instances in the training batch. This balancing favours the contrastive\nloss for more common classes, leading to a learned classifier with a more\nbalanced performance across all class frequencies. Furthermore, increasing the\nrelative weight on the contrastive head shifts performance from common (head)\nto rare (tail) classes, allowing the user to skew the performance towards these\nclasses if desired. We also show that changing the linear classifier head with\na cosine classifier yields a network that can be trained to similar performance\nin substantially fewer epochs. We obtain competitive results on both\nCIFAR-100-LT and ImageNet-LT.\n","authors":["Marc-Antoine Lavoie","Steven Waslander"],"pdf_url":"https://arxiv.org/pdf/2307.05322v1.pdf","comment":"8 pages, 2 figures, presented at the Conference on Robots and Vision\n  2023"},{"id":"http://arxiv.org/abs/2307.05317v1","updated":"2023-07-11T15:01:42Z","published":"2023-07-11T15:01:42Z","title":"Automatic Generation of Semantic Parts for Face Image Synthesis","summary":"  Semantic image synthesis (SIS) refers to the problem of generating realistic\nimagery given a semantic segmentation mask that defines the spatial layout of\nobject classes. Most of the approaches in the literature, other than the\nquality of the generated images, put effort in finding solutions to increase\nthe generation diversity in terms of style i.e. texture. However, they all\nneglect a different feature, which is the possibility of manipulating the\nlayout provided by the mask. Currently, the only way to do so is manually by\nmeans of graphical users interfaces. In this paper, we describe a network\narchitecture to address the problem of automatically manipulating or generating\nthe shape of object classes in semantic segmentation masks, with specific focus\non human faces. Our proposed model allows embedding the mask class-wise into a\nlatent space where each class embedding can be independently edited. Then, a\nbi-directional LSTM block and a convolutional decoder output a new, locally\nmanipulated mask. We report quantitative and qualitative results on the\nCelebMask-HQ dataset, which show our model can both faithfully reconstruct and\nmodify a segmentation mask at the class level. Also, we show our model can be\nput before a SIS generator, opening the way to a fully automatic generation\ncontrol of both shape and texture. Code available at\nhttps://github.com/TFonta/Semantic-VAE.\n","authors":["Tomaso Fontanini","Claudio Ferrari","Massimo Bertozzi","Andrea Prati"],"pdf_url":"https://arxiv.org/pdf/2307.05317v1.pdf","comment":"Preprint, accepted for publication at ICIAP 2023"},{"id":"http://arxiv.org/abs/2307.05314v1","updated":"2023-07-11T15:00:11Z","published":"2023-07-11T15:00:11Z","title":"Masked Vision and Language Pre-training with Unimodal and Multimodal\n  Contrastive Losses for Medical Visual Question Answering","summary":"  Medical visual question answering (VQA) is a challenging task that requires\nanswering clinical questions of a given medical image, by taking consider of\nboth visual and language information. However, due to the small scale of\ntraining data for medical VQA, pre-training fine-tuning paradigms have been a\ncommonly used solution to improve model generalization performance. In this\npaper, we present a novel self-supervised approach that learns unimodal and\nmultimodal feature representations of input images and text using medical image\ncaption datasets, by leveraging both unimodal and multimodal contrastive\nlosses, along with masked language modeling and image text matching as\npretraining objectives. The pre-trained model is then transferred to downstream\nmedical VQA tasks. The proposed approach achieves state-of-the-art (SOTA)\nperformance on three publicly available medical VQA datasets with significant\naccuracy improvements of 2.2%, 14.7%, and 1.7% respectively. Besides, we\nconduct a comprehensive analysis to validate the effectiveness of different\ncomponents of the approach and study different pre-training settings. Our codes\nand models are available at https://github.com/pengfeiliHEU/MUMC.\n","authors":["Pengfei Li","Gang Liu","Jinlong He","Zixu Zhao","Shenjun Zhong"],"pdf_url":"https://arxiv.org/pdf/2307.05314v1.pdf","comment":"accepted by MICCAI2023"},{"id":"http://arxiv.org/abs/2307.05276v1","updated":"2023-07-11T14:11:24Z","published":"2023-07-11T14:11:24Z","title":"Unbiased Scene Graph Generation via Two-stage Causal Modeling","summary":"  Despite the impressive performance of recent unbiased Scene Graph Generation\n(SGG) methods, the current debiasing literature mainly focuses on the\nlong-tailed distribution problem, whereas it overlooks another source of bias,\ni.e., semantic confusion, which makes the SGG model prone to yield false\npredictions for similar relationships. In this paper, we explore a debiasing\nprocedure for the SGG task leveraging causal inference. Our central insight is\nthat the Sparse Mechanism Shift (SMS) in causality allows independent\nintervention on multiple biases, thereby potentially preserving head category\nperformance while pursuing the prediction of high-informative tail\nrelationships. However, the noisy datasets lead to unobserved confounders for\nthe SGG task, and thus the constructed causal models are always\ncausal-insufficient to benefit from SMS. To remedy this, we propose Two-stage\nCausal Modeling (TsCM) for the SGG task, which takes the long-tailed\ndistribution and semantic confusion as confounders to the Structural Causal\nModel (SCM) and then decouples the causal intervention into two stages. The\nfirst stage is causal representation learning, where we use a novel Population\nLoss (P-Loss) to intervene in the semantic confusion confounder. The second\nstage introduces the Adaptive Logit Adjustment (AL-Adjustment) to eliminate the\nlong-tailed distribution confounder to complete causal calibration learning.\nThese two stages are model agnostic and thus can be used in any SGG model that\nseeks unbiased predictions. Comprehensive experiments conducted on the popular\nSGG backbones and benchmarks show that our TsCM can achieve state-of-the-art\nperformance in terms of mean recall rate. Furthermore, TsCM can maintain a\nhigher recall rate than other debiasing methods, which indicates that our\nmethod can achieve a better tradeoff between head and tail relationships.\n","authors":["Shuzhou Sun","Shuaifeng Zhi","Qing Liao","Janne Heikkilä","Li Liu"],"pdf_url":"https://arxiv.org/pdf/2307.05276v1.pdf","comment":"17 pages, 9 figures. Accepted by IEEE Transactions on Pattern\n  Analysis and Machine Intelligence"},{"id":"http://arxiv.org/abs/2307.05270v1","updated":"2023-07-11T14:04:12Z","published":"2023-07-11T14:04:12Z","title":"APRF: Anti-Aliasing Projection Representation Field for Inverse Problem\n  in Imaging","summary":"  Sparse-view Computed Tomography (SVCT) reconstruction is an ill-posed inverse\nproblem in imaging that aims to acquire high-quality CT images based on\nsparsely-sampled measurements. Recent works use Implicit Neural Representations\n(INRs) to build the coordinate-based mapping between sinograms and CT images.\nHowever, these methods have not considered the correlation between adjacent\nprojection views, resulting in aliasing artifacts on SV sinograms. To address\nthis issue, we propose a self-supervised SVCT reconstruction method --\nAnti-Aliasing Projection Representation Field (APRF), which can build the\ncontinuous representation between adjacent projection views via the spatial\nconstraints. Specifically, APRF only needs SV sinograms for training, which\nfirst employs a line-segment sampling module to estimate the distribution of\nprojection views in a local region, and then synthesizes the corresponding\nsinogram values using center-based line integral module. After training APRF on\na single SV sinogram itself, it can synthesize the corresponding dense-view\n(DV) sinogram with consistent continuity. High-quality CT images can be\nobtained by applying re-projection techniques on the predicted DV sinograms.\nExtensive experiments on CT images demonstrate that APRF outperforms\nstate-of-the-art methods, yielding more accurate details and fewer artifacts.\nOur code will be publicly available soon.\n","authors":["Zixuan Chen","Lingxiao Yang","Jianhuang Lai","Xiaohua Xie"],"pdf_url":"https://arxiv.org/pdf/2307.05270v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.07625v2","updated":"2023-07-11T13:47:07Z","published":"2022-11-14T18:48:08Z","title":"What Images are More Memorable to Machines?","summary":"  This paper studies the problem of measuring and predicting how memorable an\nimage is to pattern recognition machines, as a path to explore machine\nintelligence. Firstly, we propose a self-supervised machine memory\nquantification pipeline, dubbed ``MachineMem measurer'', to collect machine\nmemorability scores of images. Similar to humans, machines also tend to\nmemorize certain kinds of images, whereas the types of images that machines and\nhumans memorize are different. Through in-depth analysis and comprehensive\nvisualizations, we gradually unveil that``complex\" images are usually more\nmemorable to machines. We further conduct extensive experiments across 11\ndifferent machines (from linear classifiers to modern ViTs) and 9 pre-training\nmethods to analyze and understand machine memory. This work proposes the\nconcept of machine memorability and opens a new research direction at the\ninterface between machine memory and visual data.\n","authors":["Junlin Han","Huangying Zhan","Jie Hong","Pengfei Fang","Hongdong Li","Lars Petersson","Ian Reid"],"pdf_url":"https://arxiv.org/pdf/2211.07625v2.pdf","comment":"Code: https://github.com/JunlinHan/MachineMem Project page:\n  https://junlinhan.github.io/projects/machinemem.html"},{"id":"http://arxiv.org/abs/2307.05254v1","updated":"2023-07-11T13:36:07Z","published":"2023-07-11T13:36:07Z","title":"OpenAL: An Efficient Deep Active Learning Framework for Open-Set\n  Pathology Image Classification","summary":"  Active learning (AL) is an effective approach to select the most informative\nsamples to label so as to reduce the annotation cost. Existing AL methods\ntypically work under the closed-set assumption, i.e., all classes existing in\nthe unlabeled sample pool need to be classified by the target model. However,\nin some practical clinical tasks, the unlabeled pool may contain not only the\ntarget classes that need to be fine-grainedly classified, but also non-target\nclasses that are irrelevant to the clinical tasks. Existing AL methods cannot\nwork well in this scenario because they tend to select a large number of\nnon-target samples. In this paper, we formulate this scenario as an open-set AL\nproblem and propose an efficient framework, OpenAL, to address the challenge of\nquerying samples from an unlabeled pool with both target class and non-target\nclass samples. Experiments on fine-grained classification of pathology images\nshow that OpenAL can significantly improve the query quality of target class\nsamples and achieve higher performance than current state-of-the-art AL\nmethods. Code is available at https://github.com/miccaiif/OpenAL.\n","authors":["Linhao Qu","Yingfan Ma","Zhiwei Yang","Manning Wang","Zhijian Song"],"pdf_url":"https://arxiv.org/pdf/2307.05254v1.pdf","comment":"Accepted by MICCAI2023"},{"id":"http://arxiv.org/abs/2304.11549v2","updated":"2023-07-11T13:34:50Z","published":"2023-04-23T06:18:07Z","title":"Spectral Sensitivity Estimation Without a Camera","summary":"  A number of problems in computer vision and related fields would be mitigated\nif camera spectral sensitivities were known. As consumer cameras are not\ndesigned for high-precision visual tasks, manufacturers do not disclose\nspectral sensitivities. Their estimation requires a costly optical setup, which\ntriggered researchers to come up with numerous indirect methods that aim to\nlower cost and complexity by using color targets. However, the use of color\ntargets gives rise to new complications that make the estimation more\ndifficult, and consequently, there currently exists no simple, low-cost, robust\ngo-to method for spectral sensitivity estimation. Furthermore, even if not\nlimited by hardware or cost, researchers frequently work with imagery from\nmultiple cameras that they do not have in their possession. To provide a\npractical solution to this problem, we propose a framework for spectral\nsensitivity estimation that not only does not require any hardware, but also\ndoes not require physical access to the camera itself. Similar to other work,\nwe formulate an optimization problem that minimizes a two-term objective\nfunction: a camera-specific term from a system of equations, and a universal\nterm that bounds the solution space. Different than other work, we use publicly\navailable high-quality calibration data to construct both terms. We use the\ncolorimetric mapping matrices provided by the Adobe DNG Converter to formulate\nthe camera-specific system of equations, and constrain the solutions using an\nautoencoder trained on a database of ground-truth curves. On average, we\nachieve reconstruction errors as low as those that can arise due to\nmanufacturing imperfections between two copies of the same camera. We provide\nour code and predicted sensitivities for 1,000+ cameras, and discuss which\ntasks can become trivial when camera responses are available.\n","authors":["Grigory Solomatov","Derya Akkaynak"],"pdf_url":"https://arxiv.org/pdf/2304.11549v2.pdf","comment":"11 pages, 4 figures, conference: ICCP 2023"},{"id":"http://arxiv.org/abs/2307.05650v1","updated":"2023-07-11T13:30:37Z","published":"2023-07-11T13:30:37Z","title":"Evidence-based Hand Hygiene. Can You Trust the Fluorescent-based\n  Assessment Methods?","summary":"  Healthcare-Associated Infections present a major threat to patient safety\nglobally. According to studies, more than 50% of HAI could be prevented by\nproper hand hygiene. Effectiveness of hand hygiene is regularly evaluated with\nthe fluorescent method: performing hand hygiene with a handrub containing an\nultra violet (UV) fluorescent marker. Typically, human experts evaluate the\nhands under UV-A light, and decide whether the applied handrub covered the\nwhole hand surface. The aim of this study was to investigate how different\nexperts judge the same UV-pattern, and compare that to microbiology for\nobjective validation. Hands of volunteer participants were contaminated with\nhigh concentration of a Staphylococcus epidermidis suspension. Hands were\nincompletely disinfected with UV-labeled handrub. Four different UV-box type\ndevices were used to take CCD pictures of the hands under UV light. Size of\ninadequately disinfected areas on the hands were determined in two different\nways. First, based on microbiology; the areas where colonies were grown were\nmeasured. Second, four independent senior infection control specialists were\nasked to mark the missed areas on printed image, captured under UV light. 8\nhands of healthy volunteers were examined. Expert evaluations were highly\nuncorrelated (regarding interrater reliability) and inconsistent. Microbiology\nresults weakly correlated with the expert evaluations. In half of the cases,\nthere were more than 10% difference in the size of properly disinfected area,\nas measured by microbiology versus human experts. Considering the result of the\nexpert evaluations, variability was disconcertingly high. Evaluating the\nfluorescent method is challenging, even for highly experienced professionals. A\npatient safety quality assurance system cannot be built on these data quality.\n","authors":["Száva Bánsághi","Viola Sári","Péter Szerémy","Ákos Lehotsky","Bence Takács","Brigitta K. Tóth","Tamás Haidegger"],"pdf_url":"https://arxiv.org/pdf/2307.05650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05249v1","updated":"2023-07-11T13:29:37Z","published":"2023-07-11T13:29:37Z","title":"DRMC: A Generalist Model with Dynamic Routing for Multi-Center PET Image\n  Synthesis","summary":"  Multi-center positron emission tomography (PET) image synthesis aims at\nrecovering low-dose PET images from multiple different centers. The\ngeneralizability of existing methods can still be suboptimal for a multi-center\nstudy due to domain shifts, which result from non-identical data distribution\namong centers with different imaging systems/protocols. While some approaches\naddress domain shifts by training specialized models for each center, they are\nparameter inefficient and do not well exploit the shared knowledge across\ncenters. To address this, we develop a generalist model that shares\narchitecture and parameters across centers to utilize the shared knowledge.\nHowever, the generalist model can suffer from the center interference issue,\n\\textit{i.e.} the gradient directions of different centers can be inconsistent\nor even opposite owing to the non-identical data distribution. To mitigate such\ninterference, we introduce a novel dynamic routing strategy with cross-layer\nconnections that routes data from different centers to different experts.\nExperiments show that our generalist model with dynamic routing (DRMC) exhibits\nexcellent generalizability across centers. Code and data are available at:\nhttps://github.com/Yaziwel/Multi-Center-PET-Image-Synthesis.\n","authors":["Zhiwen Yang","Yang Zhou","Hui Zhang","Bingzheng Wei","Yubo Fan","Yan Xu"],"pdf_url":"https://arxiv.org/pdf/2307.05249v1.pdf","comment":"This article has been early accepted by MICCAI 2023,but has not been\n  fully edited. Content may change prior to final publication"},{"id":"http://arxiv.org/abs/2307.05241v1","updated":"2023-07-11T13:16:04Z","published":"2023-07-11T13:16:04Z","title":"Does pre-training on brain-related tasks results in better\n  deep-learning-based brain age biomarkers?","summary":"  Brain age prediction using neuroimaging data has shown great potential as an\nindicator of overall brain health and successful aging, as well as a disease\nbiomarker. Deep learning models have been established as reliable and efficient\nbrain age estimators, being trained to predict the chronological age of healthy\nsubjects. In this paper, we investigate the impact of a pre-training step on\ndeep learning models for brain age prediction. More precisely, instead of the\ncommon approach of pre-training on natural imaging classification, we propose\npre-training the models on brain-related tasks, which led to state-of-the-art\nresults in our experiments on ADNI data. Furthermore, we validate the resulting\nbrain age biomarker on images of patients with mild cognitive impairment and\nAlzheimer's disease. Interestingly, our results indicate that better-performing\ndeep learning models in terms of brain age prediction on healthy patients do\nnot result in more reliable biomarkers.\n","authors":["Bruno Machado Pacheco","Victor Hugo Rocha de Oliveira","Augusto Braga Fernandes Antunes","Saulo Domingos de Souza Pedro","Danilo Silva"],"pdf_url":"https://arxiv.org/pdf/2307.05241v1.pdf","comment":"Accepted at BRACIS 2023"},{"id":"http://arxiv.org/abs/2306.03430v2","updated":"2023-07-11T13:00:52Z","published":"2023-06-06T06:09:11Z","title":"Revisiting the Trade-off between Accuracy and Robustness via Weight\n  Distribution of Filters","summary":"  Adversarial attacks have been proven to be potential threats to Deep Neural\nNetworks (DNNs), and many methods are proposed to defend against adversarial\nattacks. However, while enhancing the robustness, the clean accuracy will\ndecline to a certain extent, implying a trade-off existed between the accuracy\nand robustness. In this paper, we firstly empirically find an obvious\ndistinction between standard and robust models in the filters' weight\ndistribution of the same architecture, and then theoretically explain this\nphenomenon in terms of the gradient regularization, which shows this difference\nis an intrinsic property for DNNs, and thus a static network architecture is\ndifficult to improve the accuracy and robustness at the same time. Secondly,\nbased on this observation, we propose a sample-wise dynamic network\narchitecture named Adversarial Weight-Varied Network (AW-Net), which focuses on\ndealing with clean and adversarial examples with a ``divide and rule\" weight\nstrategy. The AW-Net dynamically adjusts network's weights based on regulation\nsignals generated by an adversarial detector, which is directly influenced by\nthe input sample. Benefiting from the dynamic network architecture, clean and\nadversarial examples can be processed with different network weights, which\nprovides the potentiality to enhance the accuracy and robustness\nsimultaneously. A series of experiments demonstrate that our AW-Net is\narchitecture-friendly to handle both clean and adversarial examples and can\nachieve better trade-off performance than state-of-the-art robust models.\n","authors":["Xingxing Wei","Shiji Zhao"],"pdf_url":"https://arxiv.org/pdf/2306.03430v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.16170v2","updated":"2023-07-11T12:46:01Z","published":"2023-06-28T12:47:01Z","title":"Mitigating the Accuracy-Robustness Trade-off via Multi-Teacher\n  Adversarial Distillation","summary":"  Adversarial training is a practical approach for improving the robustness of\ndeep neural networks against adversarial attacks. Although bringing reliable\nrobustness, the performance toward clean examples is negatively affected after\nadversarial training, which means a trade-off exists between accuracy and\nrobustness. Recently, some studies have tried to use knowledge distillation\nmethods in adversarial training, achieving competitive performance in improving\nthe robustness but the accuracy for clean samples is still limited. In this\npaper, to mitigate the accuracy-robustness trade-off, we introduce the\nMulti-Teacher Adversarial Robustness Distillation (MTARD) to guide the model's\nadversarial training process by applying a strong clean teacher and a strong\nrobust teacher to handle the clean examples and adversarial examples,\nrespectively. During the optimization process, to ensure that different\nteachers show similar knowledge scales, we design the Entropy-Based Balance\nalgorithm to adjust the teacher's temperature and keep the teachers'\ninformation entropy consistent. Besides, to ensure that the student has a\nrelatively consistent learning speed from multiple teachers, we propose the\nNormalization Loss Balance algorithm to adjust the learning weights of\ndifferent types of knowledge. A series of experiments conducted on public\ndatasets demonstrate that MTARD outperforms the state-of-the-art adversarial\ntraining and distillation methods against various adversarial attacks.\n","authors":["Shiji Zhao","Xizhe Wang","Xingxing Wei"],"pdf_url":"https://arxiv.org/pdf/2306.16170v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05222v1","updated":"2023-07-11T12:45:39Z","published":"2023-07-11T12:45:39Z","title":"Generative Pretraining in Multimodality","summary":"  We present Emu, a Transformer-based multimodal foundation model, which can\nseamlessly generate images and texts in multimodal context. This omnivore model\ncan take in any single-modality or multimodal data input indiscriminately\n(e.g., interleaved image, text and video) through a one-model-for-all\nautoregressive training process. First, visual signals are encoded into\nembeddings, and together with text tokens form an interleaved input sequence.\nEmu is then end-to-end trained with a unified objective of classifying the next\ntext token or regressing the next visual embedding in the multimodal sequence.\nThis versatile multimodality empowers the exploration of diverse pretraining\ndata sources at scale, such as videos with interleaved frames and text,\nwebpages with interleaved images and text, as well as web-scale image-text\npairs and video-text pairs. Emu can serve as a generalist multimodal interface\nfor both image-to-text and text-to-image tasks, and supports in-context image\nand text generation. Across a broad range of zero-shot/few-shot tasks including\nimage captioning, visual question answering, video question answering and\ntext-to-image generation, Emu demonstrates superb performance compared to\nstate-of-the-art large multimodal models. Extended capabilities such as\nmultimodal assistants via instruction tuning are also demonstrated with\nimpressive performance.\n","authors":["Quan Sun","Qiying Yu","Yufeng Cui","Fan Zhang","Xiaosong Zhang","Yueze Wang","Hongcheng Gao","Jingjing Liu","Tiejun Huang","Xinlong Wang"],"pdf_url":"https://arxiv.org/pdf/2307.05222v1.pdf","comment":"Code and Demo: https://github.com/baaivision/Emu"},{"id":"http://arxiv.org/abs/2303.06388v2","updated":"2023-07-11T12:15:49Z","published":"2023-03-11T11:42:01Z","title":"FAC: 3D Representation Learning via Foreground Aware Feature Contrast","summary":"  Contrastive learning has recently demonstrated great potential for\nunsupervised pre-training in 3D scene understanding tasks. However, most\nexisting work randomly selects point features as anchors while building\ncontrast, leading to a clear bias toward background points that often dominate\nin 3D scenes. Also, object awareness and foreground-to-background\ndiscrimination are neglected, making contrastive learning less effective. To\ntackle these issues, we propose a general foreground-aware feature contrast\n(FAC) framework to learn more effective point cloud representations in\npre-training. FAC consists of two novel contrast designs to construct more\neffective and informative contrast pairs. The first is building positive pairs\nwithin the same foreground segment where points tend to have the same\nsemantics. The second is that we prevent over-discrimination between 3D\nsegments/objects and encourage foreground-to-background distinctions at the\nsegment level with adaptive feature learning in a Siamese correspondence\nnetwork, which adaptively learns feature correlations within and across point\ncloud views effectively. Visualization with point activation maps shows that\nour contrast pairs capture clear correspondences among foreground regions\nduring pre-training. Quantitative experiments also show that FAC achieves\nsuperior knowledge transfer and data efficiency in various downstream 3D\nsemantic segmentation and object detection tasks.\n","authors":["Kangcheng Liu","Aoran Xiao","Xiaoqin Zhang","Shijian Lu","Ling Shao"],"pdf_url":"https://arxiv.org/pdf/2303.06388v2.pdf","comment":"11 pages, IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition 2023 (CVPR 2023), the work is mainly supported by the Natural\n  Science Foundation Project of Fujian Province (2020J01826)"},{"id":"http://arxiv.org/abs/2306.12045v2","updated":"2023-07-11T12:14:10Z","published":"2023-06-21T06:30:18Z","title":"Temporal Conditioning Spiking Latent Variable Models of the Neural\n  Response to Natural Visual Scenes","summary":"  Developing computational models of neural response is crucial for\nunderstanding sensory processing and neural computations. Current\nstate-of-the-art neural network methods use temporal filters to handle temporal\ndependencies, resulting in an unrealistic and inflexible processing flow.\nMeanwhile, these methods target trial-averaged firing rates and fail to capture\nimportant features in spike trains. This work presents the temporal\nconditioning spiking latent variable models (TeCoS-LVM) to simulate the neural\nresponse to natural visual stimuli. We use spiking neurons to produce spike\noutputs that directly match the recorded trains. This approach helps to avoid\nlosing information embedded in the original spike trains. We exclude the\ntemporal dimension from the model parameter space and introduce a temporal\nconditioning operation to allow the model to adaptively explore and exploit\ntemporal dependencies in stimuli sequences in a natural paradigm. We show that\nTeCoS-LVM models can produce more realistic spike activities and accurately fit\nspike statistics than powerful alternatives. Additionally, learned TeCoS-LVM\nmodels can generalize well to longer time scales. Overall, while remaining\ncomputationally tractable, our model effectively captures key features of\nneural coding systems. It thus provides a useful tool for building accurate\npredictive computational accounts for various sensory perception circuits.\n","authors":["Gehua Ma","Runhao Jiang","Rui Yan","Huajin Tang"],"pdf_url":"https://arxiv.org/pdf/2306.12045v2.pdf","comment":"spiking neural networks, neural coding, visual coding, latent\n  variable models, variational information bottleneck, noisy spiking neural\n  networks"},{"id":"http://arxiv.org/abs/2307.05201v1","updated":"2023-07-11T12:10:42Z","published":"2023-07-11T12:10:42Z","title":"The Staged Knowledge Distillation in Video Classification: Harmonizing\n  Student Progress by a Complementary Weakly Supervised Framework","summary":"  In the context of label-efficient learning on video data, the distillation\nmethod and the structural design of the teacher-student architecture have a\nsignificant impact on knowledge distillation. However, the relationship between\nthese factors has been overlooked in previous research. To address this gap, we\npropose a new weakly supervised learning framework for knowledge distillation\nin video classification that is designed to improve the efficiency and accuracy\nof the student model. Our approach leverages the concept of substage-based\nlearning to distill knowledge based on the combination of student substages and\nthe correlation of corresponding substages. We also employ the progressive\ncascade training method to address the accuracy loss caused by the large\ncapacity gap between the teacher and the student. Additionally, we propose a\npseudo-label optimization strategy to improve the initial data label. To\noptimize the loss functions of different distillation substages during the\ntraining process, we introduce a new loss method based on feature distribution.\nWe conduct extensive experiments on both real and simulated data sets,\ndemonstrating that our proposed approach outperforms existing distillation\nmethods in terms of knowledge distillation for video classification tasks. Our\nproposed substage-based distillation approach has the potential to inform\nfuture research on label-efficient learning for video data.\n","authors":["Chao Wang","Zheng Tang"],"pdf_url":"https://arxiv.org/pdf/2307.05201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05066v2","updated":"2023-07-11T12:04:33Z","published":"2023-03-09T06:33:31Z","title":"Distortion-Disentangled Contrastive Learning","summary":"  Self-supervised learning is well known for its remarkable performance in\nrepresentation learning and various downstream computer vision tasks. Recently,\nPositive-pair-Only Contrastive Learning (POCL) has achieved reliable\nperformance without the need to construct positive-negative training sets. It\nreduces memory requirements by lessening the dependency on the batch size. The\nPOCL method typically uses a single loss function to extract the distortion\ninvariant representation (DIR) which describes the proximity of positive-pair\nrepresentations affected by different distortions. This loss function\nimplicitly enables the model to filter out or ignore the distortion variant\nrepresentation (DVR) affected by different distortions. However, existing POCL\nmethods do not explicitly enforce the disentanglement and exploitation of the\nactually valuable DVR. In addition, these POCL methods have been observed to be\nsensitive to augmentation strategies. To address these limitations, we propose\na novel POCL framework named Distortion-Disentangled Contrastive Learning\n(DDCL) and a Distortion-Disentangled Loss (DDL). Our approach is the first to\nexplicitly disentangle and exploit the DVR inside the model and feature stream\nto improve the overall representation utilization efficiency, robustness and\nrepresentation ability. Experiments carried out demonstrate the superiority of\nour framework to Barlow Twins and Simsiam in terms of convergence,\nrepresentation quality, and robustness on several benchmark datasets.\n","authors":["Jinfeng Wang","Sifan Song","Jionglong Su","S. Kevin Zhou"],"pdf_url":"https://arxiv.org/pdf/2303.05066v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.13668v2","updated":"2023-07-11T12:00:52Z","published":"2023-02-27T11:09:13Z","title":"Contrastive Video Question Answering via Video Graph Transformer","summary":"  We propose to perform video question answering (VideoQA) in a Contrastive\nmanner via a Video Graph Transformer model (CoVGT). CoVGT's uniqueness and\nsuperiority are three-fold: 1) It proposes a dynamic graph transformer module\nwhich encodes video by explicitly capturing the visual objects, their relations\nand dynamics, for complex spatio-temporal reasoning. 2) It designs separate\nvideo and text transformers for contrastive learning between the video and text\nto perform QA, instead of multi-modal transformer for answer classification.\nFine-grained video-text communication is done by additional cross-modal\ninteraction modules. 3) It is optimized by the joint fully- and self-supervised\ncontrastive objectives between the correct and incorrect answers, as well as\nthe relevant and irrelevant questions respectively. With superior video\nencoding and QA solution, we show that CoVGT can achieve much better\nperformances than previous arts on video reasoning tasks. Its performances even\nsurpass those models that are pretrained with millions of external data. We\nfurther show that CoVGT can also benefit from cross-modal pretraining, yet with\norders of magnitude smaller data. The results demonstrate the effectiveness and\nsuperiority of CoVGT, and additionally reveal its potential for more\ndata-efficient pretraining. We hope our success can advance VideoQA beyond\ncoarse recognition/description towards fine-grained relation reasoning of video\ncontents. Our code is available at https://github.com/doc-doc/CoVGT.\n","authors":["Junbin Xiao","Pan Zhou","Angela Yao","Yicong Li","Richang Hong","Shuicheng Yan","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2302.13668v2.pdf","comment":"Accepted by IEEE T-PAMI'23"},{"id":"http://arxiv.org/abs/2307.05182v1","updated":"2023-07-11T11:35:40Z","published":"2023-07-11T11:35:40Z","title":"Co-Attention Gated Vision-Language Embedding for Visual Question\n  Localized-Answering in Robotic Surgery","summary":"  Medical students and junior surgeons often rely on senior surgeons and\nspecialists to answer their questions when learning surgery. However, experts\nare often busy with clinical and academic work, and have little time to give\nguidance. Meanwhile, existing deep learning (DL)-based surgical Visual Question\nAnswering (VQA) systems can only provide simple answers without the location of\nthe answers. In addition, vision-language (ViL) embedding is still a less\nexplored research in these kinds of tasks. Therefore, a surgical Visual\nQuestion Localized-Answering (VQLA) system would be helpful for medical\nstudents and junior surgeons to learn and understand from recorded surgical\nvideos. We propose an end-to-end Transformer with Co-Attention gaTed\nVision-Language (CAT-ViL) for VQLA in surgical scenarios, which does not\nrequire feature extraction through detection models. The CAT-ViL embedding\nmodule is designed to fuse heterogeneous features from visual and textual\nsources. The fused embedding will feed a standard Data-Efficient Image\nTransformer (DeiT) module, before the parallel classifier and detector for\njoint prediction. We conduct the experimental validation on public surgical\nvideos from MICCAI EndoVis Challenge 2017 and 2018. The experimental results\nhighlight the superior performance and robustness of our proposed model\ncompared to the state-of-the-art approaches. Ablation studies further prove the\noutstanding performance of all the proposed components. The proposed method\nprovides a promising solution for surgical scene understanding, and opens up a\nprimary step in the Artificial Intelligence (AI)-based VQLA system for surgical\ntraining. Our code is publicly available.\n","authors":["Long Bai","Mobarakol Islam","Hongliang Ren"],"pdf_url":"https://arxiv.org/pdf/2307.05182v1.pdf","comment":"To appear in MICCAI 2023. Code availability:\n  https://github.com/longbai1006/CAT-ViL"},{"id":"http://arxiv.org/abs/2307.05180v1","updated":"2023-07-11T11:32:12Z","published":"2023-07-11T11:32:12Z","title":"ResMatch: Residual Attention Learning for Local Feature Matching","summary":"  Attention-based graph neural networks have made great progress in feature\nmatching learning. However, insight of how attention mechanism works for\nfeature matching is lacked in the literature. In this paper, we rethink cross-\nand self-attention from the viewpoint of traditional feature matching and\nfiltering. In order to facilitate the learning of matching and filtering, we\ninject the similarity of descriptors and relative positions into cross- and\nself-attention score, respectively. In this way, the attention can focus on\nlearning residual matching and filtering functions with reference to the basic\nfunctions of measuring visual and spatial correlation. Moreover, we mine intra-\nand inter-neighbors according to the similarity of descriptors and relative\npositions. Then sparse attention for each point can be performed only within\nits neighborhoods to acquire higher computation efficiency. Feature matching\nnetworks equipped with our full and sparse residual attention learning\nstrategies are termed ResMatch and sResMatch respectively. Extensive\nexperiments, including feature matching, pose estimation and visual\nlocalization, confirm the superiority of our networks.\n","authors":["Yuxin Deng","Jiayi Ma"],"pdf_url":"https://arxiv.org/pdf/2307.05180v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.04014v2","updated":"2023-07-11T11:16:17Z","published":"2023-07-08T16:46:16Z","title":"Novel Pipeline for Diagnosing Acute Lymphoblastic Leukemia Sensitive to\n  Related Biomarkers","summary":"  Acute Lymphoblastic Leukemia (ALL) is one of the most common types of\nchildhood blood cancer. The quick start of the treatment process is critical to\nsaving the patient's life, and for this reason, early diagnosis of this disease\nis essential. Examining the blood smear images of these patients is one of the\nmethods used by expert doctors to diagnose this disease. Deep learning-based\nmethods have numerous applications in medical fields, as they have\nsignificantly advanced in recent years. ALL diagnosis is not an exception in\nthis field, and several machine learning-based methods for this problem have\nbeen proposed. In previous methods, high diagnostic accuracy was reported, but\nour work showed that this alone is not sufficient, as it can lead to models\ntaking shortcuts and not making meaningful decisions. This issue arises due to\nthe small size of medical training datasets. To address this, we constrained\nour model to follow a pipeline inspired by experts' work. We also demonstrated\nthat, since a judgement based on only one image is insufficient, redefining the\nproblem as a multiple-instance learning problem is necessary for achieving a\npractical result. Our model is the first to provide a solution to this problem\nin a multiple-instance learning setup. We introduced a novel pipeline for\ndiagnosing ALL that approximates the process used by hematologists, is\nsensitive to disease biomarkers, and achieves an accuracy of 96.15%, an\nF1-score of 94.24%, a sensitivity of 97.56%, and a specificity of 90.91% on ALL\nIDB 1. Our method was further evaluated on an out-of-distribution dataset,\nwhich posed a challenging test and had acceptable performance. Notably, our\nmodel was trained on a relatively small dataset, highlighting the potential for\nour approach to be applied to other medical datasets with limited data\navailability.\n","authors":["Amirhossein Askari-Farsangi","Ali Sharifi-Zarchi","Mohammad Hossein Rohban"],"pdf_url":"https://arxiv.org/pdf/2307.04014v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.00679v2","updated":"2023-07-11T10:46:52Z","published":"2023-05-01T06:21:35Z","title":"Enhanced Multi-level Features for Very High Resolution Remote Sensing\n  Scene Classification","summary":"  Very high-resolution (VHR) remote sensing (RS) scene classification is a\nchallenging task due to the higher inter-class similarity and intra-class\nvariability problems. Recently, the existing deep learning (DL)-based methods\nhave shown great promise in VHR RS scene classification. However, they still\nprovide an unstable classification performance. To address such a problem, we,\nin this letter, propose a novel DL-based approach. For this, we devise an\nenhanced VHR attention module (EAM), followed by the atrous spatial pyramid\npooling (ASPP) and global average pooling (GAP). This procedure imparts the\nenhanced features from the corresponding level. Then, the multi-level feature\nfusion is performed. Experimental results on two widely-used VHR RS datasets\nshow that the proposed approach yields a competitive and stable/robust\nclassification performance with the least standard deviation of 0.001. Further,\nthe highest overall accuracies on the AID and the NWPU datasets are 95.39% and\n93.04%, respectively.\n","authors":["Chiranjibi Sitaula","Sumesh KC","Jagannath Aryal"],"pdf_url":"https://arxiv.org/pdf/2305.00679v2.pdf","comment":"This paper is under consideration in the International Journal of\n  Intelligent Systems (Wiley) journal. Based on the journal's policy and\n  restrictions, this version may be updated or deleted"},{"id":"http://arxiv.org/abs/2307.05158v1","updated":"2023-07-11T10:30:33Z","published":"2023-07-11T10:30:33Z","title":"A Modular Multimodal Architecture for Gaze Target Prediction:\n  Application to Privacy-Sensitive Settings","summary":"  Predicting where a person is looking is a complex task, requiring to\nunderstand not only the person's gaze and scene content, but also the 3D scene\nstructure and the person's situation (are they manipulating? interacting or\nobserving others? attentive?) to detect obstructions in the line of sight or\napply attention priors that humans typically have when observing others. In\nthis paper, we hypothesize that identifying and leveraging such priors can be\nbetter achieved through the exploitation of explicitly derived multimodal cues\nsuch as depth and pose. We thus propose a modular multimodal architecture\nallowing to combine these cues using an attention mechanism. The architecture\ncan naturally be exploited in privacy-sensitive situations such as surveillance\nand health, where personally identifiable information cannot be released. We\nperform extensive experiments on the GazeFollow and VideoAttentionTarget public\ndatasets, obtaining state-of-the-art performance and demonstrating very\ncompetitive results in the privacy setting case.\n","authors":["Anshul Gupta","Samy Tafasca","Jean-Marc Odobez"],"pdf_url":"https://arxiv.org/pdf/2307.05158v1.pdf","comment":"In the proceedings of the GAZE workshop at CVPR 2022"},{"id":"http://arxiv.org/abs/2307.05151v1","updated":"2023-07-11T10:14:41Z","published":"2023-07-11T10:14:41Z","title":"ExFaceGAN: Exploring Identity Directions in GAN's Learned Latent Space\n  for Synthetic Identity Generation","summary":"  Deep generative models have recently presented impressive results in\ngenerating realistic face images of random synthetic identities. To generate\nmultiple samples of a certain synthetic identity, several previous works\nproposed to disentangle the latent space of GANs by incorporating additional\nsupervision or regularization, enabling the manipulation of certain attributes,\ne.g. identity, hairstyle, pose, or expression. Most of these works require\ndesigning special loss functions and training dedicated network architectures.\nOthers proposed to disentangle specific factors in unconditional pretrained\nGANs latent spaces to control their output, which also requires supervision by\nattribute classifiers. Moreover, these attributes are entangled in GAN's latent\nspace, making it difficult to manipulate them without affecting the identity\ninformation. We propose in this work a framework, ExFaceGAN, to disentangle\nidentity information in state-of-the-art pretrained GANs latent spaces,\nenabling the generation of multiple samples of any synthetic identity. The\nvariations in our generated images are not limited to specific attributes as\nExFaceGAN explicitly aims at disentangling identity information, while other\nvisual attributes are randomly drawn from a learned GAN latent space. As an\nexample of the practical benefit of our ExFaceGAN, we empirically prove that\ndata generated by ExFaceGAN can be successfully used to train face recognition\nmodels.\n","authors":["Fadi Boutros","Marcel Klemt","Meiling Fang","Arjan Kuijper","Naser Damer"],"pdf_url":"https://arxiv.org/pdf/2307.05151v1.pdf","comment":"Accepted at IJCB 2023"},{"id":"http://arxiv.org/abs/2305.18295v2","updated":"2023-07-11T09:33:07Z","published":"2023-05-29T17:59:41Z","title":"RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths","summary":"  Text-to-image generation has recently witnessed remarkable achievements. We\nintroduce a text-conditional image diffusion model, termed RAPHAEL, to generate\nhighly artistic images, which accurately portray the text prompts, encompassing\nmultiple nouns, adjectives, and verbs. This is achieved by stacking tens of\nmixture-of-experts (MoEs) layers, i.e., space-MoE and time-MoE layers, enabling\nbillions of diffusion paths (routes) from the network input to the output. Each\npath intuitively functions as a \"painter\" for depicting a particular textual\nconcept onto a specified image region at a diffusion timestep. Comprehensive\nexperiments reveal that RAPHAEL outperforms recent cutting-edge models, such as\nStable Diffusion, ERNIE-ViLG 2.0, DeepFloyd, and DALL-E 2, in terms of both\nimage quality and aesthetic appeal. Firstly, RAPHAEL exhibits superior\nperformance in switching images across diverse styles, such as Japanese comics,\nrealism, cyberpunk, and ink illustration. Secondly, a single model with three\nbillion parameters, trained on 1,000 A100 GPUs for two months, achieves a\nstate-of-the-art zero-shot FID score of 6.61 on the COCO dataset. Furthermore,\nRAPHAEL significantly surpasses its counterparts in human evaluation on the\nViLG-300 benchmark. We believe that RAPHAEL holds the potential to propel the\nfrontiers of image generation research in both academia and industry, paving\nthe way for future breakthroughs in this rapidly evolving field. More details\ncan be found on a webpage: https://miaohua.sensetime.com/en.\n","authors":["Zeyue Xue","Guanglu Song","Qiushan Guo","Boxiao Liu","Zhuofan Zong","Yu Liu","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2305.18295v2.pdf","comment":"Project Page: https://miaohua.sensetime.com/en"},{"id":"http://arxiv.org/abs/2307.04157v2","updated":"2023-07-11T09:28:36Z","published":"2023-07-09T12:13:43Z","title":"DIFF-NST: Diffusion Interleaving For deFormable Neural Style Transfer","summary":"  Neural Style Transfer (NST) is the field of study applying neural techniques\nto modify the artistic appearance of a content image to match the style of a\nreference style image. Traditionally, NST methods have focused on texture-based\nimage edits, affecting mostly low level information and keeping most image\nstructures the same. However, style-based deformation of the content is\ndesirable for some styles, especially in cases where the style is abstract or\nthe primary concept of the style is in its deformed rendition of some content.\nWith the recent introduction of diffusion models, such as Stable Diffusion, we\ncan access far more powerful image generation techniques, enabling new\npossibilities. In our work, we propose using this new class of models to\nperform style transfer while enabling deformable style transfer, an elusive\ncapability in previous models. We show how leveraging the priors of these\nmodels can expose new artistic controls at inference time, and we document our\nfindings in exploring this new direction for the field of style transfer.\n","authors":["Dan Ruta","Gemma Canet Tarrés","Andrew Gilbert","Eli Shechtman","Nicholas Kolkin","John Collomosse"],"pdf_url":"https://arxiv.org/pdf/2307.04157v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05134v1","updated":"2023-07-11T09:23:05Z","published":"2023-07-11T09:23:05Z","title":"TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation","summary":"  The progress in the generation of synthetic images has made it crucial to\nassess their quality. While several metrics have been proposed to assess the\nrendering of images, it is crucial for Text-to-Image (T2I) models, which\ngenerate images based on a prompt, to consider additional aspects such as to\nwhich extent the generated image matches the important content of the prompt.\nMoreover, although the generated images usually result from a random starting\npoint, the influence of this one is generally not considered. In this article,\nwe propose a new metric based on prompt templates to study the alignment\nbetween the content specified in the prompt and the corresponding generated\nimages. It allows us to better characterize the alignment in terms of the type\nof the specified objects, their number, and their color. We conducted a study\non several recent T2I models about various aspects. An additional interesting\nresult we obtained with our approach is that image quality can vary drastically\ndepending on the latent noise used as a seed for the images. We also quantify\nthe influence of the number of concepts in the prompt, their order as well as\ntheir (color) attributes. Finally, our method allows us to identify some latent\nseeds that produce better images than others, opening novel directions of\nresearch on this understudied topic.\n","authors":["Paul Grimal","Hervé Le Borgne","Olivier Ferret","Julien Tourille"],"pdf_url":"https://arxiv.org/pdf/2307.05134v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.11427v2","updated":"2023-07-11T09:20:40Z","published":"2023-05-19T04:50:13Z","title":"RGB-D And Thermal Sensor Fusion: A Systematic Literature Review","summary":"  In the last decade, the computer vision field has seen significant progress\nin multimodal data fusion and learning, where multiple sensors, including\ndepth, infrared, and visual, are used to capture the environment across diverse\nspectral ranges. Despite these advancements, there has been no systematic and\ncomprehensive evaluation of fusing RGB-D and thermal modalities to date. While\nautonomous driving using LiDAR, radar, RGB, and other sensors has garnered\nsubstantial research interest, along with the fusion of RGB and depth\nmodalities, the integration of thermal cameras and, specifically, the fusion of\nRGB-D and thermal data, has received comparatively less attention. This might\nbe partly due to the limited number of publicly available datasets for such\napplications. This paper provides a comprehensive review of both,\nstate-of-the-art and traditional methods used in fusing RGB-D and thermal\ncamera data for various applications, such as site inspection, human tracking,\nfault detection, and others. The reviewed literature has been categorised into\ntechnical areas, such as 3D reconstruction, segmentation, object detection,\navailable datasets, and other related topics. Following a brief introduction\nand an overview of the methodology, the study delves into calibration and\nregistration techniques, then examines thermal visualisation and 3D\nreconstruction, before discussing the application of classic feature-based\ntechniques as well as modern deep learning approaches. The paper concludes with\na discourse on current limitations and potential future research directions. It\nis hoped that this survey will serve as a valuable reference for researchers\nlooking to familiarise themselves with the latest advancements and contribute\nto the RGB-DT research field.\n","authors":["Martin Brenner","Napoleon H. Reyes","Teo Susnjak","Andre L. C. Barczak"],"pdf_url":"https://arxiv.org/pdf/2305.11427v2.pdf","comment":"34 pages, 21 figures"},{"id":"http://arxiv.org/abs/2307.05129v1","updated":"2023-07-11T09:11:22Z","published":"2023-07-11T09:11:22Z","title":"DFR: Depth from Rotation by Uncalibrated Image Rectification with\n  Latitudinal Motion Assumption","summary":"  Despite the increasing prevalence of rotating-style capture (e.g.,\nsurveillance cameras), conventional stereo rectification techniques frequently\nfail due to the rotation-dominant motion and small baseline between views. In\nthis paper, we tackle the challenge of performing stereo rectification for\nuncalibrated rotating cameras. To that end, we propose Depth-from-Rotation\n(DfR), a novel image rectification solution that analytically rectifies two\nimages with two-point correspondences and serves for further depth estimation.\nSpecifically, we model the motion of a rotating camera as the camera rotates on\na sphere with fixed latitude. The camera's optical axis lies perpendicular to\nthe sphere's surface. We call this latitudinal motion assumption. Then we\nderive a 2-point analytical solver from directly computing the rectified\ntransformations on the two images. We also present a self-adaptive strategy to\nreduce the geometric distortion after rectification. Extensive synthetic and\nreal data experiments demonstrate that the proposed method outperforms existing\nworks in effectiveness and efficiency by a significant margin.\n","authors":["Yongcong Zhang","Yifei Xue","Ming Liao","Huiqing Zhang","Yizhen Lao"],"pdf_url":"https://arxiv.org/pdf/2307.05129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05128v1","updated":"2023-07-11T09:10:16Z","published":"2023-07-11T09:10:16Z","title":"One-Shot Learning for Periocular Recognition: Exploring the Effect of\n  Domain Adaptation and Data Bias on Deep Representations","summary":"  One weakness of machine-learning algorithms is the need to train the models\nfor a new task. This presents a specific challenge for biometric recognition\ndue to the dynamic nature of databases and, in some instances, the reliance on\nsubject collaboration for data collection. In this paper, we investigate the\nbehavior of deep representations in widely used CNN models under extreme data\nscarcity for One-Shot periocular recognition, a biometric recognition task. We\nanalyze the outputs of CNN layers as identity-representing feature vectors. We\nexamine the impact of Domain Adaptation on the network layers' output for\nunseen data and evaluate the method's robustness concerning data normalization\nand generalization of the best-performing layer. We improved state-of-the-art\nresults that made use of networks trained with biometric datasets with millions\nof images and fine-tuned for the target periocular dataset by utilizing\nout-of-the-box CNNs trained for the ImageNet Recognition Challenge and standard\ncomputer vision algorithms. For example, for the Cross-Eyed dataset, we could\nreduce the EER by 67% and 79% (from 1.70% and 3.41% to 0.56% and 0.71%) in the\nClose-World and Open-World protocols, respectively, for the periocular case. We\nalso demonstrate that traditional algorithms like SIFT can outperform CNNs in\nsituations with limited data or scenarios where the network has not been\ntrained with the test classes like the Open-World mode. SIFT alone was able to\nreduce the EER by 64% and 71.6% (from 1.7% and 3.41% to 0.6% and 0.97%) for\nCross-Eyed in the Close-World and Open-World protocols, respectively, and a\nreduction of 4.6% (from 3.94% to 3.76%) in the PolyU database for the\nOpen-World and single biometric case.\n","authors":["Kevin Hernandez-Diaz","Fernando Alonso-Fernandez","Josef Bigun"],"pdf_url":"https://arxiv.org/pdf/2307.05128v1.pdf","comment":"Submitted preprint to IEE Access"},{"id":"http://arxiv.org/abs/2211.07336v2","updated":"2023-07-11T09:01:00Z","published":"2022-11-14T13:22:29Z","title":"An Inter-observer consistent deep adversarial training for visual\n  scanpath prediction","summary":"  The visual scanpath is a sequence of points through which the human gaze\nmoves while exploring a scene. It represents the fundamental concepts upon\nwhich visual attention research is based. As a result, the ability to predict\nthem has emerged as an important task in recent years. In this paper, we\npropose an inter-observer consistent adversarial training approach for scanpath\nprediction through a lightweight deep neural network. The adversarial method\nemploys a discriminative neural network as a dynamic loss that is better suited\nto model the natural stochastic phenomenon while maintaining consistency\nbetween the distributions related to the subjective nature of scanpaths\ntraversed by different observers. Through extensive testing, we show the\ncompetitiveness of our approach in regard to state-of-the-art methods.\n","authors":["Mohamed Amine Kerkouri","Marouane Tliba","Aladine Chetouani","Alessandro Bruno"],"pdf_url":"https://arxiv.org/pdf/2211.07336v2.pdf","comment":"ICIP2023"},{"id":"http://arxiv.org/abs/2005.01344v4","updated":"2023-07-11T08:54:31Z","published":"2020-05-04T09:36:03Z","title":"Tamed Warping Network for High-Resolution Semantic Video Segmentation","summary":"  Recent approaches for fast semantic video segmentation have reduced\nredundancy by warping feature maps across adjacent frames, greatly speeding up\nthe inference phase. However, the accuracy drops seriously owing to the errors\nincurred by warping. In this paper, we propose a novel framework and design a\nsimple and effective correction stage after warping. Specifically, we build a\nnon-key-frame CNN, fusing warped context features with current spatial details.\nBased on the feature fusion, our Context Feature Rectification~(CFR) module\nlearns the model's difference from a per-frame model to correct the warped\nfeatures. Furthermore, our Residual-Guided Attention~(RGA) module utilizes the\nresidual maps in the compressed domain to help CRF focus on error-prone\nregions. Results on Cityscapes show that the accuracy significantly increases\nfrom $67.3\\%$ to $71.6\\%$, and the speed edges down from $65.5$ FPS to $61.8$\nFPS at a resolution of $1024\\times 2048$. For non-rigid categories, e.g.,\n``human'' and ``object'', the improvements are even higher than 18 percentage\npoints.\n","authors":["Songyuan Li","Junyi Feng","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2005.01344v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.16211v3","updated":"2023-07-11T08:49:38Z","published":"2022-11-26T08:48:44Z","title":"ResNeRF: Geometry-Guided Residual Neural Radiance Field for Indoor Scene\n  Novel View Synthesis","summary":"  We represent the ResNeRF, a novel geometry-guided two-stage framework for\nindoor scene novel view synthesis. Be aware of that a good geometry would\ngreatly boost the performance of novel view synthesis, and to avoid the\ngeometry ambiguity issue, we propose to characterize the density distribution\nof the scene based on a base density estimated from scene geometry and a\nresidual density parameterized by the geometry. In the first stage, we focus on\ngeometry reconstruction based on SDF representation, which would lead to a good\ngeometry surface of the scene and also a sharp density. In the second stage,\nthe residual density is learned based on the SDF learned in the first stage for\nencoding more details about the appearance. In this way, our method can better\nlearn the density distribution with the geometry prior for high-fidelity novel\nview synthesis while preserving the 3D structures. Experiments on large-scale\nindoor scenes with many less-observed and textureless areas show that with the\ngood 3D surface, our method achieves state-of-the-art performance for novel\nview synthesis.\n","authors":["Yuting Xiao","Yiqun Zhao","Yanyu Xu","Shenghua Gao"],"pdf_url":"https://arxiv.org/pdf/2211.16211v3.pdf","comment":"This is an incomplete paper"},{"id":"http://arxiv.org/abs/2306.06210v2","updated":"2023-07-11T08:37:56Z","published":"2023-05-26T13:06:38Z","title":"Single-Model Attribution of Generative Models Through Final-Layer\n  Inversion","summary":"  Recent groundbreaking developments on generative modeling have sparked\ninterest in practical single-model attribution. Such methods predict whether a\nsample was generated by a specific generator or not, for instance, to prove\nintellectual property theft. However, previous works are either limited to the\nclosed-world setting or require undesirable changes of the generative model. We\naddress these shortcomings by proposing FLIPAD, a new approach for single-model\nattribution in the open-world setting based on final-layer inversion and\nanomaly detection. We show that the utilized final-layer inversion can be\nreduced to a convex lasso optimization problem, making our approach\ntheoretically sound and computationally efficient. The theoretical findings are\naccompanied by an experimental study demonstrating the effectiveness of our\napproach, outperforming the existing methods.\n","authors":["Mike Laszkiewicz","Jonas Ricker","Johannes Lederer","Asja Fischer"],"pdf_url":"https://arxiv.org/pdf/2306.06210v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05634v1","updated":"2023-07-11T08:18:37Z","published":"2023-07-11T08:18:37Z","title":"Hyperspherical Embedding for Point Cloud Completion","summary":"  Most real-world 3D measurements from depth sensors are incomplete, and to\naddress this issue the point cloud completion task aims to predict the complete\nshapes of objects from partial observations. Previous works often adapt an\nencoder-decoder architecture, where the encoder is trained to extract\nembeddings that are used as inputs to generate predictions from the decoder.\nHowever, the learned embeddings have sparse distribution in the feature space,\nwhich leads to worse generalization results during testing. To address these\nproblems, this paper proposes a hyperspherical module, which transforms and\nnormalizes embeddings from the encoder to be on a unit hypersphere. With the\nproposed module, the magnitude and direction of the output hyperspherical\nembedding are decoupled and only the directional information is optimized. We\ntheoretically analyze the hyperspherical embedding and show that it enables\nmore stable training with a wider range of learning rates and more compact\nembedding distributions. Experiment results show consistent improvement of\npoint cloud completion in both single-task and multi-task learning, which\ndemonstrates the effectiveness of the proposed method.\n","authors":["Junming Zhang","Haomeng Zhang","Ram Vasudevan","Matthew Johnson-Roberson"],"pdf_url":"https://arxiv.org/pdf/2307.05634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.04390v2","updated":"2023-07-11T08:15:04Z","published":"2023-07-10T07:54:29Z","title":"CT-based Subchondral Bone Microstructural Analysis in Knee\n  Osteoarthritis via MR-Guided Distillation Learning","summary":"  Background: MR-based subchondral bone effectively predicts knee\nosteoarthritis. However, its clinical application is limited by the cost and\ntime of MR. Purpose: We aim to develop a novel distillation-learning-based\nmethod named SRRD for subchondral bone microstructural analysis using\neasily-acquired CT images, which leverages paired MR images to enhance the\nCT-based analysis model during training. Materials and Methods: Knee joint\nimages of both CT and MR modalities were collected from October 2020 to May\n2021. Firstly, we developed a GAN-based generative model to transform MR images\ninto CT images, which was used to establish the anatomical correspondence\nbetween the two modalities. Next, we obtained numerous patches of subchondral\nbone regions of MR images, together with their trabecular parameters (BV / TV,\nTb. Th, Tb. Sp, Tb. N) from the corresponding CT image patches via regression.\nThe distillation-learning technique was used to train the regression model and\ntransfer MR structural information to the CT-based model. The regressed\ntrabecular parameters were further used for knee osteoarthritis classification.\nResults: A total of 80 participants were evaluated. CT-based regression results\nof trabecular parameters achieved intra-class correlation coefficients (ICCs)\nof 0.804, 0.773, 0.711, and 0.622 for BV / TV, Tb. Th, Tb. Sp, and Tb. N,\nrespectively. The use of distillation learning significantly improved the\nperformance of the CT-based knee osteoarthritis classification method using the\nCNN approach, yielding an AUC score of 0.767 (95% CI, 0.681-0.853) instead of\n0.658 (95% CI, 0.574-0.742) (p<.001). Conclusions: The proposed SRRD method\nshowed high reliability and validity in MR-CT registration, regression, and\nknee osteoarthritis classification, indicating the feasibility of subchondral\nbone microstructural analysis based on CT images.\n","authors":["Yuqi Hu","Xiangyu Zhao","Gaowei Qing","Kai Xie","Chenglei Liu","Lichi Zhang"],"pdf_url":"https://arxiv.org/pdf/2307.04390v2.pdf","comment":"5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2303.13223v5","updated":"2023-07-11T08:13:55Z","published":"2023-03-23T12:39:20Z","title":"Exploring Structured Semantic Prior for Multi Label Recognition with\n  Incomplete Labels","summary":"  Multi-label recognition (MLR) with incomplete labels is very challenging.\nRecent works strive to explore the image-to-label correspondence in the\nvision-language model, \\ie, CLIP, to compensate for insufficient annotations.\nIn spite of promising performance, they generally overlook the valuable prior\nabout the label-to-label correspondence. In this paper, we advocate remedying\nthe deficiency of label supervision for the MLR with incomplete labels by\nderiving a structured semantic prior about the label-to-label correspondence\nvia a semantic prior prompter. We then present a novel Semantic Correspondence\nPrompt Network (SCPNet), which can thoroughly explore the structured semantic\nprior. A Prior-Enhanced Self-Supervised Learning method is further introduced\nto enhance the use of the prior. Comprehensive experiments and analyses on\nseveral widely used benchmark datasets show that our method significantly\noutperforms existing methods on all datasets, well demonstrating the\neffectiveness and the superiority of our method. Our code will be available at\nhttps://github.com/jameslahm/SCPNet.\n","authors":["Zixuan Ding","Ao Wang","Hui Chen","Qiang Zhang","Pengzhang Liu","Yongjun Bao","Weipeng Yan","Jungong Han"],"pdf_url":"https://arxiv.org/pdf/2303.13223v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05092v1","updated":"2023-07-11T07:52:06Z","published":"2023-07-11T07:52:06Z","title":"Offline and Online Optical Flow Enhancement for Deep Video Compression","summary":"  Video compression relies heavily on exploiting the temporal redundancy\nbetween video frames, which is usually achieved by estimating and using the\nmotion information. The motion information is represented as optical flows in\nmost of the existing deep video compression networks. Indeed, these networks\noften adopt pre-trained optical flow estimation networks for motion estimation.\nThe optical flows, however, may be less suitable for video compression due to\nthe following two factors. First, the optical flow estimation networks were\ntrained to perform inter-frame prediction as accurately as possible, but the\noptical flows themselves may cost too many bits to encode. Second, the optical\nflow estimation networks were trained on synthetic data, and may not generalize\nwell enough to real-world videos. We address the twofold limitations by\nenhancing the optical flows in two stages: offline and online. In the offline\nstage, we fine-tune a trained optical flow estimation network with the motion\ninformation provided by a traditional (non-deep) video compression scheme, e.g.\nH.266/VVC, as we believe the motion information of H.266/VVC achieves a better\nrate-distortion trade-off. In the online stage, we further optimize the latent\nfeatures of the optical flows with a gradient descent-based algorithm for the\nvideo to be compressed, so as to enhance the adaptivity of the optical flows.\nWe conduct experiments on a state-of-the-art deep video compression scheme,\nDCVC. Experimental results demonstrate that the proposed offline and online\nenhancement together achieves on average 12.8% bitrate saving on the tested\nvideos, without increasing the model or computational complexity of the decoder\nside.\n","authors":["Chuanbo Tang","Xihua Sheng","Zhuoyuan Li","Haotian Zhang","Li Li","Dong Liu"],"pdf_url":"https://arxiv.org/pdf/2307.05092v1.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2307.05087v1","updated":"2023-07-11T07:37:56Z","published":"2023-07-11T07:37:56Z","title":"SAR-NeRF: Neural Radiance Fields for Synthetic Aperture Radar Multi-View\n  Representation","summary":"  SAR images are highly sensitive to observation configurations, and they\nexhibit significant variations across different viewing angles, making it\nchallenging to represent and learn their anisotropic features. As a result,\ndeep learning methods often generalize poorly across different view angles.\nInspired by the concept of neural radiance fields (NeRF), this study combines\nSAR imaging mechanisms with neural networks to propose a novel NeRF model for\nSAR image generation. Following the mapping and projection pinciples, a set of\nSAR images is modeled implicitly as a function of attenuation coefficients and\nscattering intensities in the 3D imaging space through a differentiable\nrendering equation. SAR-NeRF is then constructed to learn the distribution of\nattenuation coefficients and scattering intensities of voxels, where the\nvectorized form of 3D voxel SAR rendering equation and the sampling\nrelationship between the 3D space voxels and the 2D view ray grids are\nanalytically derived. Through quantitative experiments on various datasets, we\nthoroughly assess the multi-view representation and generalization capabilities\nof SAR-NeRF. Additionally, it is found that SAR-NeRF augumented dataset can\nsignificantly improve SAR target classification performance under few-shot\nlearning setup, where a 10-type classification accuracy of 91.6\\% can be\nachieved by using only 12 images per class.\n","authors":["Zhengxin Lei","Feng Xu","Jiangtao Wei","Feng Cai","Feng Wang","Ya-Qiu Jin"],"pdf_url":"https://arxiv.org/pdf/2307.05087v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2307.05827v1","updated":"2023-07-11T22:36:47Z","published":"2023-07-11T22:36:47Z","title":"Relational Extraction on Wikipedia Tables using Convolutional and Memory\n  Networks","summary":"  Relation extraction (RE) is the task of extracting relations between entities\nin text. Most RE methods extract relations from free-form running text and\nleave out other rich data sources, such as tables. We explore RE from the\nperspective of applying neural methods on tabularly organized data. We\nintroduce a new model consisting of Convolutional Neural Network (CNN) and\nBidirectional-Long Short Term Memory (BiLSTM) network to encode entities and\nlearn dependencies among them, respectively. We evaluate our model on a large\nand recent dataset and compare results with previous neural methods.\nExperimental results show that our model consistently outperforms the previous\nmodel for the task of relation extraction on tabular data. We perform\ncomprehensive error analyses and ablation study to show the contribution of\nvarious components of our model. Finally, we discuss the usefulness and\ntrade-offs of our approach, and provide suggestions for fostering further\nresearch.\n","authors":["Arif Shahriar","Rohan Saha","Denilson Barbosa"],"pdf_url":"https://arxiv.org/pdf/2307.05827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05414v1","updated":"2023-07-11T16:30:45Z","published":"2023-07-11T16:30:45Z","title":"Duncode Characters Shorter","summary":"  This paper investigates the employment of various encoders in text\ntransformation, converting characters into bytes. It discusses local encoders\nsuch as ASCII and GB-2312, which encode specific characters into shorter bytes,\nand universal encoders like UTF-8 and UTF-16, which can encode the complete\nUnicode set with greater space requirements and are gaining widespread\nacceptance. Other encoders, including SCSU, BOCU-1, and binary encoders,\nhowever, lack self-synchronizing capabilities. Duncode is introduced as an\ninnovative encoding method that aims to encode the entire Unicode character set\nwith high space efficiency, akin to local encoders. It has the potential to\ncompress multiple characters of a string into a Duncode unit using fewer bytes.\nDespite offering less self-synchronizing identification information, Duncode\nsurpasses UTF8 in terms of space efficiency. The application is available at\n\\url{https://github.com/laohur/duncode}. Additionally, we have developed a\nbenchmark for evaluating character encoders across different languages. It\nencompasses 179 languages and can be accessed at\n\\url{https://github.com/laohur/wiki2txt}.\n","authors":["Changshang Xue"],"pdf_url":"https://arxiv.org/pdf/2307.05414v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05268v1","updated":"2023-07-11T14:02:06Z","published":"2023-07-11T14:02:06Z","title":"Temporal Graphs Anomaly Emergence Detection: Benchmarking For Social\n  Media Interactions","summary":"  Temporal graphs have become an essential tool for analyzing complex dynamic\nsystems with multiple agents. Detecting anomalies in temporal graphs is crucial\nfor various applications, including identifying emerging trends, monitoring\nnetwork security, understanding social dynamics, tracking disease outbreaks,\nand understanding financial dynamics. In this paper, we present a comprehensive\nbenchmarking study that compares 12 data-driven methods for anomaly detection\nin temporal graphs. We conduct experiments on two temporal graphs extracted\nfrom Twitter and Facebook, aiming to identify anomalies in group interactions.\nSurprisingly, our study reveals an unclear pattern regarding the best method\nfor such tasks, highlighting the complexity and challenges involved in anomaly\nemergence detection in large and dynamic systems. The results underscore the\nneed for further research and innovative approaches to effectively detect\nemerging anomalies in dynamic systems represented as temporal graphs.\n","authors":["Teddy Lazebnik","Or Iny"],"pdf_url":"https://arxiv.org/pdf/2307.05268v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05260v1","updated":"2023-07-11T13:51:12Z","published":"2023-07-11T13:51:12Z","title":"U-CREAT: Unsupervised Case Retrieval using Events extrAcTion","summary":"  The task of Prior Case Retrieval (PCR) in the legal domain is about\nautomatically citing relevant (based on facts and precedence) prior legal cases\nin a given query case. To further promote research in PCR, in this paper, we\npropose a new large benchmark (in English) for the PCR task: IL-PCR (Indian\nLegal Prior Case Retrieval) corpus. Given the complex nature of case relevance\nand the long size of legal documents, BM25 remains a strong baseline for\nranking the cited prior documents. In this work, we explore the role of events\nin legal case retrieval and propose an unsupervised retrieval method-based\npipeline U-CREAT (Unsupervised Case Retrieval using Events Extraction). We find\nthat the proposed unsupervised retrieval method significantly increases\nperformance compared to BM25 and makes retrieval faster by a considerable\nmargin, making it applicable to real-time case retrieval systems. Our proposed\nsystem is generic, we show that it generalizes across two different legal\nsystems (Indian and Canadian), and it shows state-of-the-art performance on the\nbenchmarks for both the legal systems (IL-PCR and COLIEE corpora).\n","authors":["Abhinav Joshi","Akshat Sharma","Sai Kiran Tanikella","Ashutosh Modi"],"pdf_url":"https://arxiv.org/pdf/2307.05260v1.pdf","comment":"Accepted at ACL 2023, 15 pages (12 main + 3 Appendix)"},{"id":"http://arxiv.org/abs/2302.11266v2","updated":"2023-07-11T13:47:09Z","published":"2023-02-22T10:25:54Z","title":"One-Shot Labeling for Automatic Relevance Estimation","summary":"  Dealing with unjudged documents (\"holes\") in relevance assessments is a\nperennial problem when evaluating search systems with offline experiments.\nHoles can reduce the apparent effectiveness of retrieval systems during\nevaluation and introduce biases in models trained with incomplete data. In this\nwork, we explore whether large language models can help us fill such holes to\nimprove offline evaluations. We examine an extreme, albeit common, evaluation\nsetting wherein only a single known relevant document per query is available\nfor evaluation. We then explore various approaches for predicting the relevance\nof unjudged documents with respect to a query and the known relevant document,\nincluding nearest neighbor, supervised, and prompting techniques. We find that\nalthough the predictions of these One-Shot Labelers (1SL) frequently disagree\nwith human assessments, the labels they produce yield a far more reliable\nranking of systems than the single labels do alone. Specifically, the strongest\napproaches can consistently reach system ranking correlations of over 0.86 with\nthe full rankings over a variety of measures. Meanwhile, the approach\nsubstantially increases the reliability of t-tests due to filling holes in\nrelevance assessments, giving researchers more confidence in results they find\nto be significant. Alongside this work, we release an easy-to-use software\npackage to enable the use of 1SL for evaluation of other ad-hoc collections or\nsystems.\n","authors":["Sean MacAvaney","Luca Soldaini"],"pdf_url":"https://arxiv.org/pdf/2302.11266v2.pdf","comment":"SIGIR 2023"},{"id":"http://arxiv.org/abs/2307.05100v1","updated":"2023-07-11T08:20:53Z","published":"2023-07-11T08:20:53Z","title":"Generative Contrastive Graph Learning for Recommendation","summary":"  By treating users' interactions as a user-item graph, graph learning models\nhave been widely deployed in Collaborative Filtering(CF) based recommendation.\nRecently, researchers have introduced Graph Contrastive Learning(GCL)\ntechniques into CF to alleviate the sparse supervision issue, which first\nconstructs contrastive views by data augmentations and then provides\nself-supervised signals by maximizing the mutual information between\ncontrastive views. Despite the effectiveness, we argue that current GCL-based\nrecommendation models are still limited as current data augmentation\ntechniques, either structure augmentation or feature augmentation. First,\nstructure augmentation randomly dropout nodes or edges, which is easy to\ndestroy the intrinsic nature of the user-item graph. Second, feature\naugmentation imposes the same scale noise augmentation on each node, which\nneglects the unique characteristics of nodes on the graph. To tackle the above\nlimitations, we propose a novel Variational Graph Generative-Contrastive\nLearning(VGCL) framework for recommendation. Specifically, we leverage\nvariational graph reconstruction to estimate a Gaussian distribution of each\nnode, then generate multiple contrastive views through multiple samplings from\nthe estimated distributions, which builds a bridge between generative and\ncontrastive learning. Besides, the estimated variances are tailored to each\nnode, which regulates the scale of contrastive loss for each node on\noptimization. Considering the similarity of the estimated distributions, we\npropose a cluster-aware twofold contrastive learning, a node-level to encourage\nconsistency of a node's contrastive views and a cluster-level to encourage\nconsistency of nodes in a cluster. Finally, extensive experimental results on\nthree public datasets clearly demonstrate the effectiveness of the proposed\nmodel.\n","authors":["Yonghui Yang","Zhengwei Wu","Le Wu","Kun Zhang","Richang Hong","Zhiqiang Zhang","Jun Zhou","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2307.05100v1.pdf","comment":"This paper is accepted to SIGIR 2023. Code is avaliable:\n  https://github.com/yimutianyang/SIGIR23-VGCL"},{"id":"http://arxiv.org/abs/2305.13825v2","updated":"2023-07-11T08:02:42Z","published":"2023-05-23T08:49:19Z","title":"Continual Learning on Dynamic Graphs via Parameter Isolation","summary":"  Many real-world graph learning tasks require handling dynamic graphs where\nnew nodes and edges emerge. Dynamic graph learning methods commonly suffer from\nthe catastrophic forgetting problem, where knowledge learned for previous\ngraphs is overwritten by updates for new graphs. To alleviate the problem,\ncontinual graph learning methods are proposed. However, existing continual\ngraph learning methods aim to learn new patterns and maintain old ones with the\nsame set of parameters of fixed size, and thus face a fundamental tradeoff\nbetween both goals. In this paper, we propose Parameter Isolation GNN (PI-GNN)\nfor continual learning on dynamic graphs that circumvents the tradeoff via\nparameter isolation and expansion. Our motivation lies in that different\nparameters contribute to learning different graph patterns. Based on the idea,\nwe expand model parameters to continually learn emerging graph patterns.\nMeanwhile, to effectively preserve knowledge for unaffected patterns, we find\nparameters that correspond to them via optimization and freeze them to prevent\nthem from being rewritten. Experiments on eight real-world datasets corroborate\nthe effectiveness of PI-GNN compared to state-of-the-art baselines.\n","authors":["Peiyan Zhang","Yuchen Yan","Chaozhuo Li","Senzhang Wang","Xing Xie","Guojie Song","Sunghun Kim"],"pdf_url":"https://arxiv.org/pdf/2305.13825v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05074v1","updated":"2023-07-11T07:16:22Z","published":"2023-07-11T07:16:22Z","title":"Retrieval-augmented GPT-3.5-based Text-to-SQL Framework with\n  Sample-aware Prompting and Dynamic Revision Chain","summary":"  Text-to-SQL aims at generating SQL queries for the given natural language\nquestions and thus helping users to query databases. Prompt learning with large\nlanguage models (LLMs) has emerged as a recent approach, which designs prompts\nto lead LLMs to understand the input question and generate the corresponding\nSQL. However, it faces challenges with strict SQL syntax requirements. Existing\nwork prompts the LLMs with a list of demonstration examples (i.e. question-SQL\npairs) to generate SQL, but the fixed prompts can hardly handle the scenario\nwhere the semantic gap between the retrieved demonstration and the input\nquestion is large. In this paper, we propose a retrieval-augmented prompting\nmethod for a LLM-based Text-to-SQL framework, involving sample-aware prompting\nand a dynamic revision chain. Our approach incorporates sample-aware\ndemonstrations, which include the composition of SQL operators and fine-grained\ninformation related to the given question. To retrieve questions sharing\nsimilar intents with input questions, we propose two strategies for assisting\nretrieval. Firstly, we leverage LLMs to simplify the original questions,\nunifying the syntax and thereby clarifying the users' intentions. To generate\nexecutable and accurate SQLs without human intervention, we design a dynamic\nrevision chain which iteratively adapts fine-grained feedback from the\npreviously generated SQL. Experimental results on three Text-to-SQL benchmarks\ndemonstrate the superiority of our method over strong baseline models.\n","authors":["Chunxi Guo","Zhiliang Tian","Jintao Tang","Shasha Li","Zhihua Wen","Kaixuan Wang","Ting Wang"],"pdf_url":"https://arxiv.org/pdf/2307.05074v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05071v1","updated":"2023-07-11T07:14:53Z","published":"2023-07-11T07:14:53Z","title":"Mining for Unknown Unknowns","summary":"  Unknown unknowns are future relevant contingencies that lack an ex ante\ndescription. While there are numerous retrospective accounts showing that\nsignificant gains or losses might have been achieved or avoided had such\ncontingencies been previously uncovered, getting hold of unknown unknowns still\nremains elusive, both in practice and conceptually. Using Formal Concept\nAnalysis (FCA) - a subfield of lattice theory which is increasingly applied for\nmining and organizing data - this paper introduces a simple framework to\nsystematically think out of the box and direct the search for unknown unknowns.\n","authors":["Bernard Sinclair-Desgagné"],"pdf_url":"https://arxiv.org/pdf/2307.05071v1.pdf","comment":"In Proceedings TARK 2023, arXiv:2307.04005"},{"id":"http://arxiv.org/abs/2307.05036v1","updated":"2023-07-11T06:29:31Z","published":"2023-07-11T06:29:31Z","title":"Neural-Symbolic Recommendation with Graph-Enhanced Information","summary":"  The recommendation system is not only a problem of inductive statistics from\ndata but also a cognitive task that requires reasoning ability. The most\nadvanced graph neural networks have been widely used in recommendation systems\nbecause they can capture implicit structured information from graph-structured\ndata. However, like most neural network algorithms, they only learn matching\npatterns from a perception perspective. Some researchers use user behavior for\nlogic reasoning to achieve recommendation prediction from the perspective of\ncognitive reasoning, but this kind of reasoning is a local one and ignores\nimplicit information on a global scale. In this work, we combine the advantages\nof graph neural networks and propositional logic operations to construct a\nneuro-symbolic recommendation model with both global implicit reasoning ability\nand local explicit logic reasoning ability. We first build an item-item graph\nbased on the principle of adjacent interaction and use graph neural networks to\ncapture implicit information in global data. Then we transform user behavior\ninto propositional logic expressions to achieve recommendations from the\nperspective of cognitive reasoning. Extensive experiments on five public\ndatasets show that our proposed model outperforms several state-of-the-art\nmethods, source code is avaliable at [https://github.com/hanzo2020/GNNLR].\n","authors":["Bang Chen","Wei Peng","Maonian Wu","Bo Zheng","Shaojun Zhu"],"pdf_url":"https://arxiv.org/pdf/2307.05036v1.pdf","comment":"12 pages, 2 figures, conference"},{"id":"http://arxiv.org/abs/2307.04996v1","updated":"2023-07-11T03:24:54Z","published":"2023-07-11T03:24:54Z","title":"Empowering recommender systems using automatically generated Knowledge\n  Graphs and Reinforcement Learning","summary":"  Personalized recommendations have a growing importance in direct marketing,\nwhich motivates research to enhance customer experiences by knowledge graph\n(KG) applications. For example, in financial services, companies may benefit\nfrom providing relevant financial articles to their customers to cultivate\nrelationships, foster client engagement and promote informed financial\ndecisions. While several approaches center on KG-based recommender systems for\nimproved content, in this study we focus on interpretable KG-based recommender\nsystems for decision making.To this end, we present two knowledge graph-based\napproaches for personalized article recommendations for a set of customers of a\nlarge multinational financial services company. The first approach employs\nReinforcement Learning and the second approach uses the XGBoost algorithm for\nrecommending articles to the customers. Both approaches make use of a KG\ngenerated from both structured (tabular data) and unstructured data (a large\nbody of text data).Using the Reinforcement Learning-based recommender system we\ncould leverage the graph traversal path leading to the recommendation as a way\nto generate interpretations (Path Directed Reasoning (PDR)). In the\nXGBoost-based approach, one can also provide explainable results using post-hoc\nmethods such as SHAP (SHapley Additive exPlanations) and ELI5 (Explain Like I\nam Five).Importantly, our approach offers explainable results, promoting better\ndecision-making. This study underscores the potential of combining advanced\nmachine learning techniques with KG-driven insights to bolster experience in\ncustomer relationship management.\n","authors":["Ghanshyam Verma","Shovon Sengupta","Simon Simanta","Huan Chen","Janos A. Perge","Devishree Pillai","John P. McCrae","Paul Buitelaar"],"pdf_url":"https://arxiv.org/pdf/2307.04996v1.pdf","comment":"Accepted at KDD (OARS) 2023 [https://oars-workshop.github.io/]"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2307.05845v1","updated":"2023-07-11T23:36:49Z","published":"2023-07-11T23:36:49Z","title":"PIGEON: Predicting Image Geolocations","summary":"  We introduce PIGEON, a multi-task end-to-end system for planet-scale image\ngeolocalization that achieves state-of-the-art performance on both external\nbenchmarks and in human evaluation. Our work incorporates semantic geocell\ncreation with label smoothing, conducts pretraining of a vision transformer on\nimages with geographic information, and refines location predictions with\nProtoNets across a candidate set of geocells. The contributions of PIGEON are\nthree-fold: first, we design a semantic geocells creation and splitting\nalgorithm based on open-source data which can be adapted to any geospatial\ndataset. Second, we show the effectiveness of intra-geocell refinement and the\napplicability of unsupervised clustering and ProtNets to the task. Finally, we\nmake our pre-trained CLIP transformer model, StreetCLIP, publicly available for\nuse in adjacent domains with applications to fighting climate change and urban\nand rural scene understanding.\n","authors":["Lukas Haas","Silas Alberti","Michal Skreta"],"pdf_url":"https://arxiv.org/pdf/2307.05845v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05834v1","updated":"2023-07-11T22:58:53Z","published":"2023-07-11T22:58:53Z","title":"Scaling Distributed Multi-task Reinforcement Learning with Experience\n  Sharing","summary":"  Recently, DARPA launched the ShELL program, which aims to explore how\nexperience sharing can benefit distributed lifelong learning agents in adapting\nto new challenges. In this paper, we address this issue by conducting both\ntheoretical and empirical research on distributed multi-task reinforcement\nlearning (RL), where a group of $N$ agents collaboratively solves $M$ tasks\nwithout prior knowledge of their identities. We approach the problem by\nformulating it as linearly parameterized contextual Markov decision processes\n(MDPs), where each task is represented by a context that specifies the\ntransition dynamics and rewards. To tackle this problem, we propose an\nalgorithm called DistMT-LSVI. First, the agents identify the tasks, and then\nthey exchange information through a central server to derive $\\epsilon$-optimal\npolicies for the tasks. Our research demonstrates that to achieve\n$\\epsilon$-optimal policies for all $M$ tasks, a single agent using DistMT-LSVI\nneeds to run a total number of episodes that is at most\n$\\tilde{\\mathcal{O}}({d^3H^6(\\epsilon^{-2}+c_{\\rm sep}^{-2})}\\cdot M/N)$, where\n$c_{\\rm sep}>0$ is a constant representing task separability, $H$ is the\nhorizon of each episode, and $d$ is the feature dimension of the dynamics and\nrewards. Notably, DistMT-LSVI improves the sample complexity of non-distributed\nsettings by a factor of $1/N$, as each agent independently learns\n$\\epsilon$-optimal policies for all $M$ tasks using\n$\\tilde{\\mathcal{O}}(d^3H^6M\\epsilon^{-2})$ episodes. Additionally, we provide\nnumerical experiments conducted on OpenAI Gym Atari environments that validate\nour theoretical findings.\n","authors":["Sanae Amani","Khushbu Pahwa","Vladimir Braverman","Lin F. Yang"],"pdf_url":"https://arxiv.org/pdf/2307.05834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05831v1","updated":"2023-07-11T22:53:09Z","published":"2023-07-11T22:53:09Z","title":"Memorization Through the Lens of Curvature of Loss Function Around\n  Samples","summary":"  Neural networks are overparametrized and easily overfit the datasets they\ntrain on. In the extreme case, it is shown that they can memorize a training\nset with fully randomized labels. We propose using the curvature of loss\nfunction around the training sample as a measure of its memorization, averaged\nover all training epochs. We use this to study the generalization versus\nmemorization properties of different samples in popular image datasets. We\nvisualize samples with the highest curvature of loss around them, and show that\nthese visually correspond to long-tailed, mislabeled or conflicting samples.\nThis analysis helps us find a, to the best of our knowledge, novel failure\nmodel on the CIFAR100 dataset, that of duplicated images with different labels.\nWe also synthetically mislabel a proportion of the dataset by randomly\ncorrupting the labels of a few samples, and show that sorting by curvature\nyields high AUROC values for identifying the mislabeled samples.\n","authors":["Isha Garg","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2307.05831v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2211.01753v2","updated":"2023-07-11T22:46:12Z","published":"2022-11-01T12:16:30Z","title":"Looking Beyond IoCs: Automatically Extracting Attack Patterns from\n  External CTI","summary":"  Public and commercial organizations extensively share cyberthreat\nintelligence (CTI) to prepare systems to defend against existing and emerging\ncyberattacks. However, traditional CTI has primarily focused on tracking known\nthreat indicators such as IP addresses and domain names, which may not provide\nlong-term value in defending against evolving attacks. To address this\nchallenge, we propose to use more robust threat intelligence signals called\nattack patterns. LADDER is a knowledge extraction framework that can extract\ntext-based attack patterns from CTI reports at scale. The framework\ncharacterizes attack patterns by capturing the phases of an attack in Android\nand enterprise networks and systematically maps them to the MITRE ATT\\&CK\npattern framework. LADDER can be used by security analysts to determine the\npresence of attack vectors related to existing and emerging threats, enabling\nthem to prepare defenses proactively. We also present several use cases to\ndemonstrate the application of LADDER in real-world scenarios. Finally, we\nprovide a new, open-access benchmark malware dataset to train future\ncyberthreat intelligence models.\n","authors":["Md Tanvirul Alam","Dipkamal Bhusal","Youngja Park","Nidhi Rastogi"],"pdf_url":"https://arxiv.org/pdf/2211.01753v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05827v1","updated":"2023-07-11T22:36:47Z","published":"2023-07-11T22:36:47Z","title":"Relational Extraction on Wikipedia Tables using Convolutional and Memory\n  Networks","summary":"  Relation extraction (RE) is the task of extracting relations between entities\nin text. Most RE methods extract relations from free-form running text and\nleave out other rich data sources, such as tables. We explore RE from the\nperspective of applying neural methods on tabularly organized data. We\nintroduce a new model consisting of Convolutional Neural Network (CNN) and\nBidirectional-Long Short Term Memory (BiLSTM) network to encode entities and\nlearn dependencies among them, respectively. We evaluate our model on a large\nand recent dataset and compare results with previous neural methods.\nExperimental results show that our model consistently outperforms the previous\nmodel for the task of relation extraction on tabular data. We perform\ncomprehensive error analyses and ablation study to show the contribution of\nvarious components of our model. Finally, we discuss the usefulness and\ntrade-offs of our approach, and provide suggestions for fostering further\nresearch.\n","authors":["Arif Shahriar","Rohan Saha","Denilson Barbosa"],"pdf_url":"https://arxiv.org/pdf/2307.05827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05825v1","updated":"2023-07-11T22:16:13Z","published":"2023-07-11T22:16:13Z","title":"Bayesian taut splines for estimating the number of modes","summary":"  The number of modes in a probability density function is representative of\nthe model's complexity and can also be viewed as the number of existing\nsubpopulations. Despite its relevance, little research has been devoted to its\nestimation. Focusing on the univariate setting, we propose a novel approach\ntargeting prediction accuracy inspired by some overlooked aspects of the\nproblem. We argue for the need for structure in the solutions, the subjective\nand uncertain nature of modes, and the convenience of a holistic view blending\nglobal and local density properties. Our method builds upon a combination of\nflexible kernel estimators and parsimonious compositional splines. Feature\nexploration, model selection and mode testing are implemented in the Bayesian\ninference paradigm, providing soft solutions and allowing to incorporate expert\njudgement in the process. The usefulness of our proposal is illustrated through\na case study in sports analytics, showcasing multiple companion visualisation\ntools. A thorough simulation study demonstrates that traditional\nmodality-driven approaches paradoxically struggle to provide accurate results.\nIn this context, our method emerges as a top-tier alternative offering\ninnovative solutions for analysts.\n","authors":["José E. Chacón","Javier Fernández Serrano"],"pdf_url":"https://arxiv.org/pdf/2307.05825v1.pdf","comment":"28 pages, 15 figures (manuscript) + 11 pages, 10 figures\n  (supplementary material)"},{"id":"http://arxiv.org/abs/2302.00766v2","updated":"2023-07-11T22:13:52Z","published":"2023-02-01T21:32:22Z","title":"Privacy Risk for anisotropic Langevin dynamics using relative entropy\n  bounds","summary":"  The privacy preserving properties of Langevin dynamics with additive\nisotropic noise have been extensively studied. However, the isotropic noise\nassumption is very restrictive: (a) when adding noise to existing learning\nalgorithms to preserve privacy and maintain the best possible accuracy one\nshould take into account the relative magnitude of the outputs and their\ncorrelations; (b) popular algorithms such as stochastic gradient descent (and\ntheir continuous time limits) appear to possess anisotropic covariance\nproperties. To study the privacy risks for the anisotropic noise case, one\nrequires general results on the relative entropy between the laws of two\nStochastic Differential Equations with different drifts and diffusion\ncoefficients. Our main contribution is to establish such a bound using\nstability estimates for solutions to the Fokker-Planck equations via functional\ninequalities. With additional assumptions, the relative entropy bound implies\nan $(\\epsilon,\\delta)$-differential privacy bound or translates to bounds on\nthe membership inference attack success and we show how anisotropic noise can\nlead to better privacy-accuracy trade-offs. Finally, the benefits of\nanisotropic noise are illustrated using numerical results in quadratic loss and\nneural network setups.\n","authors":["Anastasia Borovykh","Nikolas Kantas","Panos Parpas","Greg Pavliotis"],"pdf_url":"https://arxiv.org/pdf/2302.00766v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.01075v4","updated":"2023-07-11T22:07:48Z","published":"2023-02-02T13:05:27Z","title":"MonoFlow: Rethinking Divergence GANs via the Perspective of Wasserstein\n  Gradient Flows","summary":"  The conventional understanding of adversarial training in generative\nadversarial networks (GANs) is that the discriminator is trained to estimate a\ndivergence, and the generator learns to minimize this divergence. We argue that\ndespite the fact that many variants of GANs were developed following this\nparadigm, the current theoretical understanding of GANs and their practical\nalgorithms are inconsistent. In this paper, we leverage Wasserstein gradient\nflows which characterize the evolution of particles in the sample space, to\ngain theoretical insights and algorithmic inspiration of GANs. We introduce a\nunified generative modeling framework - MonoFlow: the particle evolution is\nrescaled via a monotonically increasing mapping of the log density ratio. Under\nour framework, adversarial training can be viewed as a procedure first\nobtaining MonoFlow's vector field via training the discriminator and the\ngenerator learns to draw the particle flow defined by the corresponding vector\nfield. We also reveal the fundamental difference between variational divergence\nminimization and adversarial training. This analysis helps us to identify what\ntypes of generator loss functions can lead to the successful training of GANs\nand suggest that GANs may have more loss designs beyond the literature (e.g.,\nnon-saturated loss), as long as they realize MonoFlow. Consistent empirical\nstudies are included to validate the effectiveness of our framework.\n","authors":["Mingxuan Yi","Zhanxing Zhu","Song Liu"],"pdf_url":"https://arxiv.org/pdf/2302.01075v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09387v3","updated":"2023-07-11T21:52:09Z","published":"2022-07-19T16:37:24Z","title":"Green, Quantized Federated Learning over Wireless Networks: An\n  Energy-Efficient Design","summary":"  In this paper, a green-quantized FL framework, which represents data with a\nfinite precision level in both local training and uplink transmission, is\nproposed. Here, the finite precision level is captured through the use of\nquantized neural networks (QNNs) that quantize weights and activations in\nfixed-precision format. In the considered FL model, each device trains its QNN\nand transmits a quantized training result to the base station. Energy models\nfor the local training and the transmission with quantization are rigorously\nderived. To minimize the energy consumption and the number of communication\nrounds simultaneously, a multi-objective optimization problem is formulated\nwith respect to the number of local iterations, the number of selected devices,\nand the precision levels for both local training and transmission while\nensuring convergence under a target accuracy constraint. To solve this problem,\nthe convergence rate of the proposed FL system is analytically derived with\nrespect to the system control variables. Then, the Pareto boundary of the\nproblem is characterized to provide efficient solutions using the normal\nboundary inspection method. Design insights on balancing the tradeoff between\nthe two objectives while achieving a target accuracy are drawn from using the\nNash bargaining solution and analyzing the derived convergence rate. Simulation\nresults show that the proposed FL framework can reduce energy consumption until\nconvergence by up to 70\\% compared to a baseline FL algorithm that represents\ndata with full precision without damaging the convergence rate.\n","authors":["Minsu Kim","Walid Saad","Mohammad Mozaffari","Merouane Debbah"],"pdf_url":"https://arxiv.org/pdf/2207.09387v3.pdf","comment":"Accepted in the IEEE Transactions on Wireless Communications"},{"id":"http://arxiv.org/abs/2307.05812v1","updated":"2023-07-11T21:38:52Z","published":"2023-07-11T21:38:52Z","title":"Safe Reinforcement Learning for Strategic Bidding of Virtual Power\n  Plants in Day-Ahead Markets","summary":"  This paper presents a novel safe reinforcement learning algorithm for\nstrategic bidding of Virtual Power Plants (VPPs) in day-ahead electricity\nmarkets. The proposed algorithm utilizes the Deep Deterministic Policy Gradient\n(DDPG) method to learn competitive bidding policies without requiring an\naccurate market model. Furthermore, to account for the complex internal\nphysical constraints of VPPs we introduce two enhancements to the DDPG method.\nFirstly, a projection-based safety shield that restricts the agent's actions to\nthe feasible space defined by the non-linear power flow equations and operating\nconstraints of distributed energy resources is derived. Secondly, a penalty for\nthe shield activation in the reward function that incentivizes the agent to\nlearn a safer policy is introduced. A case study based on the IEEE 13-bus\nnetwork demonstrates the effectiveness of the proposed approach in enabling the\nagent to learn a highly competitive, safe strategic policy.\n","authors":["Ognjen Stanojev","Lesia Mitridati","Riccardo de Nardis di Prata","Gabriela Hug"],"pdf_url":"https://arxiv.org/pdf/2307.05812v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.02309v2","updated":"2023-07-11T21:11:23Z","published":"2023-05-03T17:55:25Z","title":"CodeGen2: Lessons for Training LLMs on Programming and Natural Languages","summary":"  Large language models (LLMs) have demonstrated remarkable abilities in\nrepresentation learning for program synthesis and understanding tasks. The\nquality of the learned representations appears to be dictated by the neural\nscaling laws as a function of the number of model parameters and observations,\nwhile imposing upper bounds on the model performance by the amount of available\ndata and compute, which is costly.\n  In this study, we attempt to render the training of LLMs for program\nsynthesis more efficient by unifying four key components: (1) model\narchitectures, (2) learning methods, (3) infill sampling, and, (4) data\ndistributions. Specifically, for the model architecture, we attempt to unify\nencoder and decoder-based models into a single prefix-LM. For learning methods,\n(i) causal language modeling, (ii) span corruption, (iii) infilling are unified\ninto a simple learning algorithm. For infill sampling, we explore the claim of\na \"free lunch\" hypothesis. For data distributions, the effect of a mixture\ndistribution and multi-epoch training of programming and natural languages on\nmodel performance is explored.\n  We conduct a comprehensive series of empirical experiments on 1B LLMs, for\nwhich failures and successes of this exploration are distilled into five\nlessons. We will provide a final recipe for training and release CodeGen2\nmodels in size 1B, 3.7B, 7B, and, 16B parameters, along with the training\nframework as open-source: https://github.com/salesforce/CodeGen.\n","authors":["Erik Nijkamp","Hiroaki Hayashi","Caiming Xiong","Silvio Savarese","Yingbo Zhou"],"pdf_url":"https://arxiv.org/pdf/2305.02309v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05801v1","updated":"2023-07-11T20:52:46Z","published":"2023-07-11T20:52:46Z","title":"Differentiable Forward Projector for X-ray Computed Tomography","summary":"  Data-driven deep learning has been successfully applied to various computed\ntomographic reconstruction problems. The deep inference models may outperform\nexisting analytical and iterative algorithms, especially in ill-posed CT\nreconstruction. However, those methods often predict images that do not agree\nwith the measured projection data. This paper presents an accurate\ndifferentiable forward and back projection software library to ensure the\nconsistency between the predicted images and the original measurements. The\nsoftware library efficiently supports various projection geometry types while\nminimizing the GPU memory footprint requirement, which facilitates seamless\nintegration with existing deep learning training and inference pipelines. The\nproposed software is available as open source: https://github.com/LLNL/LEAP.\n","authors":["Hyojin Kim","Kyle Champley"],"pdf_url":"https://arxiv.org/pdf/2307.05801v1.pdf","comment":"ICML 2023 Workshop: Differentiable Almost Everything: Differentiable\n  Relaxations, Algorithms, Operators, and Simulators"},{"id":"http://arxiv.org/abs/2307.03364v2","updated":"2023-07-11T20:45:23Z","published":"2023-07-07T03:07:28Z","title":"Distilled Pruning: Using Synthetic Data to Win the Lottery","summary":"  This work introduces a novel approach to pruning deep learning models by\nusing distilled data. Unlike conventional strategies which primarily focus on\narchitectural or algorithmic optimization, our method reconsiders the role of\ndata in these scenarios. Distilled datasets capture essential patterns from\nlarger datasets, and we demonstrate how to leverage this capability to enable a\ncomputationally efficient pruning process. Our approach can find sparse,\ntrainable subnetworks (a.k.a. Lottery Tickets) up to 5x faster than Iterative\nMagnitude Pruning at comparable sparsity on CIFAR-10. The experimental results\nhighlight the potential of using distilled data for resource-efficient neural\nnetwork pruning, model compression, and neural architecture search.\n","authors":["Luke McDermott","Daniel Cummings"],"pdf_url":"https://arxiv.org/pdf/2307.03364v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05794v1","updated":"2023-07-11T20:40:31Z","published":"2023-07-11T20:40:31Z","title":"Machine Learning Study of the Extended Drug-target Interaction Network\n  informed by Pain Related Voltage-Gated Sodium Channels","summary":"  Pain is a significant global health issue, and the current treatment options\nfor pain management have limitations in terms of effectiveness, side effects,\nand potential for addiction. There is a pressing need for improved pain\ntreatments and the development of new drugs. Voltage-gated sodium channels,\nparticularly Nav1.3, Nav1.7, Nav1.8, and Nav1.9, play a crucial role in\nneuronal excitability and are predominantly expressed in the peripheral nervous\nsystem. Targeting these channels may provide a means to treat pain while\nminimizing central and cardiac adverse effects. In this study, we construct\nprotein-protein interaction (PPI) networks based on pain-related sodium\nchannels and develop a corresponding drug-target interaction (DTI) network to\nidentify potential lead compounds for pain management. To ensure reliable\nmachine learning predictions, we carefully select 111 inhibitor datasets from a\npool of over 1,000 targets in the PPI network. We employ three distinct machine\nlearning algorithms combined with advanced natural language processing\n(NLP)-based embeddings, specifically pre-trained transformer and autoencoder\nrepresentations. Through a systematic screening process, we evaluate the side\neffects and repurposing potential of over 150,000 drug candidates targeting\nNav1.7 and Nav1.8 sodium channels. Additionally, we assess the ADMET\n(absorption, distribution, metabolism, excretion, and toxicity) properties of\nthese candidates to identify leads with near-optimal characteristics. Our\nstrategy provides an innovative platform for the pharmacological development of\npain treatments, offering the potential for improved efficacy and reduced side\neffects.\n","authors":["Long Chen","Jian Jiang","Bozheng Dou","Hongsong Feng","Jie Liu","Yueying Zhu","Bengong Zhang","Tianshou Zhou","Guo-Wei Wei"],"pdf_url":"https://arxiv.org/pdf/2307.05794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05789v1","updated":"2023-07-11T20:33:33Z","published":"2023-07-11T20:33:33Z","title":"Implicit regularisation in stochastic gradient descent: from\n  single-objective to two-player games","summary":"  Recent years have seen many insights on deep learning optimisation being\nbrought forward by finding implicit regularisation effects of commonly used\ngradient-based optimisers. Understanding implicit regularisation can not only\nshed light on optimisation dynamics, but it can also be used to improve\nperformance and stability across problem domains, from supervised learning to\ntwo-player games such as Generative Adversarial Networks. An avenue for finding\nsuch implicit regularisation effects has been quantifying the discretisation\nerrors of discrete optimisers via continuous-time flows constructed by backward\nerror analysis (BEA). The current usage of BEA is not without limitations,\nsince not all the vector fields of continuous-time flows obtained using BEA can\nbe written as a gradient, hindering the construction of modified losses\nrevealing implicit regularisers. In this work, we provide a novel approach to\nuse BEA, and show how our approach can be used to construct continuous-time\nflows with vector fields that can be written as gradients. We then use this to\nfind previously unknown implicit regularisation effects, such as those induced\nby multiple stochastic gradient descent steps while accounting for the exact\ndata batches used in the updates, and in generally differentiable two-player\ngames.\n","authors":["Mihaela Rosca","Marc Peter Deisenroth"],"pdf_url":"https://arxiv.org/pdf/2307.05789v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05785v1","updated":"2023-07-11T20:25:46Z","published":"2023-07-11T20:25:46Z","title":"Making the Nyström method highly accurate for low-rank approximations","summary":"  The Nystr\\\"om method is a convenient heuristic method to obtain low-rank\napproximations to kernel matrices in nearly linear complexity. Existing studies\ntypically use the method to approximate positive semidefinite matrices with low\nor modest accuracies. In this work, we propose a series of heuristic strategies\nto make the Nystr\\\"om method reach high accuracies for nonsymmetric and/or\nrectangular matrices. The resulting methods (called high-accuracy Nystr\\\"om\nmethods) treat the Nystr\\\"om method and a skinny rank-revealing factorization\nas a fast pivoting strategy in a progressive alternating direction refinement\nprocess. Two refinement mechanisms are used: alternating the row and column\npivoting starting from a small set of randomly chosen columns, and adaptively\nincreasing the number of samples until a desired rank or accuracy is reached. A\nfast subset update strategy based on the progressive sampling of Schur\ncomplements is further proposed to accelerate the refinement process. Efficient\nrandomized accuracy control is also provided. Relevant accuracy and singular\nvalue analysis is given to support some of the heuristics. Extensive tests with\nvarious kernel functions and data sets show how the methods can quickly reach\nprespecified high accuracies in practice, sometimes with quality close to SVDs,\nusing only small numbers of progressive sampling steps.\n","authors":["Jianlin Xia"],"pdf_url":"https://arxiv.org/pdf/2307.05785v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.01413v2","updated":"2023-07-11T20:22:08Z","published":"2022-11-02T18:16:17Z","title":"Harnessing the Power of Explanations for Incremental Training: A\n  LIME-Based Approach","summary":"  Explainability of neural network prediction is essential to understand\nfeature importance and gain interpretable insight into neural network\nperformance. However, explanations of neural network outcomes are mostly\nlimited to visualization, and there is scarce work that looks to use these\nexplanations as feedback to improve model performance. In this work, model\nexplanations are fed back to the feed-forward training to help the model\ngeneralize better. To this extent, a custom weighted loss where the weights are\ngenerated by considering the Euclidean distances between true LIME (Local\nInterpretable Model-Agnostic Explanations) explanations and model-predicted\nLIME explanations is proposed. Also, in practical training scenarios,\ndeveloping a solution that can help the model learn sequentially without losing\ninformation on previous data distribution is imperative due to the\nunavailability of all the training data at once. Thus, the framework\nincorporates the custom weighted loss with Elastic Weight Consolidation (EWC)\nto maintain performance in sequential testing sets. The proposed custom\ntraining procedure results in a consistent enhancement of accuracy ranging from\n0.5% to 1.5% throughout all phases of the incremental learning setup compared\nto traditional loss-based training methods for the keyword spotting task using\nthe Google Speech Commands dataset.\n","authors":["Arnab Neelim Mazumder","Niall Lyons","Ashutosh Pandey","Avik Santra","Tinoosh Mohsenin"],"pdf_url":"https://arxiv.org/pdf/2211.01413v2.pdf","comment":"Accepted at EUSIPCO 2023"},{"id":"http://arxiv.org/abs/2307.05775v1","updated":"2023-07-11T20:06:12Z","published":"2023-07-11T20:06:12Z","title":"Weisfeiler and Lehman Go Measurement Modeling: Probing the Validity of\n  the WL Test","summary":"  The expressive power of graph neural networks is usually measured by\ncomparing how many pairs of graphs or nodes an architecture can possibly\ndistinguish as non-isomorphic to those distinguishable by the $k$-dimensional\nWeisfeiler-Lehman ($k$-WL) test. In this paper, we uncover misalignments\nbetween practitioners' conceptualizations of expressive power and $k$-WL\nthrough a systematic analysis of the reliability and validity of $k$-WL. We\nfurther conduct a survey ($n = 18$) of practitioners to surface their\nconceptualizations of expressive power and their assumptions about $k$-WL. In\ncontrast to practitioners' opinions, our analysis (which draws from graph\ntheory and benchmark auditing) reveals that $k$-WL does not guarantee isometry,\ncan be irrelevant to real-world graph tasks, and may not promote generalization\nor trustworthiness. We argue for extensional definitions and measurement of\nexpressive power based on benchmarks; we further contribute guiding questions\nfor constructing such benchmarks, which is critical for progress in graph\nmachine learning.\n","authors":["Arjun Subramonian","Adina Williams","Maximilian Nickel","Yizhou Sun","Levent Sagun"],"pdf_url":"https://arxiv.org/pdf/2307.05775v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05772v1","updated":"2023-07-11T20:00:35Z","published":"2023-07-11T20:00:35Z","title":"Random-Set Convolutional Neural Network (RS-CNN) for Epistemic Deep\n  Learning","summary":"  Machine learning is increasingly deployed in safety-critical domains where\nrobustness against adversarial attacks is crucial and erroneous predictions\ncould lead to potentially catastrophic consequences. This highlights the need\nfor learning systems to be equipped with the means to determine a model's\nconfidence in its prediction and the epistemic uncertainty associated with it,\n'to know when a model does not know'. In this paper, we propose a novel\nRandom-Set Convolutional Neural Network (RS-CNN) for classification which\npredicts belief functions rather than probability vectors over the set of\nclasses, using the mathematics of random sets, i.e., distributions over the\npower set of the sample space. Based on the epistemic deep learning approach,\nrandom-set models are capable of representing the 'epistemic' uncertainty\ninduced in machine learning by limited training sets. We estimate epistemic\nuncertainty by approximating the size of credal sets associated with the\npredicted belief functions, and experimentally demonstrate how our approach\noutperforms competing uncertainty-aware approaches in a classical evaluation\nsetting. The performance of RS-CNN is best demonstrated on OOD samples where it\nmanages to capture the true prediction while standard CNNs fail.\n","authors":["Shireen Kudukkil Manchingal","Muhammad Mubashar","Kaizheng Wang","Keivan Shariatmadar","Fabio Cuzzolin"],"pdf_url":"https://arxiv.org/pdf/2307.05772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.11197v3","updated":"2023-07-11T19:51:21Z","published":"2023-06-19T23:10:02Z","title":"Sparse Modular Activation for Efficient Sequence Modeling","summary":"  Linear State Space Models (SSMs) have demonstrated strong performance in a\nvariety of sequence modeling tasks due to their efficient encoding of the\nrecurrent structure. However, in more comprehensive tasks like language\nmodeling and machine translation, self-attention-based models still outperform\nSSMs. Hybrid models employing both SSM and self-attention generally show\npromising performance, but current approaches apply attention modules\nstatically and uniformly to all elements in the input sequences, leading to\nsub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse\nModular Activation (SMA), a general mechanism enabling neural networks to\nsparsely and dynamically activate sub-modules for sequence elements in a\ndifferentiable manner. Through allowing each element to skip non-activated\nsub-modules, SMA reduces computation and memory consumption at both training\nand inference stages of sequence modeling. As a specific instantiation of SMA,\nwe design a novel neural architecture, SeqBoat, which employs SMA to sparsely\nactivate a Gated Attention Unit (GAU) based on the state representations\nlearned from an SSM. By constraining the GAU to only conduct local attention on\nthe activated inputs, SeqBoat can achieve linear inference complexity with\ntheoretically infinite attention span, and provide substantially better\nquality-efficiency trade-off than the chunking-based models. With experiments\non a wide range of tasks, including language modeling, speech classification\nand long-range arena, SeqBoat brings new state-of-the-art results among hybrid\nmodels with linear complexity and reveals the amount of attention needed for\neach task through the learned sparse activation patterns.\n","authors":["Liliang Ren","Yang Liu","Shuohang Wang","Yichong Xu","Chenguang Zhu","ChengXiang Zhai"],"pdf_url":"https://arxiv.org/pdf/2306.11197v3.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2210.13601v2","updated":"2023-07-11T19:49:07Z","published":"2022-10-24T20:55:21Z","title":"Active Learning for Single Neuron Models with Lipschitz Non-Linearities","summary":"  We consider the problem of active learning for single neuron models, also\nsometimes called ``ridge functions'', in the agnostic setting (under\nadversarial label noise). Such models have been shown to be broadly effective\nin modeling physical phenomena, and for constructing surrogate data-driven\nmodels for partial differential equations.\n  Surprisingly, we show that for a single neuron model with any Lipschitz\nnon-linearity (such as the ReLU, sigmoid, absolute value, low-degree\npolynomial, among others), strong provable approximation guarantees can be\nobtained using a well-known active learning strategy for fitting \\emph{linear\nfunctions} in the agnostic setting. % -- i.e. for the case when there is no\nnon-linearity. Namely, we can collect samples via statistical \\emph{leverage\nscore sampling}, which has been shown to be near-optimal in other active\nlearning scenarios. We support our theoretical results with empirical\nsimulations showing that our proposed active learning strategy based on\nleverage score sampling outperforms (ordinary) uniform sampling when fitting\nsingle neuron models.\n","authors":["Aarshvi Gajjar","Chinmay Hegde","Christopher Musco"],"pdf_url":"https://arxiv.org/pdf/2210.13601v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05763v1","updated":"2023-07-11T19:40:02Z","published":"2023-07-11T19:40:02Z","title":"Realtime Spectrum Monitoring via Reinforcement Learning -- A Comparison\n  Between Q-Learning and Heuristic Methods","summary":"  Due to technological advances in the field of radio technology and its\navailability, the number of interference signals in the radio spectrum is\ncontinuously increasing. Interference signals must be detected in a timely\nfashion, in order to maintain standards and keep emergency frequencies open. To\nthis end, specialized (multi-channel) receivers are used for spectrum\nmonitoring. In this paper, the performances of two different approaches for\ncontrolling the available receiver resources are compared. The methods used for\nresource management (ReMa) are linear frequency tuning as a heuristic approach\nand a Q-learning algorithm from the field of reinforcement learning. To test\nthe methods to be investigated, a simplified scenario was designed with two\nreceiver channels monitoring ten non-overlapping frequency bands with\nnon-uniform signal activity. For this setting, it is shown that the Q-learning\nalgorithm used has a significantly higher detection rate than the heuristic\napproach at the expense of a smaller exploration rate. In particular, the\nQ-learning approach can be parameterized to allow for a suitable trade-off\nbetween detection and exploration rate.\n","authors":["Tobias Braun","Tobias Korzyzkowske","Larissa Putzar","Jan Mietzner","Peter A. Hoeher"],"pdf_url":"https://arxiv.org/pdf/2307.05763v1.pdf","comment":"5 pages, 6 figures"},{"id":"http://arxiv.org/abs/2212.10258v2","updated":"2023-07-11T19:33:44Z","published":"2022-12-20T14:06:50Z","title":"In and Out-of-Domain Text Adversarial Robustness via Label Smoothing","summary":"  Recently it has been shown that state-of-the-art NLP models are vulnerable to\nadversarial attacks, where the predictions of a model can be drastically\naltered by slight modifications to the input (such as synonym substitutions).\nWhile several defense techniques have been proposed, and adapted, to the\ndiscrete nature of text adversarial attacks, the benefits of general-purpose\nregularization methods such as label smoothing for language models, have not\nbeen studied. In this paper, we study the adversarial robustness provided by\nvarious label smoothing strategies in foundational models for diverse NLP tasks\nin both in-domain and out-of-domain settings. Our experiments show that label\nsmoothing significantly improves adversarial robustness in pre-trained models\nlike BERT, against various popular attacks. We also analyze the relationship\nbetween prediction confidence and robustness, showing that label smoothing\nreduces over-confident errors on adversarial examples.\n","authors":["Yahan Yang","Soham Dan","Dan Roth","Insup Lee"],"pdf_url":"https://arxiv.org/pdf/2212.10258v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.02422v3","updated":"2023-07-11T19:10:32Z","published":"2023-06-04T17:54:11Z","title":"A Generalized Alternating Method for Bilevel Learning under the\n  Polyak-Łojasiewicz Condition","summary":"  Bilevel optimization has recently regained interest owing to its applications\nin emerging machine learning fields such as hyperparameter optimization,\nmeta-learning, and reinforcement learning. Recent results have shown that\nsimple alternating (implicit) gradient-based algorithms can achieve the same\nconvergence rate of single-level gradient descent (GD) for bilevel problems\nwith a strongly convex lower-level objective. However, it remains unclear\nwhether this result can be generalized to bilevel problems beyond this basic\nsetting. In this paper, we propose a Generalized ALternating mEthod for bilevel\nopTimization (GALET) with a nonconvex lower-level objective that satisfies the\nPolyak-{\\L}ojasiewicz (PL) condition. We first introduce a stationary metric\nfor the considered bilevel problems, which generalizes the existing metric. We\nthen establish that GALET achieves an $\\epsilon$-stationary metric for the\nconsidered problem within $\\tilde{\\cal O}(\\epsilon^{-1})$ iterations, which\nmatches the iteration complexity of GD for smooth nonconvex problems.\n","authors":["Quan Xiao","Songtao Lu","Tianyi Chen"],"pdf_url":"https://arxiv.org/pdf/2306.02422v3.pdf","comment":"Correct a typo of Figure 2"},{"id":"http://arxiv.org/abs/2106.10865v3","updated":"2023-07-11T19:04:17Z","published":"2021-06-21T05:34:36Z","title":"Benign Overfitting in Multiclass Classification: All Roads Lead to\n  Interpolation","summary":"  The literature on \"benign overfitting\" in overparameterized models has been\nmostly restricted to regression or binary classification; however, modern\nmachine learning operates in the multiclass setting. Motivated by this\ndiscrepancy, we study benign overfitting in multiclass linear classification.\nSpecifically, we consider the following training algorithms on separable data:\n(i) empirical risk minimization (ERM) with cross-entropy loss, which converges\nto the multiclass support vector machine (SVM) solution; (ii) ERM with\nleast-squares loss, which converges to the min-norm interpolating (MNI)\nsolution; and, (iii) the one-vs-all SVM classifier. First, we provide a simple\nsufficient deterministic condition under which all three algorithms lead to\nclassifiers that interpolate the training data and have equal accuracy. When\nthe data is generated from Gaussian mixtures or a multinomial logistic model,\nthis condition holds under high enough effective overparameterization. We also\nshow that this sufficient condition is satisfied under \"neural collapse\", a\nphenomenon that is observed in training deep neural networks. Second, we derive\nnovel bounds on the accuracy of the MNI classifier, thereby showing that all\nthree training algorithms lead to benign overfitting under sufficient\noverparameterization. Ultimately, our analysis shows that good generalization\nis possible for SVM solutions beyond the realm in which typical margin-based\nbounds apply.\n","authors":["Ke Wang","Vidya Muthukumar","Christos Thrampoulidis"],"pdf_url":"https://arxiv.org/pdf/2106.10865v3.pdf","comment":"Corrected subtle issue in the proofs of Lemmas 4 an 5. Relaxed\n  Assumptions 1 and 2 and added error bound for ETF geometry"},{"id":"http://arxiv.org/abs/2307.05735v1","updated":"2023-07-11T19:03:17Z","published":"2023-07-11T19:03:17Z","title":"GOKU-UI: Ubiquitous Inference through Attention and Multiple Shooting\n  for Continuous-time Generative Models","summary":"  Scientific Machine Learning (SciML) is a burgeoning field that\nsynergistically combines domain-aware and interpretable models with agnostic\nmachine learning techniques. In this work, we introduce GOKU-UI, an evolution\nof the SciML generative model GOKU-nets. The GOKU-UI broadens the original\nmodel's spectrum to incorporate other classes of differential equations, such\nas Stochastic Differential Equations (SDEs), and integrates a distributed, i.e.\nubiquitous, inference through attention mechanisms and a novel multiple\nshooting training strategy in the latent space. These enhancements have led to\na significant increase in its performance in both reconstruction and forecast\ntasks, as demonstrated by our evaluation of simulated and empirical data.\nSpecifically, GOKU-UI outperformed all baseline models on synthetic datasets\neven with a training set 32-fold smaller, underscoring its remarkable data\nefficiency. Furthermore, when applied to empirical human brain data, while\nincorporating stochastic Stuart-Landau oscillators into its dynamical core, it\nnot only surpassed state-of-the-art baseline methods in the reconstruction\ntask, but also demonstrated better prediction of future brain activity up to 12\nseconds ahead. By training GOKU-UI on resting-state fMRI data, we encoded\nwhole-brain dynamics into a latent representation, learning an effective\nlow-dimensional dynamical system model that could offer insights into brain\nfunctionality and open avenues for practical applications such as mental state\nor psychiatric condition classification. Ultimately, our research provides\nfurther impetus for the field of Scientific Machine Learning, showcasing the\npotential for advancements when established scientific insights are interwoven\nwith modern machine learning.\n","authors":["Germán Abrevaya","Mahta Ramezanian-Panahi","Jean-Christophe Gagnon-Audet","Irina Rish","Pablo Polosecki","Silvina Ponce Dawson","Guillermo Cecchi","Guillaume Dumas"],"pdf_url":"https://arxiv.org/pdf/2307.05735v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05728v1","updated":"2023-07-11T18:55:27Z","published":"2023-07-11T18:55:27Z","title":"Towards A Scalable Solution for Improving Multi-Group Fairness in\n  Compositional Classification","summary":"  Despite the rich literature on machine learning fairness, relatively little\nattention has been paid to remediating complex systems, where the final\nprediction is the combination of multiple classifiers and where multiple groups\nare present. In this paper, we first show that natural baseline approaches for\nimproving equal opportunity fairness scale linearly with the product of the\nnumber of remediated groups and the number of remediated prediction labels,\nrendering them impractical. We then introduce two simple techniques, called\n{\\em task-overconditioning} and {\\em group-interleaving}, to achieve a constant\nscaling in this multi-group multi-label setup. Our experimental results in\nacademic and real-world environments demonstrate the effectiveness of our\nproposal at mitigation within this environment.\n","authors":["James Atwood","Tina Tian","Ben Packer","Meghana Deodhar","Jilin Chen","Alex Beutel","Flavien Prost","Ahmad Beirami"],"pdf_url":"https://arxiv.org/pdf/2307.05728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.09452v2","updated":"2023-07-11T18:46:45Z","published":"2022-10-17T21:43:32Z","title":"Multiple Instance Learning via Iterative Self-Paced Supervised\n  Contrastive Learning","summary":"  Learning representations for individual instances when only bag-level labels\nare available is a fundamental challenge in multiple instance learning (MIL).\nRecent works have shown promising results using contrastive self-supervised\nlearning (CSSL), which learns to push apart representations corresponding to\ntwo different randomly-selected instances. Unfortunately, in real-world\napplications such as medical image classification, there is often class\nimbalance, so randomly-selected instances mostly belong to the same majority\nclass, which precludes CSSL from learning inter-class differences. To address\nthis issue, we propose a novel framework, Iterative Self-paced Supervised\nContrastive Learning for MIL Representations (ItS2CLR), which improves the\nlearned representation by exploiting instance-level pseudo labels derived from\nthe bag-level labels. The framework employs a novel self-paced sampling\nstrategy to ensure the accuracy of pseudo labels. We evaluate ItS2CLR on three\nmedical datasets, showing that it improves the quality of instance-level pseudo\nlabels and representations, and outperforms existing MIL methods in terms of\nboth bag and instance level accuracy. Code is available at\nhttps://github.com/Kangningthu/ItS2CLR\n","authors":["Kangning Liu","Weicheng Zhu","Yiqiu Shen","Sheng Liu","Narges Razavian","Krzysztof J. Geras","Carlos Fernandez-Granda"],"pdf_url":"https://arxiv.org/pdf/2210.09452v2.pdf","comment":"CVPR 2023 camera-ready version. The first two authors contribute\n  equally. The last two authors are joint last authors"},{"id":"http://arxiv.org/abs/2307.05707v1","updated":"2023-07-11T18:17:50Z","published":"2023-07-11T18:17:50Z","title":"MoP-CLIP: A Mixture of Prompt-Tuned CLIP Models for Domain Incremental\n  Learning","summary":"  Despite the recent progress in incremental learning, addressing catastrophic\nforgetting under distributional drift is still an open and important problem.\nIndeed, while state-of-the-art domain incremental learning (DIL) methods\nperform satisfactorily within known domains, their performance largely degrades\nin the presence of novel domains. This limitation hampers their\ngeneralizability, and restricts their scalability to more realistic settings\nwhere train and test data are drawn from different distributions. To address\nthese limitations, we present a novel DIL approach based on a mixture of\nprompt-tuned CLIP models (MoP-CLIP), which generalizes the paradigm of\nS-Prompting to handle both in-distribution and out-of-distribution data at\ninference. In particular, at the training stage we model the features\ndistribution of every class in each domain, learning individual text and visual\nprompts to adapt to a given domain. At inference, the learned distributions\nallow us to identify whether a given test sample belongs to a known domain,\nselecting the correct prompt for the classification task, or from an unseen\ndomain, leveraging a mixture of the prompt-tuned CLIP models. Our empirical\nevaluation reveals the poor performance of existing DIL methods under domain\nshift, and suggests that the proposed MoP-CLIP performs competitively in the\nstandard DIL settings while outperforming state-of-the-art methods in OOD\nscenarios. These results demonstrate the superiority of MoP-CLIP, offering a\nrobust and general solution to the problem of domain incremental learning.\n","authors":["Julien Nicolas","Florent Chiaroni","Imtiaz Ziko","Ola Ahmad","Christian Desrosiers","Jose Dolz"],"pdf_url":"https://arxiv.org/pdf/2307.05707v1.pdf","comment":"13 pages, 5 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2302.13668v2","updated":"2023-07-11T12:00:52Z","published":"2023-02-27T11:09:13Z","title":"Contrastive Video Question Answering via Video Graph Transformer","summary":"  We propose to perform video question answering (VideoQA) in a Contrastive\nmanner via a Video Graph Transformer model (CoVGT). CoVGT's uniqueness and\nsuperiority are three-fold: 1) It proposes a dynamic graph transformer module\nwhich encodes video by explicitly capturing the visual objects, their relations\nand dynamics, for complex spatio-temporal reasoning. 2) It designs separate\nvideo and text transformers for contrastive learning between the video and text\nto perform QA, instead of multi-modal transformer for answer classification.\nFine-grained video-text communication is done by additional cross-modal\ninteraction modules. 3) It is optimized by the joint fully- and self-supervised\ncontrastive objectives between the correct and incorrect answers, as well as\nthe relevant and irrelevant questions respectively. With superior video\nencoding and QA solution, we show that CoVGT can achieve much better\nperformances than previous arts on video reasoning tasks. Its performances even\nsurpass those models that are pretrained with millions of external data. We\nfurther show that CoVGT can also benefit from cross-modal pretraining, yet with\norders of magnitude smaller data. The results demonstrate the effectiveness and\nsuperiority of CoVGT, and additionally reveal its potential for more\ndata-efficient pretraining. We hope our success can advance VideoQA beyond\ncoarse recognition/description towards fine-grained relation reasoning of video\ncontents. Our code is available at https://github.com/doc-doc/CoVGT.\n","authors":["Junbin Xiao","Pan Zhou","Angela Yao","Yicong Li","Richang Hong","Shuicheng Yan","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2302.13668v2.pdf","comment":"Accepted by IEEE T-PAMI'23"},{"id":"http://arxiv.org/abs/2307.05107v1","updated":"2023-07-11T08:34:11Z","published":"2023-07-11T08:34:11Z","title":"Optimizing Feature Extraction for Symbolic Music","summary":"  This paper presents a comprehensive investigation of existing feature\nextraction tools for symbolic music and contrasts their performance to\ndetermine the set of features that best characterizes the musical style of a\ngiven music score. In this regard, we propose a novel feature extraction tool,\nnamed musif, and evaluate its efficacy on various repertoires and file formats,\nincluding MIDI, MusicXML, and **kern. Musif approximates existing tools such as\njSymbolic and music21 in terms of computational efficiency while attempting to\nenhance the usability for custom feature development. The proposed tool also\nenhances classification accuracy when combined with other sets of features. We\ndemonstrate the contribution of each set of features and the computational\nresources they require. Our findings indicate that the optimal tool for feature\nextraction is a combination of the best features from each tool rather than\nthose of a single one. To facilitate future research in music information\nretrieval, we release the source code of the tool and benchmarks.\n","authors":["Federico Simonetta","Ana Llorens","Martín Serrano","Eduardo García-Portugués","Álvaro Torrente"],"pdf_url":"https://arxiv.org/pdf/2307.05107v1.pdf","comment":"Published at ISMIR 2023"},{"id":"http://arxiv.org/abs/2305.09381v5","updated":"2023-07-11T06:12:43Z","published":"2023-05-16T12:09:30Z","title":"AMD: Autoregressive Motion Diffusion","summary":"  Human motion generation aims to produce plausible human motion sequences\naccording to various conditional inputs, such as text or audio. Despite the\nfeasibility of existing methods in generating motion based on short prompts and\nsimple motion patterns, they encounter difficulties when dealing with long\nprompts or complex motions. The challenges are two-fold: 1) the scarcity of\nhuman motion-captured data for long prompts and complex motions. 2) the high\ndiversity of human motions in the temporal domain and the substantial\ndivergence of distributions from conditional modalities, leading to a\nmany-to-many mapping problem when generating motion with complex and long\ntexts. In this work, we address these gaps by 1) elaborating the first dataset\npairing long textual descriptions and 3D complex motions (HumanLong3D), and 2)\nproposing an autoregressive motion diffusion model (AMD). Specifically, AMD\nintegrates the text prompt at the current timestep with the text prompt and\naction sequences at the previous timestep as conditional information to predict\nthe current action sequences in an iterative manner. Furthermore, we present\nits generalization for X-to-Motion with \"No Modality Left Behind\", enabling for\nthe first time the generation of high-definition and high-fidelity human\nmotions based on user-defined modality input.\n","authors":["Bo Han","Hao Peng","Minjing Dong","Chang Xu","Yi Ren","Yixuan Shen","Yuheng Li"],"pdf_url":"https://arxiv.org/pdf/2305.09381v5.pdf","comment":null}]},"2023-07-10T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2212.09651v4","updated":"2023-07-10T22:27:15Z","published":"2022-12-19T17:29:37Z","title":"Cross-Lingual Retrieval Augmented Prompt for Low-Resource Languages","summary":"  Multilingual Pretrained Language Models (MPLMs) have shown their strong\nmultilinguality in recent empirical cross-lingual transfer studies. In this\npaper, we propose the Prompts Augmented by Retrieval Crosslingually (PARC)\npipeline to improve the zero-shot performance on low-resource languages (LRLs)\nby augmenting the context with semantically similar sentences retrieved from a\nhigh-resource language (HRL) as prompts. PARC improves the zero-shot\nperformance on three downstream tasks (binary sentiment classification, topic\ncategorization and natural language inference) with multilingual parallel test\nsets across 10 LRLs covering 6 language families in both unlabeled settings\n(+5.1%) and labeled settings (+16.3%). PARC-labeled also outperforms the\nfinetuning baseline by 3.7%. We find a significant positive correlation between\ncross-lingual transfer performance on one side, and the similarity between the\nhigh- and low-resource languages as well as the amount of low-resource\npretraining data on the other side. A robustness analysis suggests that PARC\nhas the potential to achieve even stronger performance with more powerful\nMPLMs.\n","authors":["Ercong Nie","Sheng Liang","Helmut Schmid","Hinrich Schütze"],"pdf_url":"https://arxiv.org/pdf/2212.09651v4.pdf","comment":"Accepted to Findings of ACL 2023"},{"id":"http://arxiv.org/abs/2307.04907v1","updated":"2023-07-10T21:16:46Z","published":"2023-07-10T21:16:46Z","title":"SimpleMTOD: A Simple Language Model for Multimodal Task-Oriented\n  Dialogue with Symbolic Scene Representation","summary":"  SimpleMTOD is a simple language model which recasts several sub-tasks in\nmultimodal task-oriented dialogues as sequence prediction tasks. SimpleMTOD is\nbuilt on a large-scale transformer-based auto-regressive architecture, which\nhas already proven to be successful in uni-modal task-oriented dialogues, and\neffectively leverages transfer learning from pre-trained GPT-2. In-order to\ncapture the semantics of visual scenes, we introduce both local and\nde-localized tokens for objects within a scene. De-localized tokens represent\nthe type of an object rather than the specific object itself and so possess a\nconsistent meaning across the dataset. SimpleMTOD achieves a state-of-the-art\nBLEU score (0.327) in the Response Generation sub-task of the SIMMC 2.0\ntest-std dataset while performing on par in other multimodal sub-tasks:\nDisambiguation, Coreference Resolution, and Dialog State Tracking. This is\ndespite taking a minimalist approach for extracting visual (and non-visual)\ninformation. In addition the model does not rely on task-specific architectural\nchanges such as classification heads.\n","authors":["Bhathiya Hemanthage","Christian Dondrup","Phil Bartie","Oliver Lemon"],"pdf_url":"https://arxiv.org/pdf/2307.04907v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.06960v2","updated":"2023-07-10T21:02:05Z","published":"2022-07-14T14:39:30Z","title":"Forming Trees with Treeformers","summary":"  Human language is known to exhibit a nested, hierarchical structure, allowing\nus to form complex sentences out of smaller pieces. However, many\nstate-of-the-art neural networks models such as Transformers have no explicit\nhierarchical structure in its architecture -- that is, they don't have an\ninductive bias toward hierarchical structure. Additionally, Transformers are\nknown to perform poorly on compositional generalization tasks which require\nsuch structures. In this paper, we introduce Treeformer, a general-purpose\nencoder module inspired by the CKY algorithm which learns a composition\noperator and pooling function to construct hierarchical encodings for phrases\nand sentences. Our extensive experiments demonstrate the benefits of\nincorporating hierarchical structure into the Transformer and show significant\nimprovements in compositional generalization as well as in downstream tasks\nsuch as machine translation, abstractive summarization, and various natural\nlanguage understanding tasks.\n","authors":["Nilay Patel","Jeffrey Flanigan"],"pdf_url":"https://arxiv.org/pdf/2207.06960v2.pdf","comment":"Accepted to RANLP 2023"},{"id":"http://arxiv.org/abs/2304.02168v2","updated":"2023-07-10T20:41:34Z","published":"2023-04-04T23:51:48Z","title":"I2I: Initializing Adapters with Improvised Knowledge","summary":"  Adapters present a promising solution to the catastrophic forgetting problem\nin continual learning. However, training independent Adapter modules for every\nnew task misses an opportunity for cross-task knowledge transfer. We propose\nImprovise to Initialize (I2I), a continual learning algorithm that initializes\nAdapters for incoming tasks by distilling knowledge from previously-learned\ntasks' Adapters. We evaluate I2I on CLiMB, a multimodal continual learning\nbenchmark, by conducting experiments on sequences of visual question answering\ntasks. Adapters trained with I2I consistently achieve better task accuracy than\nindependently-trained Adapters, demonstrating that our algorithm facilitates\nknowledge transfer between task Adapters. I2I also results in better cross-task\nknowledge transfer than the state-of-the-art AdapterFusion without incurring\nthe associated parametric cost.\n","authors":["Tejas Srinivasan","Furong Jia","Mohammad Rostami","Jesse Thomason"],"pdf_url":"https://arxiv.org/pdf/2304.02168v2.pdf","comment":"Accepted at 2nd Conference on Lifelong Learning Agents (CoLLAs), 2023"},{"id":"http://arxiv.org/abs/2307.04892v1","updated":"2023-07-10T20:30:27Z","published":"2023-07-10T20:30:27Z","title":"Entity Identifier: A Natural Text Parsing-based Framework For Entity\n  Relation Extraction","summary":"  The field of programming has a diversity of paradigms that are used according\nto the working framework. While current neural code generation methods are able\nto learn and generate code directly from text, we believe that this approach is\nnot optimal for certain code tasks, particularly the generation of classes in\nan object-oriented project. Specifically, we use natural language processing\ntechniques to extract structured information from requirements descriptions, in\norder to automate the generation of CRUD (Create, Read, Update, Delete) class\ncode. To facilitate this process, we introduce a pipeline for extracting entity\nand relation information, as well as a representation called an \"Entity Tree\"\nto model this information. We also create a dataset to evaluate the\neffectiveness of our approach.\n","authors":["El Mehdi Chouham","Jessica López Espejel","Mahaman Sanoussi Yahaya Alassan","Walid Dahhane","El Hassane Ettifouri"],"pdf_url":"https://arxiv.org/pdf/2307.04892v1.pdf","comment":"Under review for Elsevier's Natural Language Processing Journal"},{"id":"http://arxiv.org/abs/2307.02738v2","updated":"2023-07-10T20:17:53Z","published":"2023-07-06T02:51:54Z","title":"RecallM: An Architecture for Temporal Context Understanding and Question\n  Answering","summary":"  The ideal long-term memory mechanism for Large Language Model (LLM) based\nchatbots, would lay the foundation for continual learning, complex reasoning\nand allow sequential and temporal dependencies to be learnt. Creating this type\nof memory mechanism is an extremely challenging problem. In this paper we\nexplore different methods of achieving the effect of long-term memory. We\npropose a new architecture focused on creating adaptable and updatable\nlong-term memory for AGI systems. We demonstrate through various experiments\nthe benefits of the RecallM architecture, particularly the improved temporal\nunderstanding of knowledge it provides.\n","authors":["Brandon Kynoch","Hugo Latapie"],"pdf_url":"https://arxiv.org/pdf/2307.02738v2.pdf","comment":"12 pages, 6 figures, Our code is publicly available online at:\n  https://github.com/cisco-open/DeepVision/tree/main/recallm"},{"id":"http://arxiv.org/abs/2304.02819v3","updated":"2023-07-10T18:48:45Z","published":"2023-04-06T01:51:15Z","title":"GPT detectors are biased against non-native English writers","summary":"  The rapid adoption of generative language models has brought about\nsubstantial advancements in digital communication, while simultaneously raising\nconcerns regarding the potential misuse of AI-generated content. Although\nnumerous detection methods have been proposed to differentiate between AI and\nhuman-generated content, the fairness and robustness of these detectors remain\nunderexplored. In this study, we evaluate the performance of several\nwidely-used GPT detectors using writing samples from native and non-native\nEnglish writers. Our findings reveal that these detectors consistently\nmisclassify non-native English writing samples as AI-generated, whereas native\nwriting samples are accurately identified. Furthermore, we demonstrate that\nsimple prompting strategies can not only mitigate this bias but also\neffectively bypass GPT detectors, suggesting that GPT detectors may\nunintentionally penalize writers with constrained linguistic expressions. Our\nresults call for a broader conversation about the ethical implications of\ndeploying ChatGPT content detectors and caution against their use in evaluative\nor educational settings, particularly when they may inadvertently penalize or\nexclude non-native English speakers from the global discourse. The published\nversion of this study can be accessed at:\nwww.cell.com/patterns/fulltext/S2666-3899(23)00130-7\n","authors":["Weixin Liang","Mert Yuksekgonul","Yining Mao","Eric Wu","James Zou"],"pdf_url":"https://arxiv.org/pdf/2304.02819v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.08614v7","updated":"2023-07-10T18:30:18Z","published":"2021-08-19T10:50:52Z","title":"UNIQORN: Unified Question Answering over RDF Knowledge Graphs and\n  Natural Language Text","summary":"  Question answering over knowledge graphs and other RDF data has been greatly\nadvanced, with a number of good techniques providing crisp answers for natural\nlanguage questions or telegraphic queries. Some of these systems incorporate\ntextual sources as additional evidence for the answering process, but cannot\ncompute answers that are present in text alone. Conversely, techniques from the\nIR and NLP communities have addressed QA over text, but such systems barely\nutilize semantic data and knowledge. This paper presents a method for complex\nquestions that can seamlessly operate over a mixture of RDF datasets and text\ncorpora, or individual sources, in a unified framework. Our method, called\nUNIQORN, builds a context graph on-the-fly, by retrieving question-relevant\nevidences from the RDF data and/or a text corpus, using fine-tuned BERT models.\nThe resulting graph typically contains all question-relevant evidences but also\na lot of noise. UNIQORN copes with this input by a graph algorithm for Group\nSteiner Trees, that identifies the best answer candidates in the context graph.\nExperimental results on several benchmarks of complex questions with multiple\nentities and relations, show that UNIQORN significantly outperforms\nstate-of-the-art methods for heterogeneous QA. The graph-based methodology\nprovides user-interpretable evidence for the complete answering process.\n","authors":["Soumajit Pramanik","Jesujoba Oluwadara Alabi","Rishiraj Saha Roy","Gerhard Weikum"],"pdf_url":"https://arxiv.org/pdf/2108.08614v7.pdf","comment":"35 pages"},{"id":"http://arxiv.org/abs/2107.00439v3","updated":"2023-07-10T18:08:42Z","published":"2021-07-01T13:32:55Z","title":"What do End-to-End Speech Models Learn about Speaker, Language and\n  Channel Information? A Layer-wise and Neuron-level Analysis","summary":"  Deep neural networks are inherently opaque and challenging to interpret.\nUnlike hand-crafted feature-based models, we struggle to comprehend the\nconcepts learned and how they interact within these models. This understanding\nis crucial not only for debugging purposes but also for ensuring fairness in\nethical decision-making. In our study, we conduct a post-hoc functional\ninterpretability analysis of pretrained speech models using the probing\nframework [1]. Specifically, we analyze utterance-level representations of\nspeech models trained for various tasks such as speaker recognition and dialect\nidentification. We conduct layer and neuron-wise analyses, probing for speaker,\nlanguage, and channel properties. Our study aims to answer the following\nquestions: i) what information is captured within the representations? ii) how\nis it represented and distributed? and iii) can we identify a minimal subset of\nthe network that possesses this information?\n  Our results reveal several novel findings, including: i) channel and gender\ninformation are distributed across the network, ii) the information is\nredundantly available in neurons with respect to a task, iii) complex\nproperties such as dialectal information are encoded only in the task-oriented\npretrained network, iv) and is localised in the upper layers, v) we can extract\na minimal subset of neurons encoding the pre-defined property, vi) salient\nneurons are sometimes shared between properties, vii) our analysis highlights\nthe presence of biases (for example gender) in the network. Our\ncross-architectural comparison indicates that: i) the pretrained models capture\nspeaker-invariant information, and ii) CNN models are competitive with\nTransformer models in encoding various understudied properties.\n","authors":["Shammur Absar Chowdhury","Nadir Durrani","Ahmed Ali"],"pdf_url":"https://arxiv.org/pdf/2107.00439v3.pdf","comment":"Accepted in CSL journal. Keywords: Speech, Neuron Analysis,\n  Interpretibility, Diagnostic Classifier, AI explainability, End-to-End\n  Architecture"},{"id":"http://arxiv.org/abs/2307.05591v1","updated":"2023-07-10T17:59:21Z","published":"2023-07-10T17:59:21Z","title":"SITTA: A Semantic Image-Text Alignment for Image Captioning","summary":"  Textual and semantic comprehension of images is essential for generating\nproper captions. The comprehension requires detection of objects, modeling of\nrelations between them, an assessment of the semantics of the scene and,\nfinally, representing the extracted knowledge in a language space. To achieve\nrich language capabilities while ensuring good image-language mappings,\npretrained language models (LMs) were conditioned on pretrained multi-modal\n(image-text) models that allow for image inputs. This requires an alignment of\nthe image representation of the multi-modal model with the language\nrepresentations of a generative LM. However, it is not clear how to best\ntransfer semantics detected by the vision encoder of the multi-modal model to\nthe LM. We introduce two novel ways of constructing a linear mapping that\nsuccessfully transfers semantics between the embedding spaces of the two\npretrained models. The first aligns the embedding space of the multi-modal\nlanguage encoder with the embedding space of the pretrained LM via token\ncorrespondences. The latter leverages additional data that consists of\nimage-text pairs to construct the mapping directly from vision to language\nspace. Using our semantic mappings, we unlock image captioning for LMs without\naccess to gradient information. By using different sources of data we achieve\nstrong captioning performance on MS-COCO and Flickr30k datasets. Even in the\nface of limited data, our method partly exceeds the performance of other\nzero-shot and even finetuned competitors. Our ablation studies show that even\nLMs at a scale of merely 250M parameters can generate decent captions employing\nour semantic mappings. Our approach makes image captioning more accessible for\ninstitutions with restricted computational resources.\n","authors":["Fabian Paischer","Thomas Adler","Markus Hofmarcher","Sepp Hochreiter"],"pdf_url":"https://arxiv.org/pdf/2307.05591v1.pdf","comment":"10 pages (+ references and appendix), Code:\n  https://github.com/ml-jku/semantic-image-text-alignment"},{"id":"http://arxiv.org/abs/2307.04721v1","updated":"2023-07-10T17:32:13Z","published":"2023-07-10T17:32:13Z","title":"Large Language Models as General Pattern Machines","summary":"  We observe that pre-trained large language models (LLMs) are capable of\nautoregressively completing complex token sequences -- from arbitrary ones\nprocedurally generated by probabilistic context-free grammars (PCFG), to more\nrich spatial patterns found in the Abstract Reasoning Corpus (ARC), a general\nAI benchmark, prompted in the style of ASCII art. Surprisingly, pattern\ncompletion proficiency can be partially retained even when the sequences are\nexpressed using tokens randomly sampled from the vocabulary. These results\nsuggest that without any additional training, LLMs can serve as general\nsequence modelers, driven by in-context learning. In this work, we investigate\nhow these zero-shot capabilities may be applied to problems in robotics -- from\nextrapolating sequences of numbers that represent states over time to complete\nsimple motions, to least-to-most prompting of reward-conditioned trajectories\nthat can discover and represent closed-loop policies (e.g., a stabilizing\ncontroller for CartPole). While difficult to deploy today for real systems due\nto latency, context size limitations, and compute costs, the approach of using\nLLMs to drive low-level control may provide an exciting glimpse into how the\npatterns among words could be transferred to actions.\n","authors":["Suvir Mirchandani","Fei Xia","Pete Florence","Brian Ichter","Danny Driess","Montserrat Gonzalez Arenas","Kanishka Rao","Dorsa Sadigh","Andy Zeng"],"pdf_url":"https://arxiv.org/pdf/2307.04721v1.pdf","comment":"18 pages, 23 figures"},{"id":"http://arxiv.org/abs/2211.12316v2","updated":"2023-07-10T17:15:23Z","published":"2022-11-22T15:10:48Z","title":"Simplicity Bias in Transformers and their Ability to Learn Sparse\n  Boolean Functions","summary":"  Despite the widespread success of Transformers on NLP tasks, recent works\nhave found that they struggle to model several formal languages when compared\nto recurrent models. This raises the question of why Transformers perform well\nin practice and whether they have any properties that enable them to generalize\nbetter than recurrent models. In this work, we conduct an extensive empirical\nstudy on Boolean functions to demonstrate the following: (i) Random\nTransformers are relatively more biased towards functions of low sensitivity.\n(ii) When trained on Boolean functions, both Transformers and LSTMs prioritize\nlearning functions of low sensitivity, with Transformers ultimately converging\nto functions of lower sensitivity. (iii) On sparse Boolean functions which have\nlow sensitivity, we find that Transformers generalize near perfectly even in\nthe presence of noisy labels whereas LSTMs overfit and achieve poor\ngeneralization accuracy. Overall, our results provide strong quantifiable\nevidence that suggests differences in the inductive biases of Transformers and\nrecurrent models which may help explain Transformer's effective generalization\nperformance despite relatively limited expressiveness.\n","authors":["Satwik Bhattamishra","Arkil Patel","Varun Kanade","Phil Blunsom"],"pdf_url":"https://arxiv.org/pdf/2211.12316v2.pdf","comment":"ACL 2023"},{"id":"http://arxiv.org/abs/2208.12306v3","updated":"2023-07-10T16:51:34Z","published":"2022-08-25T19:04:28Z","title":"Multimedia Generative Script Learning for Task Planning","summary":"  Goal-oriented generative script learning aims to generate subsequent steps to\nreach a particular goal, which is an essential task to assist robots or humans\nin performing stereotypical activities. An important aspect of this process is\nthe ability to capture historical states visually, which provides detailed\ninformation that is not covered by text and will guide subsequent steps.\nTherefore, we propose a new task, Multimedia Generative Script Learning, to\ngenerate subsequent steps by tracking historical states in both text and vision\nmodalities, as well as presenting the first benchmark containing 5,652 tasks\nand 79,089 multimedia steps. This task is challenging in three aspects: the\nmultimedia challenge of capturing the visual states in images, the induction\nchallenge of performing unseen tasks, and the diversity challenge of covering\ndifferent information in individual steps. We propose to encode visual state\nchanges through a selective multimedia encoder to address the multimedia\nchallenge, transfer knowledge from previously observed tasks using a\nretrieval-augmented decoder to overcome the induction challenge, and further\npresent distinct information at each step by optimizing a diversity-oriented\ncontrastive learning objective. We define metrics to evaluate both generation\nand inductive quality. Experiment results demonstrate that our approach\nsignificantly outperforms strong baselines.\n","authors":["Qingyun Wang","Manling Li","Hou Pong Chan","Lifu Huang","Julia Hockenmaier","Girish Chowdhary","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2208.12306v3.pdf","comment":"21 pages, Accepted by Findings of the Association for Computational\n  Linguistics: ACL 2023, Code and Resources at\n  https://github.com/EagleW/Multimedia-Generative-Script-Learning"},{"id":"http://arxiv.org/abs/2306.17649v3","updated":"2023-07-10T16:22:41Z","published":"2023-06-30T13:35:24Z","title":"Biomedical Language Models are Robust to Sub-optimal Tokenization","summary":"  As opposed to general English, many concepts in biomedical terminology have\nbeen designed in recent history by biomedical professionals with the goal of\nbeing precise and concise. This is often achieved by concatenating meaningful\nbiomedical morphemes to create new semantic units. Nevertheless, most modern\nbiomedical language models (LMs) are pre-trained using standard domain-specific\ntokenizers derived from large scale biomedical corpus statistics without\nexplicitly leveraging the agglutinating nature of biomedical language. In this\nwork, we first find that standard open-domain and biomedical tokenizers are\nlargely unable to segment biomedical terms into meaningful components.\nTherefore, we hypothesize that using a tokenizer which segments biomedical\nterminology more accurately would enable biomedical LMs to improve their\nperformance on downstream biomedical NLP tasks, especially ones which involve\nbiomedical terms directly such as named entity recognition (NER) and entity\nlinking. Surprisingly, we find that pre-training a biomedical LM using a more\naccurate biomedical tokenizer does not improve the entity representation\nquality of a language model as measured by several intrinsic and extrinsic\nmeasures such as masked language modeling prediction (MLM) accuracy as well as\nNER and entity linking performance. These quantitative findings, along with a\ncase study which explores entity representation quality more directly, suggest\nthat the biomedical pre-training process is quite robust to instances of\nsub-optimal tokenization.\n","authors":["Bernal Jiménez Gutiérrez","Huan Sun","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2306.17649v3.pdf","comment":"BioNLP @ ACL 2023"},{"id":"http://arxiv.org/abs/2306.15666v2","updated":"2023-07-10T16:14:33Z","published":"2023-06-21T16:29:44Z","title":"Testing of Detection Tools for AI-Generated Text","summary":"  Recent advances in generative pre-trained transformer large language models\nhave emphasised the potential risks of unfair use of artificial intelligence\n(AI) generated content in an academic environment and intensified efforts in\nsearching for solutions to detect such content. The paper examines the general\nfunctionality of detection tools for artificial intelligence generated text and\nevaluates them based on accuracy and error type analysis. Specifically, the\nstudy seeks to answer research questions about whether existing detection tools\ncan reliably differentiate between human-written text and ChatGPT-generated\ntext, and whether machine translation and content obfuscation techniques affect\nthe detection of AI-generated text. The research covers 12 publicly available\ntools and two commercial systems (Turnitin and PlagiarismCheck) that are widely\nused in the academic setting. The researchers conclude that the available\ndetection tools are neither accurate nor reliable and have a main bias towards\nclassifying the output as human-written rather than detecting AI-generated\ntext. Furthermore, content obfuscation techniques significantly worsen the\nperformance of tools. The study makes several significant contributions. First,\nit summarises up-to-date similar scientific and non-scientific efforts in the\nfield. Second, it presents the result of one of the most comprehensive tests\nconducted so far, based on a rigorous research methodology, an original\ndocument set, and a broad coverage of tools. Third, it discusses the\nimplications and drawbacks of using detection tools for AI-generated text in\nacademic settings.\n","authors":["Debora Weber-Wulff","Alla Anohina-Naumeca","Sonja Bjelobaba","Tomáš Foltýnek","Jean Guerrero-Dib","Olumide Popoola","Petr Šigut","Lorna Waddington"],"pdf_url":"https://arxiv.org/pdf/2306.15666v2.pdf","comment":"38 pages, 13 figures and 10 tables, and an appendix with 18 figures.\n  Submitted to the International Journal for Educational Integrity"},{"id":"http://arxiv.org/abs/2307.03131v2","updated":"2023-07-10T15:59:49Z","published":"2023-07-06T16:59:30Z","title":"BLEURT Has Universal Translations: An Analysis of Automatic Metrics by\n  Minimum Risk Training","summary":"  Automatic metrics play a crucial role in machine translation. Despite the\nwidespread use of n-gram-based metrics, there has been a recent surge in the\ndevelopment of pre-trained model-based metrics that focus on measuring sentence\nsemantics. However, these neural metrics, while achieving higher correlations\nwith human evaluations, are often considered to be black boxes with potential\nbiases that are difficult to detect. In this study, we systematically analyze\nand compare various mainstream and cutting-edge automatic metrics from the\nperspective of their guidance for training machine translation systems. Through\nMinimum Risk Training (MRT), we find that certain metrics exhibit robustness\ndefects, such as the presence of universal adversarial translations in BLEURT\nand BARTScore. In-depth analysis suggests two main causes of these robustness\ndeficits: distribution biases in the training datasets, and the tendency of the\nmetric paradigm. By incorporating token-level constraints, we enhance the\nrobustness of evaluation metrics, which in turn leads to an improvement in the\nperformance of machine translation systems. Codes are available at\n\\url{https://github.com/powerpuffpomelo/fairseq_mrt}.\n","authors":["Yiming Yan","Tao Wang","Chengqi Zhao","Shujian Huang","Jiajun Chen","Mingxuan Wang"],"pdf_url":"https://arxiv.org/pdf/2307.03131v2.pdf","comment":"Accepted to ACL 2023 main conference"},{"id":"http://arxiv.org/abs/2307.04657v1","updated":"2023-07-10T15:56:17Z","published":"2023-07-10T15:56:17Z","title":"BeaverTails: Towards Improved Safety Alignment of LLM via a\n  Human-Preference Dataset","summary":"  In this paper, we introduce the BeaverTails dataset, aimed at fostering\nresearch on safety alignment in large language models (LLMs). This dataset\nuniquely separates annotations of helpfulness and harmlessness for\nquestion-answering pairs, thus offering distinct perspectives on these crucial\nattributes. In total, we have compiled safety meta-labels for 30,207\nquestion-answer (QA) pairs and gathered 30,144 pairs of expert comparison data\nfor both the helpfulness and harmlessness metrics. We further showcase\napplications of BeaverTails in content moderation and reinforcement learning\nwith human feedback (RLHF), emphasizing its potential for practical safety\nmeasures in LLMs. We believe this dataset provides vital resources for the\ncommunity, contributing towards the safe development and deployment of LLMs.\nOur project page is available at the following URL:\nhttps://sites.google.com/view/pku-beavertails.\n","authors":["Jiaming Ji","Mickel Liu","Juntao Dai","Xuehai Pan","Chi Zhang","Ce Bian","Chi Zhang","Ruiyang Sun","Yizhou Wang","Yaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2307.04657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.04626v1","updated":"2023-07-10T15:10:56Z","published":"2023-07-10T15:10:56Z","title":"Measuring Lexical Diversity in Texts: The Twofold Length Problem","summary":"  The impact of text length on the estimation of lexical diversity has captured\nthe attention of the scientific community for more than a century. Numerous\nindices have been proposed, and many studies have been conducted to evaluate\nthem, but the problem remains. This methodological review provides a critical\nanalysis not only of the most commonly used indices in language learning\nstudies, but also of the length problem itself, as well as of the methodology\nfor evaluating the proposed solutions. The analysis of three datasets of\nEnglish language-learners' texts revealed that indices that reduce all texts to\nthe same length using a probabilistic or an algorithmic approach solve the\nlength dependency problem; however, all these indices failed to address the\nsecond problem, which is their sensitivity to the parameter that determines the\nlength to which the texts are reduced. The paper concludes with recommendations\nfor optimizing lexical diversity analysis.\n","authors":["Yves Bestgen"],"pdf_url":"https://arxiv.org/pdf/2307.04626v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2307.05578v1","updated":"2023-07-10T13:23:36Z","published":"2023-07-10T13:23:36Z","title":"Hate Speech Detection via Dual Contrastive Learning","summary":"  The fast spread of hate speech on social media impacts the Internet\nenvironment and our society by increasing prejudice and hurting people.\nDetecting hate speech has aroused broad attention in the field of natural\nlanguage processing. Although hate speech detection has been addressed in\nrecent work, this task still faces two inherent unsolved challenges. The first\nchallenge lies in the complex semantic information conveyed in hate speech,\nparticularly the interference of insulting words in hate speech detection. The\nsecond challenge is the imbalanced distribution of hate speech and non-hate\nspeech, which may significantly deteriorate the performance of models. To\ntackle these challenges, we propose a novel dual contrastive learning (DCL)\nframework for hate speech detection. Our framework jointly optimizes the\nself-supervised and the supervised contrastive learning loss for capturing\nspan-level information beyond the token-level emotional semantics used in\nexisting models, particularly detecting speech containing abusive and insulting\nwords. Moreover, we integrate the focal loss into the dual contrastive learning\nframework to alleviate the problem of data imbalance. We conduct experiments on\ntwo publicly available English datasets, and experimental results show that the\nproposed model outperforms the state-of-the-art models and precisely detects\nhate speeches.\n","authors":["Junyu Lu","Hongfei Lin","Xiaokun Zhang","Zhaoqing Li","Tongyue Zhang","Linlin Zong","Fenglong Ma","Bo Xu"],"pdf_url":"https://arxiv.org/pdf/2307.05578v1.pdf","comment":"The paper has been accepted in IEEE/ACM Transactions on Audio,\n  Speech, and Language Processing (TASLP)"},{"id":"http://arxiv.org/abs/2305.18185v2","updated":"2023-07-10T13:10:40Z","published":"2023-05-29T16:24:01Z","title":"Syntax and Semantics Meet in the \"Middle\": Probing the Syntax-Semantics\n  Interface of LMs Through Agentivity","summary":"  Recent advances in large language models have prompted researchers to examine\ntheir abilities across a variety of linguistic tasks, but little has been done\nto investigate how models handle the interactions in meaning across words and\nlarger syntactic forms -- i.e. phenomena at the intersection of syntax and\nsemantics. We present the semantic notion of agentivity as a case study for\nprobing such interactions. We created a novel evaluation dataset by utilitizing\nthe unique linguistic properties of a subset of optionally transitive English\nverbs. This dataset was used to prompt varying sizes of three model classes to\nsee if they are sensitive to agentivity at the lexical level, and if they can\nappropriately employ these word-level priors given a specific syntactic\ncontext. Overall, GPT-3 text-davinci-003 performs extremely well across all\nexperiments, outperforming all other models tested by far. In fact, the results\nare even better correlated with human judgements than both syntactic and\nsemantic corpus statistics. This suggests that LMs may potentially serve as\nmore useful tools for linguistic annotation, theory testing, and discovery than\nselect corpora for certain tasks. Code is available at\nhttps://github.com/lindiatjuatja/lm_sem\n","authors":["Lindia Tjuatja","Emmy Liu","Lori Levin","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2305.18185v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.04507v1","updated":"2023-07-10T12:01:18Z","published":"2023-07-10T12:01:18Z","title":"Improving Factuality of Abstractive Summarization via Contrastive Reward\n  Learning","summary":"  Modern abstractive summarization models often generate summaries that contain\nhallucinated or contradictory information. In this paper, we propose a simple\nbut effective contrastive learning framework that incorporates recent\ndevelopments in reward learning and factuality metrics. Empirical studies\ndemonstrate that the proposed framework enables summarization models to learn\nfrom feedback of factuality metrics using contrastive reward learning, leading\nto more factual summaries by human evaluations. This suggests that further\nadvances in learning and evaluation algorithms can feed directly into providing\nmore factual summaries.\n","authors":["I-Chun Chern","Zhiruo Wang","Sanjan Das","Bhavuk Sharma","Pengfei Liu","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2307.04507v1.pdf","comment":"TrustNLP @ ACL 2023"},{"id":"http://arxiv.org/abs/2307.03667v2","updated":"2023-07-10T11:38:21Z","published":"2023-07-07T15:37:50Z","title":"Testing the Predictions of Surprisal Theory in 11 Languages","summary":"  A fundamental result in psycholinguistics is that less predictable words take\na longer time to process. One theoretical explanation for this finding is\nSurprisal Theory (Hale, 2001; Levy, 2008), which quantifies a word's\npredictability as its surprisal, i.e. its negative log-probability given a\ncontext. While evidence supporting the predictions of Surprisal Theory have\nbeen replicated widely, most have focused on a very narrow slice of data:\nnative English speakers reading English texts. Indeed, no comprehensive\nmultilingual analysis exists. We address this gap in the current literature by\ninvestigating the relationship between surprisal and reading times in eleven\ndifferent languages, distributed across five language families. Deriving\nestimates from language models trained on monolingual and multilingual corpora,\nwe test three predictions associated with surprisal theory: (i) whether\nsurprisal is predictive of reading times; (ii) whether expected surprisal, i.e.\ncontextual entropy, is predictive of reading times; (iii) and whether the\nlinking function between surprisal and reading times is linear. We find that\nall three predictions are borne out crosslinguistically. By focusing on a more\ndiverse set of languages, we argue that these results offer the most robust\nlink to-date between information theory and incremental language processing\nacross languages.\n","authors":["Ethan Gotlieb Wilcox","Tiago Pimentel","Clara Meister","Ryan Cotterell","Roger P. Levy"],"pdf_url":"https://arxiv.org/pdf/2307.03667v2.pdf","comment":"This is a pre-MIT Press publication version of the paper"},{"id":"http://arxiv.org/abs/2307.05722v1","updated":"2023-07-10T11:29:41Z","published":"2023-07-10T11:29:41Z","title":"Exploring Large Language Model for Graph Data Understanding in Online\n  Job Recommendations","summary":"  Large Language Models (LLMs) have revolutionized natural language processing\ntasks, demonstrating their exceptional capabilities in various domains.\nHowever, their potential for behavior graph understanding in job\nrecommendations remains largely unexplored. This paper focuses on unveiling the\ncapability of large language models in understanding behavior graphs and\nleveraging this understanding to enhance recommendations in online recruitment,\nincluding the promotion of out-of-distribution (OOD) application. We present a\nnovel framework that harnesses the rich contextual information and semantic\nrepresentations provided by large language models to analyze behavior graphs\nand uncover underlying patterns and relationships. Specifically, we propose a\nmeta-path prompt constructor that leverages LLM recommender to understand\nbehavior graphs for the first time and design a corresponding path augmentation\nmodule to alleviate the prompt bias introduced by path-based sequence input. By\nleveraging this capability, our framework enables personalized and accurate job\nrecommendations for individual users. We evaluate the effectiveness of our\napproach on a comprehensive dataset and demonstrate its ability to improve the\nrelevance and quality of recommended quality. This research not only sheds\nlight on the untapped potential of large language models but also provides\nvaluable insights for developing advanced recommendation systems in the\nrecruitment market. The findings contribute to the growing field of natural\nlanguage processing and offer practical implications for enhancing job search\nexperiences.\n","authors":["Likang Wu","Zhaopeng Qiu","Zhi Zheng","Hengshu Zhu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2307.05722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.09170v3","updated":"2023-07-10T11:22:20Z","published":"2023-06-15T14:47:03Z","title":"Can ChatGPT pass the Vietnamese National High School Graduation\n  Examination?","summary":"  This research article highlights the potential of AI-powered chatbots in\neducation and presents the results of using ChatGPT, a large language model, to\ncomplete the Vietnamese National High School Graduation Examination (VNHSGE).\nThe study dataset included 30 essays in the literature test case and 1,700\nmultiple-choice questions designed for other subjects. The results showed that\nChatGPT was able to pass the examination with an average score of 6-7,\ndemonstrating the technology's potential to revolutionize the educational\nlandscape. The analysis of ChatGPT performance revealed its proficiency in a\nrange of subjects, including mathematics, English, physics, chemistry, biology,\nhistory, geography, civic education, and literature, which suggests its\npotential to provide effective support for learners. However, further research\nis needed to assess ChatGPT performance on more complex exam questions and its\npotential to support learners in different contexts. As technology continues to\nevolve and improve, we can expect to see the use of AI tools like ChatGPT\nbecome increasingly common in educational settings, ultimately enhancing the\neducational experience for both students and educators.\n","authors":["Xuan-Quy Dao","Ngoc-Bich Le","Xuan-Dung Phan","Bac-Bien Ngo"],"pdf_url":"https://arxiv.org/pdf/2306.09170v3.pdf","comment":"9 pages, 13 figures, 4 tables"},{"id":"http://arxiv.org/abs/2304.00215v3","updated":"2023-07-10T10:27:06Z","published":"2023-04-01T03:49:47Z","title":"Inductive Relation Prediction from Relational Paths and Context with\n  Hierarchical Transformers","summary":"  Relation prediction on knowledge graphs (KGs) is a key research topic.\nDominant embedding-based methods mainly focus on the transductive setting and\nlack the inductive ability to generalize to new entities for inference.\nExisting methods for inductive reasoning mostly mine the connections between\nentities, i.e., relational paths, without considering the nature of head and\ntail entities contained in the relational context. This paper proposes a novel\nmethod that captures both connections between entities and the intrinsic nature\nof entities, by simultaneously aggregating RElational Paths and cOntext with a\nunified hieRarchical Transformer framework, namely REPORT. REPORT relies solely\non relation semantics and can naturally generalize to the fully-inductive\nsetting, where KGs for training and inference have no common entities. In the\nexperiments, REPORT performs consistently better than all baselines on almost\nall the eight version subsets of two fully-inductive datasets. Moreover. REPORT\nis interpretable by providing each element's contribution to the prediction\nresults.\n","authors":["Jiaang Li","Quan Wang","Zhendong Mao"],"pdf_url":"https://arxiv.org/pdf/2304.00215v3.pdf","comment":"Accepted by ICASSP 2023 (Oral). The code is available at:\n  https://github.com/JiaangL/REPORT"},{"id":"http://arxiv.org/abs/2306.17181v2","updated":"2023-07-10T10:12:37Z","published":"2023-06-19T10:22:12Z","title":"Unsupervised Text Embedding Space Generation Using Generative\n  Adversarial Networks for Text Synthesis","summary":"  Generative Adversarial Networks (GAN) is a model for data synthesis, which\ncreates plausible data through the competition of generator and discriminator.\nAlthough GAN application to image synthesis is extensively studied, it has\ninherent limitations to natural language generation. Because natural language\nis composed of discrete tokens, a generator has difficulty updating its\ngradient through backpropagation; therefore, most text-GAN studies generate\nsentences starting with a random token based on a reward system. Thus, the\ngenerators of previous studies are pre-trained in an autoregressive way before\nadversarial training, causing data memorization that synthesized sentences\nreproduce the training data. In this paper, we synthesize sentences using a\nframework similar to the original GAN. More specifically, we propose Text\nEmbedding Space Generative Adversarial Networks (TESGAN) which generate\ncontinuous text embedding spaces instead of discrete tokens to solve the\ngradient backpropagation problem. Furthermore, TESGAN conducts unsupervised\nlearning which does not directly refer to the text of the training data to\novercome the data memorization issue. By adopting this novel method, TESGAN can\nsynthesize new sentences, showing the potential of unsupervised learning for\ntext synthesis. We expect to see extended research combining Large Language\nModels with a new perspective of viewing text as an continuous space.\n","authors":["Jun-Min Lee","Tae-Bin Ha"],"pdf_url":"https://arxiv.org/pdf/2306.17181v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.04412v1","updated":"2023-07-10T08:32:45Z","published":"2023-07-10T08:32:45Z","title":"Enhancing Biomedical Text Summarization and Question-Answering: On the\n  Utility of Domain-Specific Pre-Training","summary":"  Biomedical summarization requires large datasets to train for text\ngeneration. We show that while transfer learning offers a viable option for\naddressing this challenge, an in-domain pre-training does not always offer\nadvantages in a BioASQ summarization task. We identify a suitable model\narchitecture and use it to show a benefit of a general-domain pre-training\nfollowed by a task-specific fine-tuning in the context of a BioASQ\nsummarization task, leading to a novel three-step fine-tuning approach that\nworks with only a thousand in-domain examples. Our results indicate that a\nLarge Language Model without domain-specific pre-training can have a\nsignificant edge in some domain-specific biomedical text generation tasks.\n","authors":["Dima Galat","Marian-Andrei Rizoiu"],"pdf_url":"https://arxiv.org/pdf/2307.04412v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05360v1","updated":"2023-07-10T08:20:34Z","published":"2023-07-10T08:20:34Z","title":"Unmasking the giant: A comprehensive evaluation of ChatGPT's proficiency\n  in coding algorithms and data structures","summary":"  The transformative influence of Large Language Models (LLMs) is profoundly\nreshaping the Artificial Intelligence (AI) technology domain. Notably, ChatGPT\ndistinguishes itself within these models, demonstrating remarkable performance\nin multi-turn conversations and exhibiting code proficiency across an array of\nlanguages. In this paper, we carry out a comprehensive evaluation of ChatGPT's\ncoding capabilities based on what is to date the largest catalog of coding\nchallenges. Our focus is on the python programming language and problems\ncentered on data structures and algorithms, two topics at the very foundations\nof Computer Science. We evaluate ChatGPT for its ability to generate correct\nsolutions to the problems fed to it, its code quality, and nature of run-time\nerrors thrown by its code. Where ChatGPT code successfully executes, but fails\nto solve the problem at hand, we look into patterns in the test cases passed in\norder to gain some insights into how wrong ChatGPT code is in these kinds of\nsituations. To infer whether ChatGPT might have directly memorized some of the\ndata that was used to train it, we methodically design an experiment to\ninvestigate this phenomena. Making comparisons with human performance whenever\nfeasible, we investigate all the above questions from the context of both its\nunderlying learning models (GPT-3.5 and GPT-4), on a vast array sub-topics\nwithin the main topics, and on problems having varying degrees of difficulty.\n","authors":["Sayed Erfan Arefin","Tasnia Ashrafi Heya","Hasan Al-Qudah","Ynes Ineza","Abdul Serwadda"],"pdf_url":"https://arxiv.org/pdf/2307.05360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.04408v1","updated":"2023-07-10T08:15:40Z","published":"2023-07-10T08:15:40Z","title":"TIM: Teaching Large Language Models to Translate with Comparison","summary":"  Open-sourced large language models (LLMs) have demonstrated remarkable\nefficacy in various tasks with instruction tuning. However, these models can\nsometimes struggle with tasks that require more specialized knowledge such as\ntranslation. One possible reason for such deficiency is that instruction tuning\naims to generate fluent and coherent text that continues from a given\ninstruction without being constrained by any task-specific requirements.\nMoreover, it can be more challenging for tuning smaller LLMs with lower-quality\ntraining data. To address this issue, we propose a novel framework using\nexamples in comparison to teach LLMs to learn translation. Our approach\ninvolves presenting the model with examples of correct and incorrect\ntranslations and using a preference loss to guide the model's learning. We\nevaluate our method on WMT2022 test sets and show that it outperforms existing\nmethods. Our findings offer a new perspective on fine-tuning LLMs for\ntranslation tasks and provide a promising solution for generating high-quality\ntranslations. Please refer to Github for more details:\nhttps://github.com/lemon0830/TIM.\n","authors":["Jiali Zeng","Fandong Meng","Yongjing Yin","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2307.04408v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.04401v1","updated":"2023-07-10T08:03:41Z","published":"2023-07-10T08:03:41Z","title":"Ethicist: Targeted Training Data Extraction Through Loss Smoothed Soft\n  Prompting and Calibrated Confidence Estimation","summary":"  Large pre-trained language models achieve impressive results across many\ntasks. However, recent works point out that pre-trained language models may\nmemorize a considerable fraction of their training data, leading to the privacy\nrisk of information leakage. In this paper, we propose a method named Ethicist\nfor targeted training data extraction through loss smoothed soft prompting and\ncalibrated confidence estimation, investigating how to recover the suffix in\nthe training data when given a prefix. To elicit memorization in the attacked\nmodel, we tune soft prompt embeddings while keeping the model fixed. We further\npropose a smoothing loss that smooths the loss distribution of the suffix\ntokens to make it easier to sample the correct suffix. In order to select the\nmost probable suffix from a collection of sampled suffixes and estimate the\nprediction confidence, we propose a calibrated confidence estimation method,\nwhich normalizes the confidence of the generated suffixes with a local\nestimation. We show that Ethicist significantly improves the extraction\nperformance on a recently proposed public benchmark. We also investigate\nseveral factors influencing the data extraction performance, including decoding\nstrategy, model scale, prefix length, and suffix length. Our code is available\nat https://github.com/thu-coai/Targeted-Data-Extraction.\n","authors":["Zhexin Zhang","Jiaxin Wen","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2307.04401v1.pdf","comment":"ACL 2023 Long Paper (Main Conference)"},{"id":"http://arxiv.org/abs/2307.02863v2","updated":"2023-07-10T07:54:23Z","published":"2023-07-06T09:03:10Z","title":"ValiTex -- a unified validation framework for computational text-based\n  measures of social science constructs","summary":"  Guidance on how to validate computational text-based measures of social\nscience constructs is fragmented. Whereas scholars are generally acknowledging\nthe importance of validating their text-based measures, they often lack common\nterminology and a unified framework to do so. This paper introduces a new\nvalidation framework called ValiTex, designed to assist scholars to measure\nsocial science constructs based on textual data. The framework draws on a\nlong-established tradition within psychometrics while extending the framework\nfor the purpose of computational text analysis. ValiTex consists of two\ncomponents, a conceptual model, and a dynamic checklist. Whereas the conceptual\nmodel provides a general structure along distinct phases on how to approach\nvalidation, the dynamic checklist defines specific validation steps and\nprovides guidance on which steps might be considered recommendable (i.e.,\nproviding relevant and necessary validation evidence) or optional (i.e., useful\nfor providing additional supporting validation evidence. The utility of the\nframework is demonstrated by applying it to a use case of detecting sexism from\nsocial media data.\n","authors":["Lukas Birkenmaier","Clemens Lechner","Claudia Wagner"],"pdf_url":"https://arxiv.org/pdf/2307.02863v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.08917v2","updated":"2023-07-10T06:21:55Z","published":"2022-10-17T10:14:22Z","title":"Mars: Modeling Context & State Representations with Contrastive Learning\n  for End-to-End Task-Oriented Dialog","summary":"  Traditional end-to-end task-oriented dialog systems first convert dialog\ncontext into belief state and action state before generating the system\nresponse. The system response performance is significantly affected by the\nquality of the belief state and action state. We first explore what dialog\ncontext representation is beneficial to improving the quality of the belief\nstate and action state, which further enhances the generated response quality.\nTo tackle our exploration, we propose Mars, an end-to-end task-oriented dialog\nsystem with two contrastive learning strategies to model the relationship\nbetween dialog context and belief/action state representations. Empirical\nresults show dialog context representations, which are more different from\nsemantic state representations, are more conducive to multi-turn task-oriented\ndialog. Moreover, our proposed Mars achieves state-of-the-art performance on\nthe MultiWOZ 2.0, CamRest676, and CrossWOZ.\n","authors":["Haipeng Sun","Junwei Bao","Youzheng Wu","Xiaodong He"],"pdf_url":"https://arxiv.org/pdf/2210.08917v2.pdf","comment":"Findings of ACL2023"},{"id":"http://arxiv.org/abs/2307.04361v1","updated":"2023-07-10T06:17:33Z","published":"2023-07-10T06:17:33Z","title":"Enhancing Cross-lingual Transfer via Phonemic Transcription Integration","summary":"  Previous cross-lingual transfer methods are restricted to orthographic\nrepresentation learning via textual scripts. This limitation hampers\ncross-lingual transfer and is biased towards languages sharing similar\nwell-known scripts. To alleviate the gap between languages from different\nwriting scripts, we propose PhoneXL, a framework incorporating phonemic\ntranscriptions as an additional linguistic modality beyond the traditional\northographic transcriptions for cross-lingual transfer. Particularly, we\npropose unsupervised alignment objectives to capture (1) local one-to-one\nalignment between the two different modalities, (2) alignment via\nmulti-modality contexts to leverage information from additional modalities, and\n(3) alignment via multilingual contexts where additional bilingual dictionaries\nare incorporated. We also release the first phonemic-orthographic alignment\ndataset on two token-level tasks (Named Entity Recognition and Part-of-Speech\nTagging) among the understudied but interconnected\nChinese-Japanese-Korean-Vietnamese (CJKV) languages. Our pilot study reveals\nphonemic transcription provides essential information beyond the orthography to\nenhance cross-lingual transfer and bridge the gap among CJKV languages, leading\nto consistent improvements on cross-lingual token-level tasks over\northographic-based multilingual PLMs.\n","authors":["Hoang H. Nguyen","Chenwei Zhang","Tao Zhang","Eugene Rohrbaugh","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2307.04361v1.pdf","comment":"11 pages,1 figure, 7 tables. To appear in Findings of ACL 2023"},{"id":"http://arxiv.org/abs/2210.15097v2","updated":"2023-07-10T06:08:55Z","published":"2022-10-27T00:58:21Z","title":"Contrastive Decoding: Open-ended Text Generation as Optimization","summary":"  Given a language model (LM), maximum probability is a poor decoding objective\nfor open-ended generation, because it produces short and repetitive text. On\nthe other hand, sampling can often produce incoherent text that drifts from the\noriginal topics. We propose contrastive decoding (CD), a reliable decoding\napproach that optimizes a contrastive objective subject to a plausibility\nconstraint. The contrastive objective returns the difference between the\nlikelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM\n(called the amateur, e.g. OPT-125M), and the constraint ensures that the\noutputs are plausible. CD is inspired by the fact that the failures of larger\nLMs (e.g., repetition, incoherence) are even more prevalent in smaller LMs, and\nthat this difference signals which texts should be preferred. CD requires zero\nadditional training, and produces higher quality text than decoding from the\nlarger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and\nsignificantly outperforms four strong decoding algorithms (e.g., nucleus,\ntop-k) in automatic and human evaluations across wikipedia, news and story\ndomains.\n","authors":["Xiang Lisa Li","Ari Holtzman","Daniel Fried","Percy Liang","Jason Eisner","Tatsunori Hashimoto","Luke Zettlemoyer","Mike Lewis"],"pdf_url":"https://arxiv.org/pdf/2210.15097v2.pdf","comment":"Main conference long paper at ACL 2023"},{"id":"http://arxiv.org/abs/2307.04349v1","updated":"2023-07-10T05:18:18Z","published":"2023-07-10T05:18:18Z","title":"RLTF: Reinforcement Learning from Unit Test Feedback","summary":"  The goal of program synthesis, or code generation, is to generate executable\ncode based on given descriptions. Recently, there has been an increasing number\nof studies employing reinforcement learning (RL) to improve the performance of\nlarge language models (LLMs) for code. However, these RL methods have only used\noffline frameworks, limiting their exploration of new sample spaces.\nAdditionally, current approaches that utilize unit test signals are rather\nsimple, not accounting for specific error locations within the code. To address\nthese issues, we proposed RLTF, i.e., Reinforcement Learning from Unit Test\nFeedback, a novel online RL framework with unit test feedback of\nmulti-granularity for refining code LLMs. Our approach generates data in\nreal-time during training and simultaneously utilizes fine-grained feedback\nsignals to guide the model towards producing higher-quality code. Extensive\nexperiments show that RLTF achieves state-of-the-art performance on the APPS\nand the MBPP benchmarks. Our code can be found at:\nhttps://github.com/Zyq-scut/RLTF.\n","authors":["Jiate Liu","Yiqin Zhu","Kaiwen Xiao","Qiang Fu","Xiao Han","Wei Yang","Deheng Ye"],"pdf_url":"https://arxiv.org/pdf/2307.04349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05567v1","updated":"2023-07-10T01:46:15Z","published":"2023-07-10T01:46:15Z","title":"Event Extraction as Question Generation and Answering","summary":"  Recent work on Event Extraction has reframed the task as Question Answering\n(QA), with promising results. The advantage of this approach is that it\naddresses the error propagation issue found in traditional token-based\nclassification approaches by directly predicting event arguments without\nextracting candidates first. However, the questions are typically based on\nfixed templates and they rarely leverage contextual information such as\nrelevant arguments. In addition, prior QA-based approaches have difficulty\nhandling cases where there are multiple arguments for the same role. In this\npaper, we propose QGA-EE, which enables a Question Generation (QG) model to\ngenerate questions that incorporate rich contextual information instead of\nusing fixed templates. We also propose dynamic templates to assist the training\nof QG model. Experiments show that QGA-EE outperforms all prior\nsingle-task-based models on the ACE05 English dataset.\n","authors":["Di Lu","Shihao Ran","Joel Tetreault","Alejandro Jaimes"],"pdf_url":"https://arxiv.org/pdf/2307.05567v1.pdf","comment":"Accepted to ACL 2023"},{"id":"http://arxiv.org/abs/2307.04303v1","updated":"2023-07-10T01:44:13Z","published":"2023-07-10T01:44:13Z","title":"Learning to Generate Equitable Text in Dialogue from Biased Training\n  Data","summary":"  The ingrained principles of fairness in a dialogue system's decision-making\nprocess and generated responses are crucial for user engagement, satisfaction,\nand task achievement. Absence of equitable and inclusive principles can hinder\nthe formation of common ground, which in turn negatively impacts the overall\nperformance of the system. For example, misusing pronouns in a user interaction\nmay cause ambiguity about the intended subject. Yet, there is no comprehensive\nstudy of equitable text generation in dialogue. Aptly, in this work, we use\ntheories of computational learning to study this problem. We provide formal\ndefinitions of equity in text generation, and further, prove formal connections\nbetween learning human-likeness and learning equity: algorithms for improving\nequity ultimately reduce to algorithms for improving human-likeness (on\naugmented data). With this insight, we also formulate reasonable conditions\nunder which text generation algorithms can learn to generate equitable text\nwithout any modifications to the biased training data on which they learn. To\nexemplify our theory in practice, we look at a group of algorithms for the\nGuessWhat?! visual dialogue game and, using this example, test our theory\nempirically. Our theory accurately predicts relative-performance of multiple\nalgorithms in generating equitable text as measured by both human and automated\nevaluation.\n","authors":["Anthony Sicilia","Malihe Alikhani"],"pdf_url":"https://arxiv.org/pdf/2307.04303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.04285v1","updated":"2023-07-10T00:24:27Z","published":"2023-07-10T00:24:27Z","title":"HistRED: A Historical Document-Level Relation Extraction Dataset","summary":"  Despite the extensive applications of relation extraction (RE) tasks in\nvarious domains, little has been explored in the historical context, which\ncontains promising data across hundreds and thousands of years. To promote the\nhistorical RE research, we present HistRED constructed from Yeonhaengnok.\nYeonhaengnok is a collection of records originally written in Hanja, the\nclassical Chinese writing, which has later been translated into Korean. HistRED\nprovides bilingual annotations such that RE can be performed on Korean and\nHanja texts. In addition, HistRED supports various self-contained subtexts with\ndifferent lengths, from a sentence level to a document level, supporting\ndiverse context settings for researchers to evaluate the robustness of their RE\nmodels. To demonstrate the usefulness of our dataset, we propose a bilingual RE\nmodel that leverages both Korean and Hanja contexts to predict relations\nbetween entities. Our model outperforms monolingual baselines on HistRED,\nshowing that employing multiple language contexts supplements the RE\npredictions. The dataset is publicly available at:\nhttps://huggingface.co/datasets/Soyoung/HistRED under CC BY-NC-ND 4.0 license.\n","authors":["Soyoung Yang","Minseok Choi","Youngwoo Cho","Jaegul Choo"],"pdf_url":"https://arxiv.org/pdf/2307.04285v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2307.04923v1","updated":"2023-07-10T22:14:56Z","published":"2023-07-10T22:14:56Z","title":"Ranking with Long-Term Constraints","summary":"  The feedback that users provide through their choices (e.g., clicks,\npurchases) is one of the most common types of data readily available for\ntraining search and recommendation algorithms. However, myopically training\nsystems based on choice data may only improve short-term engagement, but not\nthe long-term sustainability of the platform and the long-term benefits to its\nusers, content providers, and other stakeholders. In this paper, we thus\ndevelop a new framework in which decision makers (e.g., platform operators,\nregulators, users) can express long-term goals for the behavior of the platform\n(e.g., fairness, revenue distribution, legal requirements). These goals take\nthe form of exposure or impact targets that go well beyond individual sessions,\nand we provide new control-based algorithms to achieve these goals. In\nparticular, the controllers are designed to achieve the stated long-term goals\nwith minimum impact on short-term engagement. Beyond the principled theoretical\nderivation of the controllers, we evaluate the algorithms on both synthetic and\nreal-world data. While all controllers perform well, we find that they provide\ninteresting trade-offs in efficiency, robustness, and the ability to plan\nahead.\n","authors":["Kianté Brantley","Zhichong Fang","Sarah Dean","Thorsten Joachims"],"pdf_url":"https://arxiv.org/pdf/2307.04923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.05995v2","updated":"2023-07-10T21:51:34Z","published":"2023-01-15T01:36:46Z","title":"Collective Privacy Recovery: Data-sharing Coordination via Decentralized\n  Artificial Intelligence","summary":"  Collective privacy loss becomes a colossal problem, an emergency for personal\nfreedoms and democracy. But, are we prepared to handle personal data as scarce\nresource and collectively share data under the doctrine: as little as possible,\nas much as necessary? We hypothesize a significant privacy recovery if a\npopulation of individuals, the data collective, coordinates to share minimum\ndata for running online services with the required quality. Here we show how to\nautomate and scale-up complex collective arrangements for privacy recovery\nusing decentralized artificial intelligence. For this, we compare for first\ntime attitudinal, intrinsic, rewarded and coordinated data sharing in a\nrigorous living-lab experiment of high realism involving >27,000 real data\ndisclosures. Using causal inference and cluster analysis, we differentiate\ncriteria predicting privacy and five key data-sharing behaviors. Strikingly,\ndata-sharing coordination proves to be a win-win for all: remarkable privacy\nrecovery for people with evident costs reduction for service providers.\n","authors":["Evangelos Pournaras","Mark Christopher Ballandies","Stefano Bennati","Chien-fei Chen"],"pdf_url":"https://arxiv.org/pdf/2301.05995v2.pdf","comment":"Contains Supplementary Information"},{"id":"http://arxiv.org/abs/2108.08614v7","updated":"2023-07-10T18:30:18Z","published":"2021-08-19T10:50:52Z","title":"UNIQORN: Unified Question Answering over RDF Knowledge Graphs and\n  Natural Language Text","summary":"  Question answering over knowledge graphs and other RDF data has been greatly\nadvanced, with a number of good techniques providing crisp answers for natural\nlanguage questions or telegraphic queries. Some of these systems incorporate\ntextual sources as additional evidence for the answering process, but cannot\ncompute answers that are present in text alone. Conversely, techniques from the\nIR and NLP communities have addressed QA over text, but such systems barely\nutilize semantic data and knowledge. This paper presents a method for complex\nquestions that can seamlessly operate over a mixture of RDF datasets and text\ncorpora, or individual sources, in a unified framework. Our method, called\nUNIQORN, builds a context graph on-the-fly, by retrieving question-relevant\nevidences from the RDF data and/or a text corpus, using fine-tuned BERT models.\nThe resulting graph typically contains all question-relevant evidences but also\na lot of noise. UNIQORN copes with this input by a graph algorithm for Group\nSteiner Trees, that identifies the best answer candidates in the context graph.\nExperimental results on several benchmarks of complex questions with multiple\nentities and relations, show that UNIQORN significantly outperforms\nstate-of-the-art methods for heterogeneous QA. The graph-based methodology\nprovides user-interpretable evidence for the complete answering process.\n","authors":["Soumajit Pramanik","Jesujoba Oluwadara Alabi","Rishiraj Saha Roy","Gerhard Weikum"],"pdf_url":"https://arxiv.org/pdf/2108.08614v7.pdf","comment":"35 pages"},{"id":"http://arxiv.org/abs/2307.04644v1","updated":"2023-07-10T15:40:49Z","published":"2023-07-10T15:40:49Z","title":"Fairness and Diversity in Recommender Systems: A Survey","summary":"  Recommender systems are effective tools for mitigating information overload\nand have seen extensive applications across various domains. However, the\nsingle focus on utility goals proves to be inadequate in addressing real-world\nconcerns, leading to increasing attention to fairness-aware and diversity-aware\nrecommender systems. While most existing studies explore fairness and diversity\nindependently, we identify strong connections between these two domains. In\nthis survey, we first discuss each of them individually and then dive into\ntheir connections. Additionally, motivated by the concepts of user-level and\nitem-level fairness, we broaden the understanding of diversity to encompass not\nonly the item level but also the user level. With this expanded perspective on\nuser and item-level diversity, we re-interpret fairness studies from the\nviewpoint of diversity. This fresh perspective enhances our understanding of\nfairness-related work and paves the way for potential future research\ndirections. Papers discussed in this survey along with public code links are\navailable at\nhttps://github.com/YuyingZhao/Awesome-Fairness-and-Diversity-Papers-in-Recommender-Systems .\n","authors":["Yuying Zhao","Yu Wang","Yunchao Liu","Xueqi Cheng","Charu Aggarwal","Tyler Derr"],"pdf_url":"https://arxiv.org/pdf/2307.04644v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.04601v1","updated":"2023-07-10T14:39:43Z","published":"2023-07-10T14:39:43Z","title":"InPars Toolkit: A Unified and Reproducible Synthetic Data Generation\n  Pipeline for Neural Information Retrieval","summary":"  Recent work has explored Large Language Models (LLMs) to overcome the lack of\ntraining data for Information Retrieval (IR) tasks. The generalization\nabilities of these models have enabled the creation of synthetic in-domain data\nby providing instructions and a few examples on a prompt. InPars and\nPromptagator have pioneered this approach and both methods have demonstrated\nthe potential of using LLMs as synthetic data generators for IR tasks. This\nmakes them an attractive solution for IR tasks that suffer from a lack of\nannotated data. However, the reproducibility of these methods was limited,\nbecause InPars' training scripts are based on TPUs -- which are not widely\naccessible -- and because the code for Promptagator was not released and its\nproprietary LLM is not publicly accessible. To fully realize the potential of\nthese methods and make their impact more widespread in the research community,\nthe resources need to be accessible and easy to reproduce by researchers and\npractitioners. Our main contribution is a unified toolkit for end-to-end\nreproducible synthetic data generation research, which includes generation,\nfiltering, training and evaluation. Additionally, we provide an interface to IR\nlibraries widely used by the community and support for GPU. Our toolkit not\nonly reproduces the InPars method and partially reproduces Promptagator, but\nalso provides a plug-and-play functionality allowing the use of different LLMs,\nexploring filtering methods and finetuning various reranker models on the\ngenerated data. We also made available all the synthetic data generated in this\nwork for the 18 different datasets in the BEIR benchmark which took more than\n2,000 GPU hours to be generated as well as the reranker models finetuned on the\nsynthetic data. Code and data are available at\nhttps://github.com/zetaalphavector/InPars\n","authors":["Hugo Abonizio","Luiz Bonifacio","Vitor Jeronymo","Roberto Lotufo","Jakub Zavrel","Rodrigo Nogueira"],"pdf_url":"https://arxiv.org/pdf/2307.04601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.04573v1","updated":"2023-07-10T14:07:28Z","published":"2023-07-10T14:07:28Z","title":"A Semi-Automated Solution Approach Selection Tool for Any Use Case via\n  Scopus and OpenAI: a Case Study for AI/ML in Oncology","summary":"  In today's vast literature landscape, a manual review is very time-consuming.\nTo address this challenge, this paper proposes a semi-automated tool for\nsolution method review and selection. It caters to researchers, practitioners,\nand decision-makers while serving as a benchmark for future work. The tool\ncomprises three modules: (1) paper selection and scoring, using a keyword\nselection scheme to query Scopus API and compute relevancy; (2) solution method\nextraction in papers utilizing OpenAI API; (3) sensitivity analysis and\npost-analyzes. It reveals trends, relevant papers, and methods. AI in the\noncology case study and several use cases are presented with promising results,\ncomparing the tool to manual ground truth.\n","authors":["Deniz Kenan Kılıç","Alex Elkjær Vasegaard","Aurélien Desoeuvres","Peter Nielsen"],"pdf_url":"https://arxiv.org/pdf/2307.04573v1.pdf","comment":"The paper is under review in Expert Systems with Applications,\n  Elsevier"},{"id":"http://arxiv.org/abs/2307.04571v1","updated":"2023-07-10T14:03:34Z","published":"2023-07-10T14:03:34Z","title":"Alleviating Matthew Effect of Offline Reinforcement Learning in\n  Interactive Recommendation","summary":"  Offline reinforcement learning (RL), a technology that offline learns a\npolicy from logged data without the need to interact with online environments,\nhas become a favorable choice in decision-making processes like interactive\nrecommendation. Offline RL faces the value overestimation problem. To address\nit, existing methods employ conservatism, e.g., by constraining the learned\npolicy to be close to behavior policies or punishing the rarely visited\nstate-action pairs. However, when applying such offline RL to recommendation,\nit will cause a severe Matthew effect, i.e., the rich get richer and the poor\nget poorer, by promoting popular items or categories while suppressing the less\npopular ones. It is a notorious issue that needs to be addressed in practical\nrecommender systems.\n  In this paper, we aim to alleviate the Matthew effect in offline RL-based\nrecommendation. Through theoretical analyses, we find that the conservatism of\nexisting methods fails in pursuing users' long-term satisfaction. It inspires\nus to add a penalty term to relax the pessimism on states with high entropy of\nthe logging policy and indirectly penalizes actions leading to less diverse\nstates. This leads to the main technical contribution of the work: Debiased\nmodel-based Offline RL (DORL) method. Experiments show that DORL not only\ncaptures user interests well but also alleviates the Matthew effect. The\nimplementation is available via https://github.com/chongminggao/DORL-codes.\n","authors":["Chongming Gao","Kexin Huang","Jiawei Chen","Yuan Zhang","Biao Li","Peng Jiang","Shiqi Wang","Zhong Zhang","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2307.04571v1.pdf","comment":"SIGIR 2023 Full Paper"},{"id":"http://arxiv.org/abs/2307.05722v1","updated":"2023-07-10T11:29:41Z","published":"2023-07-10T11:29:41Z","title":"Exploring Large Language Model for Graph Data Understanding in Online\n  Job Recommendations","summary":"  Large Language Models (LLMs) have revolutionized natural language processing\ntasks, demonstrating their exceptional capabilities in various domains.\nHowever, their potential for behavior graph understanding in job\nrecommendations remains largely unexplored. This paper focuses on unveiling the\ncapability of large language models in understanding behavior graphs and\nleveraging this understanding to enhance recommendations in online recruitment,\nincluding the promotion of out-of-distribution (OOD) application. We present a\nnovel framework that harnesses the rich contextual information and semantic\nrepresentations provided by large language models to analyze behavior graphs\nand uncover underlying patterns and relationships. Specifically, we propose a\nmeta-path prompt constructor that leverages LLM recommender to understand\nbehavior graphs for the first time and design a corresponding path augmentation\nmodule to alleviate the prompt bias introduced by path-based sequence input. By\nleveraging this capability, our framework enables personalized and accurate job\nrecommendations for individual users. We evaluate the effectiveness of our\napproach on a comprehensive dataset and demonstrate its ability to improve the\nrelevance and quality of recommended quality. This research not only sheds\nlight on the untapped potential of large language models but also provides\nvaluable insights for developing advanced recommendation systems in the\nrecruitment market. The findings contribute to the growing field of natural\nlanguage processing and offer practical implications for enhancing job search\nexperiences.\n","authors":["Likang Wu","Zhaopeng Qiu","Zhi Zheng","Hengshu Zhu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2307.05722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.04386v1","updated":"2023-07-10T07:45:06Z","published":"2023-07-10T07:45:06Z","title":"Counterfactual Explanation for Fairness in Recommendation","summary":"  Fairness-aware recommendation eliminates discrimination issues to build\ntrustworthy recommendation systems.Explaining the causes of unfair\nrecommendations is critical, as it promotes fairness diagnostics, and thus\nsecures users' trust in recommendation models. Existing fairness explanation\nmethods suffer high computation burdens due to the large-scale search space and\nthe greedy nature of the explanation search process. Besides, they perform\nscore-based optimizations with continuous values, which are not applicable to\ndiscrete attributes such as gender and race. In this work, we adopt the novel\nparadigm of counterfactual explanation from causal inference to explore how\nminimal alterations in explanations change model fairness, to abandon the\ngreedy search for explanations. We use real-world attributes from Heterogeneous\nInformation Networks (HINs) to empower counterfactual reasoning on discrete\nattributes. We propose a novel Counterfactual Explanation for Fairness\n(CFairER) that generates attribute-level counterfactual explanations from HINs\nfor recommendation fairness. Our CFairER conducts off-policy reinforcement\nlearning to seek high-quality counterfactual explanations, with an attentive\naction pruning reducing the search space of candidate counterfactuals. The\ncounterfactual explanations help to provide rational and proximate explanations\nfor model fairness, while the attentive action pruning narrows the search space\nof attributes. Extensive experiments demonstrate our proposed model can\ngenerate faithful explanations while maintaining favorable recommendation\nperformance.\n","authors":["Xiangmeng Wang","Qian Li","Dianer Yu","Qing Li","Guandong Xu"],"pdf_url":"https://arxiv.org/pdf/2307.04386v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.04384v1","updated":"2023-07-10T07:43:05Z","published":"2023-07-10T07:43:05Z","title":"Causal Neural Graph Collaborative Filtering","summary":"  Graph collaborative filtering (GCF) has gained considerable attention in\nrecommendation systems by leveraging graph learning techniques to enhance\ncollaborative filtering (CF) models. One classical approach in GCF is to learn\nuser and item embeddings by modeling complex graph relations and utilizing\nthese embeddings for CF models. However, the quality of the embeddings\nsignificantly impacts the recommendation performance of GCF models. In this\npaper, we argue that existing graph learning methods are insufficient in\ngenerating satisfactory embeddings for CF models. This is because they\naggregate neighboring node messages directly, which can result in incorrect\nestimations of user-item correlations. To overcome this limitation, we propose\na novel approach that incorporates causal modeling to explicitly encode the\ncausal effects of neighboring nodes on the target node. This approach enables\nus to identify spurious correlations and uncover the root causes of user\npreferences. We introduce Causal Neural Graph Collaborative Filtering (CNGCF),\nthe first causality-aware graph learning framework for CF. CNGCF integrates\ncausal modeling into the graph representation learning process, explicitly\ncoupling causal effects between node pairs into the core message-passing\nprocess of graph learning. As a result, CNGCF yields causality-aware embeddings\nthat promote robust recommendations. Our extensive experiments demonstrate that\nCNGCF provides precise recommendations that align with user preferences.\nTherefore, our proposed framework can address the limitations of existing GCF\nmodels and offer a more effective solution for recommendation systems.\n","authors":["Xiangmeng Wang","Qian Li","Dianer Yu","Wei Huang","Guandong Xu"],"pdf_url":"https://arxiv.org/pdf/2307.04384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.11936v3","updated":"2023-07-10T04:56:01Z","published":"2022-01-28T05:14:37Z","title":"Consistent Collaborative Filtering via Tensor Decomposition","summary":"  Collaborative filtering is the de facto standard for analyzing users'\nactivities and building recommendation systems for items. In this work we\ndevelop Sliced Anti-symmetric Decomposition (SAD), a new model for\ncollaborative filtering based on implicit feedback. In contrast to traditional\ntechniques where a latent representation of users (user vectors) and items\n(item vectors) are estimated, SAD introduces one additional latent vector to\neach item, using a novel three-way tensor view of user-item interactions. This\nnew vector extends user-item preferences calculated by standard dot products to\ngeneral inner products, producing interactions between items when evaluating\ntheir relative preferences. SAD reduces to state-of-the-art (SOTA)\ncollaborative filtering models when the vector collapses to 1, while in this\npaper we allow its value to be estimated from data. Allowing the values of the\nnew item vector to be different from 1 has profound implications. It suggests\nusers may have nonlinear mental models when evaluating items, allowing the\nexistence of cycles in pairwise comparisons. We demonstrate the efficiency of\nSAD in both simulated and real world datasets containing over 1M user-item\ninteractions. By comparing with seven SOTA collaborative filtering models with\nimplicit feedbacks, SAD produces the most consistent personalized preferences,\nin the meanwhile maintaining top-level of accuracy in personalized\nrecommendations. We release the model and inference algorithms in a Python\nlibrary https://github.com/apple/ml-sad.\n","authors":["Shiwen Zhao","Charles Crissman","Guillermo R Sapiro"],"pdf_url":"https://arxiv.org/pdf/2201.11936v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.04322v1","updated":"2023-07-10T03:31:33Z","published":"2023-07-10T03:31:33Z","title":"Graph Contrastive Learning with Multi-Objective for Personalized Product\n  Retrieval in Taobao Search","summary":"  In e-commerce search, personalized retrieval is a crucial technique for\nimproving user shopping experience. Recent works in this domain have achieved\nsignificant improvements by the representation learning paradigm, e.g.,\nembedding-based retrieval (EBR) and collaborative filtering (CF). EBR methods\ndo not sufficiently exploit the useful collaborative signal and are difficult\nto learn the representations of long-tail item well. Graph-based CF methods\nimprove personalization by modeling collaborative signal within the user click\ngraph. However, existing Graph-based methods ignore user's multiple behaviours,\nsuch as click/purchase and the relevance constraint between user behaviours and\nitems.In this paper, we propose a Graph Contrastive Learning with\nMulti-Objective (GCL-MO) collaborative filtering model, which solves the\nproblems of weak relevance and incomplete personalization in e-commerce search.\nSpecifically, GCL-MO builds a homogeneous graph of items and then optimizes a\nmulti-objective function of personalization and relevance. Moreover, we propose\na modified contrastive loss for multi-objectives graph learning, which avoids\nthe mutual suppression among positive samples and thus improves the\ngeneralization and robustness of long-tail item representations. These learned\nitem embeddings are then used for personalized retrieval by constructing an\nefficient offline-to-online inverted table. GCL-MO outperforms the online\ncollaborative filtering baseline in both offline/online experimental metrics\nand shows a significant improvement in the online A/B testing of Taobao search.\n","authors":["Longbin Li","Chao Zhang","Sen Li","Yun Zhong","Qingwen Liu","Xiaoyi Zeng"],"pdf_url":"https://arxiv.org/pdf/2307.04322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05698v1","updated":"2023-07-10T00:41:08Z","published":"2023-07-10T00:41:08Z","title":"Online Ad Procurement in Non-stationary Autobidding Worlds","summary":"  Today's online advertisers procure digital ad impressions through interacting\nwith autobidding platforms: advertisers convey high level procurement goals via\nsetting levers such as budget, target return-on-investment, max cost per click,\netc.. Then ads platforms subsequently procure impressions on advertisers'\nbehalf, and report final procurement conversions (e.g. click) to advertisers.\nIn practice, advertisers may receive minimal information on platforms'\nprocurement details, and procurement outcomes are subject to non-stationary\nfactors like seasonal patterns, occasional system corruptions, and market\ntrends which make it difficult for advertisers to optimize lever decisions\neffectively. Motivated by this, we present an online learning framework that\nhelps advertisers dynamically optimize ad platform lever decisions while\nsubject to general long-term constraints in a realistic bandit feedback\nenvironment with non-stationary procurement outcomes. In particular, we\nintroduce a primal-dual algorithm for online decision making with\nmulti-dimension decision variables, bandit feedback and long-term uncertain\nconstraints. We show that our algorithm achieves low regret in many worlds when\nprocurement outcomes are generated through procedures that are stochastic,\nadversarial, adversarially corrupted, periodic, and ergodic, respectively,\nwithout having to know which procedure is the ground truth. Finally, we\nemphasize that our proposed algorithm and theoretical results extend beyond the\napplications of online advertising.\n","authors":["Jason Cheuk Nam Liang","Haihao Lu","Baoyu Zhou"],"pdf_url":"https://arxiv.org/pdf/2307.05698v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2307.04749v1","updated":"2023-07-10T17:54:57Z","published":"2023-07-10T17:54:57Z","title":"Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image\n  Alignment with Iterative VQA Feedback","summary":"  The field of text-conditioned image generation has made unparalleled progress\nwith the recent advent of latent diffusion models. While remarkable, as the\ncomplexity of given text input increases, the state-of-the-art diffusion models\nmay still fail in generating images which accurately convey the semantics of\nthe given prompt. Furthermore, it has been observed that such misalignments are\noften left undetected by pretrained multi-modal models such as CLIP. To address\nthese problems, in this paper we explore a simple yet effective decompositional\napproach towards both evaluation and improvement of text-to-image alignment. In\nparticular, we first introduce a Decompositional-Alignment-Score which given a\ncomplex prompt decomposes it into a set of disjoint assertions. The alignment\nof each assertion with generated images is then measured using a VQA model.\nFinally, alignment scores for different assertions are combined aposteriori to\ngive the final text-to-image alignment score. Experimental analysis reveals\nthat the proposed alignment metric shows significantly higher correlation with\nhuman ratings as opposed to traditional CLIP, BLIP scores. Furthermore, we also\nfind that the assertion level alignment scores provide a useful feedback which\ncan then be used in a simple iterative procedure to gradually increase the\nexpression of different assertions in the final image outputs. Human user\nstudies indicate that the proposed approach surpasses previous state-of-the-art\nby 8.7% in overall text-to-image alignment accuracy. Project page for our paper\nis available at https://1jsingh.github.io/divide-evaluate-and-refine\n","authors":["Jaskirat Singh","Liang Zheng"],"pdf_url":"https://arxiv.org/pdf/2307.04749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.03718v3","updated":"2023-07-10T09:27:17Z","published":"2023-06-06T14:28:57Z","title":"Emotion-Conditioned Melody Harmonization with Hierarchical Variational\n  Autoencoder","summary":"  Existing melody harmonization models have made great progress in improving\nthe quality of generated harmonies, but most of them ignored the emotions\nbeneath the music. Meanwhile, the variability of harmonies generated by\nprevious methods is insufficient. To solve these problems, we propose a novel\nLSTM-based Hierarchical Variational Auto-Encoder (LHVAE) to investigate the\ninfluence of emotional conditions on melody harmonization, while improving the\nquality of generated harmonies and capturing the abundant variability of chord\nprogressions. Specifically, LHVAE incorporates latent variables and emotional\nconditions at different levels (piece- and bar-level) to model the global and\nlocal music properties. Additionally, we introduce an attention-based melody\ncontext vector at each step to better learn the correspondence between melodies\nand harmonies. Objective experimental results show that our proposed model\noutperforms other LSTM-based models. Through subjective evaluation, we conclude\nthat only altering the type of chord hardly changes the overall emotion of the\nmusic. The qualitative analysis demonstrates the ability of our model to\ngenerate variable harmonies.\n","authors":["Shulei Ji","Xinyu Yang"],"pdf_url":"https://arxiv.org/pdf/2306.03718v3.pdf","comment":"Accepted by IEEE SMC 2023"}]},"2023-07-09T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2306.01505v2","updated":"2023-07-09T23:27:27Z","published":"2023-06-02T12:52:38Z","title":"Supervised Adversarial Contrastive Learning for Emotion Recognition in\n  Conversations","summary":"  Extracting generalized and robust representations is a major challenge in\nemotion recognition in conversations (ERC). To address this, we propose a\nsupervised adversarial contrastive learning (SACL) framework for learning\nclass-spread structured representations in a supervised manner. SACL applies\ncontrast-aware adversarial training to generate worst-case samples and uses\njoint class-spread contrastive learning to extract structured representations.\nIt can effectively utilize label-level feature consistency and retain\nfine-grained intra-class features. To avoid the negative impact of adversarial\nperturbations on context-dependent data, we design a contextual adversarial\ntraining (CAT) strategy to learn more diverse features from context and enhance\nthe model's context robustness. Under the framework with CAT, we develop a\nsequence-based SACL-LSTM to learn label-consistent and context-robust features\nfor ERC. Experiments on three datasets show that SACL-LSTM achieves\nstate-of-the-art performance on ERC. Extended experiments prove the\neffectiveness of SACL and CAT.\n","authors":["Dou Hu","Yinan Bao","Lingwei Wei","Wei Zhou","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2306.01505v2.pdf","comment":"16 pages, accepted by ACL 2023"},{"id":"http://arxiv.org/abs/2307.04276v1","updated":"2023-07-09T23:02:19Z","published":"2023-07-09T23:02:19Z","title":"Automated Essay Scoring in Argumentative Writing: DeBERTeachingAssistant","summary":"  Automated Essay scoring has been explored as a research and industry problem\nfor over 50 years. It has drawn a lot of attention from the NLP community\nbecause of its clear educational value as a research area that can engender the\ncreation of valuable time-saving tools for educators around the world. Yet,\nthese tools are generally focused on detecting good grammar, spelling mistakes,\nand organization quality but tend to fail at incorporating persuasiveness\nfeatures in their final assessment. The responsibility to give actionable\nfeedback to the student to improve the strength of their arguments is left\nsolely on the teacher's shoulders. In this work, we present a transformer-based\narchitecture capable of achieving above-human accuracy in annotating\nargumentative writing discourse elements for their persuasiveness quality and\nwe expand on planned future work investigating the explainability of our model\nso that actionable feedback can be offered to the student and thus potentially\nenable a partnership between the teacher's advice and the machine's advice.\n","authors":["Yann Hicke","Tonghua Tian","Karan Jha","Choong Hee Kim"],"pdf_url":"https://arxiv.org/pdf/2307.04276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05564v1","updated":"2023-07-09T22:39:37Z","published":"2023-07-09T22:39:37Z","title":"Augmenters at SemEval-2023 Task 1: Enhancing CLIP in Handling\n  Compositionality and Ambiguity for Zero-Shot Visual WSD through Prompt\n  Augmentation and Text-To-Image Diffusion","summary":"  This paper describes our zero-shot approaches for the Visual Word Sense\nDisambiguation (VWSD) Task in English. Our preliminary study shows that the\nsimple approach of matching candidate images with the phrase using CLIP suffers\nfrom the many-to-many nature of image-text pairs. We find that the CLIP text\nencoder may have limited abilities in capturing the compositionality in natural\nlanguage. Conversely, the descriptive focus of the phrase varies from instance\nto instance. We address these issues in our two systems, Augment-CLIP and\nStable Diffusion Sampling (SD Sampling). Augment-CLIP augments the text prompt\nby generating sentences that contain the context phrase with the help of large\nlanguage models (LLMs). We further explore CLIP models in other languages, as\nthe an ambiguous word may be translated into an unambiguous one in the other\nlanguage. SD Sampling uses text-to-image Stable Diffusion to generate multiple\nimages from the given phrase, increasing the likelihood that a subset of images\nmatch the one that paired with the text.\n","authors":["Jie S. Li","Yow-Ting Shiue","Yong-Siang Shih","Jonas Geiping"],"pdf_url":"https://arxiv.org/pdf/2307.05564v1.pdf","comment":"Proceedings of the 17th International Workshop on Semantic Evaluation\n  (SemEval-2023)"},{"id":"http://arxiv.org/abs/2307.04274v1","updated":"2023-07-09T22:32:46Z","published":"2023-07-09T22:32:46Z","title":"Assessing the efficacy of large language models in generating accurate\n  teacher responses","summary":"  (Tack et al., 2023) organized the shared task hosted by the 18th Workshop on\nInnovative Use of NLP for Building Educational Applications on generation of\nteacher language in educational dialogues. Following the structure of the\nshared task, in this study, we attempt to assess the generative abilities of\nlarge language models in providing informative and helpful insights to\nstudents, thereby simulating the role of a knowledgeable teacher. To this end,\nwe present an extensive evaluation of several benchmarking generative models,\nincluding GPT-4 (few-shot, in-context learning), fine-tuned GPT-2, and\nfine-tuned DialoGPT. Additionally, to optimize for pedagogical quality, we\nfine-tuned the Flan-T5 model using reinforcement learning. Our experimental\nfindings on the Teacher-Student Chatroom Corpus subset indicate the efficacy of\nGPT-4 over other fine-tuned models, measured using BERTScore and DialogRPT.\n  We hypothesize that several dataset characteristics, including sampling,\nrepresentativeness, and dialog completeness, pose significant challenges to\nfine-tuning, thus contributing to the poor generalizability of the fine-tuned\nmodels. Finally, we note the need for these generative models to be evaluated\nwith a metric that relies not only on dialog coherence and matched language\nmodeling distribution but also on the model's ability to showcase pedagogical\nskills.\n","authors":["Yann Hicke","Abhishek Masand","Wentao Guo","Tushaar Gangavarapu"],"pdf_url":"https://arxiv.org/pdf/2307.04274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.15745v3","updated":"2023-07-09T21:15:36Z","published":"2023-06-27T18:56:28Z","title":"Identity Construction in a Misogynist Incels Forum","summary":"  Online communities of involuntary celibates (incels) are a prominent source\nof misogynist hate speech. In this paper, we use quantitative text and network\nanalysis approaches to examine how identity groups are discussed on\nincels-dot-is, the largest black-pilled incels forum. We find that this\ncommunity produces a wide range of novel identity terms and, while terms for\nwomen are most common, mentions of other minoritized identities are increasing.\nAn analysis of the associations made with identity groups suggests an\nessentialist ideology where physical appearance, as well as gender and racial\nhierarchies, determine human value. We discuss implications for research into\nautomated misogynist hate speech detection.\n","authors":["Michael Miller Yoder","Chloe Perry","David West Brown","Kathleen M. Carley","Meredith L. Pruden"],"pdf_url":"https://arxiv.org/pdf/2306.15745v3.pdf","comment":"Workshop on Online Abuse and Harms (WOAH) 2023; Minor edits to author\n  names and abstracts in most recent version"},{"id":"http://arxiv.org/abs/2307.04251v1","updated":"2023-07-09T19:28:46Z","published":"2023-07-09T19:28:46Z","title":"ChatGPT in the Age of Generative AI and Large Language Models: A Concise\n  Survey","summary":"  ChatGPT is a large language model (LLM) created by OpenAI that has been\ncarefully trained on a large amount of data. It has revolutionized the field of\nnatural language processing (NLP) and has pushed the boundaries of LLM\ncapabilities. ChatGPT has played a pivotal role in enabling widespread public\ninteraction with generative artificial intelligence (GAI) on a large scale. It\nhas also sparked research interest in developing similar technologies and\ninvestigating their applications and implications. In this paper, our primary\ngoal is to provide a concise survey on the current lines of research on ChatGPT\nand its evolution. We considered both the glass box and black box views of\nChatGPT, encompassing the components and foundational elements of the\ntechnology, as well as its applications, impacts, and implications. The glass\nbox approach focuses on understanding the inner workings of the technology, and\nthe black box approach embraces it as a complex system, and thus examines its\ninputs, outputs, and effects. This paves the way for a comprehensive\nexploration of the technology and provides a road map for further research and\nexperimentation. We also lay out essential foundational literature on LLMs and\nGAI in general and their connection with ChatGPT. This overview sheds light on\nexisting and missing research lines in the emerging field of LLMs, benefiting\nboth public users and developers. Furthermore, the paper delves into the broad\nspectrum of applications and significant concerns in fields such as education,\nresearch, healthcare, finance, etc.\n","authors":["Salman Mohamadi","Ghulam Mujtaba","Ngan Le","Gianfranco Doretto","Donald A. Adjeroh"],"pdf_url":"https://arxiv.org/pdf/2307.04251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.10264v5","updated":"2023-07-09T18:27:27Z","published":"2022-08-18T17:54:49Z","title":"Using Large Language Models to Simulate Multiple Humans and Replicate\n  Human Subject Studies","summary":"  We introduce a new type of test, called a Turing Experiment (TE), for\nevaluating to what extent a given language model, such as GPT models, can\nsimulate different aspects of human behavior. A TE can also reveal consistent\ndistortions in a language model's simulation of a specific human behavior.\nUnlike the Turing Test, which involves simulating a single arbitrary\nindividual, a TE requires simulating a representative sample of participants in\nhuman subject research. We carry out TEs that attempt to replicate\nwell-established findings from prior studies. We design a methodology for\nsimulating TEs and illustrate its use to compare how well different language\nmodels are able to reproduce classic economic, psycholinguistic, and social\npsychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock\nExperiment, and Wisdom of Crowds. In the first three TEs, the existing findings\nwere replicated using recent models, while the last TE reveals a\n\"hyper-accuracy distortion\" present in some language models (including ChatGPT\nand GPT-4), which could affect downstream applications in education and the\narts.\n","authors":["Gati Aher","Rosa I. Arriaga","Adam Tauman Kalai"],"pdf_url":"https://arxiv.org/pdf/2208.10264v5.pdf","comment":"Accepted for oral presentation at International Conference on Machine\n  Learning (ICML) 2023"},{"id":"http://arxiv.org/abs/2304.14108v3","updated":"2023-07-09T18:16:31Z","published":"2023-04-27T11:37:18Z","title":"DataComp: In search of the next generation of multimodal datasets","summary":"  Multimodal datasets are a critical component in recent breakthroughs such as\nStable Diffusion and GPT-4, yet their design does not receive the same research\nattention as model architectures or training algorithms. To address this\nshortcoming in the ML ecosystem, we introduce DataComp, a testbed for dataset\nexperiments centered around a new candidate pool of 12.8 billion image-text\npairs from Common Crawl. Participants in our benchmark design new filtering\ntechniques or curate new data sources and then evaluate their new dataset by\nrunning our standardized CLIP training code and testing the resulting model on\n38 downstream test sets. Our benchmark consists of multiple compute scales\nspanning four orders of magnitude, which enables the study of scaling trends\nand makes the benchmark accessible to researchers with varying resources. Our\nbaseline experiments show that the DataComp workflow leads to better training\nsets. In particular, our best baseline, DataComp-1B, enables training a CLIP\nViT-L/14 from scratch to 79.2% zero-shot accuracy on ImageNet, outperforming\nOpenAI's CLIP ViT-L/14 by 3.7 percentage points while using the same training\nprocedure and compute. We release DataComp and all accompanying code at\nwww.datacomp.ai.\n","authors":["Samir Yitzhak Gadre","Gabriel Ilharco","Alex Fang","Jonathan Hayase","Georgios Smyrnis","Thao Nguyen","Ryan Marten","Mitchell Wortsman","Dhruba Ghosh","Jieyu Zhang","Eyal Orgad","Rahim Entezari","Giannis Daras","Sarah Pratt","Vivek Ramanujan","Yonatan Bitton","Kalyani Marathe","Stephen Mussmann","Richard Vencu","Mehdi Cherti","Ranjay Krishna","Pang Wei Koh","Olga Saukh","Alexander Ratner","Shuran Song","Hannaneh Hajishirzi","Ali Farhadi","Romain Beaumont","Sewoong Oh","Alex Dimakis","Jenia Jitsev","Yair Carmon","Vaishaal Shankar","Ludwig Schmidt"],"pdf_url":"https://arxiv.org/pdf/2304.14108v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.00008v3","updated":"2023-07-09T18:00:02Z","published":"2023-03-27T18:00:01Z","title":"On the Creativity of Large Language Models","summary":"  Large Language Models (LLMs) are revolutionizing several areas of Artificial\nIntelligence. One of the most remarkable applications is creative writing,\ne.g., poetry or storytelling: the generated outputs are often of astonishing\nquality. However, a natural question arises: can LLMs be really considered\ncreative? In this article we firstly analyze the development of LLMs under the\nlens of creativity theories, investigating the key open questions and\nchallenges. In particular, we focus our discussion around the dimensions of\nvalue, novelty and surprise as proposed by Margaret Boden in her work. Then, we\nconsider different classic perspectives, namely product, process, press and\nperson. We discuss a set of ``easy'' and ``hard'' problems in machine\ncreativity, presenting them in relation to LLMs. Finally, we examine the\nsocietal impact of these technologies with a particular focus on the creative\nindustries, analyzing the opportunities offered by them, the challenges arising\nby them and the potential associated risks, from both legal and ethical points\nof view.\n","authors":["Giorgio Franceschelli","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2304.00008v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.03734v2","updated":"2023-07-09T17:17:39Z","published":"2023-06-06T14:52:15Z","title":"A Cross-Linguistic Pressure for Uniform Information Density in Word\n  Order","summary":"  While natural languages differ widely in both canonical word order and word\norder flexibility, their word orders still follow shared cross-linguistic\nstatistical patterns, often attributed to functional pressures. In the effort\nto identify these pressures, prior work has compared real and counterfactual\nword orders. Yet one functional pressure has been overlooked in such\ninvestigations: the uniform information density (UID) hypothesis, which holds\nthat information should be spread evenly throughout an utterance. Here, we ask\nwhether a pressure for UID may have influenced word order patterns\ncross-linguistically. To this end, we use computational models to test whether\nreal orders lead to greater information uniformity than counterfactual orders.\nIn our empirical study of 10 typologically diverse languages, we find that: (i)\namong SVO languages, real word orders consistently have greater uniformity than\nreverse word orders, and (ii) only linguistically implausible counterfactual\norders consistently exceed the uniformity of real orders. These findings are\ncompatible with a pressure for information uniformity in the development and\nusage of natural languages.\n","authors":["Thomas Hikaru Clark","Clara Meister","Tiago Pimentel","Michael Hahn","Ryan Cotterell","Richard Futrell","Roger Levy"],"pdf_url":"https://arxiv.org/pdf/2306.03734v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05560v1","updated":"2023-07-09T16:19:35Z","published":"2023-07-09T16:19:35Z","title":"Automatic Coding at Scale: Design and Deployment of a Nationwide System\n  for Normalizing Referrals in the Chilean Public Healthcare System","summary":"  The disease coding task involves assigning a unique identifier from a\ncontrolled vocabulary to each disease mentioned in a clinical document. This\ntask is relevant since it allows information extraction from unstructured data\nto perform, for example, epidemiological studies about the incidence and\nprevalence of diseases in a determined context. However, the manual coding\nprocess is subject to errors as it requires medical personnel to be competent\nin coding rules and terminology. In addition, this process consumes a lot of\ntime and energy, which could be allocated to more clinically relevant tasks.\nThese difficulties can be addressed by developing computational systems that\nautomatically assign codes to diseases. In this way, we propose a two-step\nsystem for automatically coding diseases in referrals from the Chilean public\nhealthcare system. Specifically, our model uses a state-of-the-art NER model\nfor recognizing disease mentions and a search engine system based on\nElasticsearch for assigning the most relevant codes associated with these\ndisease mentions. The system's performance was evaluated on referrals manually\ncoded by clinical experts. Our system obtained a MAP score of 0.63 for the\nsubcategory level and 0.83 for the category level, close to the best-performing\nmodels in the literature. This system could be a support tool for health\nprofessionals, optimizing the coding and management process. Finally, to\nguarantee reproducibility, we publicly release the code of our models and\nexperiments.\n","authors":["Fabián Villena","Matías Rojas","Felipe Arias","Jorge Pacheco","Paulina Vera","Jocelyn Dunstan"],"pdf_url":"https://arxiv.org/pdf/2307.05560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04526v2","updated":"2023-07-09T16:13:25Z","published":"2023-03-08T11:51:26Z","title":"Student's t-Distribution: On Measuring the Inter-Rater Reliability When\n  the Observations are Scarce","summary":"  In natural language processing (NLP) we always rely on human judgement as the\ngolden quality evaluation method. However, there has been an ongoing debate on\nhow to better evaluate inter-rater reliability (IRR) levels for certain\nevaluation tasks, such as translation quality evaluation (TQE), especially when\nthe data samples (observations) are very scarce. In this work, we first\nintroduce the study on how to estimate the confidence interval for the\nmeasurement value when only one data (evaluation) point is available. Then,\nthis leads to our example with two human-generated observational scores, for\nwhich, we introduce ``Student's \\textit{t}-Distribution'' method and explain\nhow to use it to measure the IRR score using only these two data points, as\nwell as the confidence intervals (CIs) of the quality evaluation. We give\nquantitative analysis on how the evaluation confidence can be greatly improved\nby introducing more observations, even if only one extra observation. We\nencourage researchers to report their IRR scores in all possible means, e.g.\nusing Student's \\textit{t}-Distribution method whenever possible; thus making\nthe NLP evaluation more meaningful, transparent, and trustworthy. This\n\\textit{t}-Distribution method can be also used outside of NLP fields to\nmeasure IRR level for trustworthy evaluation of experimental investigations,\nwhenever the observational data is scarce.\n  Keywords: Inter-Rater Reliability (IRR); Scarce Observations; Confidence\nIntervals (CIs); Natural Language Processing (NLP); Translation Quality\nEvaluation (TQE); Student's \\textit{t}-Distribution\n","authors":["Serge Gladkoff","Lifeng Han","Goran Nenadic"],"pdf_url":"https://arxiv.org/pdf/2303.04526v2.pdf","comment":"Accepted to RANLP2023: Recent Advances in Natural Language\n  Processing, Varna, Bulgaria. 30 Aug - 8 Sep\n  \\url{https://ranlp.org/ranlp2023/}"},{"id":"http://arxiv.org/abs/2307.01370v2","updated":"2023-07-09T15:21:22Z","published":"2023-07-03T21:54:28Z","title":"Multilingual Language Models are not Multicultural: A Case Study in\n  Emotion","summary":"  Emotions are experienced and expressed differently across the world. In order\nto use Large Language Models (LMs) for multilingual tasks that require\nemotional sensitivity, LMs must reflect this cultural variation in emotion. In\nthis study, we investigate whether the widely-used multilingual LMs in 2023\nreflect differences in emotional expressions across cultures and languages. We\nfind that embeddings obtained from LMs (e.g., XLM-RoBERTa) are Anglocentric,\nand generative LMs (e.g., ChatGPT) reflect Western norms, even when responding\nto prompts in other languages. Our results show that multilingual LMs do not\nsuccessfully learn the culturally appropriate nuances of emotion and we\nhighlight possible research directions towards correcting this.\n","authors":["Shreya Havaldar","Sunny Rai","Bhumika Singhal","Langchen Liu","Sharath Chandra Guntuku","Lyle Ungar"],"pdf_url":"https://arxiv.org/pdf/2307.01370v2.pdf","comment":"Accepted to WASSA at ACL 2023"},{"id":"http://arxiv.org/abs/2307.04192v1","updated":"2023-07-09T14:54:30Z","published":"2023-07-09T14:54:30Z","title":"SAS Video-QA: Self-Adaptive Sampling for Efficient Video\n  Question-Answering","summary":"  Video question--answering is a fundamental task in the field of video\nunderstanding. Although current vision--language models (VLMs) equipped with\nVideo Transformers have enabled temporal modeling and yielded superior results,\nthey are at the cost of huge computational power and thus too expensive to\ndeploy in real-time application scenarios. An economical workaround only\nsamples a small portion of frames to represent the main content of that video\nand tune an image--text model on these sampled frames. Recent video\nunderstanding models usually randomly sample a set of frames or clips,\nregardless of internal correlations between their visual contents, nor their\nrelevance to the problem. We argue that such kinds of aimless sampling may omit\nthe key frames from which the correct answer can be deduced, and the situation\ngets worse when the sampling sparsity increases, which always happens as the\nvideo lengths increase. To mitigate this issue, we propose two frame sampling\nstrategies, namely the most domain frames (MDF) and most implied frames (MIF),\nto maximally preserve those frames that are most likely vital to the given\nquestions. MDF passively minimizes the risk of key frame omission in a\nbootstrap manner, while MIS actively searches key frames customized for each\nvideo--question pair with the assistance of auxiliary models. The experimental\nresults on three public datasets from three advanced VLMs (CLIP, GIT and\nAll-in-one) demonstrate that our proposed strategies can boost the performance\nfor image--text pretrained models. The source codes pertaining to the method\nproposed in this paper are publicly available at\nhttps://github.com/declare-lab/sas-vqa.\n","authors":["Wei Han","Hui Chen","Min-Yen Kan","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2307.04192v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2205.02364v3","updated":"2023-07-09T14:06:02Z","published":"2022-05-04T23:53:23Z","title":"KenSwQuAD -- A Question Answering Dataset for Swahili Low Resource\n  Language","summary":"  The need for Question Answering datasets in low resource languages is the\nmotivation of this research, leading to the development of Kencorpus Swahili\nQuestion Answering Dataset, KenSwQuAD. This dataset is annotated from raw story\ntexts of Swahili low resource language, which is a predominantly spoken in\nEastern African and in other parts of the world. Question Answering (QA)\ndatasets are important for machine comprehension of natural language for tasks\nsuch as internet search and dialog systems. Machine learning systems need\ntraining data such as the gold standard Question Answering set developed in\nthis research. The research engaged annotators to formulate QA pairs from\nSwahili texts collected by the Kencorpus project, a Kenyan languages corpus.\nThe project annotated 1,445 texts from the total 2,585 texts with at least 5 QA\npairs each, resulting into a final dataset of 7,526 QA pairs. A quality\nassurance set of 12.5% of the annotated texts confirmed that the QA pairs were\nall correctly annotated. A proof of concept on applying the set to the QA task\nconfirmed that the dataset can be usable for such tasks. KenSwQuAD has also\ncontributed to resourcing of the Swahili language.\n","authors":["Barack W. Wanjawa","Lilian D. A. Wanzare","Florence Indede","Owen McOnyango","Lawrence Muchemi","Edward Ombui"],"pdf_url":"https://arxiv.org/pdf/2205.02364v3.pdf","comment":"17 pages, 1 figure, 10 tables"},{"id":"http://arxiv.org/abs/2211.05750v2","updated":"2023-07-09T13:40:30Z","published":"2022-11-10T18:31:56Z","title":"Nano: Nested Human-in-the-Loop Reward Learning for Few-shot Language\n  Model Control","summary":"  Pretrained language models have demonstrated extraordinary capabilities in\nlanguage generation. However, real-world tasks often require controlling the\ndistribution of generated text in order to mitigate bias, promote fairness, and\nachieve personalization. Existing techniques for controlling the distribution\nof generated text only work with quantified distributions, which require\npre-defined categories, proportions of the distribution, or an existing corpus\nfollowing the desired distributions. However, many important distributions,\nsuch as personal preferences, are unquantified. In this work, we tackle the\nproblem of generating text following arbitrary distributions (quantified and\nunquantified) by proposing Nano, a few-shot human-in-the-loop training\nalgorithm that continuously learns from human feedback. Nano achieves\nstate-of-the-art results on single topic/attribute as well as quantified\ndistribution control compared to previous works. We also show that Nano is able\nto learn unquantified distributions, achieves personalization, and captures\ndifferences between different individuals' personal preferences with high\nsample efficiency.\n","authors":["Xiang Fan","Yiwei Lyu","Paul Pu Liang","Ruslan Salakhutdinov","Louis-Philippe Morency"],"pdf_url":"https://arxiv.org/pdf/2211.05750v2.pdf","comment":"Accepted to ACL Findings 2023"},{"id":"http://arxiv.org/abs/2307.04172v1","updated":"2023-07-09T13:38:25Z","published":"2023-07-09T13:38:25Z","title":"Can Generative Large Language Models Perform ASR Error Correction?","summary":"  ASR error correction continues to serve as an important part of\npost-processing for speech recognition systems. Traditionally, these models are\ntrained with supervised training using the decoding results of the underlying\nASR system and the reference text. This approach is computationally intensive\nand the model needs to be re-trained when switching the underlying ASR model.\nRecent years have seen the development of large language models and their\nability to perform natural language processing tasks in a zero-shot manner. In\nthis paper, we take ChatGPT as an example to examine its ability to perform ASR\nerror correction in the zero-shot or 1-shot settings. We use the ASR N-best\nlist as model input and propose unconstrained error correction and N-best\nconstrained error correction methods. Results on a Conformer-Transducer model\nand the pre-trained Whisper model show that we can largely improve the ASR\nsystem performance with error correction using the powerful ChatGPT model.\n","authors":["Rao Ma","Mengjie Qian","Potsawee Manakul","Mark Gales","Kate Knill"],"pdf_url":"https://arxiv.org/pdf/2307.04172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.04167v1","updated":"2023-07-09T13:24:58Z","published":"2023-07-09T13:24:58Z","title":"Dream Content Discovery from Reddit with an Unsupervised Mixed-Method\n  Approach","summary":"  Dreaming is a fundamental but not fully understood part of human experience\nthat can shed light on our thought patterns. Traditional dream analysis\npractices, while popular and aided by over 130 unique scales and rating\nsystems, have limitations. Mostly based on retrospective surveys or lab\nstudies, they struggle to be applied on a large scale or to show the importance\nand connections between different dream themes. To overcome these issues, we\ndeveloped a new, data-driven mixed-method approach for identifying topics in\nfree-form dream reports through natural language processing. We tested this\nmethod on 44,213 dream reports from Reddit's r/Dreams subreddit, where we found\n217 topics, grouped into 22 larger themes: the most extensive collection of\ndream topics to date. We validated our topics by comparing it to the\nwidely-used Hall and van de Castle scale. Going beyond traditional scales, our\nmethod can find unique patterns in different dream types (like nightmares or\nrecurring dreams), understand topic importance and connections, and observe\nchanges in collective dream experiences over time and around major events, like\nthe COVID-19 pandemic and the recent Russo-Ukrainian war. We envision that the\napplications of our method will provide valuable insights into the intricate\nnature of dreaming.\n","authors":["Anubhab Das","Sanja Šćepanović","Luca Maria Aiello","Remington Mallett","Deirdre Barrett","Daniele Quercia"],"pdf_url":"https://arxiv.org/pdf/2307.04167v1.pdf","comment":"20 pages, 6 figures, 4 tables, 4 pages of supplementary information"},{"id":"http://arxiv.org/abs/2307.05553v1","updated":"2023-07-09T11:04:13Z","published":"2023-07-09T11:04:13Z","title":"Review of feedback in Automated Essay Scoring","summary":"  The first automated essay scoring system was developed 50 years ago.\nAutomated essay scoring systems are developing into systems with richer\nfunctions than the previous simple scoring systems. Its purpose is not only to\nscore essays but also as a learning tool to improve the writing skill of users.\nFeedback is the most important aspect of making an automated essay scoring\nsystem useful in real life. The importance of feedback was already emphasized\nin the first AES system. This paper reviews research on feedback including\ndifferent feedback types and essay traits on automated essay scoring. We also\nreviewed the latest case studies of the automated essay scoring system that\nprovides feedback.\n","authors":["You-Jin Jong","Yong-Jin Kim","Ok-Chol Ri"],"pdf_url":"https://arxiv.org/pdf/2307.05553v1.pdf","comment":"21 pages, 5 figures, 5 tables"},{"id":"http://arxiv.org/abs/2307.04123v1","updated":"2023-07-09T08:32:14Z","published":"2023-07-09T08:32:14Z","title":"Towards cross-language prosody transfer for dialog","summary":"  Speech-to-speech translation systems today do not adequately support use for\ndialog purposes. In particular, nuances of speaker intent and stance can be\nlost due to improper prosody transfer. We present an exploration of what needs\nto be done to overcome this. First, we developed a data collection protocol in\nwhich bilingual speakers re-enact utterances from an earlier conversation in\ntheir other language, and used this to collect an English-Spanish corpus, so\nfar comprising 1871 matched utterance pairs. Second, we developed a simple\nprosodic dissimilarity metric based on Euclidean distance over a broad set of\nprosodic features. We then used these to investigate cross-language prosodic\ndifferences, measure the likely utility of three simple baseline models, and\nidentify phenomena which will require more powerful modeling. Our findings\nshould inform future research on cross-language prosody and the design of\nspeech-to-speech translation systems capable of effective prosody transfer.\n","authors":["Jonathan E. Avila","Nigel G. Ward"],"pdf_url":"https://arxiv.org/pdf/2307.04123v1.pdf","comment":"Accepted to Interspeech 2023"},{"id":"http://arxiv.org/abs/2306.01657v2","updated":"2023-07-09T08:09:49Z","published":"2023-06-02T16:26:21Z","title":"DiffusEmp: A Diffusion Model-Based Framework with Multi-Grained Control\n  for Empathetic Response Generation","summary":"  Empathy is a crucial factor in open-domain conversations, which naturally\nshows one's caring and understanding to others. Though several methods have\nbeen proposed to generate empathetic responses, existing works often lead to\nmonotonous empathy that refers to generic and safe expressions. In this paper,\nwe propose to use explicit control to guide the empathy expression and design a\nframework DiffusEmp based on conditional diffusion language model to unify the\nutilization of dialogue context and attribute-oriented control signals.\nSpecifically, communication mechanism, intent, and semantic frame are imported\nas multi-grained signals that control the empathy realization from coarse to\nfine levels. We then design a specific masking strategy to reflect the\nrelationship between multi-grained signals and response tokens, and integrate\nit into the diffusion model to influence the generative process. Experimental\nresults on a benchmark dataset EmpatheticDialogue show that our framework\noutperforms competitive baselines in terms of controllability, informativeness,\nand diversity without the loss of context-relatedness.\n","authors":["Guanqun Bi","Lei Shen","Yanan Cao","Meng Chen","Yuqiang Xie","Zheng Lin","Xiaodong He"],"pdf_url":"https://arxiv.org/pdf/2306.01657v2.pdf","comment":"accepted by ACL 2023 main conference (Oral)"},{"id":"http://arxiv.org/abs/2307.04114v1","updated":"2023-07-09T08:07:43Z","published":"2023-07-09T08:07:43Z","title":"FILM: How can Few-Shot Image Classification Benefit from Pre-Trained\n  Language Models?","summary":"  Few-shot learning aims to train models that can be generalized to novel\nclasses with only a few samples. Recently, a line of works are proposed to\nenhance few-shot learning with accessible semantic information from class\nnames. However, these works focus on improving existing modules such as visual\nprototypes and feature extractors of the standard few-shot learning framework.\nThis limits the full potential use of semantic information. In this paper, we\npropose a novel few-shot learning framework that uses pre-trained language\nmodels based on contrastive learning. To address the challenge of alignment\nbetween visual features and textual embeddings obtained from text-based\npre-trained language model, we carefully design the textual branch of our\nframework and introduce a metric module to generalize the cosine similarity.\nFor better transferability, we let the metric module adapt to different\nfew-shot tasks and adopt MAML to train the model via bi-level optimization.\nMoreover, we conduct extensive experiments on multiple benchmarks to\ndemonstrate the effectiveness of our method.\n","authors":["Zihao Jiang","Yunkai Dang","Dong Pang","Huishuai Zhang","Weiran Huang"],"pdf_url":"https://arxiv.org/pdf/2307.04114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.01385v2","updated":"2023-07-09T06:31:46Z","published":"2023-06-02T09:11:06Z","title":"Task-Agnostic Structured Pruning of Speech Representation Models","summary":"  Self-supervised pre-trained models such as Wav2vec2, Hubert, and WavLM have\nbeen shown to significantly improve many speech tasks. However, their large\nmemory and strong computational requirements hinder their industrial\napplicability. Structured pruning is a hardware-friendly model compression\ntechnique but usually results in a larger loss of accuracy. In this paper, we\npropose a fine-grained attention head pruning method to compensate for the\nperformance degradation. In addition, we also introduce the straight through\nestimator into the L0 regularization to further accelerate the pruned model.\nExperiments on the SUPERB benchmark show that our model can achieve comparable\nperformance to the dense model in multiple tasks and outperforms the Wav2vec\n2.0 base model on average, with 72% fewer parameters and 2 times faster\ninference speed.\n","authors":["Haoyu Wang","Siyuan Wang","Wei-Qiang Zhang","Hongbin Suo","Yulong Wan"],"pdf_url":"https://arxiv.org/pdf/2306.01385v2.pdf","comment":"Accepted by INTERSPEECH 2023"},{"id":"http://arxiv.org/abs/2307.04096v1","updated":"2023-07-09T04:52:31Z","published":"2023-07-09T04:52:31Z","title":"Optimal Transport Posterior Alignment for Cross-lingual Semantic Parsing","summary":"  Cross-lingual semantic parsing transfers parsing capability from a\nhigh-resource language (e.g., English) to low-resource languages with scarce\ntraining data. Previous work has primarily considered silver-standard data\naugmentation or zero-shot methods, however, exploiting few-shot gold data is\ncomparatively unexplored. We propose a new approach to cross-lingual semantic\nparsing by explicitly minimizing cross-lingual divergence between probabilistic\nlatent variables using Optimal Transport. We demonstrate how this direct\nguidance improves parsing from natural languages using fewer examples and less\ntraining. We evaluate our method on two datasets, MTOP and MultiATIS++SQL,\nestablishing state-of-the-art results under a few-shot cross-lingual regime.\nAblation studies further reveal that our method improves performance even\nwithout parallel input translations. In addition, we show that our model better\ncaptures cross-lingual structure in the latent space to improve semantic\nrepresentation similarity.\n","authors":["Tom Sherborne","Tom Hosking","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2307.04096v1.pdf","comment":"Accepted to TACL 2023. Pre-MIT Press publication. 17 pages, 3\n  figures, 6 tables"},{"id":"http://arxiv.org/abs/2306.07848v3","updated":"2023-07-09T04:21:54Z","published":"2023-06-13T15:28:10Z","title":"GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio\n  Pretraining for Speech Emotion Recognition","summary":"  Contrastive learning based pretraining methods have recently exhibited\nimpressive success in diverse fields. In this paper, we propose GEmo-CLAP, a\nkind of efficient gender-attribute-enhanced contrastive language-audio\npretraining (CLAP) model for speech emotion recognition. To be specific, we\nfirst build an effective emotion CLAP model Emo-CLAP for emotion recognition,\nutilizing various self-supervised learning based pre-trained models. Then,\nconsidering the importance of the gender attribute in speech emotion modeling,\ntwo GEmo-CLAP approaches are further proposed to integrate the emotion and\ngender information of speech signals, forming more reasonable objectives.\nExtensive experiments on the IEMOCAP corpus demonstrate that our proposed two\nGEmo-CLAP approaches consistently outperform the baseline Emo-CLAP with\ndifferent pre-trained models, while also achieving superior recognition\nperformance compared with other state-of-the-art methods.\n","authors":["Yu Pan","Yanni Hu","Yuguang Yang","Jixun Yao","Wen Fei","Lei Ma","Heng Lu"],"pdf_url":"https://arxiv.org/pdf/2306.07848v3.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2307.04090v1","updated":"2023-07-09T04:19:19Z","published":"2023-07-09T04:19:19Z","title":"DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge\n  Graphs","summary":"  Recent work within the Argument Mining community has shown the applicability\nof Natural Language Processing systems for solving problems found within\ncompetitive debate. One of the most important tasks within competitive debate\nis for debaters to create high quality debate cases. We show that effective\ndebate cases can be constructed using constrained shortest path traversals on\nArgumentative Semantic Knowledge Graphs. We study this potential in the context\nof a type of American Competitive Debate, called Policy Debate, which already\nhas a large scale dataset targeting it called DebateSum. We significantly\nimprove upon DebateSum by introducing 53180 new examples, as well as further\nuseful metadata for every example, to the dataset. We leverage the txtai\nsemantic search and knowledge graph toolchain to produce and contribute 9\nsemantic knowledge graphs built on this dataset. We create a unique method for\nevaluating which knowledge graphs are better in the context of producing policy\ndebate cases. A demo which automatically generates debate cases, along with all\nother code and the Knowledge Graphs, are open-sourced and made available to the\npublic here: https://github.com/Hellisotherpeople/DebateKG\n","authors":["Allen Roush"],"pdf_url":"https://arxiv.org/pdf/2307.04090v1.pdf","comment":"8 pages, knife-edge reject from EACL 2023 and workshops, System\n  Demonstration paper"},{"id":"http://arxiv.org/abs/2208.01307v2","updated":"2023-07-09T02:06:43Z","published":"2022-08-02T08:27:00Z","title":"Multilingual Coreference Resolution in Multiparty Dialogue","summary":"  Existing multiparty dialogue datasets for entity coreference resolution are\nnascent, and many challenges are still unaddressed. We create a large-scale\ndataset, Multilingual Multiparty Coref (MMC), for this task based on TV\ntranscripts. Due to the availability of gold-quality subtitles in multiple\nlanguages, we propose reusing the annotations to create silver coreference\nresolution data in other languages (Chinese and Farsi) via annotation\nprojection. On the gold (English) data, off-the-shelf models perform relatively\npoorly on MMC, suggesting that MMC has broader coverage of multiparty\ncoreference than prior datasets. On the silver data, we find success both using\nit for data augmentation and training from scratch, which effectively simulates\nthe zero-shot cross-lingual setting.\n","authors":["Boyuan Zheng","Patrick Xia","Mahsa Yarmohammadi","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2208.01307v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05696v1","updated":"2023-07-09T01:19:08Z","published":"2023-07-09T01:19:08Z","title":"A Personalized Reinforcement Learning Summarization Service for Learning\n  Structure from Unstructured Data","summary":"  The exponential growth of textual data has created a crucial need for tools\nthat assist users in extracting meaningful insights. Traditional document\nsummarization approaches often fail to meet individual user requirements and\nlack structure for efficient information processing. To address these\nlimitations, we propose Summation, a hierarchical personalized concept-based\nsummarization approach. It synthesizes documents into a concise hierarchical\nconcept map and actively engages users by learning and adapting to their\npreferences. Using a Reinforcement Learning algorithm, Summation generates\npersonalized summaries for unseen documents on specific topics. This framework\nenhances comprehension, enables effective navigation, and empowers users to\nextract meaningful insights from large document collections aligned with their\nunique requirements.\n","authors":["Samira Ghodratnama","Amin Beheshti","Mehrdad Zakershahrak"],"pdf_url":"https://arxiv.org/pdf/2307.05696v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2108.09443"},{"id":"http://arxiv.org/abs/2306.11270v2","updated":"2023-07-09T00:39:30Z","published":"2023-06-20T03:48:51Z","title":"Evaluating the Zero-shot Robustness of Instruction-tuned Language Models","summary":"  Instruction fine-tuning has recently emerged as a promising approach for\nimproving the zero-shot capabilities of Large Language Models (LLMs) on new\ntasks. This technique has shown particular strength in improving the\nperformance of modestly sized LLMs, sometimes inducing performance competitive\nwith much larger model variants. In this paper we ask two questions: (1) How\nsensitive are instruction-tuned models to the particular phrasings of\ninstructions, and, (2) How can we make them more robust to such natural\nlanguage variation? To answer the former, we collect a set of 319 instructions\nmanually written by NLP practitioners for over 80 unique tasks included in\nwidely used benchmarks, and we evaluate the variance and average performance of\nthese instructions as compared to instruction phrasings observed during\ninstruction fine-tuning. We find that using novel (unobserved) but appropriate\ninstruction phrasings consistently degrades model performance, sometimes\nsubstantially so. Further, such natural instructions yield a wide variance in\ndownstream performance, despite their semantic equivalence. Put another way,\ninstruction-tuned models are not especially robust to instruction re-phrasings.\nWe propose a simple method to mitigate this issue by introducing ``soft\nprompt'' embedding parameters and optimizing these to maximize the similarity\nbetween representations of semantically equivalent instructions. We show that\nthis method consistently improves the robustness of instruction-tuned models.\n","authors":["Jiuding Sun","Chantal Shaib","Byron C. Wallace"],"pdf_url":"https://arxiv.org/pdf/2306.11270v2.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2306.13374v2","updated":"2023-07-09T11:01:01Z","published":"2023-06-23T08:53:41Z","title":"Human Activity Behavioural Pattern Recognition in Smarthome with\n  Long-hour Data Collection","summary":"  The research on human activity recognition has provided novel solutions to\nmany applications like healthcare, sports, and user profiling. Considering the\ncomplex nature of human activities, it is still challenging even after\neffective and efficient sensors are available. The existing works on human\nactivity recognition using smartphone sensors focus on recognizing basic human\nactivities like sitting, sleeping, standing, stair up and down and running.\nHowever, more than these basic activities is needed to analyze human\nbehavioural pattern. The proposed framework recognizes basic human activities\nusing deep learning models. Also, ambient sensors like PIR, pressure sensors,\nand smartphone-based sensors like accelerometers and gyroscopes are combined to\nmake it hybrid-sensor-based human activity recognition. The hybrid approach\nhelped derive more activities than the basic ones, which also helped derive\nhuman activity patterns or user profiling. User profiling provides sufficient\ninformation to identify daily living activity patterns and predict whether any\nanomaly exists. The framework provides the base for applications such as\nelderly monitoring when they are alone at home. The GRU model's accuracy of\n95\\% is observed to recognize the basic activities. Finally, Human activity\npatterns over time are recognized based on the duration and frequency of the\nactivities. It is observed that human activity pattern, like, morning walking\nduration, varies depending on the day of the week.\n","authors":["Ranjit Kolkar","Geetha V"],"pdf_url":"https://arxiv.org/pdf/2306.13374v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05694v1","updated":"2023-07-09T10:55:11Z","published":"2023-07-09T10:55:11Z","title":"A Survey on Figure Classification Techniques in Scientific Documents","summary":"  Figures visually represent an essential piece of information and provide an\neffective means to communicate scientific facts. Recently there have been many\nefforts toward extracting data directly from figures, specifically from tables,\ndiagrams, and plots, using different Artificial Intelligence and Machine\nLearning techniques. This is because removing information from figures could\nlead to deeper insights into the concepts highlighted in the scientific\ndocuments. In this survey paper, we systematically categorize figures into five\nclasses - tables, photos, diagrams, maps, and plots, and subsequently present a\ncritical review of the existing methodologies and data sets that address the\nproblem of figure classification. Finally, we identify the current research\ngaps and provide possible directions for further research on figure\nclassification.\n","authors":["Anurag Dhote","Mohammed Javed","David S Doermann"],"pdf_url":"https://arxiv.org/pdf/2307.05694v1.pdf","comment":"Some contents of this paper appears in the accepted paper - \"A Survey\n  and Approach to Chart Classification\" at 15th IAPR GREC 2023 at 17th ICDAR\n  2023, August 21-26, San Jose, USA. arXiv admin note: text overlap with\n  arXiv:2307.04147"},{"id":"http://arxiv.org/abs/2307.04090v1","updated":"2023-07-09T04:19:19Z","published":"2023-07-09T04:19:19Z","title":"DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge\n  Graphs","summary":"  Recent work within the Argument Mining community has shown the applicability\nof Natural Language Processing systems for solving problems found within\ncompetitive debate. One of the most important tasks within competitive debate\nis for debaters to create high quality debate cases. We show that effective\ndebate cases can be constructed using constrained shortest path traversals on\nArgumentative Semantic Knowledge Graphs. We study this potential in the context\nof a type of American Competitive Debate, called Policy Debate, which already\nhas a large scale dataset targeting it called DebateSum. We significantly\nimprove upon DebateSum by introducing 53180 new examples, as well as further\nuseful metadata for every example, to the dataset. We leverage the txtai\nsemantic search and knowledge graph toolchain to produce and contribute 9\nsemantic knowledge graphs built on this dataset. We create a unique method for\nevaluating which knowledge graphs are better in the context of producing policy\ndebate cases. A demo which automatically generates debate cases, along with all\nother code and the Knowledge Graphs, are open-sourced and made available to the\npublic here: https://github.com/Hellisotherpeople/DebateKG\n","authors":["Allen Roush"],"pdf_url":"https://arxiv.org/pdf/2307.04090v1.pdf","comment":"8 pages, knife-edge reject from EACL 2023 and workshops, System\n  Demonstration paper"},{"id":"http://arxiv.org/abs/2205.13128v2","updated":"2023-07-09T01:32:35Z","published":"2022-05-26T03:25:45Z","title":"Cascading Residual Graph Convolutional Network for Multi-Behavior\n  Recommendation","summary":"  Multi-behavior recommendation exploits multiple types of user-item\ninteractions to alleviate the data sparsity problem faced by the traditional\nmodels that often utilize only one type of interaction for recommendation. In\nreal scenarios, users often take a sequence of actions to interact with an\nitem, in order to get more information about the item and thus accurately\nevaluate whether an item fits personal preference. Those interaction behaviors\noften obey a certain order, and different behaviors reveal different\ninformation or aspects of user preferences towards the target item. Most\nexisting multi-behavior recommendation methods take the strategy to first\nextract information from different behaviors separately and then fuse them for\nfinal prediction. However, they have not exploited the connections between\ndifferent behaviors to learn user preferences. Besides, they often introduce\ncomplex model structures and more parameters to model multiple behaviors,\nlargely increasing the space and time complexity. In this work, we propose a\nlightweight multi-behavior recommendation model named Cascading Residual Graph\nConvolutional Network (CRGCN for short), which can explicitly exploit the\nconnections between different behaviors into the embedding learning process\nwithout introducing any additional parameters. In particular, we design a\ncascading residual graph convolutional network structure, which enables our\nmodel to learn user preferences by continuously refining user embeddings across\ndifferent types of behaviors. The multi-task learning method is adopted to\njointly optimize our model based on different behaviors. Extensive experimental\nresults on two real-world benchmark datasets show that CRGCN can substantially\noutperform state-of-the-art methods. Further studies also analyze the effects\nof leveraging multi-behaviors in different numbers and orders on the final\nperformance.\n","authors":["Mingshi Yan","Zhiyong Cheng","Chen Gao","Jing Sun","Fan Liu","Fuming Sun","Haojie Li"],"pdf_url":"https://arxiv.org/pdf/2205.13128v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05696v1","updated":"2023-07-09T01:19:08Z","published":"2023-07-09T01:19:08Z","title":"A Personalized Reinforcement Learning Summarization Service for Learning\n  Structure from Unstructured Data","summary":"  The exponential growth of textual data has created a crucial need for tools\nthat assist users in extracting meaningful insights. Traditional document\nsummarization approaches often fail to meet individual user requirements and\nlack structure for efficient information processing. To address these\nlimitations, we propose Summation, a hierarchical personalized concept-based\nsummarization approach. It synthesizes documents into a concise hierarchical\nconcept map and actively engages users by learning and adapting to their\npreferences. Using a Reinforcement Learning algorithm, Summation generates\npersonalized summaries for unseen documents on specific topics. This framework\nenhances comprehension, enables effective navigation, and empowers users to\nextract meaningful insights from large document collections aligned with their\nunique requirements.\n","authors":["Samira Ghodratnama","Amin Beheshti","Mehrdad Zakershahrak"],"pdf_url":"https://arxiv.org/pdf/2307.05696v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2108.09443"}],"Multimedia":[{"id":"http://arxiv.org/abs/2307.04192v1","updated":"2023-07-09T14:54:30Z","published":"2023-07-09T14:54:30Z","title":"SAS Video-QA: Self-Adaptive Sampling for Efficient Video\n  Question-Answering","summary":"  Video question--answering is a fundamental task in the field of video\nunderstanding. Although current vision--language models (VLMs) equipped with\nVideo Transformers have enabled temporal modeling and yielded superior results,\nthey are at the cost of huge computational power and thus too expensive to\ndeploy in real-time application scenarios. An economical workaround only\nsamples a small portion of frames to represent the main content of that video\nand tune an image--text model on these sampled frames. Recent video\nunderstanding models usually randomly sample a set of frames or clips,\nregardless of internal correlations between their visual contents, nor their\nrelevance to the problem. We argue that such kinds of aimless sampling may omit\nthe key frames from which the correct answer can be deduced, and the situation\ngets worse when the sampling sparsity increases, which always happens as the\nvideo lengths increase. To mitigate this issue, we propose two frame sampling\nstrategies, namely the most domain frames (MDF) and most implied frames (MIF),\nto maximally preserve those frames that are most likely vital to the given\nquestions. MDF passively minimizes the risk of key frame omission in a\nbootstrap manner, while MIS actively searches key frames customized for each\nvideo--question pair with the assistance of auxiliary models. The experimental\nresults on three public datasets from three advanced VLMs (CLIP, GIT and\nAll-in-one) demonstrate that our proposed strategies can boost the performance\nfor image--text pretrained models. The source codes pertaining to the method\nproposed in this paper are publicly available at\nhttps://github.com/declare-lab/sas-vqa.\n","authors":["Wei Han","Hui Chen","Min-Yen Kan","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2307.04192v1.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2307.04187v1","updated":"2023-07-09T14:40:54Z","published":"2023-07-09T14:40:54Z","title":"Predictive Coding For Animation-Based Video Compression","summary":"  We address the problem of efficiently compressing video for conferencing-type\napplications. We build on recent approaches based on image animation, which can\nachieve good reconstruction quality at very low bitrate by representing face\nmotions with a compact set of sparse keypoints. However, these methods encode\nvideo in a frame-by-frame fashion, i.e. each frame is reconstructed from a\nreference frame, which limits the reconstruction quality when the bandwidth is\nlarger. Instead, we propose a predictive coding scheme which uses image\nanimation as a predictor, and codes the residual with respect to the actual\ntarget frame. The residuals can be in turn coded in a predictive manner, thus\nremoving efficiently temporal dependencies. Our experiments indicate a\nsignificant bitrate gain, in excess of 70% compared to the HEVC video standard\nand over 30% compared to VVC, on a datasetof talking-head videos\n","authors":["Goluck Konuko","Stéphane Lathuilière","Giuseppe Valenzise"],"pdf_url":"https://arxiv.org/pdf/2307.04187v1.pdf","comment":"Accepted paper: ICIP 2023"},{"id":"http://arxiv.org/abs/2307.04114v1","updated":"2023-07-09T08:07:43Z","published":"2023-07-09T08:07:43Z","title":"FILM: How can Few-Shot Image Classification Benefit from Pre-Trained\n  Language Models?","summary":"  Few-shot learning aims to train models that can be generalized to novel\nclasses with only a few samples. Recently, a line of works are proposed to\nenhance few-shot learning with accessible semantic information from class\nnames. However, these works focus on improving existing modules such as visual\nprototypes and feature extractors of the standard few-shot learning framework.\nThis limits the full potential use of semantic information. In this paper, we\npropose a novel few-shot learning framework that uses pre-trained language\nmodels based on contrastive learning. To address the challenge of alignment\nbetween visual features and textual embeddings obtained from text-based\npre-trained language model, we carefully design the textual branch of our\nframework and introduce a metric module to generalize the cosine similarity.\nFor better transferability, we let the metric module adapt to different\nfew-shot tasks and adopt MAML to train the model via bi-level optimization.\nMoreover, we conduct extensive experiments on multiple benchmarks to\ndemonstrate the effectiveness of our method.\n","authors":["Zihao Jiang","Yunkai Dang","Dong Pang","Huishuai Zhang","Weiran Huang"],"pdf_url":"https://arxiv.org/pdf/2307.04114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.07848v3","updated":"2023-07-09T04:21:54Z","published":"2023-06-13T15:28:10Z","title":"GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio\n  Pretraining for Speech Emotion Recognition","summary":"  Contrastive learning based pretraining methods have recently exhibited\nimpressive success in diverse fields. In this paper, we propose GEmo-CLAP, a\nkind of efficient gender-attribute-enhanced contrastive language-audio\npretraining (CLAP) model for speech emotion recognition. To be specific, we\nfirst build an effective emotion CLAP model Emo-CLAP for emotion recognition,\nutilizing various self-supervised learning based pre-trained models. Then,\nconsidering the importance of the gender attribute in speech emotion modeling,\ntwo GEmo-CLAP approaches are further proposed to integrate the emotion and\ngender information of speech signals, forming more reasonable objectives.\nExtensive experiments on the IEMOCAP corpus demonstrate that our proposed two\nGEmo-CLAP approaches consistently outperform the baseline Emo-CLAP with\ndifferent pre-trained models, while also achieving superior recognition\nperformance compared with other state-of-the-art methods.\n","authors":["Yu Pan","Yanni Hu","Yuguang Yang","Jixun Yao","Wen Fei","Lei Ma","Heng Lu"],"pdf_url":"https://arxiv.org/pdf/2306.07848v3.pdf","comment":"5 pages"}]},"2023-07-08T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2307.04057v1","updated":"2023-07-08T23:25:55Z","published":"2023-07-08T23:25:55Z","title":"Bidirectional Attention as a Mixture of Continuous Word Experts","summary":"  Bidirectional attention $\\unicode{x2013}$ composed of self-attention with\npositional encodings and the masked language model (MLM) objective\n$\\unicode{x2013}$ has emerged as a key component of modern large language\nmodels (LLMs). Despite its empirical success, few studies have examined its\nstatistical underpinnings: What statistical model is bidirectional attention\nimplicitly fitting? What sets it apart from its non-attention predecessors? We\nexplore these questions in this paper. The key observation is that fitting a\nsingle-layer single-head bidirectional attention, upon reparameterization, is\nequivalent to fitting a continuous bag of words (CBOW) model with\nmixture-of-experts (MoE) weights. Further, bidirectional attention with\nmultiple heads and multiple layers is equivalent to stacked MoEs and a mixture\nof MoEs, respectively. This statistical viewpoint reveals the distinct use of\nMoE in bidirectional attention, which aligns with its practical effectiveness\nin handling heterogeneous data. It also suggests an immediate extension to\ncategorical tabular data, if we view each word location in a sentence as a\ntabular feature. Across empirical studies, we find that this extension\noutperforms existing tabular extensions of transformers in out-of-distribution\n(OOD) generalization. Finally, this statistical perspective of bidirectional\nattention enables us to theoretically characterize when linear word analogies\nare present in its word embeddings. These analyses show that bidirectional\nattention can require much stronger assumptions to exhibit linear word\nanalogies than its non-attention predecessors.\n","authors":["Kevin Christian Wibisono","Yixin Wang"],"pdf_url":"https://arxiv.org/pdf/2307.04057v1.pdf","comment":"31 pages"},{"id":"http://arxiv.org/abs/2307.04053v1","updated":"2023-07-08T22:03:00Z","published":"2023-07-08T22:03:00Z","title":"How is Fatherhood Framed Online in Singapore?","summary":"  The proliferation of discussion about fatherhood in Singapore attests to its\nsignificance, indicating the need for an exploration of how fatherhood is\nframed, aiding policy-making around fatherhood in Singapore. Sound and holistic\npolicy around fatherhood in Singapore may reduce stigma and apprehension around\nbeing a parent, critical to improving the nations flagging birth rate. We\nanalyzed 15,705 articles and 56,221 posts to study how fatherhood is framed in\nSingapore across a range of online platforms (news outlets, parenting forums,\nTwitter). We used NLP techniques to understand these differences. While\nfatherhood was framed in a range of ways on the Singaporean online environment,\nit did not seem that fathers were framed as central to the Singaporean family\nunit. A strength of our work is how the different techniques we have applied\nvalidate each other.\n","authors":["Tran Hien Van","Abhay Goyal","Muhammad Siddique","Lam Yin Cheung","Nimay Parekh","Jonathan Y Huang","Keri McCrickerd","Edson C Tandoc Jr.","Gerard Chung","Navin Kumar"],"pdf_url":"https://arxiv.org/pdf/2307.04053v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.10515v2","updated":"2023-07-08T21:59:43Z","published":"2022-12-20T18:31:50Z","title":"CausalDialogue: Modeling Utterance-level Causality in Conversations","summary":"  Despite their widespread adoption, neural conversation models have yet to\nexhibit natural chat capabilities with humans. In this research, we examine\nuser utterances as causes and generated responses as effects, recognizing that\nchanges in a cause should produce a different effect. To further explore this\nconcept, we have compiled and expanded upon a new dataset called CausalDialogue\nthrough crowd-sourcing. This dataset includes multiple cause-effect pairs\nwithin a directed acyclic graph (DAG) structure. Our analysis reveals that\ntraditional loss functions struggle to effectively incorporate the DAG\nstructure, leading us to propose a causality-enhanced method called Exponential\nMaximum Average Treatment Effect (ExMATE) to enhance the impact of causality at\nthe utterance level in training neural conversation models. To evaluate the\nneeds of considering causality in dialogue generation, we built a comprehensive\nbenchmark on CausalDialogue dataset using different models, inference, and\ntraining methods. Through experiments, we find that a causality-inspired loss\nlike ExMATE can improve the diversity and agility of conventional loss function\nand there is still room for improvement to reach human-level quality on this\nnew dataset.\n","authors":["Yi-Lin Tuan","Alon Albalak","Wenda Xu","Michael Saxon","Connor Pryor","Lise Getoor","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2212.10515v2.pdf","comment":"Accepted to ACL-Findings 2023"},{"id":"http://arxiv.org/abs/2301.11489v2","updated":"2023-07-08T21:41:23Z","published":"2023-01-27T01:54:16Z","title":"Talk the Walk: Synthetic Data Generation for Conversational Music\n  Recommendation","summary":"  Recommendation systems are ubiquitous yet often difficult for users to\ncontrol and adjust when recommendation quality is poor. This has motivated the\ndevelopment of conversational recommendation systems (CRSs), with control over\nrecommendations provided through natural language feedback. However, building\nconversational recommendation systems requires conversational training data\ninvolving user utterances paired with items that cover a diverse range of\npreferences. Such data has proved challenging to collect scalably using\nconventional methods like crowdsourcing. We address it in the context of\nitem-set recommendation, noting the increasing attention to this task motivated\nby use cases like music, news and recipe recommendation. We present a new\ntechnique, TalkTheWalk, that synthesizes realistic high-quality conversational\ndata by leveraging domain expertise encoded in widely available curated item\ncollections, showing how these can be transformed into corresponding item set\ncuration conversations. Specifically, TalkTheWalk generates a sequence of\nhypothetical yet plausible item sets returned by a system, then uses a language\nmodel to produce corresponding user utterances. Applying TalkTheWalk to music\nrecommendation, we generate over one million diverse playlist curation\nconversations. A human evaluation shows that the conversations contain\nconsistent utterances with relevant item sets, nearly matching the quality of\nsmall human-collected conversational data for this task. At the same time, when\nthe synthetic corpus is used to train a CRS, it improves Hits@100 by 10.5\npoints on a benchmark dataset over standard baselines and is preferred over the\ntop-performing baseline in an online evaluation.\n","authors":["Megan Leszczynski","Ravi Ganti","Shu Zhang","Krisztian Balog","Filip Radlinski","Fernando Pereira","Arun Tejasvi Chaganty"],"pdf_url":"https://arxiv.org/pdf/2301.11489v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.12081v2","updated":"2023-07-08T20:37:28Z","published":"2022-08-25T13:27:14Z","title":"Kencorpus: A Kenyan Language Corpus of Swahili, Dholuo and Luhya for\n  Natural Language Processing Tasks","summary":"  Indigenous African languages are categorized as under-served in Natural\nLanguage Processing. They therefore experience poor digital inclusivity and\ninformation access. The processing challenge with such languages has been how\nto use machine learning and deep learning models without the requisite data.\nThe Kencorpus project intends to bridge this gap by collecting and storing text\nand speech data that is good enough for data-driven solutions in applications\nsuch as machine translation, question answering and transcription in\nmultilingual communities. The Kencorpus dataset is a text and speech corpus for\nthree languages predominantly spoken in Kenya: Swahili, Dholuo and Luhya. Data\ncollection was done by researchers from communities, schools, media, and\npublishers. The Kencorpus' dataset has a collection of 5,594 items - 4,442\ntexts (5.6M words) and 1,152 speech files (177hrs). Based on this data, Part of\nSpeech tagging sets for Dholuo and Luhya (50,000 and 93,000 words respectively)\nwere developed. We developed 7,537 Question-Answer pairs for Swahili and\ncreated a text translation set of 13,400 sentences from Dholuo and Luhya into\nSwahili. The datasets are useful for downstream machine learning tasks such as\nmodel training and translation. We also developed two proof of concept systems:\nfor Kiswahili speech-to-text and machine learning system for Question Answering\ntask, with results of 18.87% word error rate and 80% Exact Match (EM)\nrespectively. These initial results give great promise to the usability of\nKencorpus to the machine learning community. Kencorpus is one of few public\ndomain corpora for these three low resource languages and forms a basis of\nlearning and sharing experiences for similar works especially for low resource\nlanguages.\n","authors":["Barack Wanjawa","Lilian Wanzare","Florence Indede","Owen McOnyango","Edward Ombui","Lawrence Muchemi"],"pdf_url":"https://arxiv.org/pdf/2208.12081v2.pdf","comment":"24 pages, 6 figures"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2305.09918v3","updated":"2023-07-08T23:12:48Z","published":"2023-05-17T02:59:13Z","title":"Unconfounded Propensity Estimation for Unbiased Ranking","summary":"  The goal of unbiased learning to rank (ULTR) is to leverage implicit user\nfeedback for optimizing learning-to-rank systems. Among existing solutions,\nautomatic ULTR algorithms that jointly learn user bias models (i.e., propensity\nmodels) with unbiased rankers have received a lot of attention due to their\nsuperior performance and low deployment cost in practice. Despite their\ntheoretical soundness, the effectiveness is usually justified under a weak\nlogging policy, where the ranking model can barely rank documents according to\ntheir relevance to the query. However, when the logging policy is strong, e.g.,\nan industry-deployed ranking policy, the reported effectiveness cannot be\nreproduced. In this paper, we first investigate ULTR from a causal perspective\nand uncover a negative result: existing ULTR algorithms fail to address the\nissue of propensity overestimation caused by the query-document relevance\nconfounder. Then, we propose a new learning objective based on backdoor\nadjustment and highlight its differences from conventional propensity models,\nwhich reveal the prevalence of propensity overestimation. On top of that, we\nintroduce a novel propensity model called Logging-Policy-aware Propensity (LPP)\nmodel and its distinctive two-step optimization strategy, which allows for the\njoint learning of LPP and ranking models within the automatic ULTR framework,\nand actualize the unconfounded propensity estimation for ULTR. Extensive\nexperiments on two benchmarks demonstrate the effectiveness and\ngeneralizability of the proposed method.\n","authors":["Dan Luo","Lixin Zou","Qingyao Ai","Zhiyu Chen","Chenliang Li","Dawei Yin","Brian D. Davison"],"pdf_url":"https://arxiv.org/pdf/2305.09918v3.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2301.11489v2","updated":"2023-07-08T21:41:23Z","published":"2023-01-27T01:54:16Z","title":"Talk the Walk: Synthetic Data Generation for Conversational Music\n  Recommendation","summary":"  Recommendation systems are ubiquitous yet often difficult for users to\ncontrol and adjust when recommendation quality is poor. This has motivated the\ndevelopment of conversational recommendation systems (CRSs), with control over\nrecommendations provided through natural language feedback. However, building\nconversational recommendation systems requires conversational training data\ninvolving user utterances paired with items that cover a diverse range of\npreferences. Such data has proved challenging to collect scalably using\nconventional methods like crowdsourcing. We address it in the context of\nitem-set recommendation, noting the increasing attention to this task motivated\nby use cases like music, news and recipe recommendation. We present a new\ntechnique, TalkTheWalk, that synthesizes realistic high-quality conversational\ndata by leveraging domain expertise encoded in widely available curated item\ncollections, showing how these can be transformed into corresponding item set\ncuration conversations. Specifically, TalkTheWalk generates a sequence of\nhypothetical yet plausible item sets returned by a system, then uses a language\nmodel to produce corresponding user utterances. Applying TalkTheWalk to music\nrecommendation, we generate over one million diverse playlist curation\nconversations. A human evaluation shows that the conversations contain\nconsistent utterances with relevant item sets, nearly matching the quality of\nsmall human-collected conversational data for this task. At the same time, when\nthe synthetic corpus is used to train a CRS, it improves Hits@100 by 10.5\npoints on a benchmark dataset over standard baselines and is preferred over the\ntop-performing baseline in an online evaluation.\n","authors":["Megan Leszczynski","Ravi Ganti","Shu Zhang","Krisztian Balog","Filip Radlinski","Fernando Pereira","Arun Tejasvi Chaganty"],"pdf_url":"https://arxiv.org/pdf/2301.11489v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.14464v3","updated":"2023-07-08T20:34:47Z","published":"2022-12-29T21:58:37Z","title":"Result Diversification in Search and Recommendation: A Survey","summary":"  Diversifying return results is an important research topic in retrieval\nsystems in order to satisfy both the various interests of customers and the\nequal market exposure of providers. There has been growing attention on\ndiversity-aware research during recent years, accompanied by a proliferation of\nliterature on methods to promote diversity in search and recommendation.\nHowever, diversity-aware studies in retrieval systems lack a systematic\norganization and are rather fragmented. In this survey, we are the first to\npropose a unified taxonomy for classifying the metrics and approaches of\ndiversification in both search and recommendation, which are two of the most\nextensively researched fields of retrieval systems. We begin the survey with a\nbrief discussion of why diversity is important in retrieval systems, followed\nby a summary of the various diversity concerns in search and recommendation,\nhighlighting their relationship and differences. For the survey's main body, we\npresent a unified taxonomy of diversification metrics and approaches in\nretrieval systems, from both the search and recommendation perspectives. In the\nlater part of the survey, we discuss the open research questions of\ndiversity-aware research in search and recommendation in an effort to inspire\nfuture innovations and encourage the implementation of diversity in real-world\nsystems.\n","authors":["Haolun Wu","Yansen Zhang","Chen Ma","Fuyuan Lyu","Bowei He","Bhaskar Mitra","Xue Liu"],"pdf_url":"https://arxiv.org/pdf/2212.14464v3.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2305.06569v4","updated":"2023-07-08T18:57:21Z","published":"2023-05-11T05:02:37Z","title":"How to Index Item IDs for Recommendation Foundation Models","summary":"  Recommendation foundation model utilizes large language models (LLM) for\nrecommendation by converting recommendation tasks into natural language tasks.\nIt enables generative recommendation which directly generates the item(s) to\nrecommend rather than calculating a ranking score for each and every candidate\nitem in traditional recommendation models, simplifying the recommendation\npipeline from multi-stage filtering to single-stage filtering. To avoid\ngenerating excessively long text when deciding which item(s) to recommend,\ncreating LLM-compatible item IDs is essential for recommendation foundation\nmodels. In this study, we systematically examine the item indexing problem for\nrecommendation foundation models, using P5 as the representative backbone model\nand replicating its results with various indexing methods. To emphasize the\nimportance of item indexing, we first discuss the issues of several trivial\nitem indexing methods, such as independent indexing, title indexing, and random\nindexing. We then propose four simple yet effective solutions, including\nsequential indexing, collaborative indexing, semantic (content-based) indexing,\nand hybrid indexing. Our reproducibility study of P5 highlights the significant\ninfluence of item indexing methods on the model performance, and our results on\nreal-world datasets validate the effectiveness of our proposed solutions.\n","authors":["Wenyue Hua","Shuyuan Xu","Yingqiang Ge","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.06569v4.pdf","comment":"12 pages, 6 figures"},{"id":"http://arxiv.org/abs/2307.03929v1","updated":"2023-07-08T08:09:06Z","published":"2023-07-08T08:09:06Z","title":"Fairness-Aware Graph Neural Networks: A Survey","summary":"  Graph Neural Networks (GNNs) have become increasingly important due to their\nrepresentational power and state-of-the-art predictive performance on many\nfundamental learning tasks. Despite this success, GNNs suffer from fairness\nissues that arise as a result of the underlying graph data and the fundamental\naggregation mechanism that lies at the heart of the large class of GNN models.\nIn this article, we examine and categorize fairness techniques for improving\nthe fairness of GNNs. Previous work on fair GNN models and techniques are\ndiscussed in terms of whether they focus on improving fairness during a\npreprocessing step, during training, or in a post-processing phase.\nFurthermore, we discuss how such techniques can be used together whenever\nappropriate, and highlight the advantages and intuition as well. We also\nintroduce an intuitive taxonomy for fairness evaluation metrics including\ngraph-level fairness, neighborhood-level fairness, embedding-level fairness,\nand prediction-level fairness metrics. In addition, graph datasets that are\nuseful for benchmarking the fairness of GNN models are summarized succinctly.\nFinally, we highlight key open problems and challenges that remain to be\naddressed.\n","authors":["April Chen","Ryan A. Rossi","Namyong Park","Puja Trivedi","Yu Wang","Tong Yu","Sungchul Kim","Franck Dernoncourt","Nesreen K. Ahmed"],"pdf_url":"https://arxiv.org/pdf/2307.03929v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.10837v2","updated":"2023-07-08T08:03:47Z","published":"2023-05-18T09:34:05Z","title":"Adaptive Graph Contrastive Learning for Recommendation","summary":"  Graph neural networks (GNNs) have recently emerged as an effective\ncollaborative filtering (CF) approaches for recommender systems. The key idea\nof GNN-based recommender systems is to recursively perform message passing\nalong user-item interaction edges to refine encoded embeddings, relying on\nsufficient and high-quality training data. However, user behavior data in\npractical recommendation scenarios is often noisy and exhibits skewed\ndistribution. To address these issues, some recommendation approaches, such as\nSGL, leverage self-supervised learning to improve user representations. These\napproaches conduct self-supervised learning through creating contrastive views,\nbut they depend on the tedious trial-and-error selection of augmentation\nmethods. In this paper, we propose a novel Adaptive Graph Contrastive Learning\n(AdaGCL) framework that conducts data augmentation with two adaptive\ncontrastive view generators to better empower the CF paradigm. Specifically, we\nuse two trainable view generators - a graph generative model and a graph\ndenoising model - to create adaptive contrastive views. With two adaptive\ncontrastive views, AdaGCL introduces additional high-quality training signals\ninto the CF paradigm, helping to alleviate data sparsity and noise issues.\nExtensive experiments on three real-world datasets demonstrate the superiority\nof our model over various state-of-the-art recommendation methods. Our model\nimplementation codes are available at the link https://github.com/HKUDS/AdaGCL.\n","authors":["Yangqin Jiang","Chao Huang","Lianghao Xia"],"pdf_url":"https://arxiv.org/pdf/2305.10837v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18683v3","updated":"2023-07-08T07:25:07Z","published":"2023-05-30T01:47:50Z","title":"Known by the Company it Keeps: Proximity-Based Indexing for Physical\n  Content in Archival Repositories","summary":"  Despite the plethora of born-digital content, vast troves of important\ncontent remain accessible only on physical media such as paper or microfilm.\nThe traditional approach to indexing undigitized content is using manually\ncreated metadata that describes it at some level of aggregation (e.g., folder,\nbox, or collection). Searchers led in this way to some subset of the content\noften must then manually examine substantial quantities of physical media to\nfind what they are looking for. This paper proposes a complementary approach,\nin which selective digitization of a small portion of the content is used as a\nbasis for proximity-based indexing as a way of bringing the user closer to the\nspecific content for which they are looking. Experiments with 35 boxes of\npartially digitized US State Department records indicate that box-level indexes\nbuilt in this way can provide a useful basis for search.\n","authors":["Douglas W. Oard"],"pdf_url":"https://arxiv.org/pdf/2305.18683v3.pdf","comment":"To be published in Theory and Practice of Digital Libraries (TPDL)\n  2023"},{"id":"http://arxiv.org/abs/2307.03892v1","updated":"2023-07-08T03:59:58Z","published":"2023-07-08T03:59:58Z","title":"Embedding Mental Health Discourse for Community Recommendation","summary":"  Our paper investigates the use of discourse embedding techniques to develop a\ncommunity recommendation system that focuses on mental health support groups on\nsocial media. Social media platforms provide a means for users to anonymously\nconnect with communities that cater to their specific interests. However, with\nthe vast number of online communities available, users may face difficulties in\nidentifying relevant groups to address their mental health concerns. To address\nthis challenge, we explore the integration of discourse information from\nvarious subreddit communities using embedding techniques to develop an\neffective recommendation system. Our approach involves the use of content-based\nand collaborative filtering techniques to enhance the performance of the\nrecommendation system. Our findings indicate that the proposed approach\noutperforms the use of each technique separately and provides interpretability\nin the recommendation process.\n","authors":["Hy Dang","Bang Nguyen","Noah Ziems","Meng Jiang"],"pdf_url":"https://arxiv.org/pdf/2307.03892v1.pdf","comment":"Accepted to the 4th workshop on Computational Approaches to Discourse\n  (CODI-2023) at ACL 2023"}],"Multimedia":[{"id":"http://arxiv.org/abs/2307.04015v1","updated":"2023-07-08T16:47:31Z","published":"2023-07-08T16:47:31Z","title":"Emotion-Guided Music Accompaniment Generation Based on Variational\n  Autoencoder","summary":"  Music accompaniment generation is a crucial aspect in the composition\nprocess. Deep neural networks have made significant strides in this field, but\nit remains a challenge for AI to effectively incorporate human emotions to\ncreate beautiful accompaniments. Existing models struggle to effectively\ncharacterize human emotions within neural network models while composing music.\nTo address this issue, we propose the use of an easy-to-represent emotion flow\nmodel, the Valence/Arousal Curve, which allows for the compatibility of\nemotional information within the model through data transformation and enhances\ninterpretability of emotional factors by utilizing a Variational Autoencoder as\nthe model structure. Further, we used relative self-attention to maintain the\nstructure of the music at music phrase level and to generate a richer\naccompaniment when combined with the rules of music theory.\n","authors":["Qi Wang","Shubing Zhang","Li Zhou"],"pdf_url":"https://arxiv.org/pdf/2307.04015v1.pdf","comment":"Accepted By International Joint Conference on Neural Networks\n  2023(IJCNN2023)"}]},"2023-07-07T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2307.03826v1","updated":"2023-07-07T20:41:26Z","published":"2023-07-07T20:41:26Z","title":"How does AI chat change search behaviors?","summary":"  Generative AI tools such as chatGPT are poised to change the way people\nengage with online information. Recently, Microsoft announced their \"new Bing\"\nsearch system which incorporates chat and generative AI technology from OpenAI.\nGoogle has announced plans to deploy search interfaces that incorporate similar\ntypes of technology. These new technologies will transform how people can\nsearch for information. The research presented here is an early investigation\ninto how people make use of a generative AI chat system (referred to simply as\nchat from here on) as part of a search process, and how the incorporation of\nchat systems with existing search tools may effect users search behaviors and\nstrategies.\n  We report on an exploratory user study with 10 participants who used a\ncombined Chat+Search system that utilized the OpenAI GPT-3.5 API and the Bing\nWeb Search v5 API. Participants completed three search tasks. In this pre-print\npaper of preliminary results, we report on ways that users integrated AI chat\ninto their search process, things they liked and disliked about the chat\nsystem, their trust in the chat responses, and their mental models of how the\nchat system generated responses.\n","authors":["Robert Capra","Jaime Arguello"],"pdf_url":"https://arxiv.org/pdf/2307.03826v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.01304v2","updated":"2023-07-07T09:57:54Z","published":"2023-06-02T07:04:33Z","title":"JEPOO: Highly Accurate Joint Estimation of Pitch, Onset and Offset for\n  Music Information Retrieval","summary":"  Melody extraction is a core task in music information retrieval, and the\nestimation of pitch, onset and offset are key sub-tasks in melody extraction.\nExisting methods have limited accuracy, and work for only one type of data,\neither single-pitch or multipitch. In this paper, we propose a highly accurate\nmethod for joint estimation of pitch, onset and offset, named JEPOO. We address\nthe challenges of joint learning optimization and handling both single-pitch\nand multi-pitch data through novel model design and a new optimization\ntechnique named Pareto modulated loss with loss weight regularization. This is\nthe first method that can accurately handle both single-pitch and multi-pitch\nmusic data, and even a mix of them. A comprehensive experimental study on a\nwide range of real datasets shows that JEPOO outperforms state-ofthe-art\nmethods by up to 10.6%, 8.3% and 10.3% for the prediction of Pitch, Onset and\nOffset, respectively, and JEPOO is robust for various types of data and\ninstruments. The ablation study shows the effectiveness of each component of\nJEPOO.\n","authors":["Haojie Wei","Jun Yuan","Rui Zhang","Yueguo Chen","Gang Wang"],"pdf_url":"https://arxiv.org/pdf/2306.01304v2.pdf","comment":"This paper has been accepted by IJCAI 2023; 11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2307.05469v1","updated":"2023-07-07T06:48:58Z","published":"2023-07-07T06:48:58Z","title":"AdaptiveRec: Adaptively Construct Pairs for Contrastive Learning in\n  Sequential Recommendation","summary":"  This paper presents a solution to the challenges faced by contrastive\nlearning in sequential recommendation systems. In particular, it addresses the\nissue of false negative, which limits the effectiveness of recommendation\nalgorithms. By introducing an advanced approach to contrastive learning, the\nproposed method improves the quality of item embeddings and mitigates the\nproblem of falsely categorizing similar instances as dissimilar. Experimental\nresults demonstrate performance enhancements compared to existing systems. The\nflexibility and applicability of the proposed approach across various\nrecommendation scenarios further highlight its value in enhancing sequential\nrecommendation systems.\n","authors":["Jaeheyoung Jeon","Jung Hyun Ryu","Jewoong Cho","Myungjoo Kang"],"pdf_url":"https://arxiv.org/pdf/2307.05469v1.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2307.03399v1","updated":"2023-07-07T05:51:04Z","published":"2023-07-07T05:51:04Z","title":"A Network Resource Allocation Recommendation Method with An Improved\n  Similarity Measure","summary":"  Recommender systems have been acknowledged as efficacious tools for managing\ninformation overload. Nevertheless, conventional algorithms adopted in such\nsystems primarily emphasize precise recommendations and, consequently, overlook\nother vital aspects like the coverage, diversity, and novelty of items. This\napproach results in less exposure for long-tail items. In this paper, to\npersonalize the recommendations and allocate recommendation resources more\npurposively, a method named PIM+RA is proposed. This method utilizes a\nbipartite network that incorporates self-connecting edges and weights.\nFurthermore, an improved Pearson correlation coefficient is employed for better\nredistribution. The evaluation of PIM+RA demonstrates a significant enhancement\nnot only in accuracy but also in coverage, diversity, and novelty of the\nrecommendation. It leads to a better balance in recommendation frequency by\nproviding effective exposure to long-tail items, while allowing customized\nparameters to adjust the recommendation list bias.\n","authors":["Huiyu Li","Pei Liang","Junhua Hu"],"pdf_url":"https://arxiv.org/pdf/2307.03399v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.10128v3","updated":"2023-07-07T04:08:33Z","published":"2022-06-21T06:21:23Z","title":"Bridging the Gap Between Indexing and Retrieval for Differentiable\n  Search Index with Query Generation","summary":"  The Differentiable Search Index (DSI) is an emerging paradigm for information\nretrieval. Unlike traditional retrieval architectures where index and retrieval\nare two different and separate components, DSI uses a single transformer model\nto perform both indexing and retrieval.\n  In this paper, we identify and tackle an important issue of current DSI\nmodels: the data distribution mismatch that occurs between the DSI indexing and\nretrieval processes. Specifically, we argue that, at indexing, current DSI\nmethods learn to build connections between the text of long documents and the\nidentifier of the documents, but then retrieval of document identifiers is\nbased on queries that are commonly much shorter than the indexed documents.\nThis problem is further exacerbated when using DSI for cross-lingual retrieval,\nwhere document text and query text are in different languages.\n  To address this fundamental problem of current DSI models, we propose a\nsimple yet effective indexing framework for DSI, called DSI-QG. When indexing,\nDSI-QG represents documents with a number of potentially relevant queries\ngenerated by a query generation model and re-ranked and filtered by a\ncross-encoder ranker. The presence of these queries at indexing allows the DSI\nmodels to connect a document identifier to a set of queries, hence mitigating\ndata distribution mismatches present between the indexing and the retrieval\nphases. Empirical results on popular mono-lingual and cross-lingual passage\nretrieval datasets show that DSI-QG significantly outperforms the original DSI\nmodel.\n","authors":["Shengyao Zhuang","Houxing Ren","Linjun Shou","Jian Pei","Ming Gong","Guido Zuccon","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2206.10128v3.pdf","comment":"11 pages"}],"Multimedia":[{"id":"http://arxiv.org/abs/2307.04827v1","updated":"2023-07-07T16:25:59Z","published":"2023-07-07T16:25:59Z","title":"LaunchpadGPT: Language Model as Music Visualization Designer on\n  Launchpad","summary":"  Launchpad is a musical instrument that allows users to create and perform\nmusic by pressing illuminated buttons. To assist and inspire the design of the\nLaunchpad light effect, and provide a more accessible approach for beginners to\ncreate music visualization with this instrument, we proposed the LaunchpadGPT\nmodel to generate music visualization designs on Launchpad automatically. Based\non the language model with excellent generation ability, our proposed\nLaunchpadGPT takes an audio piece of music as input and outputs the lighting\neffects of Launchpad-playing in the form of a video (Launchpad-playing video).\nWe collect Launchpad-playing videos and process them to obtain music and\ncorresponding video frame of Launchpad-playing as prompt-completion pairs, to\ntrain the language model. The experiment result shows the proposed method can\ncreate better music visualization than random generation methods and hold the\npotential for a broader range of music visualization applications. Our code is\navailable at https://github.com/yunlong10/LaunchpadGPT/.\n","authors":["Siting Xu","Yunlong Tang","Feng Zheng"],"pdf_url":"https://arxiv.org/pdf/2307.04827v1.pdf","comment":"Accepted by International Computer Music Conference (ICMC) 2023"},{"id":"http://arxiv.org/abs/2307.03638v1","updated":"2023-07-07T14:57:34Z","published":"2023-07-07T14:57:34Z","title":"Physical-aware Cross-modal Adversarial Network for Wearable Sensor-based\n  Human Action Recognition","summary":"  Wearable sensor-based Human Action Recognition (HAR) has made significant\nstrides in recent times. However, the accuracy performance of wearable\nsensor-based HAR is currently still lagging behind that of visual\nmodalities-based systems, such as RGB video and depth data. Although diverse\ninput modalities can provide complementary cues and improve the accuracy\nperformance of HAR, wearable devices can only capture limited kinds of\nnon-visual time series input, such as accelerometers and gyroscopes. This\nlimitation hinders the deployment of multimodal simultaneously using visual and\nnon-visual modality data in parallel on current wearable devices. To address\nthis issue, we propose a novel Physical-aware Cross-modal Adversarial (PCA)\nframework that utilizes only time-series accelerometer data from four inertial\nsensors for the wearable sensor-based HAR problem. Specifically, we propose an\neffective IMU2SKELETON network to produce corresponding synthetic skeleton\njoints from accelerometer data. Subsequently, we imposed additional constraints\non the synthetic skeleton data from a physical perspective, as accelerometer\ndata can be regarded as the second derivative of the skeleton sequence\ncoordinates. After that, the original accelerometer as well as the constrained\nskeleton sequence were fused together to make the final classification. In this\nway, when individuals wear wearable devices, the devices can not only capture\naccelerometer data, but can also generate synthetic skeleton sequences for\nreal-time wearable sensor-based HAR applications that need to be conducted\nanytime and anywhere. To demonstrate the effectiveness of our proposed PCA\nframework, we conduct extensive experiments on Berkeley-MHAD, UTD-MHAD, and\nMMAct datasets. The results confirm that the proposed PCA approach has\ncompetitive performance compared to the previous methods on the mono\nsensor-based HAR classification problem.\n","authors":["Jianyuan Ni","Hao Tang","Anne H. H. Ngu","Gaowen Liu","Yan Yan"],"pdf_url":"https://arxiv.org/pdf/2307.03638v1.pdf","comment":"First IMU2SKELETON GANs approach for wearable HAR problem. arXiv\n  admin note: text overlap with arXiv:2208.08090"},{"id":"http://arxiv.org/abs/2306.01304v2","updated":"2023-07-07T09:57:54Z","published":"2023-06-02T07:04:33Z","title":"JEPOO: Highly Accurate Joint Estimation of Pitch, Onset and Offset for\n  Music Information Retrieval","summary":"  Melody extraction is a core task in music information retrieval, and the\nestimation of pitch, onset and offset are key sub-tasks in melody extraction.\nExisting methods have limited accuracy, and work for only one type of data,\neither single-pitch or multipitch. In this paper, we propose a highly accurate\nmethod for joint estimation of pitch, onset and offset, named JEPOO. We address\nthe challenges of joint learning optimization and handling both single-pitch\nand multi-pitch data through novel model design and a new optimization\ntechnique named Pareto modulated loss with loss weight regularization. This is\nthe first method that can accurately handle both single-pitch and multi-pitch\nmusic data, and even a mix of them. A comprehensive experimental study on a\nwide range of real datasets shows that JEPOO outperforms state-ofthe-art\nmethods by up to 10.6%, 8.3% and 10.3% for the prediction of Pitch, Onset and\nOffset, respectively, and JEPOO is robust for various types of data and\ninstruments. The ablation study shows the effectiveness of each component of\nJEPOO.\n","authors":["Haojie Wei","Jun Yuan","Rui Zhang","Yueguo Chen","Gang Wang"],"pdf_url":"https://arxiv.org/pdf/2306.01304v2.pdf","comment":"This paper has been accepted by IJCAI 2023; 11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2307.03436v1","updated":"2023-07-07T07:47:45Z","published":"2023-07-07T07:47:45Z","title":"Anableps: Adapting Bitrate for Real-Time Communication Using VBR-encoded\n  Video","summary":"  Content providers increasingly replace traditional constant bitrate with\nvariable bitrate (VBR) encoding in real-time video communication systems for\nbetter video quality. However, VBR encoding often leads to large and frequent\nbitrate fluctuation, inevitably deteriorating the efficiency of existing\nadaptive bitrate (ABR) methods. To tackle it, we propose the Anableps to\nconsider the network dynamics and VBR-encoding-induced video bitrate\nfluctuations jointly for deploying the best ABR policy. With this aim, Anableps\nuses sender-side information from the past to predict the video bitrate range\nof upcoming frames. Such bitrate range is then combined with the receiver-side\nobservations to set the proper bitrate target for video encoding using a\nreinforcement-learning-based ABR model. As revealed by extensive experiments on\na real-world trace-driven testbed, our Anableps outperforms the GCC with\nsignificant improvement of quality of experience, e.g., 1.88x video quality,\n57% less bitrate consumption, 85% less stalling, and 74% shorter interaction\ndelay.\n","authors":["Zicheng Zhang","Hao Chen","Xun Cao","Zhan Ma"],"pdf_url":"https://arxiv.org/pdf/2307.03436v1.pdf","comment":"This paper will be presented at IEEE ICME 2023"},{"id":"http://arxiv.org/abs/2307.03394v1","updated":"2023-07-07T05:33:54Z","published":"2023-07-07T05:33:54Z","title":"Towards Robust SDRTV-to-HDRTV via Dual Inverse Degradation Network","summary":"  Recently, the transformation of standard dynamic range TV (SDRTV) to high\ndynamic range TV (HDRTV) is in high demand due to the scarcity of HDRTV\ncontent. However, the conversion of SDRTV to HDRTV often amplifies the existing\ncoding artifacts in SDRTV which deteriorate the visual quality of the output.\nIn this study, we propose a dual inverse degradation SDRTV-to-HDRTV network\nDIDNet to address the issue of coding artifact restoration in converted HDRTV,\nwhich has not been previously studied. Specifically, we propose a\ntemporal-spatial feature alignment module and dual modulation convolution to\nremove coding artifacts and enhance color restoration ability. Furthermore, a\nwavelet attention module is proposed to improve SDRTV features in the frequency\ndomain. An auxiliary loss is introduced to decouple the learning process for\neffectively restoring from dual degradation. The proposed method outperforms\nthe current state-of-the-art method in terms of quantitative results, visual\nquality, and inference times, thus enhancing the performance of the\nSDRTV-to-HDRTV method in real-world scenarios.\n","authors":["Kepeng Xu","Gang He","Li Xu","Xingchao Yang","Ming Sun","Yuzhi Wang","Zijia Ma","Haoqiang Fan","Xing Wen"],"pdf_url":"https://arxiv.org/pdf/2307.03394v1.pdf","comment":"10 pages"}]},"2023-07-06T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2307.03313v1","updated":"2023-07-06T21:55:15Z","published":"2023-07-06T21:55:15Z","title":"InfoSync: Information Synchronization across Multilingual\n  Semi-structured Tables","summary":"  Information Synchronization of semi-structured data across languages is\nchallenging. For instance, Wikipedia tables in one language should be\nsynchronized across languages. To address this problem, we introduce a new\ndataset InfoSyncC and a two-step method for tabular synchronization. InfoSync\ncontains 100K entity-centric tables (Wikipedia Infoboxes) across 14 languages,\nof which a subset (3.5K pairs) are manually annotated. The proposed method\nincludes 1) Information Alignment to map rows and 2) Information Update for\nupdating missing/outdated information for aligned tables across multilingual\ntables. When evaluated on InfoSync, information alignment achieves an F1 score\nof 87.91 (en <-> non-en). To evaluate information updation, we perform\nhuman-assisted Wikipedia edits on Infoboxes for 603 table pairs. Our approach\nobtains an acceptance rate of 77.28% on Wikipedia, showing the effectiveness of\nthe proposed method.\n","authors":["Siddharth Khincha","Chelsi Jain","Vivek Gupta","Tushar Kataria","Shuo Zhang"],"pdf_url":"https://arxiv.org/pdf/2307.03313v1.pdf","comment":"22 pages, 7 figures, 20 tables, ACL 2023 (Toronto, Canada)"},{"id":"http://arxiv.org/abs/2305.06300v2","updated":"2023-07-06T18:47:02Z","published":"2023-05-10T16:40:52Z","title":"Evaluating Embedding APIs for Information Retrieval","summary":"  The ever-increasing size of language models curtails their widespread\navailability to the community, thereby galvanizing many companies into offering\naccess to large language models through APIs. One particular type, suitable for\ndense retrieval, is a semantic embedding service that builds vector\nrepresentations of input text. With a growing number of publicly available\nAPIs, our goal in this paper is to analyze existing offerings in realistic\nretrieval scenarios, to assist practitioners and researchers in finding\nsuitable services according to their needs. Specifically, we investigate the\ncapabilities of existing semantic embedding APIs on domain generalization and\nmultilingual retrieval. For this purpose, we evaluate these services on two\nstandard benchmarks, BEIR and MIRACL. We find that re-ranking BM25 results\nusing the APIs is a budget-friendly approach and is most effective in English,\nin contrast to the standard practice of employing them as first-stage\nretrievers. For non-English retrieval, re-ranking still improves the results,\nbut a hybrid model with BM25 works best, albeit at a higher cost. We hope our\nwork lays the groundwork for evaluating semantic embedding APIs that are\ncritical in search and more broadly, for information access.\n","authors":["Ehsan Kamalloo","Xinyu Zhang","Odunayo Ogundepo","Nandan Thakur","David Alfonso-Hermelo","Mehdi Rezagholizadeh","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2305.06300v2.pdf","comment":"ACL 2023 Industry Track"},{"id":"http://arxiv.org/abs/2307.03153v1","updated":"2023-07-06T17:29:34Z","published":"2023-07-06T17:29:34Z","title":"MultiVENT: Multilingual Videos of Events with Aligned Natural Text","summary":"  Everyday news coverage has shifted from traditional broadcasts towards a wide\nrange of presentation formats such as first-hand, unedited video footage.\nDatasets that reflect the diverse array of multimodal, multilingual news\nsources available online could be used to teach models to benefit from this\nshift, but existing news video datasets focus on traditional news broadcasts\nproduced for English-speaking audiences. We address this limitation by\nconstructing MultiVENT, a dataset of multilingual, event-centric videos\ngrounded in text documents across five target languages. MultiVENT includes\nboth news broadcast videos and non-professional event footage, which we use to\nanalyze the state of online news videos and how they can be leveraged to build\nrobust, factually accurate models. Finally, we provide a model for complex,\nmultilingual video retrieval to serve as a baseline for information retrieval\nusing MultiVENT.\n","authors":["Kate Sanders","David Etter","Reno Kriz","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2307.03153v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.03591v1","updated":"2023-07-06T16:04:56Z","published":"2023-07-06T16:04:56Z","title":"Structure Guided Multi-modal Pre-trained Transformer for Knowledge Graph\n  Reasoning","summary":"  Multimodal knowledge graphs (MKGs), which intuitively organize information in\nvarious modalities, can benefit multiple practical downstream tasks, such as\nrecommendation systems, and visual question answering. However, most MKGs are\nstill far from complete, which motivates the flourishing of MKG reasoning\nmodels. Recently, with the development of general artificial architectures, the\npretrained transformer models have drawn increasing attention, especially for\nmultimodal scenarios. However, the research of multimodal pretrained\ntransformer (MPT) for knowledge graph reasoning (KGR) is still at an early\nstage. As the biggest difference between MKG and other multimodal data, the\nrich structural information underlying the MKG still cannot be fully leveraged\nin existing MPT models. Most of them only utilize the graph structure as a\nretrieval map for matching images and texts connected with the same entity.\nThis manner hinders their reasoning performances. To this end, we propose the\ngraph Structure Guided Multimodal Pretrained Transformer for knowledge graph\nreasoning, termed SGMPT. Specifically, the graph structure encoder is adopted\nfor structural feature encoding. Then, a structure-guided fusion module with\ntwo different strategies, i.e., weighted summation and alignment constraint, is\nfirst designed to inject the structural information into both the textual and\nvisual features. To the best of our knowledge, SGMPT is the first MPT model for\nmultimodal KGR, which mines the structural information underlying the knowledge\ngraph. Extensive experiments on FB15k-237-IMG and WN18-IMG, demonstrate that\nour SGMPT outperforms existing state-of-the-art models, and prove the\neffectiveness of the designed strategies.\n","authors":["Ke Liang","Sihang Zhou","Yue Liu","Lingyuan Meng","Meng Liu","Xinwang Liu"],"pdf_url":"https://arxiv.org/pdf/2307.03591v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessed"},{"id":"http://arxiv.org/abs/2306.12118v2","updated":"2023-07-06T15:11:20Z","published":"2023-06-21T09:01:53Z","title":"Visualizing Relation Between (De)Motivating Topics and Public Stance\n  toward COVID-19 Vaccine","summary":"  While social media plays a vital role in communication nowadays,\nmisinformation and trolls can easily take over the conversation and steer\npublic opinion on these platforms. We saw the effect of misinformation during\nthe COVID-19 pandemic when public health officials faced significant push-back\nwhile trying to motivate the public to vaccinate. To tackle the current and any\nfuture threats in emergencies and motivate the public towards a common goal, it\nis essential to understand how public motivation shifts and which topics\nresonate among the general population. In this study, we proposed an\ninteractive visualization tool to inspect and analyze the topics that resonated\namong Twitter-sphere during the COVID-19 pandemic and understand the key\nfactors that shifted public stance for vaccination. This tool can easily be\ngeneralized for any scenario for visual analysis and to increase the\ntransparency of social media data for researchers and the general population\nalike.\n","authors":["Ashiqur Rahman","Hamed Alhoori"],"pdf_url":"https://arxiv.org/pdf/2306.12118v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.03045v1","updated":"2023-07-06T15:10:29Z","published":"2023-07-06T15:10:29Z","title":"Track Mix Generation on Music Streaming Services using Transformers","summary":"  This paper introduces Track Mix, a personalized playlist generation system\nreleased in 2022 on the music streaming service Deezer. Track Mix automatically\ngenerates \"mix\" playlists inspired by initial music tracks, allowing users to\ndiscover music similar to their favorite content. To generate these mixes, we\nconsider a Transformer model trained on millions of track sequences from user\nplaylists. In light of the growing popularity of Transformers in recent years,\nwe analyze the advantages, drawbacks, and technical challenges of using such a\nmodel for mix generation on the service, compared to a more traditional\ncollaborative filtering approach. Since its release, Track Mix has been\ngenerating playlists for millions of users daily, enhancing their music\ndiscovery experience on Deezer.\n","authors":["Walid Bendada","Théo Bontempelli","Mathieu Morlon","Benjamin Chapus","Thibault Cador","Thomas Bouabça","Guillaume Salha-Galvan"],"pdf_url":"https://arxiv.org/pdf/2307.03045v1.pdf","comment":"RecSys 2023 - Industry track with oral presentation"},{"id":"http://arxiv.org/abs/2307.03027v1","updated":"2023-07-06T14:44:07Z","published":"2023-07-06T14:44:07Z","title":"Improving Retrieval-Augmented Large Language Models via Data Importance\n  Learning","summary":"  Retrieval augmentation enables large language models to take advantage of\nexternal knowledge, for example on tasks like question answering and data\nimputation. However, the performance of such retrieval-augmented models is\nlimited by the data quality of their underlying retrieval corpus. In this\npaper, we propose an algorithm based on multilinear extension for evaluating\nthe data importance of retrieved data points. There are exponentially many\nterms in the multilinear extension, and one key contribution of this paper is a\npolynomial time algorithm that computes exactly, given a retrieval-augmented\nmodel with an additive utility function and a validation set, the data\nimportance of data points in the retrieval corpus using the multilinear\nextension of the model's utility function. We further proposed an even more\nefficient ({\\epsilon}, {\\delta})-approximation algorithm. Our experimental\nresults illustrate that we can enhance the performance of large language models\nby only pruning or reweighting the retrieval corpus, without requiring further\ntraining. For some tasks, this even allows a small model (e.g., GPT-JT),\naugmented with a search engine API, to outperform GPT-3.5 (without retrieval\naugmentation). Moreover, we show that weights based on multilinear extension\ncan be computed efficiently in practice (e.g., in less than ten minutes for a\ncorpus with 100 million elements).\n","authors":["Xiaozhong Lyu","Stefan Grafberger","Samantha Biegel","Shaopeng Wei","Meng Cao","Sebastian Schelter","Ce Zhang"],"pdf_url":"https://arxiv.org/pdf/2307.03027v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.11182v3","updated":"2023-07-06T13:52:31Z","published":"2023-06-19T22:12:37Z","title":"Co-design Hardware and Algorithm for Vector Search","summary":"  Vector search has emerged as the foundation for large-scale information\nretrieval and machine learning systems, with search engines like Google and\nBing processing tens of thousands of queries per second on petabyte-scale\ndocument datasets by evaluating vector similarities between encoded query texts\nand web documents. As performance demands for vector search systems surge,\naccelerated hardware offers a promising solution in the post-Moore's Law era.\nWe introduce \\textit{FANNS}, an end-to-end and scalable vector search framework\non FPGAs. Given a user-provided recall requirement on a dataset and a hardware\nresource budget, \\textit{FANNS} automatically co-designs hardware and\nalgorithm, subsequently generating the corresponding accelerator. The framework\nalso supports scale-out by incorporating a hardware TCP/IP stack in the\naccelerator. \\textit{FANNS} attains up to 23.0$\\times$ and 37.2$\\times$ speedup\ncompared to FPGA and CPU baselines, respectively, and demonstrates superior\nscalability to GPUs, achieving 5.5$\\times$ and 7.6$\\times$ speedup in median\nand 95\\textsuperscript{th} percentile (P95) latency within an eight-accelerator\nconfiguration. The remarkable performance of \\textit{FANNS} lays a robust\ngroundwork for future FPGA integration in data centers and AI supercomputers.\n","authors":["Wenqi Jiang","Shigang Li","Yu Zhu","Johannes de Fine Licht","Zhenhao He","Runbin Shi","Cedric Renggli","Shuai Zhang","Theodoros Rekatsinas","Torsten Hoefler","Gustavo Alonso"],"pdf_url":"https://arxiv.org/pdf/2306.11182v3.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2307.02936v1","updated":"2023-07-06T12:02:38Z","published":"2023-07-06T12:02:38Z","title":"A Meta-Evaluation of C/W/L/A Metrics: System Ranking Similarity, System\n  Ranking Consistency and Discriminative Power","summary":"  Recently, Moffat et al. proposed an analytic framework, namely C/W/L/A, for\noffline evaluation metrics. This framework allows information retrieval (IR)\nresearchers to design evaluation metrics through the flexible combination of\nuser browsing models and user gain aggregations. However, the statistical\nstability of C/W/L/A metrics with different aggregations is not yet\ninvestigated. In this study, we investigate the statistical stability of\nC/W/L/A metrics from the perspective of: (1) the system ranking similarity\namong aggregations, (2) the system ranking consistency of aggregations and (3)\nthe discriminative power of aggregations. More specifically, we combined\nvarious aggregation functions with the browsing model of Precision, Discounted\nCumulative Gain (DCG), Rank-Biased Precision (RBP), INST, Average Precision\n(AP) and Expected Reciprocal Rank (ERR), examing their performances in terms of\nsystem ranking similarity, system ranking consistency and discriminative power\non two offline test collections. Our experimental result suggests that, in\nterms of system ranking consistency and discriminative power, the aggregation\nfunction of expected rate of gain (ERG) has an outstanding performance while\nthe aggregation function of maximum relevance usually has an insufficient\nperformance. The result also suggests that Precision, DCG, RBP, INST and AP\nwith their canonical aggregation all have favourable performances in system\nranking consistency and discriminative power; but for ERR, replacing its\ncanonical aggregation with ERG can further strengthen the discriminative power\nwhile obtaining a system ranking list similar to the canonical version at the\nsame time.\n","authors":["Nuo Chen","Tetsuya Sakai"],"pdf_url":"https://arxiv.org/pdf/2307.02936v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.05697v1","updated":"2023-07-06T09:25:38Z","published":"2023-07-06T09:25:38Z","title":"A Machine-Learned Ranking Algorithm for Dynamic and Personalised Car\n  Pooling Services","summary":"  Car pooling is expected to significantly help in reducing traffic congestion\nand pollution in cities by enabling drivers to share their cars with travellers\nwith similar itineraries and time schedules. A number of car pooling matching\nservices have been designed in order to efficiently find successful ride\nmatches in a given pool of drivers and potential passengers. However, it is now\nrecognised that many non-monetary aspects and social considerations, besides\nsimple mobility needs, may influence the individual willingness of sharing a\nride, which are difficult to predict. To address this problem, in this study we\npropose GoTogether, a recommender system for car pooling services that\nleverages on learning-to-rank techniques to automatically derive the\npersonalised ranking model of each user from the history of her choices (i.e.,\nthe type of accepted or rejected shared rides). Then, GoTogether builds the\nlist of recommended rides in order to maximise the success rate of the offered\nmatches. To test the performance of our scheme we use real data from Twitter\nand Foursquare sources in order to generate a dataset of plausible mobility\npatterns and ride requests in a metropolitan area. The results show that the\nproposed solution quickly obtain an accurate prediction of the personalised\nuser's choice model both in static and dynamic conditions.\n","authors":["Mattia Giovanni Campana","Franca Delmastro","Raffaele Bruno"],"pdf_url":"https://arxiv.org/pdf/2307.05697v1.pdf","comment":"Accepted from the IEEE 19th International Conference on Intelligent\n  Transportation Systems (ITSC), 2016"},{"id":"http://arxiv.org/abs/2307.02865v1","updated":"2023-07-06T09:04:58Z","published":"2023-07-06T09:04:58Z","title":"PLIERS: a Popularity-Based Recommender System for Content Dissemination\n  in Online Social Networks","summary":"  In this paper, we propose a novel tag-based recommender system called PLIERS,\nwhich relies on the assumption that users are mainly interested in items and\ntags with similar popularity to those they already own. PLIERS is aimed at\nreaching a good tradeoff between algorithmic complexity and the level of\npersonalization of recommended items. To evaluate PLIERS, we performed a set of\nexperiments on real OSN datasets, demonstrating that it outperforms\nstate-of-the-art solutions in terms of personalization, relevance, and novelty\nof recommendations.\n","authors":["Valerio Arnaboldi","Mattia Giovanni Campana","Franca Delmastro","Elena Pagani"],"pdf_url":"https://arxiv.org/pdf/2307.02865v1.pdf","comment":"Published in SAC '16: Proceedings of the 31st Annual ACM Symposium on\n  Applied Computing"},{"id":"http://arxiv.org/abs/2206.13102v2","updated":"2023-07-06T07:24:25Z","published":"2022-06-27T08:16:59Z","title":"Modeling Content Creator Incentives on Algorithm-Curated Platforms","summary":"  Content creators compete for user attention. Their reach crucially depends on\nalgorithmic choices made by developers on online platforms. To maximize\nexposure, many creators adapt strategically, as evidenced by examples like the\nsprawling search engine optimization industry. This begets competition for the\nfinite user attention pool. We formalize these dynamics in what we call an\nexposure game, a model of incentives induced by algorithms, including modern\nfactorization and (deep) two-tower architectures. We prove that seemingly\ninnocuous algorithmic choices, e.g., non-negative vs. unconstrained\nfactorization, significantly affect the existence and character of (Nash)\nequilibria in exposure games. We proffer use of creator behavior models, like\nexposure games, for an (ex-ante) pre-deployment audit. Such an audit can\nidentify misalignment between desirable and incentivized content, and thus\ncomplement post-hoc measures like content filtering and moderation. To this\nend, we propose tools for numerically finding equilibria in exposure games, and\nillustrate results of an audit on the MovieLens and LastFM datasets. Among\nelse, we find that the strategically produced content exhibits strong\ndependence between algorithmic exploration and content diversity, and between\nmodel expressivity and bias towards gender-based user and creator groups.\n","authors":["Jiri Hron","Karl Krauth","Michael I. Jordan","Niki Kilbertus","Sarah Dean"],"pdf_url":"https://arxiv.org/pdf/2206.13102v2.pdf","comment":"presented at ICLR 2023 (top 5%)"},{"id":"http://arxiv.org/abs/2307.02797v1","updated":"2023-07-06T06:12:37Z","published":"2023-07-06T06:12:37Z","title":"BHEISR: Nudging from Bias to Balance -- Promoting Belief Harmony by\n  Eliminating Ideological Segregation in Knowledge-based Recommendations","summary":"  In the realm of personalized recommendation systems, the increasing concern\nis the amplification of belief imbalance and user biases, a phenomenon\nprimarily attributed to the filter bubble. Addressing this critical issue, we\nintroduce an innovative intermediate agency (BHEISR) between users and existing\nrecommendation systems to attenuate the negative repercussions of the filter\nbubble effect in extant recommendation systems. The main objective is to strike\na belief balance for users while minimizing the detrimental influence caused by\nfilter bubbles. The BHEISR model amalgamates principles from nudge theory while\nupholding democratic and transparent principles. It harnesses user-specific\ncategory information to stimulate curiosity, even in areas users might\ninitially deem uninteresting. By progressively stimulating interest in novel\ncategories, the model encourages users to broaden their belief horizons and\nexplore the information they typically overlook. Our model is time-sensitive\nand operates on a user feedback loop. It utilizes the existing recommendation\nalgorithm of the model and incorporates user feedback from the prior time\nframe. This approach endeavors to transcend the constraints of the filter\nbubble, enrich recommendation diversity, and strike a belief balance among\nusers while also catering to user preferences and system-specific business\nrequirements. To validate the effectiveness and reliability of the BHEISR\nmodel, we conducted a series of comprehensive experiments with real-world\ndatasets. These experiments compared the performance of the BHEISR model\nagainst several baseline models using nearly 200 filter bubble-impacted users\nas test subjects. Our experimental results conclusively illustrate the superior\nperformance of the BHEISR model in mitigating filter bubbles and balancing user\nperspectives.\n","authors":["Mengyan Wang","Yuxuan Hu","Zihan Yuan","Chenting Jiang","Weihua Li","Shiqing Wu","Quan Bai"],"pdf_url":"https://arxiv.org/pdf/2307.02797v1.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2307.03679v1","updated":"2023-07-06T04:10:40Z","published":"2023-07-06T04:10:40Z","title":"Undecimated Wavelet Transform for Word Embedded Semantic Marginal\n  Autoencoder in Security improvement and Denoising different Languages","summary":"  By combining the undecimated wavelet transform within a Word Embedded\nSemantic Marginal Autoencoder (WESMA), this research study provides a novel\nstrategy for improving security measures and denoising multiple languages. The\nincorporation of these strategies is intended to address the issues of\nrobustness, privacy, and multilingualism in data processing applications. The\nundecimated wavelet transform is used as a feature extraction tool to identify\nprominent language patterns and structural qualities in the input data. The\nproposed system may successfully capture significant information while\npreserving the temporal and geographical links within the data by employing\nthis transform. This improves security measures by increasing the system's\nability to detect abnormalities, discover hidden patterns, and distinguish\nbetween legitimate content and dangerous threats. The Word Embedded Semantic\nMarginal Autoencoder also functions as an intelligent framework for\ndimensionality and noise reduction. The autoencoder effectively learns the\nunderlying semantics of the data and reduces noise components by exploiting\nword embeddings and semantic context. As a result, data quality and accuracy\nare increased in following processing stages. The suggested methodology is\ntested using a diversified dataset that includes several languages and security\nscenarios. The experimental results show that the proposed approach is\neffective in attaining security enhancement and denoising capabilities across\nmultiple languages. The system is strong in dealing with linguistic variances,\nproducing consistent outcomes regardless of the language used. Furthermore,\nincorporating the undecimated wavelet transform considerably improves the\nsystem's ability to efficiently address complex security concerns\n","authors":["Shreyanth S"],"pdf_url":"https://arxiv.org/pdf/2307.03679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.03206v1","updated":"2023-07-06T04:10:12Z","published":"2023-07-06T04:10:12Z","title":"Optimal Bandwidth Selection for DENCLUE","summary":"  In modern day industry, clustering algorithms are daily routines of algorithm\nengineers. Although clustering algorithms experienced rapid growth before 2010.\nInnovation related to the research topic has stagnated after deep learning\nbecame the de facto industrial standard for machine learning applications. In\n2007, a density-based clustering algorithm named DENCLUE was invented to solve\nclustering problem for nonlinear data structures. However, its parameter\nselection problem was largely neglected until 2011. In this paper, we propose a\nnew approach to compute the optimal parameters for the DENCLUE algorithm, and\ndiscuss its performance in the experiment section.\n","authors":["Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2307.03206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.02761v1","updated":"2023-07-06T03:54:19Z","published":"2023-07-06T03:54:19Z","title":"Cross-Modal Content Inference and Feature Enrichment for Cold-Start\n  Recommendation","summary":"  Multimedia recommendation aims to fuse the multi-modal information of items\nfor feature enrichment to improve the recommendation performance. However,\nexisting methods typically introduce multi-modal information based on\ncollaborative information to improve the overall recommendation precision,\nwhile failing to explore its cold-start recommendation performance. Meanwhile,\nthese above methods are only applicable when such multi-modal data is\navailable. To address this problem, this paper proposes a recommendation\nframework, named Cross-modal Content Inference and Feature Enrichment\nRecommendation (CIERec), which exploits the multi-modal information to improve\nits cold-start recommendation performance. Specifically, CIERec first\nintroduces image annotation as the privileged information to help guide the\nmapping of unified features from the visual space to the semantic space in the\ntraining phase. And then CIERec enriches the content representation with the\nfusion of collaborative, visual, and cross-modal inferred representations, so\nas to improve its cold-start recommendation performance. Experimental results\non two real-world datasets show that the content representations learned by\nCIERec are able to achieve superior cold-start recommendation performance over\nexisting visually-aware recommendation algorithms. More importantly, CIERec can\nconsistently achieve significant improvements with different conventional\nvisually-aware backbones, which verifies its universality and effectiveness.\n","authors":["Haokai Ma","Zhuang Qi","Xinxin Dong","Xiangxian Li","Yuze Zheng","Xiangxu Mengand Lei Meng"],"pdf_url":"https://arxiv.org/pdf/2307.02761v1.pdf","comment":"Accepted by International Joint Conference on Neural Networks 2023\n  (IJCNN)"},{"id":"http://arxiv.org/abs/2307.02759v1","updated":"2023-07-06T03:44:40Z","published":"2023-07-06T03:44:40Z","title":"Knowledge Graph Self-Supervised Rationalization for Recommendation","summary":"  In this paper, we introduce a new self-supervised rationalization method,\ncalled KGRec, for knowledge-aware recommender systems. To effectively identify\ninformative knowledge connections, we propose an attentive knowledge\nrationalization mechanism that generates rational scores for knowledge\ntriplets. With these scores, KGRec integrates generative and contrastive\nself-supervised tasks for recommendation through rational masking. To highlight\nrationales in the knowledge graph, we design a novel generative task in the\nform of masking-reconstructing. By masking important knowledge with high\nrational scores, KGRec is trained to rebuild and highlight useful knowledge\nconnections that serve as rationales. To further rationalize the effect of\ncollaborative interactions on knowledge graph learning, we introduce a\ncontrastive learning task that aligns signals from knowledge and user-item\ninteraction views. To ensure noise-resistant contrasting, potential noisy edges\nin both graphs judged by the rational scores are masked. Extensive experiments\non three real-world datasets demonstrate that KGRec outperforms\nstate-of-the-art methods. We also provide the implementation codes for our\napproach at https://github.com/HKUDS/KGRec.\n","authors":["Yuhao Yang","Chao Huang","Lianghao Xia","Chunzhen Huang"],"pdf_url":"https://arxiv.org/pdf/2307.02759v1.pdf","comment":"Accepted by KDD'23"},{"id":"http://arxiv.org/abs/2307.05680v1","updated":"2023-07-06T02:59:54Z","published":"2023-07-06T02:59:54Z","title":"LogitMat : Zeroshot Learning Algorithm for Recommender Systems without\n  Transfer Learning or Pretrained Models","summary":"  Recommender system is adored in the internet industry as one of the most\nprofitable technologies. Unlike other sectors such as fraud detection in the\nFintech industry, recommender system is both deep and broad. In recent years,\nmany researchers start to focus on the cold-start problem of recommender\nsystems. In spite of the large volume of research literature, the majority of\nthe research utilizes transfer learning / meta learning and pretrained model to\nsolve the problem. Although the researchers claim the effectiveness of the\napproaches, everyone of them does rely on extra input data from other sources.\nIn 2021 and 2022, several zeroshot learning algorithm for recommender system\nsuch as ZeroMat, DotMat, PoissonMat and PowerMat were invented. They are the\nfirst batch of the algorithms that rely on no transfer learning or pretrained\nmodels to tackle the problem. In this paper, we follow this line and invent a\nnew zeroshot learning algorithm named LogitMat. We take advantage of the Zipf\nLaw property of the user item rating values and logistic regression model to\ntackle the cold-start problem and generate competitive results with other\ncompeting techniques. We prove in experiments that our algorithm is fast,\nrobust and effective.\n","authors":["Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2307.05680v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.02740v1","updated":"2023-07-06T02:59:47Z","published":"2023-07-06T02:59:47Z","title":"Dense Retrieval Adaptation using Target Domain Description","summary":"  In information retrieval (IR), domain adaptation is the process of adapting a\nretrieval model to a new domain whose data distribution is different from the\nsource domain. Existing methods in this area focus on unsupervised domain\nadaptation where they have access to the target document collection or\nsupervised (often few-shot) domain adaptation where they additionally have\naccess to (limited) labeled data in the target domain. There also exists\nresearch on improving zero-shot performance of retrieval models with no\nadaptation. This paper introduces a new category of domain adaptation in IR\nthat is as-yet unexplored. Here, similar to the zero-shot setting, we assume\nthe retrieval model does not have access to the target document collection. In\ncontrast, it does have access to a brief textual description that explains the\ntarget domain. We define a taxonomy of domain attributes in retrieval tasks to\nunderstand different properties of a source domain that can be adapted to a\ntarget domain. We introduce a novel automatic data construction pipeline that\nproduces a synthetic document collection, query set, and pseudo relevance\nlabels, given a textual domain description. Extensive experiments on five\ndiverse target domains show that adapting dense retrieval models using the\nconstructed synthetic data leads to effective retrieval performance on the\ntarget domain.\n","authors":["Helia Hashemi","Yong Zhuang","Sachith Sri Ram Kothur","Srivas Prasad","Edgar Meij","W. Bruce Croft"],"pdf_url":"https://arxiv.org/pdf/2307.02740v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2307.03153v1","updated":"2023-07-06T17:29:34Z","published":"2023-07-06T17:29:34Z","title":"MultiVENT: Multilingual Videos of Events with Aligned Natural Text","summary":"  Everyday news coverage has shifted from traditional broadcasts towards a wide\nrange of presentation formats such as first-hand, unedited video footage.\nDatasets that reflect the diverse array of multimodal, multilingual news\nsources available online could be used to teach models to benefit from this\nshift, but existing news video datasets focus on traditional news broadcasts\nproduced for English-speaking audiences. We address this limitation by\nconstructing MultiVENT, a dataset of multilingual, event-centric videos\ngrounded in text documents across five target languages. MultiVENT includes\nboth news broadcast videos and non-professional event footage, which we use to\nanalyze the state of online news videos and how they can be leveraged to build\nrobust, factually accurate models. Finally, we provide a model for complex,\nmultilingual video retrieval to serve as a baseline for information retrieval\nusing MultiVENT.\n","authors":["Kate Sanders","David Etter","Reno Kriz","Benjamin Van Durme"],"pdf_url":"https://arxiv.org/pdf/2307.03153v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.04832v2","updated":"2023-07-06T12:46:49Z","published":"2022-06-10T01:30:15Z","title":"Unifying Multimodal Source and Propagation Graph for Rumour Detection on\n  Social Media with Missing Features","summary":"  With the rapid development of online social media platforms, the spread of\nrumours has become a critical societal concern. Current methods for rumour\ndetection can be categorized into image-text pair classification and\nsource-reply graph classification. In this paper, we propose a novel approach\nthat combines multimodal source and propagation graph features for rumour\nclassification. We introduce the Unified Multimodal Graph Transformer Network\n(UMGTN) which integrates Transformer encoders to fuse these features. Given\nthat not every message in social media is associated with an image and\ncommunity responses in propagation graphs do not immediately follow source\nmessages, our aim is to build a network architecture that handles missing\nfeatures such as images or replies. To enhance the model's robustness to data\nwith missing features, we adopt a multitask learning framework that\nsimultaneously learns representations between samples with complete and missing\nfeatures. We evaluate our proposed method on four real-world datasets,\naugmenting them by recovering images and replies from Twitter and Weibo.\nExperimental results demonstrate that our UMGTN with multitask learning\nachieves state-of-the-art performance, improving F1-score by 1.0% to 4.0%,\nwhile maintaining detection robustness to missing features within 2% accuracy\nand F1-score compared to models trained without the multitask learning\nframework. We have made our models and datasets publicly available at:\nhttps://thcheung.github.io/umgtn/.\n","authors":["Tsun-Hin Cheung","Kin-Man Lam"],"pdf_url":"https://arxiv.org/pdf/2206.04832v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.09817v3","updated":"2023-07-06T07:43:18Z","published":"2023-06-16T12:59:51Z","title":"INDCOR white paper 4: Evaluation of Interactive Narrative Design For\n  Complexity Representations","summary":"  While a strength of Interactive Digital Narratives (IDN) is its support for\nmultiperspectivity, this also poses a substantial challenge to its evaluation.\nMoreover, evaluation has to assess the system's ability to represent a complex\nreality as well as the user's understanding of that complex reality as a result\nof the experience of interacting with the system. This is needed to measure an\nIDN's efficiency and effectiveness in representing the chosen complex\nphenomenon. We here present some empirical methods employed by INDCOR members\nin their research including UX toolkits and scales. Particularly, we consider\nthe impact of IDN on transformative learning and its evaluation through\nself-reporting and other alternatives.\n","authors":["Christian Roth","Breanne Pitt","Lāsma Šķestere","Jonathan Barbara","Agnes Karolina Bakk","Kirsty Dunlop","Maria del Mar Grandio","Miguel Barreda","Despoina Sampatakou","Michael Schlauch"],"pdf_url":"https://arxiv.org/pdf/2306.09817v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2010.10135"},{"id":"http://arxiv.org/abs/2211.04750v2","updated":"2023-07-06T06:14:17Z","published":"2022-11-09T09:01:19Z","title":"Errorless Robust JPEG Steganography using Outputs of JPEG Coders","summary":"  Robust steganography is a technique of hiding secret messages in images so\nthat the message can be recovered after additional image processing. One of the\nmost popular processing operations is JPEG recompression. Unfortunately, most\nof today's steganographic methods addressing this issue only provide a\nprobabilistic guarantee of recovering the secret and are consequently not\nerrorless. That is unacceptable since even a single unexpected change can make\nthe whole message unreadable if it is encrypted. We propose to create a robust\nset of DCT coefficients by inspecting their behavior during recompression,\nwhich requires access to the targeted JPEG compressor. This is done by dividing\nthe DCT coefficients into 64 non-overlapping lattices because one embedding\nchange can potentially affect many other coefficients from the same DCT block\nduring recompression. The robustness is then combined with standard\nsteganographic costs creating a lattice embedding scheme robust against JPEG\nrecompression. Through experiments, we show that the size of the robust set and\nthe scheme's security depends on the ordering of lattices during embedding. We\nverify the validity of the proposed method with three typical JPEG compressors\nand the {\\it Slack} instant messaging application. We benchmark its security\nfor various embedding payloads, three different ways of ordering the lattices,\nand a range of Quality Factors. Finally, this method is errorless by\nconstruction, meaning the embedded message will always be readable.\n","authors":["Jan Butora","Pauline Puteaux","Patrick Bas"],"pdf_url":"https://arxiv.org/pdf/2211.04750v2.pdf","comment":"13 pages, 13 figures, 5 tables, submitted to IEEE Transactions on\n  Dependable and Secure Computing"}]},"2023-07-05T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2307.03691v1","updated":"2023-07-05T23:19:18Z","published":"2023-07-05T23:19:18Z","title":"Comparing Apples to Apples: Generating Aspect-Aware Comparative\n  Sentences from User Review","summary":"  It is time-consuming to find the best product among many similar\nalternatives. Comparative sentences can help to contrast one item from others\nin a way that highlights important features of an item that stand out. Given\nreviews of one or multiple items and relevant item features, we generate\ncomparative review sentences to aid users to find the best fit. Specifically,\nour model consists of three successive components in a transformer: (i) an item\nencoding module to encode an item for comparison, (ii) a comparison generation\nmodule that generates comparative sentences in an autoregressive manner, (iii)\na novel decoding method for user personalization. We show that our pipeline\ngenerates fluent and diverse comparative sentences. We run experiments on the\nrelevance and fidelity of our generated sentences in a human evaluation study\nand find that our algorithm creates comparative review sentences that are\nrelevant and truthful.\n","authors":["Jessica Echterhoff","An Yan","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2307.03691v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.02300v1","updated":"2023-07-05T13:58:26Z","published":"2023-07-05T13:58:26Z","title":"Improving Address Matching using Siamese Transformer Networks","summary":"  Matching addresses is a critical task for companies and post offices involved\nin the processing and delivery of packages. The ramifications of incorrectly\ndelivering a package to the wrong recipient are numerous, ranging from harm to\nthe company's reputation to economic and environmental costs. This research\nintroduces a deep learning-based model designed to increase the efficiency of\naddress matching for Portuguese addresses. The model comprises two parts: (i) a\nbi-encoder, which is fine-tuned to create meaningful embeddings of Portuguese\npostal addresses, utilized to retrieve the top 10 likely matches of the\nun-normalized target address from a normalized database, and (ii) a\ncross-encoder, which is fine-tuned to accurately rerank the 10 addresses\nobtained by the bi-encoder. The model has been tested on a real-case scenario\nof Portuguese addresses and exhibits a high degree of accuracy, exceeding 95%\nat the door level. When utilized with GPU computations, the inference speed is\nabout 4.5 times quicker than other traditional approaches such as BM25. An\nimplementation of this system in a real-world scenario would substantially\nincrease the effectiveness of the distribution process. Such an implementation\nis currently under investigation.\n","authors":["André V. Duarte","Arlindo L. Oliveira"],"pdf_url":"https://arxiv.org/pdf/2307.02300v1.pdf","comment":"To be published in the 22nd EPIA Conference on Artificial\n  Intelligence, EPIA 2023, Faial Island - Azores, Portugal, 5-8 September 2023,\n  Proceedings"},{"id":"http://arxiv.org/abs/2307.02183v1","updated":"2023-07-05T10:22:53Z","published":"2023-07-05T10:22:53Z","title":"An Equivalent Graph Reconstruction Model and its Application in\n  Recommendation Prediction","summary":"  Recommendation algorithm plays an important role in recommendation system\n(RS), which predicts users' interests and preferences for some given items\nbased on their known information. Recently, a recommendation algorithm based on\nthe graph Laplacian regularization was proposed, which treats the prediction\nproblem of the recommendation system as a reconstruction issue of small samples\nof the graph signal under the same graph model. Such a technique takes into\naccount both known and unknown labeled samples information, thereby obtaining\ngood prediction accuracy. However, when the data size is large, solving the\nreconstruction model is computationally expensive even with an approximate\nstrategy. In this paper, we propose an equivalent reconstruction model that can\nbe solved exactly with extremely low computational cost. Finally, a final\nprediction algorithm is proposed. We find in the experiments that the proposed\nmethod significantly reduces the computational cost while maintaining a good\nprediction accuracy.\n","authors":["Guangrui Yang","Lihua Yang","Qing Zhang","Zhihua Yang"],"pdf_url":"https://arxiv.org/pdf/2307.02183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.02157v1","updated":"2023-07-05T09:58:08Z","published":"2023-07-05T09:58:08Z","title":"Generative Job Recommendations with Large Language Model","summary":"  The rapid development of online recruitment services has encouraged the\nutilization of recommender systems to streamline the job seeking process.\nPredominantly, current job recommendations deploy either collaborative\nfiltering or person-job matching strategies. However, these models tend to\noperate as \"black-box\" systems and lack the capacity to offer explainable\nguidance to job seekers. Moreover, conventional matching-based recommendation\nmethods are limited to retrieving and ranking existing jobs in the database,\nrestricting their potential as comprehensive career AI advisors. To this end,\nhere we present GIRL (GeneratIve job Recommendation based on Large language\nmodels), a novel approach inspired by recent advancements in the field of Large\nLanguage Models (LLMs). We initially employ a Supervised Fine-Tuning (SFT)\nstrategy to instruct the LLM-based generator in crafting suitable Job\nDescriptions (JDs) based on the Curriculum Vitae (CV) of a job seeker.\nMoreover, we propose to train a model which can evaluate the matching degree\nbetween CVs and JDs as a reward model, and we use Proximal Policy Optimization\n(PPO)-based Reinforcement Learning (RL) method to further fine-tine the\ngenerator. This aligns the generator with recruiter feedback, tailoring the\noutput to better meet employer preferences. In particular, GIRL serves as a job\nseeker-centric generative model, providing job suggestions without the need of\na candidate set. This capability also enhances the performance of existing job\nrecommendation models by supplementing job seeking features with generated\ncontent. With extensive experiments on a large-scale real-world dataset, we\ndemonstrate the substantial effectiveness of our approach. We believe that GIRL\nintroduces a paradigm-shifting approach to job recommendation systems,\nfostering a more personalized and comprehensive job-seeking experience.\n","authors":["Zhi Zheng","Zhaopeng Qiu","Xiao Hu","Likang Wu","Hengshu Zhu","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2307.02157v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2003.10699v2","updated":"2023-07-05T09:56:30Z","published":"2020-03-24T07:40:33Z","title":"Utilizing Human Memory Processes to Model Genre Preferences for\n  Personalized Music Recommendations","summary":"  In this paper, we introduce a psychology-inspired approach to model and\npredict the music genre preferences of different groups of users by utilizing\nhuman memory processes. These processes describe how humans access information\nunits in their memory by considering the factors of (i) past usage frequency,\n(ii) past usage recency, and (iii) the current context. Using a publicly\navailable dataset of more than a billion music listening records shared on the\nmusic streaming platform Last.fm, we find that our approach provides\nsignificantly better prediction accuracy results than various baseline\nalgorithms for all evaluated user groups, i.e., (i) low-mainstream music\nlisteners, (ii) medium-mainstream music listeners, and (iii) high-mainstream\nmusic listeners. Furthermore, our approach is based on a simple psychological\nmodel, which contributes to the transparency and explainability of the\ncalculated predictions.\n","authors":["Dominik Kowald","Elisabeth Lex","Markus Schedl"],"pdf_url":"https://arxiv.org/pdf/2003.10699v2.pdf","comment":"Dominik Kowald and Elisabeth Lex contributed equally to this work"},{"id":"http://arxiv.org/abs/2307.02147v1","updated":"2023-07-05T09:42:51Z","published":"2023-07-05T09:42:51Z","title":"Recommendation Unlearning via Influence Function","summary":"  Recommendation unlearning is an emerging task to serve users for erasing\nunusable data (e.g., some historical behaviors) from a well-trained recommender\nmodel. Existing methods process unlearning requests by fully or partially\nretraining the model after removing the unusable data. However, these methods\nare impractical due to the high computation cost of full retraining and the\nhighly possible performance damage of partial training. In this light, a\ndesired recommendation unlearning method should obtain a similar model as full\nretraining in a more efficient manner, i.e., achieving complete, efficient and\ninnocuous unlearning. In this work, we propose an Influence Function-based\nRecommendation Unlearning (IFRU) framework, which efficiently updates the model\nwithout retraining by estimating the influence of the unusable data on the\nmodel via the influence function. In the light that recent recommender models\nuse historical data for both the constructions of the optimization loss and the\ncomputational graph (e.g., neighborhood aggregation), IFRU jointly estimates\nthe direct influence of unusable data on optimization loss and the spillover\ninfluence on the computational graph to pursue complete unlearning.\nFurthermore, we propose an importance-based pruning algorithm to reduce the\ncost of the influence function. IFRU is innocuous and applicable to mainstream\ndifferentiable models. Extensive experiments demonstrate that IFRU achieves\nmore than250times acceleration compared to retraining-based methods with\nrecommendation performance comparable to full retraining.\n","authors":["Yang Zhang","Zhiyu Hu","Yimeng Bai","Fuli Feng","Jiancan Wu","Qifan Wang","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2307.02147v1.pdf","comment":"23 pages, 7 figures"},{"id":"http://arxiv.org/abs/2307.02046v1","updated":"2023-07-05T06:03:40Z","published":"2023-07-05T06:03:40Z","title":"Recommender Systems in the Era of Large Language Models (LLMs)","summary":"  With the prosperity of e-commerce and web applications, Recommender Systems\n(RecSys) have become an important component of our daily life, providing\npersonalized suggestions that cater to user preferences. While Deep Neural\nNetworks (DNNs) have made significant advancements in enhancing recommender\nsystems by modeling user-item interactions and incorporating textual side\ninformation, DNN-based methods still face limitations, such as difficulties in\nunderstanding users' interests and capturing textual side information,\ninabilities in generalizing to various recommendation scenarios and reasoning\non their predictions, etc. Meanwhile, the emergence of Large Language Models\n(LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural\nLanguage Processing (NLP) and Artificial Intelligence (AI), due to their\nremarkable abilities in fundamental responsibilities of language understanding\nand generation, as well as impressive generalization and reasoning\ncapabilities. As a result, recent studies have attempted to harness the power\nof LLMs to enhance recommender systems. Given the rapid evolution of this\nresearch direction in recommender systems, there is a pressing need for a\nsystematic overview that summarizes existing LLM-empowered recommender systems,\nto provide researchers in relevant fields with an in-depth understanding.\nTherefore, in this paper, we conduct a comprehensive review of LLM-empowered\nrecommender systems from various aspects including Pre-training, Fine-tuning,\nand Prompting. More specifically, we first introduce representative methods to\nharness the power of LLMs (as a feature encoder) for learning representations\nof users and items. Then, we review recent techniques of LLMs for enhancing\nrecommender systems from three paradigms, namely pre-training, fine-tuning, and\nprompting. Finally, we comprehensively discuss future directions in this\nemerging field.\n","authors":["Wenqi Fan","Zihuai Zhao","Jiatong Li","Yunqing Liu","Xiaowei Mei","Yiqi Wang","Jiliang Tang","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2307.02046v1.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2307.05476v1","updated":"2023-07-05T05:58:56Z","published":"2023-07-05T05:58:56Z","title":"Fisher-Weighted Merge of Contrastive Learning Models in Sequential\n  Recommendation","summary":"  Along with the exponential growth of online platforms and services,\nrecommendation systems have become essential for identifying relevant items\nbased on user preferences. The domain of sequential recommendation aims to\ncapture evolving user preferences over time. To address dynamic preference,\nvarious contrastive learning methods have been proposed to target data\nsparsity, a challenge in recommendation systems due to the limited user-item\ninteractions. In this paper, we are the first to apply the Fisher-Merging\nmethod to Sequential Recommendation, addressing and resolving practical\nchallenges associated with it. This approach ensures robust fine-tuning by\nmerging the parameters of multiple models, resulting in improved overall\nperformance. Through extensive experiments, we demonstrate the effectiveness of\nour proposed methods, highlighting their potential to advance the\nstate-of-the-art in sequential learning and recommendation systems.\n","authors":["Jung Hyun Ryu","Jaeheyoung Jeon","Jewoong Cho","Myungjoo Kang 1"],"pdf_url":"https://arxiv.org/pdf/2307.05476v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2306.17256v2","updated":"2023-07-05T05:43:48Z","published":"2023-06-29T18:50:12Z","title":"Towards Personalized Cold-Start Recommendation with Prompts","summary":"  Recommender systems play a crucial role in helping users discover information\nthat aligns with their interests based on their past behaviors. However,\ndeveloping personalized recommendation systems becomes challenging when\nhistorical records of user-item interactions are unavailable, leading to what\nis known as the system cold-start recommendation problem. This issue is\nparticularly prominent in start-up businesses or platforms with insufficient\nuser engagement history. Previous studies focus on user or item cold-start\nscenarios, where systems could make recommendations for new users or items but\nare still trained with historical user-item interactions in the same domain,\nwhich cannot solve our problem. To bridge the gap, our research introduces an\ninnovative and effective approach, capitalizing on the capabilities of\npre-trained language models. We transform the recommendation process into\nsentiment analysis of natural languages containing information of user profiles\nand item attributes, where the sentiment polarity is predicted with prompt\nlearning. By harnessing the extensive knowledge housed within language models,\nthe prediction can be made without historical user-item interaction records. A\nbenchmark is also introduced to evaluate the proposed method under the\ncold-start setting, and the results demonstrate the effectiveness of our\nmethod. To the best of our knowledge, this is the first study to tackle the\nsystem cold-start recommendation problem. The benchmark and implementation of\nthe method are available at https://github.com/JacksonWuxs/PromptRec.\n","authors":["Xuansheng Wu","Huachi Zhou","Wenlin Yao","Xiao Huang","Ninghao Liu"],"pdf_url":"https://arxiv.org/pdf/2306.17256v2.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2307.02007v1","updated":"2023-07-05T03:32:49Z","published":"2023-07-05T03:32:49Z","title":"Remote Sensing Image Change Detection with Graph Interaction","summary":"  Modern remote sensing image change detection has witnessed substantial\nadvancements by harnessing the potent feature extraction capabilities of CNNs\nand Transforms.Yet,prevailing change detection techniques consistently\nprioritize extracting semantic features related to significant\nalterations,overlooking the viability of directly interacting with bitemporal\nimage features.In this letter,we propose a bitemporal image graph Interaction\nnetwork for remote sensing change detection,namely BGINet-CD. More\nspecifically,by leveraging the concept of non-local operations and mapping the\nfeatures obtained from the backbone network to the graph structure space,we\npropose a unified self-focus mechanism for bitemporal images.This approach\nenhances the information coupling between the two temporal images while\neffectively suppressing task-irrelevant interference,Based on a streamlined\nbackbone architecture,namely ResNet18,our model demonstrates superior\nperformance compared to other state-of-the-art methods (SOTA) on the GZ CD\ndataset. Moreover,the model exhibits an enhanced trade-off between accuracy and\ncomputational efficiency,further improving its overall effectiveness\n","authors":["Chenglong Liu"],"pdf_url":"https://arxiv.org/pdf/2307.02007v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2011.04106v2","updated":"2023-07-05T03:27:45Z","published":"2020-11-08T23:37:58Z","title":"Ensemble Knowledge Distillation for CTR Prediction","summary":"  Recently, deep learning-based models have been widely studied for\nclick-through rate (CTR) prediction and lead to improved prediction accuracy in\nmany industrial applications. However, current research focuses primarily on\nbuilding complex network architectures to better capture sophisticated feature\ninteractions and dynamic user behaviors. The increased model complexity may\nslow down online inference and hinder its adoption in real-time applications.\nInstead, our work targets at a new model training strategy based on knowledge\ndistillation (KD). KD is a teacher-student learning framework to transfer\nknowledge learned from a teacher model to a student model. The KD strategy not\nonly allows us to simplify the student model as a vanilla DNN model but also\nachieves significant accuracy improvements over the state-of-the-art teacher\nmodels. The benefits thus motivate us to further explore the use of a powerful\nensemble of teachers for more accurate student model training. We also propose\nsome novel techniques to facilitate ensembled CTR prediction, including teacher\ngating and early stopping by distillation loss. We conduct comprehensive\nexperiments against 12 existing models and across three industrial datasets.\nBoth offline and online A/B testing results show the effectiveness of our\nKD-based training strategy.\n","authors":["Jieming Zhu","Jinyang Liu","Weiqi Li","Jincai Lai","Xiuqiang He","Liang Chen","Zibin Zheng"],"pdf_url":"https://arxiv.org/pdf/2011.04106v2.pdf","comment":"Published in CIKM'2020"}],"Multimedia":[{"id":"http://arxiv.org/abs/2303.11473v2","updated":"2023-07-05T20:41:25Z","published":"2023-03-20T22:03:44Z","title":"Sandwiched Video Compression: Efficiently Extending the Reach of\n  Standard Codecs with Neural Wrappers","summary":"  We propose sandwiched video compression -- a video compression system that\nwraps neural networks around a standard video codec. The sandwich framework\nconsists of a neural pre- and post-processor with a standard video codec\nbetween them. The networks are trained jointly to optimize a rate-distortion\nloss function with the goal of significantly improving over the standard codec\nin various compression scenarios. End-to-end training in this setting requires\na differentiable proxy for the standard video codec, which incorporates\ntemporal processing with motion compensation, inter/intra mode decisions, and\nin-loop filtering. We propose differentiable approximations to key video codec\ncomponents and demonstrate that, in addition to providing meaningful\ncompression improvements over the standard codec, the neural codes of the\nsandwich lead to significantly better rate-distortion performance in two\nimportant scenarios.When transporting high-resolution video via low-resolution\nHEVC, the sandwich system obtains 6.5 dB improvements over standard HEVC. More\nimportantly, using the well-known perceptual similarity metric, LPIPS, we\nobserve 30% improvements in rate at the same quality over HEVC. Last but not\nleast, we show that pre- and post-processors formed by very\nmodestly-parameterized, light-weight networks can closely approximate these\nresults.\n","authors":["Berivan Isik","Onur G. Guleryuz","Danhang Tang","Jonathan Taylor","Philip A. Chou"],"pdf_url":"https://arxiv.org/pdf/2303.11473v2.pdf","comment":"Published at the International Conference on Image Processing (ICIP),\n  2023"},{"id":"http://arxiv.org/abs/2307.02457v1","updated":"2023-07-05T17:31:44Z","published":"2023-07-05T17:31:44Z","title":"DeSRA: Detect and Delete the Artifacts of GAN-based Real-World\n  Super-Resolution Models","summary":"  Image super-resolution (SR) with generative adversarial networks (GAN) has\nachieved great success in restoring realistic details. However, it is notorious\nthat GAN-based SR models will inevitably produce unpleasant and undesirable\nartifacts, especially in practical scenarios. Previous works typically suppress\nartifacts with an extra loss penalty in the training phase. They only work for\nin-distribution artifact types generated during training. When applied in\nreal-world scenarios, we observe that those improved methods still generate\nobviously annoying artifacts during inference. In this paper, we analyze the\ncause and characteristics of the GAN artifacts produced in unseen test data\nwithout ground-truths. We then develop a novel method, namely, DeSRA, to Detect\nand then Delete those SR Artifacts in practice. Specifically, we propose to\nmeasure a relative local variance distance from MSE-SR results and GAN-SR\nresults, and locate the problematic areas based on the above distance and\nsemantic-aware thresholds. After detecting the artifact regions, we develop a\nfinetune procedure to improve GAN-based SR models with a few samples, so that\nthey can deal with similar types of artifacts in more unseen real data.\nEquipped with our DeSRA, we can successfully eliminate artifacts from inference\nand improve the ability of SR models to be applied in real-world scenarios. The\ncode will be available at https://github.com/TencentARC/DeSRA.\n","authors":["Liangbin Xie","Xintao Wang","Xiangyu Chen","Gen Li","Ying Shan","Jiantao Zhou","Chao Dong"],"pdf_url":"https://arxiv.org/pdf/2307.02457v1.pdf","comment":"The code and models will be made publicly at\n  https://github.com/TencentARC/DeSRA"},{"id":"http://arxiv.org/abs/2210.14321v3","updated":"2023-07-05T12:25:35Z","published":"2022-10-25T20:25:49Z","title":"Artificial ASMR: A Cyber-Psychological Approach","summary":"  The popularity of Autonomous Sensory Meridian Response (ASMR) has skyrockted\nover the past decade, but scientific studies on what exactly triggered ASMR\neffect remain few and immature, one most commonly acknowledged trigger is that\nASMR clips typically provide rich semantic information. With our attention\ncaught by the common acoustic patterns in ASMR audios, we investigate the\ncorrelation between the cyclic features of audio signals and their\neffectiveness in triggering ASMR effects. A cyber-psychological approach that\ncombines signal processing, artificial intelligence, and experimental\npsychology is taken, with which we are able to quantize ASMR-related acoustic\nfeatures, and therewith synthesize ASMR clips with random cyclic patterns but\nnot delivering identifiably scenarios to the audience, which were proven to be\neffective in triggering ASMR effects.\n","authors":["Zexin Fang","Bin Han","C. Clark Cao","Hans. D. Schotten"],"pdf_url":"https://arxiv.org/pdf/2210.14321v3.pdf","comment":"Accepted by IEEE MLSP 2023"},{"id":"http://arxiv.org/abs/2307.02227v1","updated":"2023-07-05T12:08:56Z","published":"2023-07-05T12:08:56Z","title":"MAE-DFER: Efficient Masked Autoencoder for Self-supervised Dynamic\n  Facial Expression Recognition","summary":"  Dynamic facial expression recognition (DFER) is essential to the development\nof intelligent and empathetic machines. Prior efforts in this field mainly fall\ninto supervised learning paradigm, which is restricted by the limited labeled\ndata in existing datasets. Inspired by recent unprecedented success of masked\nautoencoders (e.g., VideoMAE), this paper proposes MAE-DFER, a novel\nself-supervised method which leverages large-scale self-supervised pre-training\non abundant unlabeled data to advance the development of DFER. Since the\nvanilla Vision Transformer (ViT) employed in VideoMAE requires substantial\ncomputation during fine-tuning, MAE-DFER develops an efficient local-global\ninteraction Transformer (LGI-Former) as the encoder. LGI-Former first\nconstrains self-attention in local spatiotemporal regions and then utilizes a\nsmall set of learnable representative tokens to achieve efficient local-global\ninformation exchange, thus avoiding the expensive computation of global\nspace-time self-attention in ViT. Moreover, in addition to the standalone\nappearance content reconstruction in VideoMAE, MAE-DFER also introduces\nexplicit facial motion modeling to encourage LGI-Former to excavate both static\nappearance and dynamic motion information. Extensive experiments on six\ndatasets show that MAE-DFER consistently outperforms state-of-the-art\nsupervised methods by significant margins, verifying that it can learn powerful\ndynamic facial representations via large-scale self-supervised pre-training.\nBesides, it has comparable or even better performance than VideoMAE, while\nlargely reducing the computational cost (about 38\\% FLOPs). We believe MAE-DFER\nhas paved a new way for the advancement of DFER and can inspire more relavant\nresearch in this field and even other related tasks. Codes and models are\npublicly available at https://github.com/sunlicai/MAE-DFER.\n","authors":["Licai Sun","Zheng Lian","Bin Liu","Jianhua Tao"],"pdf_url":"https://arxiv.org/pdf/2307.02227v1.pdf","comment":"17 pages, 11 figures, 14 tables"}]}}