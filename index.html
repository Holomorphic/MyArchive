<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2023-07-12T00:00:00Z">2023-07-12</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">31</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instruction Mining: High-Quality Instruction Data Selection for Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06290v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06290v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihan Cao, Yanbin Kang, Lichao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models typically undergo two training stages, pretraining and
finetuning. Despite that large-scale pretraining endows the model with strong
capabilities to generate natural language responses, these pretrained models
can still fail to understand human instructions at times. To enhance language
models' ability of interpreting and responding to instructions, instruction
finetuning has emerged as a critical method in this area. Recent studies found
that large language models can be finetuned to perform well even with a small
amount of high-quality instruction-following data. However, the selection of
high-quality datasets for finetuning language models still lacks clear
guidelines to follow. In this paper, we propose InstructMining, a linear rule
for evaluating instruction-following data quality. We formulate InstructMining
using specific natural language indicators. To investigate the relationship
between data quality and these indicators, we further conduct extensive
finetuning experiments. The experiment results are then applied to estimating
parameters in InstructMining. To further investigate its performance, we use
InstructMining to select high-quality data from unseen datasets. Results
demonstrate that InstructMining can help select relatively high-quality samples
from various instruction-following datasets. Compared to models finetuned on
unfiltered datasets, models finetuned on InstructMining selected datasets
perform better on 42.5% cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress. 12 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMBench: Is Your Multi-modal Model an All-around Player? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, Dahua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models have recently achieved remarkable progress,
exhibiting great perception and reasoning abilities concerning visual
information. However, how to effectively evaluate these large vision-language
models remains a major obstacle, hindering future model development.
Traditional benchmarks like VQAv2 or COCO Caption provide quantitative
performance measurements but suffer from a lack of fine-grained ability
assessment and non-robust evaluation metrics. Recent subjective benchmarks,
such as OwlEval, offer comprehensive evaluations of a model's abilities by
incorporating human labor, but they are not scalable and display significant
bias. In response to these challenges, we propose MMBench, a novel
multi-modality benchmark. MMBench methodically develops a comprehensive
evaluation pipeline, primarily comprised of two elements. The first element is
a meticulously curated dataset that surpasses existing similar benchmarks in
terms of the number and variety of evaluation questions and abilities. The
second element introduces a novel CircularEval strategy and incorporates the
use of ChatGPT. This implementation is designed to convert free-form
predictions into pre-defined choices, thereby facilitating a more robust
evaluation of the model's predictions. MMBench is a systematically-designed
objective benchmark for robustly evaluating the various abilities of
vision-language models. We hope MMBench will assist the research community in
better evaluating their models and encourage future advancements in this
domain. Project page: https://opencompass.org.cn/mmbench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ashaar: Automatic Analysis and Generation of Arabic Poetry Using Deep
  Learning Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zaid Alyafeai, Maged S. Al-Shaibani, Moataz Ahmed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Poetry holds immense significance within the cultural and traditional fabric
of any nation. It serves as a vehicle for poets to articulate their emotions,
preserve customs, and convey the essence of their culture. Arabic poetry is no
exception, having played a cherished role in the heritage of the Arabic
community throughout history and maintaining its relevance in the present era.
Typically, comprehending Arabic poetry necessitates the expertise of a linguist
who can analyze its content and assess its quality. This paper presents the
introduction of a framework called \textit{Ashaar}
https://github.com/ARBML/Ashaar, which encompasses a collection of datasets and
pre-trained models designed specifically for the analysis and generation of
Arabic poetry. The pipeline established within our proposed approach
encompasses various aspects of poetry, such as meter, theme, and era
classification. It also incorporates automatic poetry diacritization, enabling
more intricate analyses like automated extraction of the \textit{Arudi} style.
Additionally, we explore the feasibility of generating conditional poetry
through the pre-training of a character-based GPT model. Furthermore, as part
of this endeavor, we provide four datasets: one for poetry generation, another
for diacritization, and two for Arudi-style prediction. These datasets aim to
facilitate research and development in the field of Arabic poetry by enabling
researchers and enthusiasts to delve into the nuances of this rich literary
tradition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathalia Nascimento, Paulo Alencar, Donald Cowan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In autonomic computing, self-adaptation has been proposed as a fundamental
paradigm to manage the complexity of multiagent systems (MASs). This achieved
by extending a system with support to monitor and adapt itself to achieve
specific concerns of interest. Communication in these systems is key given that
in scenarios involving agent interaction, it enhances cooperation and reduces
coordination challenges by enabling direct, clear information exchange.
However, improving the expressiveness of the interaction communication with
MASs is not without challenges. In this sense, the interplay between
self-adaptive systems and effective communication is crucial for future MAS
advancements. In this paper, we propose the integration of large language
models (LLMs) such as GPT-based technologies into multiagent systems. We anchor
our methodology on the MAPE-K model, which is renowned for its robust support
in monitoring, analyzing, planning, and executing system adaptations in
response to dynamic environments. We also present a practical illustration of
the proposed approach, in which we implement and assess a basic MAS-based
application. The approach significantly advances the state-of-the-art of
self-adaptive systems by proposing a new paradigm for MAS self-adaptation of
autonomous systems based on LLM capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, submitted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Portuguese Sign Language Animation with Dynamic Timing and
  Mouthing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06124v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06124v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inês Lacerda, Hugo Nicolau, Luisa Coheur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current signing avatars are often described as unnatural as they cannot
accurately reproduce all the subtleties of synchronized body behaviors of a
human signer. In this paper, we propose a new dynamic approach for transitions
between signs, focusing on mouthing animations for Portuguese Sign Language.
Although native signers preferred animations with dynamic transitions, we did
not find significant differences in comprehension and perceived naturalness
scores. On the other hand, we show that including mouthing behaviors improved
comprehension and perceived naturalness for novice sign language learners.
Results have implications in computational linguistics, human-computer
interaction, and synthetic animation of signing avatars.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VELMA: Verbalization Embodiment of LLM Agents for Vision and Language
  Navigation in Street View 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06082v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06082v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raphael Schumann, Wanrong Zhu, Weixi Feng, Tsu-Jui Fu, Stefan Riezler, William Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incremental decision making in real-world environments is one of the most
challenging tasks in embodied artificial intelligence. One particularly
demanding scenario is Vision and Language Navigation~(VLN) which requires
visual and natural language understanding as well as spatial and temporal
reasoning capabilities. The embodied agent needs to ground its understanding of
navigation instructions in observations of a real-world environment like Street
View. Despite the impressive results of LLMs in other research areas, it is an
ongoing problem of how to best connect them with an interactive visual
environment. In this work, we propose VELMA, an embodied LLM agent that uses a
verbalization of the trajectory and of visual environment observations as
contextual prompt for the next action. Visual information is verbalized by a
pipeline that extracts landmarks from the human written navigation instructions
and uses CLIP to determine their visibility in the current panorama view. We
show that VELMA is able to successfully follow navigation instructions in
Street View with only two in-context examples. We further finetune the LLM
agent on a few thousand examples and achieve 25%-30% relative improvement in
task completion over the previous state-of-the-art for two datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpreting deep embeddings for disease progression clustering <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06060v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06060v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Munoz-Farre, Antonios Poulakakis-Daktylidis, Dilini Mahesha Kothalawala, Andrea Rodriguez-Martinez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel approach for interpreting deep embeddings in the context
of patient clustering. We evaluate our approach on a dataset of participants
with type 2 diabetes from the UK Biobank, and demonstrate clinically meaningful
insights into disease progression patterns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Workshop on Interpretable ML in Healthcare at International
  Conference on Machine Learning (ICML), Honolulu, Hawaii, USA. 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Study on the Appropriate size of the Mongolian general corpus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunsoo Choi, Ganbat Tsend
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study aims to determine the appropriate size of the Mongolian general
corpus. This study used the Heaps function and Type Token Ratio to determine
the appropriate size of the Mongolian general corpus. The sample corpus of
906,064 tokens comprised texts from 10 domains of newspaper politics, economy,
society, culture, sports, world articles and laws, middle and high school
literature textbooks, interview articles, and podcast transcripts. First, we
estimated the Heaps function with this sample corpus. Next, we observed changes
in the number of types and TTR values while increasing the number of tokens by
one million using the estimated Heaps function. As a result of observation, we
found that the TTR value hardly changed when the number of tokens exceeded from
39 to 42 million. Thus, we conclude that an appropriate size for a Mongolian
general corpus is from 39 to 42 million tokens.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pluggable Neural Machine Translation Models via Memory-augmented
  Adapters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06029v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06029v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhuang Xu, Shuo Wang, Peng Li, Xuebo Liu, Xiaolong Wang, Weidong Liu, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although neural machine translation (NMT) models perform well in the general
domain, it remains rather challenging to control their generation behavior to
satisfy the requirement of different users. Given the expensive training cost
and the data scarcity challenge of learning a new model from scratch for each
user requirement, we propose a memory-augmented adapter to steer pretrained NMT
models in a pluggable manner. Specifically, we construct a multi-granular
memory based on the user-provided text samples and propose a new adapter
architecture to combine the model representations and the retrieved results. We
also propose a training strategy using memory dropout to reduce spurious
dependencies between the NMT model and the memory. We validate our approach on
both style- and domain-specific experiments and the results indicate that our
method can outperform several representative pluggable baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PolyLM: An Open Source Polyglot Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangpeng Wei, Haoran Wei, Huan Lin, Tianhao Li, Pei Zhang, Xingzhang Ren, Mei Li, Yu Wan, Zhiwei Cao, Binbin Xie, Tianxiang Hu, Shangjie Li, Binyuan Hui, Bowen Yu, Dayiheng Liu, Baosong Yang, Fei Huang, Jun Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) demonstrate remarkable ability to comprehend,
reason, and generate following nature language instructions. However, the
development of LLMs has been primarily focused on high-resource languages, such
as English, thereby limiting their applicability and research in other
languages. Consequently, we present PolyLM, a multilingual LLM trained on 640
billion (B) tokens, avaliable in two model sizes: 1.7B and 13B. To enhance its
multilingual capabilities, we 1) integrate bilingual data into training data;
and 2) adopt a curriculum learning strategy that increases the proportion of
non-English data from 30% in the first stage to 60% in the final stage during
pre-training. Further, we propose a multilingual self-instruct method which
automatically generates 132.7K diverse multilingual instructions for model
fine-tuning. To assess the model's performance, we collect several existing
multilingual tasks, including multilingual understanding, question answering,
generation, and translation. Extensive experiments show that PolyLM surpasses
other open-source models such as LLaMA and BLOOM on multilingual tasks while
maintaining comparable performance in English. Our models, alone with the
instruction data and multilingual benchmark, are available at:
\url{https://modelscope.cn/models/damo/nlp_polylm_13b_text_generation}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DDNAS: Discretized Differentiable Neural Architecture Search for Text
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuan-Chun Chen, Cheng-Te Li, Kuo-Jung Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Architecture Search (NAS) has shown promising capability in learning
text representation. However, existing text-based NAS neither performs a
learnable fusion of neural operations to optimize the architecture, nor encodes
the latent hierarchical categorization behind text input. This paper presents a
novel NAS method, Discretized Differentiable Neural Architecture Search
(DDNAS), for text representation learning and classification. With the
continuous relaxation of architecture representation, DDNAS can use gradient
descent to optimize the search. We also propose a novel discretization layer
via mutual information maximization, which is imposed on every search node to
model the latent hierarchical categorization in text representation. Extensive
experiments conducted on eight diverse real datasets exhibit that DDNAS can
consistently outperform the state-of-the-art NAS methods. While DDNAS relies on
only three basic operations, i.e., convolution, pooling, and none, to be the
candidates of NAS building blocks, its promising performance is noticeable and
extensible to obtain further improvement by adding more different operations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM Trans. Intell. Syst. Technol. (TIST) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VoxPoser: Composable 3D Value Maps for Robotic Manipulation with
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, Li Fei-Fei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are shown to possess a wealth of actionable
knowledge that can be extracted for robot manipulation in the form of reasoning
and planning. Despite the progress, most still rely on pre-defined motion
primitives to carry out the physical interactions with the environment, which
remains a major bottleneck. In this work, we aim to synthesize robot
trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a
large variety of manipulation tasks given an open-set of instructions and an
open-set of objects. We achieve this by first observing that LLMs excel at
inferring affordances and constraints given a free-form language instruction.
More importantly, by leveraging their code-writing capabilities, they can
interact with a visual-language model (VLM) to compose 3D value maps to ground
the knowledge into the observation space of the agent. The composed value maps
are then used in a model-based planning framework to zero-shot synthesize
closed-loop robot trajectories with robustness to dynamic perturbations. We
further demonstrate how the proposed framework can benefit from online
experiences by efficiently learning a dynamics model for scenes that involve
contact-rich interactions. We present a large-scale study of the proposed
method in both simulated and real-robot environments, showcasing the ability to
perform a large variety of everyday manipulation tasks specified in free-form
natural language. Project website: https://voxposer.github.io
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Distilled Quantization: Achieving High Compression Rates in
  <span class="highlight-title">Transformer</span>-Based Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05972v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05972v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James O' Neill, Sourav Dutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the effects of post-training quantization and
quantization-aware training on the generalization of Transformer language
models. We present a new method called self-distilled quantization (SDQ) that
minimizes accumulative quantization errors and outperforms baselines. We apply
SDQ to multilingual models XLM-R-Base and InfoXLM-Base and demonstrate that
both models can be reduced from 32-bit floating point weights to 8-bit integer
weights while maintaining a high level of performance on the XGLUE benchmark.
Our results also highlight the challenges of quantizing multilingual models,
which must generalize to languages they were not fine-tuned on.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prototypical Contrastive Transfer Learning for Multimodal Language
  Understanding <span class="chip">IROS23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05942v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05942v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seitaro Otsuki, Shintaro Ishikawa, Komei Sugiura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although domestic service robots are expected to assist individuals who
require support, they cannot currently interact smoothly with people through
natural language. For example, given the instruction "Bring me a bottle from
the kitchen," it is difficult for such robots to specify the bottle in an
indoor environment. Most conventional models have been trained on real-world
datasets that are labor-intensive to collect, and they have not fully leveraged
simulation data through a transfer learning framework. In this study, we
propose a novel transfer learning approach for multimodal language
understanding called Prototypical Contrastive Transfer Learning (PCTL), which
uses a new contrastive loss called Dual ProtoNCE. We introduce PCTL to the task
of identifying target objects in domestic environments according to free-form
natural language instructions. To validate PCTL, we built new real-world and
simulation datasets. Our experiment demonstrated that PCTL outperformed
existing methods. Specifically, PCTL achieved an accuracy of 78.1%, whereas
simple fine-tuning achieved an accuracy of 73.4%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at IROS23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM
  Decoding <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05908v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05908v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seongjun Yang, Gibbeum Lee, Jaewoong Cho, Dimitris Papailiopoulos, Kangwook Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents "Predictive Pipelined Decoding (PPD)," an approach that
speeds up greedy decoding in Large Language Models (LLMs) while maintaining the
exact same output as the original decoding. Unlike conventional strategies, PPD
employs additional compute resources to parallelize the initiation of
subsequent token decoding during the current token decoding. This innovative
method reduces decoding latency and reshapes the understanding of trade-offs in
LLM decoding strategies. We have developed a theoretical framework that allows
us to analyze the trade-off between computation and latency. Using this
framework, we can analytically estimate the potential reduction in latency
associated with our proposed method, achieved through the assessment of the
match rate, represented as p_correct. The results demonstrate that the use of
extra computational resources has the potential to accelerate LLM greedy
decoding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ES-FoMo Workshop at ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contextualized End-to-End Speech Recognition with Contextual Phrase
  Prediction Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12493v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12493v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaixun Huang, Ao Zhang, Zhanheng Yang, Pengcheng Guo, Bingshen Mu, Tianyi Xu, Lei Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contextual information plays a crucial role in speech recognition
technologies and incorporating it into the end-to-end speech recognition models
has drawn immense interest recently. However, previous deep bias methods lacked
explicit supervision for bias tasks. In this study, we introduce a contextual
phrase prediction network for an attention-based deep bias method. This network
predicts context phrases in utterances using contextual embeddings and
calculates bias loss to assist in the training of the contextualized model. Our
method achieved a significant word error rate (WER) reduction across various
end-to-end speech recognition models. Experiments on the LibriSpeech corpus
show that our proposed model obtains a 12.1% relative WER improvement over the
baseline model, and the WER of the context phrases decreases relatively by
40.5%. Moreover, by applying a context phrase filtering strategy, we also
effectively eliminate the WER degradation when using a larger biasing list.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by interspeech2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Human-Language Model Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09746v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09746v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mina Lee, Megha Srivastava, Amelia Hardy, John Thickstun, Esin Durmus, Ashwin Paranjape, Ines Gerard-Ursin, Xiang Lisa Li, Faisal Ladhak, Frieda Rong, Rose E. Wang, Minae Kwon, Joon Sung Park, Hancheng Cao, Tony Lee, Rishi Bommasani, Michael Bernstein, Percy Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many real-world applications of language models (LMs), such as writing
assistance and code autocomplete, involve human-LM interaction. However, most
benchmarks are non-interactive in that a model produces output without human
involvement. To evaluate human-LM interaction, we develop a new framework,
Human-AI Language-based Interaction Evaluation (HALIE), that defines the
components of interactive systems and dimensions to consider when designing
evaluation metrics. Compared to standard, non-interactive evaluation, HALIE
captures (i) the interactive process, not only the final output; (ii) the
first-person subjective experience, not just a third-party assessment; and
(iii) notions of preference beyond quality (e.g., enjoyment and ownership). We
then design five tasks to cover different forms of interaction: social
dialogue, question answering, crossword puzzles, summarization, and metaphor
generation. With four state-of-the-art LMs (three variants of OpenAI's GPT-3
and AI21 Labs' Jurassic-1), we find that better non-interactive performance
does not always translate to better human-LM interaction. In particular, we
highlight three cases where the results from non-interactive and interactive
metrics diverge and underscore the importance of human-LM interaction for LM
evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Local Grammar-Based Coding Revisited 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.13636v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.13636v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Łukasz Dębowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We revisit the problem of minimal local grammar-based coding. In this
setting, the local grammar encoder encodes grammars symbol by symbol, whereas
the minimal grammar transform minimizes the grammar length in a preset class of
grammars as given by the length of local grammar encoding. It has been known
that such minimal codes are strongly universal for a strictly positive entropy
rate, whereas the number of rules in the minimal grammar constitutes an upper
bound for the mutual information of the source. Whereas the fully minimal code
is likely intractable, the constrained minimal block code can be efficiently
computed. In this article, we present a new, simpler, and more general proof of
strong universality of the minimal block code, regardless of the entropy rate.
The proof is based on a simple Zipfian bound for ranked probabilities. By the
way, we also show empirically that the number of rules in the minimal block
code cannot clearly discriminate between long-memory and memoryless sources,
such as a text in English and a random permutation of its characters. This
contradicts our previous expectations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Evaluation of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03109v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03109v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, Xing Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are gaining increasing popularity in both
academia and industry, owing to their unprecedented performance in various
applications. As LLMs continue to play a vital role in both research and daily
use, their evaluation becomes increasingly critical, not only at the task
level, but also at the society level for better understanding of their
potential risks. Over the past years, significant efforts have been made to
examine LLMs from various perspectives. This paper presents a comprehensive
review of these evaluation methods for LLMs, focusing on three key dimensions:
what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide
an overview from the perspective of evaluation tasks, encompassing general
natural language processing tasks, reasoning, medical usage, ethics,
educations, natural and social sciences, agent applications, and other areas.
Secondly, we answer the `where' and `how' questions by diving into the
evaluation methods and benchmarks, which serve as crucial components in
assessing performance of LLMs. Then, we summarize the success and failure cases
of LLMs in different tasks. Finally, we shed light on several future challenges
that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to
researchers in the realm of LLMs evaluation, thereby aiding the development of
more proficient LLMs. Our key point is that evaluation should be treated as an
essential discipline to better assist the development of LLMs. We consistently
maintain the related open-source materials at:
https://github.com/MLGroupJLU/LLM-eval-survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages; code is at https://github.com/MLGroupJLU/LLM-eval-survey</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03042v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03042v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aryo Pradipta Gema, Luke Daines, Pasquale Minervini, Beatrice Alex
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adapting pretrained language models to novel domains, such as clinical
applications, traditionally involves retraining their entire set of parameters.
However, this approach is increasingly proven to be impractical owing to the
substantial computational requirements associated with training such large
language models. To address this issue, Parameter-Efficient Fine-Tuning (PEFT)
techniques offer a viable solution by selectively fine-tuning a small subset of
additional parameters, significantly reducing the computational requirements
for domain adaptation. In this study, we propose Clinical LLaMA-LoRA, a PEFT
adapter layer built upon the open-sourced LLaMA model. Clinical LLaMA-LoRA is
trained using clinical notes obtained from the MIMIC-IV database, thereby
creating a specialised adapter designed for the clinical domain. Additionally,
we propose a two-step PEFT framework which fuses Clinical LLaMA-LoRA with
Downstream LLaMA-LoRA, another PEFT adapter specialised for downstream tasks.
We evaluate this framework on multiple clinical outcome prediction datasets,
comparing it to clinically trained language models. Our proposed framework
achieves a state-of-the-art AUROC score averaged across all clinical downstream
tasks. We observe substantial improvements of 6-9% AUROC score in the
large-scale multilabel classification tasks, such as diagnoses and procedures
classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Computational Modeling of Meaning: Embodied Cognition Intertwined
  with Emotion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04518v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04518v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Casey Kennington
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This document chronicles this author's attempt to explore how words come to
mean what they do, with a particular focus on child language acquisition and
what that means for models of language understanding.\footnote{I say
\emph{historical} because I synthesize the ideas based on when I discovered
them and how those ideas influenced my later thinking.} I explain the setting
for child language learning, how embodiment -- being able to perceive and enact
in the world, including knowledge of concrete and abstract concepts -- is
crucial, and how emotion and cognition relate to each other and the language
learning process. I end with what I think are some of the requirements for a
language-learning agent that learns language in a setting similar to that of
children. This paper can act as a potential guide for ongoing and future work
in modeling language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpeechBlender: Speech Augmentation Framework for Mispronunciation Data
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.00923v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.00923v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yassine El Kheir, Shammur Absar Chowdhury, Ahmed Ali, Hamdy Mubarak, Shazia Afzal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The lack of labeled second language (L2) speech data is a major challenge in
designing mispronunciation detection models. We introduce SpeechBlender - a
fine-grained data augmentation pipeline for generating mispronunciation errors
to overcome such data scarcity. The SpeechBlender utilizes varieties of masks
to target different regions of phonetic units, and use the mixing factors to
linearly interpolate raw speech signals while augmenting pronunciation. The
masks facilitate smooth blending of the signals, generating more effective
samples than the `Cut/Paste' method. Our proposed technique achieves
state-of-the-art results, with Speechocean762, on ASR dependent
mispronunciation detection models at phoneme level, with a 2.0% gain in Pearson
Correlation Coefficient (PCC) compared to the previous state-of-the-art [1].
Additionally, we demonstrate a 5.0% improvement at the phoneme level compared
to our baseline. We also observed a 4.6% increase in F1-score with Arabic
AraVoiceL2 testset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ANTONIO: Towards a Systematic Method of Generating NLP Benchmarks for
  Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.04003v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.04003v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Casadio, Luca Arnaboldi, Matthew L. Daggitt, Omri Isac, Tanvi Dinkar, Daniel Kienitz, Verena Rieser, Ekaterina Komendantskaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Verification of machine learning models used in Natural Language Processing
(NLP) is known to be a hard problem. In particular, many known neural network
verification methods that work for computer vision and other numeric datasets
do not work for NLP. Here, we study technical reasons that underlie this
problem. Based on this analysis, we propose practical methods and heuristics
for preparing NLP datasets and models in a way that renders them amenable to
known verification methods based on abstract interpretation. We implement these
methods as a Python library called ANTONIO that links to the neural network
verifiers ERAN and Marabou. We perform evaluation of the tool using an NLP
dataset R-U-A-Robot suggested as a benchmark for verifying legally critical NLP
applications. We hope that, thanks to its general applicability, this work will
open novel possibilities for including NLP verification problems into neural
network verification competitions, and will popularise NLP problems within this
community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in proceedings of 6th Workshop on Formal Methods for
  ML-Enabled Autonomous Systems (Affiliated with CAV 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language-Specific Representation of Emotion-Concept Knowledge Causally
  Supports Emotion Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.09582v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.09582v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Li, Yusheng Su, Hsiu-Yuan Huang, Jiali Cheng, Xin Hu, Xinmiao Zhang, Huadong Wang, Yujia Qin, Xiaozhi Wang, Zhiyuan Liu, Dan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding how language supports emotion inference remains a topic of
debate in emotion science. The present study investigated whether
language-derived emotion-concept knowledge would causally support emotion
inference by manipulating the language-specific knowledge representations in
large language models. Using the prompt technique, 14 attributes of emotion
concepts were found to be represented by distinct artificial neuron
populations. By manipulating these attribute-related neurons, the majority of
the emotion inference tasks showed performance deterioration compared to random
manipulations. The attribute-specific performance deterioration was related to
the importance of different attributes in human mental space. Our findings
provide causal evidence in support of a language-based mechanism for emotion
inference and highlight the contributions of emotion-concept knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 13 figures, 2 tables, major revisions over previous
  versions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KIT's Multilingual Speech Translation System for IWSLT 2023 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05320v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05320v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danni Liu, Thai Binh Nguyen, Sai Koneru, Enes Yavuz Ugan, Ngoc-Quan Pham, Tuan-Nam Nguyen, Tu Anh Dinh, Carlos Mullov, Alexander Waibel, Jan Niehues
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many existing speech translation benchmarks focus on native-English speech in
high-quality recording conditions, which often do not match the conditions in
real-life use-cases. In this paper, we describe our speech translation system
for the multilingual track of IWSLT 2023, which evaluates translation quality
on scientific conference talks. The test condition features accented input
speech and terminology-dense contents. The task requires translation into 10
languages of varying amounts of resources. In absence of training data from the
target domain, we use a retrieval-based approach (kNN-MT) for effective
adaptation (+0.8 BLEU for speech translation). We also use adapters to easily
integrate incremental training data from data augmentation, and show that it
matches the performance of re-training. We observe that cascaded systems are
more easily adaptable towards specific target domains, due to their separate
modules. Our cascaded speech system substantially outperforms its end-to-end
counterpart on scientific talk translation, although their performance remains
similar on TED talks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IWSLT 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do CoNLL-2003 Named Entity Taggers Still Work Well in 2023? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09747v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09747v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuheng Liu, Alan Ritter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The CoNLL-2003 English named entity recognition (NER) dataset has been widely
used to train and evaluate NER models for almost 20 years. However, it is
unclear how well models that are trained on this 20-year-old data and developed
over a period of decades using the same test set will perform when applied on
modern data. In this paper, we evaluate the generalization of over 20 different
models trained on CoNLL-2003, and show that NER models have very different
generalization. Surprisingly, we find no evidence of performance degradation in
pre-trained Transformers, such as RoBERTa and T5, even when fine-tuned using
decades-old data. We investigate why some models generalize well to new data
while others do not, and attempt to disentangle the effects of temporal drift
and overfitting due to test reuse. Our analysis suggests that most
deterioration is due to temporal mismatch between the pre-training corpora and
the downstream test sets. We found that four factors are important for good
generalization: model architecture, number of parameters, time period of the
pre-training corpus, in addition to the amount of fine-tuning data. We suggest
current evaluation methods have, in some sense, underestimated progress on NER
over the past 20 years, as NER models have not only improved on the original
CoNLL-2003 test set, but improved even more on modern data. Our datasets can be
found at https://github.com/ShuhengL/acl2023_conllpp.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pattern<span class="highlight-title">GPT</span> :A Pattern-Driven Framework for Large Language Model Text
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.00470v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.00470v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Le Xiao, Xin Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models(LLMS) have shown excellent text generation
capabilities,capable of generating fluent responses for many downstream tasks.
However,applying large language models to real-world critical tasks remains
challenging due to their susceptibility to hallucinations and inability to
directly use external knowledge. To address the above challenges,this paper
proposes PatternGPT, a pattern-driven text generation framework for large
language models. First,the framework utilizes the extraction capabilities of
large language models to generate rich and diverse patterns and later draws on
the idea of federated learning. Using multiple agents to achieve sharing to
obtain more diverse patterns. Finally, it searches for high-quality patterns
using judgment criteria and optimization algorithms and uses the searched
patterns to guide the model for generation. This framework has the advantages
of generating diversified patterns, protecting data privacy,combining external
knowledge, and improving the quality of generation, which provides an effective
method to optimize the text generation capability of large language models,and
make it better applied to the field of intelligent dialogue and content
generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating the Effectiveness of Chat<span class="highlight-title">GPT</span> in Mathematical Reasoning and
  Problem Solving: Evidence from the Vietnamese National High School Graduation
  Examination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan-Quy Dao, Ngoc-Bich Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study offers a complete analysis of ChatGPT's mathematics abilities in
responding to multiple-choice questions for the Vietnamese National High School
Graduation Examination (VNHSGE) on a range of subjects and difficulty levels.
The dataset included 250 questions divided into four levels: knowledge (K),
comprehension (C), application (A), and high application (H), and it included
ten themes that covered diverse mathematical concepts. The outcomes demonstrate
that ChatGPT's performance varies depending on the difficulty level and
subject. It performed best on questions at Level (K), with an accuracy rate of
$83\%$; but, as the difficulty level rose, it scored poorly, with an accuracy
rate of $10\%$. The study has also shown that ChatGPT significantly succeeds in
providing responses to questions on subjects including exponential and
logarithmic functions, geometric progression, and arithmetic progression. The
study found that ChatGPT had difficulty correctly answering questions on topics
including derivatives and applications, spatial geometry, and Oxyz spatial
calculus. Additionally, this study contrasted ChatGPT outcomes with Vietnamese
students in VNHSGE and in other math competitions. ChatGPT dominated in the SAT
Math competition with a success rate of $70\%$, followed by VNHSGE mathematics
($58.8\%)$. However, its success rates were lower on other exams, such as AP
Statistics, the GRE Quantitative, AMC 10, AMC 12, and AP Calculus BC. These
results suggest that ChatGPT has the potential to be an effective teaching tool
for mathematics, but more work is needed to enhance its handling of graphical
data and address the challenges presented by questions that are getting more
challenging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 13 images</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Judging LLM-as-a-judge with MT-Bench and Chatbot Arena 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05685v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05685v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating large language model (LLM) based chat assistants is challenging
due to their broad capabilities and the inadequacy of existing benchmarks in
measuring human preferences. To address this, we explore using strong LLMs as
judges to evaluate these models on more open-ended questions. We examine the
usage and limitations of LLM-as-a-judge, including position, verbosity, and
self-enhancement biases, as well as limited reasoning ability, and propose
solutions to mitigate some of them. We then verify the agreement between LLM
judges and human preferences by introducing two benchmarks: MT-bench, a
multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our
results reveal that strong LLM judges like GPT-4 can match both controlled and
crowdsourced human preferences well, achieving over 80\% agreement, the same
level of agreement between humans. Hence, LLM-as-a-judge is a scalable and
explainable way to approximate human preferences, which are otherwise very
expensive to obtain. Additionally, we show our benchmark and traditional
benchmarks complement each other by evaluating several variants of LLaMA and
Vicuna. We will publicly release MT-bench questions, 3K expert votes, and 30K
conversations with human preferences from Chatbot Arena.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MGR: Multi-generator Based Rationalization <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.04492v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.04492v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Liu, Haozhao Wang, Jun Wang, Ruixuan Li, Xinyang Li, Yuankai Zhang, Yang Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rationalization is to employ a generator and a predictor to construct a
self-explaining NLP model in which the generator selects a subset of
human-intelligible pieces of the input text to the following predictor.
However, rationalization suffers from two key challenges, i.e., spurious
correlation and degeneration, where the predictor overfits the spurious or
meaningless pieces solely selected by the not-yet well-trained generator and in
turn deteriorates the generator. Although many studies have been proposed to
address the two challenges, they are usually designed separately and do not
take both of them into account. In this paper, we propose a simple yet
effective method named MGR to simultaneously solve the two problems. The key
idea of MGR is to employ multiple generators such that the occurrence stability
of real pieces is improved and more meaningful pieces are delivered to the
predictor. Empirically, we show that MGR improves the F1 score by up to 20.9%
as compared to state-of-the-art methods. Codes are available at
https://github.com/jugechengzi/Rationalization-MGR .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023, oral presentation. arXiv admin note: text overlap with
  arXiv:2209.08285</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synthetic <span class="highlight-title">Dataset</span> for Evaluating Complex Compositional Knowledge for
  Natural Language Inference <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05034v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05034v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sushma Anand Akoju, Robert Vacareanu, Haris Riaz, Eduardo Blanco, Mihai Surdeanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a synthetic dataset called Sentences Involving Complex
Compositional Knowledge (SICCK) and a novel analysis that investigates the
performance of Natural Language Inference (NLI) models to understand
compositionality in logic. We produce 1,304 sentence pairs by modifying 15
examples from the SICK dataset (Marelli et al., 2014). To this end, we modify
the original texts using a set of phrases - modifiers that correspond to
universal quantifiers, existential quantifiers, negation, and other concept
modifiers in Natural Logic (NL) (MacCartney, 2009). We use these phrases to
modify the subject, verb, and object parts of the premise and hypothesis.
Lastly, we annotate these modified texts with the corresponding entailment
labels following NL rules. We conduct a preliminary verification of how well
the change in the structural and semantic composition is captured by neural NLI
models, in both zero-shot and fine-tuned scenarios. We found that the
performance of NLI models under the zero-shot setting is poor, especially for
modified sentences with negation and existential quantifiers. After fine-tuning
this dataset, we observe that models continue to perform poorly over negation,
existential and universal modifiers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Natural Language Reasoning and Structured Explanations
  (NLRSE) Workshop, ACL 2023. For dataset, please refer
  https://github.com/clulab/releases/tree/master/acl2023-nlrse-sicck and
  https://github.com/sushmaakoju/natural-logic</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">77</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Free-Viewpoint Relighting for Glossy Indirect Illumination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nithin Raghavan, Yan Xiao, Kai-En Lin, Tiancheng Sun, Sai Bi, Zexiang Xu, Tzu-Mao Li, Ravi Ramamoorthi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precomputed Radiance Transfer (PRT) remains an attractive solution for
real-time rendering of complex light transport effects such as glossy global
illumination. After precomputation, we can relight the scene with new
environment maps while changing viewpoint in real-time. However, practical PRT
methods are usually limited to low-frequency spherical harmonic lighting.
All-frequency techniques using wavelets are promising but have so far had
little practical impact. The curse of dimensionality and much higher data
requirements have typically limited them to relighting with fixed view or only
direct lighting with triple product integrals. In this paper, we demonstrate a
hybrid neural-wavelet PRT solution to high-frequency indirect illumination,
including glossy reflection, for relighting with changing view. Specifically,
we seek to represent the light transport function in the Haar wavelet basis.
For global illumination, we learn the wavelet transport using a small
multi-layer perceptron (MLP) applied to a feature field as a function of
spatial location and wavelet index, with reflected direction and material
parameters being other MLP inputs. We optimize/learn the feature field
(compactly represented by a tensor decomposition) and MLP parameters from
multiple images of the scene under different lighting and viewing conditions.
We demonstrate real-time (512 x 512 at 24 FPS, 800 x 600 at 13 FPS) precomputed
rendering of challenging scenes involving view-dependent reflections and even
caustics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures, to appear in cgf proceedings of egsr 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning of Crystalline Defects from TEM images: A Solution for the
  Problem of "Never Enough Training Data" 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kishan Govind, Daniela Oliveros, Antonin Dlouhy, Marc Legros, Stefan Sandfeld
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crystalline defects, such as line-like dislocations, play an important role
for the performance and reliability of many metallic devices. Their interaction
and evolution still poses a multitude of open questions to materials science
and materials physics. In-situ TEM experiments can provide important insights
into how dislocations behave and move. During such experiments, the dislocation
microstructure is captured in form of videos. The analysis of individual video
frames can provide useful insights but is limited by the capabilities of
automated identification, digitization, and quantitative extraction of the
dislocations as curved objects. The vast amount of data also makes manual
annotation very time consuming, thereby limiting the use of Deep
Learning-based, automated image analysis and segmentation of the dislocation
microstructure. In this work, a parametric model for generating synthetic
training data for segmentation of dislocations is developed. Even though domain
scientists might dismiss synthetic training images sometimes as too artificial,
our findings show that they can result in superior performance, particularly
regarding the generalizing of the Deep Learning models with respect to
different microstructures and imaging conditions. Additionally, we propose an
enhanced deep learning method optimized for segmenting overlapping or
intersecting dislocation lines. Upon testing this framework on four distinct
real datasets, we find that our synthetic training data are able to yield
high-quality results also on real images-even more so if fine-tune on a few
real images was done.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Correlation-Aware Mutual Learning for Semi-supervised Medical Image
  Segmentation <span class="chip">MICCAI2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengbo Gao, Ziji Zhang, Jiechao Ma, Zihao Li, Shu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-supervised learning has become increasingly popular in medical image
segmentation due to its ability to leverage large amounts of unlabeled data to
extract additional information. However, most existing semi-supervised
segmentation methods only focus on extracting information from unlabeled data,
disregarding the potential of labeled data to further improve the performance
of the model. In this paper, we propose a novel Correlation Aware Mutual
Learning (CAML) framework that leverages labeled data to guide the extraction
of information from unlabeled data. Our approach is based on a mutual learning
strategy that incorporates two modules: the Cross-sample Mutual Attention
Module (CMA) and the Omni-Correlation Consistency Module (OCC). The CMA module
establishes dense cross-sample correlations among a group of samples, enabling
the transfer of label prior knowledge to unlabeled data. The OCC module
constructs omni-correlations between the unlabeled and labeled datasets and
regularizes dual models by constraining the omni-correlation matrix of each
sub-model to be consistent. Experiments on the Atrial Segmentation Challenge
dataset demonstrate that our proposed approach outperforms state-of-the-art
methods, highlighting the effectiveness of our framework in medical image
segmentation tasks. The codes, pre-trained weights, and data are publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI2023 early accepted, camera ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Facial Reenactment Through a Personalized Generator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ariel Elazary, Yotam Nitzan, Daniel Cohen-Or
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the role of image generative models in facial reenactment
has been steadily increasing. Such models are usually subject-agnostic and
trained on domain-wide datasets. The appearance of the reenacted individual is
learned from a single image, and hence, the entire breadth of the individual's
appearance is not entirely captured, leading these methods to resort to
unfaithful hallucination. Thanks to recent advancements, it is now possible to
train a personalized generative model tailored specifically to a given
individual. In this paper, we propose a novel method for facial reenactment
using a personalized generator. We train the generator using frames from a
short, yet varied, self-scan video captured using a simple commodity camera.
Images synthesized by the personalized generator are guaranteed to preserve
identity. The premise of our work is that the task of reenactment is thus
reduced to accurately mimicking head poses and expressions. To this end, we
locate the desired frames in the latent space of the personalized generator
using carefully designed latent optimization. Through extensive evaluation, we
demonstrate state-of-the-art performance for facial reenactment. Furthermore,
we show that since our reenactment takes place in a semantic latent space, it
can be semantically edited and stylized in post-processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage: https://arielazary.github.io/PGR/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Patch n' Pack: NaViT, a Vision <span class="highlight-title">Transformer</span> for any Aspect Ratio and
  Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, Avital Oliver, Piotr Padlewski, Alexey Gritsenko, Mario Lučić, Neil Houlsby
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ubiquitous and demonstrably suboptimal choice of resizing images to a
fixed resolution before processing them with computer vision models has not yet
been successfully challenged. However, models such as the Vision Transformer
(ViT) offer flexible sequence-based modeling, and hence varying input sequence
lengths. We take advantage of this with NaViT (Native Resolution ViT) which
uses sequence packing during training to process inputs of arbitrary
resolutions and aspect ratios. Alongside flexible model usage, we demonstrate
improved training efficiency for large-scale supervised and contrastive
image-text pretraining. NaViT can be efficiently transferred to standard tasks
such as image and video classification, object detection, and semantic
segmentation and leads to improved results on robustness and fairness
benchmarks. At inference time, the input resolution flexibility can be used to
smoothly navigate the test-time cost-performance trade-off. We believe that
NaViT marks a departure from the standard, CNN-designed, input and modelling
pipeline used by most computer vision models, and represents a promising
direction for ViTs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Real-time Image Smoothing with Weak Structures Preserved and
  High-contrast Details Removed 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengchun Wang, Wencheng Wang, Fei Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image smoothing is by reducing pixel-wise gradients to smooth out details. As
existing methods always rely on gradients to determine smoothing manners, it is
difficult to distinguish structures and details to handle distinctively due to
the overlapped ranges of gradients for structures and details. Thus, it is
still challenging to achieve high-quality results, especially on preserving
weak structures and removing high-contrast details. In this paper, we address
this challenge by improving the real-time optimization-based method via
iterative least squares (called ILS). We observe that 1) ILS uses gradients as
the independent variable in its penalty function for determining smoothing
manners, and 2) the framework of ILS can still work for image smoothing when we
use some values instead of gradients in the penalty function. Thus,
corresponding to the properties of pixels on structures or not, we compute some
values to use in the penalty function to determine smoothing manners, and so we
can handle structures and details distinctively, no matter whether their
gradients are high or low. As a result, we can conveniently remove
high-contrast details while preserving weak structures. Moreover, such values
can be adjusted to accelerate optimization computation, so that we can use
fewer iterations than the original ILS method for efficiency. This also reduces
the changes onto structures to help structure preservation. Experimental
results show our advantages over existing methods on efficiency and quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMBench: Is Your Multi-modal Model an All-around Player? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, Dahua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models have recently achieved remarkable progress,
exhibiting great perception and reasoning abilities concerning visual
information. However, how to effectively evaluate these large vision-language
models remains a major obstacle, hindering future model development.
Traditional benchmarks like VQAv2 or COCO Caption provide quantitative
performance measurements but suffer from a lack of fine-grained ability
assessment and non-robust evaluation metrics. Recent subjective benchmarks,
such as OwlEval, offer comprehensive evaluations of a model's abilities by
incorporating human labor, but they are not scalable and display significant
bias. In response to these challenges, we propose MMBench, a novel
multi-modality benchmark. MMBench methodically develops a comprehensive
evaluation pipeline, primarily comprised of two elements. The first element is
a meticulously curated dataset that surpasses existing similar benchmarks in
terms of the number and variety of evaluation questions and abilities. The
second element introduces a novel CircularEval strategy and incorporates the
use of ChatGPT. This implementation is designed to convert free-form
predictions into pre-defined choices, thereby facilitating a more robust
evaluation of the model's predictions. MMBench is a systematically-designed
objective benchmark for robustly evaluating the various abilities of
vision-language models. We hope MMBench will assist the research community in
better evaluating their models and encourage future advancements in this
domain. Project page: https://opencompass.org.cn/mmbench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stochastic Light Field Holography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Schiffers, Praneeth Chakravarthula, Nathan Matsuda, Grace Kuo, Ethan Tseng, Douglas Lanman, Felix Heide, Oliver Cossairt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Visual Turing Test is the ultimate goal to evaluate the realism of
holographic displays. Previous studies have focused on addressing challenges
such as limited \'etendue and image quality over a large focal volume, but they
have not investigated the effect of pupil sampling on the viewing experience in
full 3D holograms. In this work, we tackle this problem with a novel hologram
generation algorithm motivated by matching the projection operators of
incoherent Light Field and coherent Wigner Function light transport. To this
end, we supervise hologram computation using synthesized photographs, which are
rendered on-the-fly using Light Field refocusing from stochastically sampled
pupil states during optimization. The proposed method produces holograms with
correct parallax and focus cues, which are important for passing the Visual
Turing Test. We validate that our approach compares favorably to
state-of-the-art CGH algorithms that use Light Field and Focal Stack
supervision. Our experiments demonstrate that our algorithm significantly
improves the realism of the viewing experience for a variety of different pupil
states.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exposing the Fake: Effective Diffusion-Generated Images Detection <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruipeng Ma, Jinhao Duan, Fei Kong, Xiaoshuang Shi, Kaidi Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image synthesis has seen significant advancements with the advent of
diffusion-based generative models like Denoising Diffusion Probabilistic Models
(DDPM) and text-to-image diffusion models. Despite their efficacy, there is a
dearth of research dedicated to detecting diffusion-generated images, which
could pose potential security and privacy risks. This paper addresses this gap
by proposing a novel detection method called Stepwise Error for
Diffusion-generated Image Detection (SeDID). Comprising statistical-based
$\text{SeDID}_{\text{Stat}}$ and neural network-based
$\text{SeDID}_{\text{NNs}}$, SeDID exploits the unique attributes of diffusion
models, namely deterministic reverse and deterministic denoising computation
errors. Our evaluations demonstrate SeDID's superior performance over existing
methods when applied to diffusion models. Thus, our work makes a pivotal
contribution to distinguishing diffusion model-generated images, marking a
significant step in the domain of artificial intelligence security.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AdvML-Frontiers@ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UGCANet: A Unified Global Context-Aware <span class="highlight-title">Transformer</span>-based Network with
  Feature Alignment for Endoscopic Image Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pham Vu Hung, Nguyen Duy Manh, Nguyen Thi Oanh, Nguyen Thi Thuy, Dinh Viet Sang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gastrointestinal endoscopy is a medical procedure that utilizes a flexible
tube equipped with a camera and other instruments to examine the digestive
tract. This minimally invasive technique allows for diagnosing and managing
various gastrointestinal conditions, including inflammatory bowel disease,
gastrointestinal bleeding, and colon cancer. The early detection and
identification of lesions in the upper gastrointestinal tract and the
identification of malignant polyps that may pose a risk of cancer development
are critical components of gastrointestinal endoscopy's diagnostic and
therapeutic applications. Therefore, enhancing the detection rates of
gastrointestinal disorders can significantly improve a patient's prognosis by
increasing the likelihood of timely medical intervention, which may prolong the
patient's lifespan and improve overall health outcomes. This paper presents a
novel Transformer-based deep neural network designed to perform multiple tasks
simultaneously, thereby enabling accurate identification of both upper
gastrointestinal tract lesions and colon polyps. Our approach proposes a unique
global context-aware module and leverages the powerful MiT backbone, along with
a feature alignment block, to enhance the network's representation capability.
This novel design leads to a significant improvement in performance across
various endoscopic diagnosis tasks. Extensive experiments demonstrate the
superior performance of our method compared to other state-of-the-art
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Importance of Denoising when Learning to Compress Images <span class="chip">WACV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benoit Brummer, Christophe De Vleeschouwer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image noise is ubiquitous in photography. However, image noise is not
compressible nor desirable, thus attempting to convey the noise in compressed
image bitstreams yields sub-par results in both rate and distortion. We propose
to explicitly learn the image denoising task when training a codec. Therefore,
we leverage the Natural Image Noise Dataset, which offers a wide variety of
scenes captured with various ISO numbers, leading to different noise levels,
including insignificant ones. Given this training set, we supervise the codec
with noisy-clean image pairs, and show that a single model trained based on a
mixture of images with variable noise levels appears to yield best-in-class
results with both noisy and clean images, achieving better rate-distortion than
a compression-only model or even than a pair of denoising-then-compression
models with almost one order of magnitude fewer GMac operations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in 2023 IEEE/CVF Winter Conference on Applications of
  Computer Vision (WACV)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SepVAE: a contrastive VAE to separate pathological patterns from healthy
  ones <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robin Louiset, Edouard Duchesnay, Antoine Grigis, Benoit Dufumier, Pietro Gori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Analysis VAE (CA-VAEs) is a family of Variational auto-encoders
(VAEs) that aims at separating the common factors of variation between a
background dataset (BG) (i.e., healthy subjects) and a target dataset (TG)
(i.e., patients) from the ones that only exist in the target dataset. To do so,
these methods separate the latent space into a set of salient features (i.e.,
proper to the target dataset) and a set of common features (i.e., exist in both
datasets). Currently, all models fail to prevent the sharing of information
between latent spaces effectively and to capture all salient factors of
variation. To this end, we introduce two crucial regularization losses: a
disentangling term between common and salient representations and a
classification term between background and target samples in the salient space.
We show a better performance than previous CA-VAEs methods on three medical
applications and a natural images dataset (CelebA). Code and datasets are
available on GitHub https://github.com/neurospin-projects/2023_rlouiset_sepvae.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Workshop on Interpretable ML in Healthcare at International
  Conference on Machine Learning (ICML), Honolulu, Hawaii, USA. 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CellGAN: Conditional Cervical Cell Synthesis for Augmenting
  Cytopathological Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenrong Shen, Maosong Cao, Sheng Wang, Lichi Zhang, Qian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic examination of thin-prep cytologic test (TCT) slides can assist
pathologists in finding cervical abnormality for accurate and efficient cancer
screening. Current solutions mostly need to localize suspicious cells and
classify abnormality based on local patches, concerning the fact that whole
slide images of TCT are extremely large. It thus requires many annotations of
normal and abnormal cervical cells, to supervise the training of the
patch-level classifier for promising performance. In this paper, we propose
CellGAN to synthesize cytopathological images of various cervical cell types
for augmenting patch-level cell classification. Built upon a lightweight
backbone, CellGAN is equipped with a non-linear class mapping network to
effectively incorporate cell type information into image generation. We also
propose the Skip-layer Global Context module to model the complex spatial
relationship of the cells, and attain high fidelity of the synthesized images
through adversarial learning. Our experiments demonstrate that CellGAN can
produce visually plausible TCT cytopathological images for different cell
types. We also validate the effectiveness of using CellGAN to greatly augment
patch-level cell classification performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Class Separation is not what you need for Relational
  Reasoning-based OOD Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Li Lu, Giulia D'Ascenzi, Francesco Cappio Borlino, Tatiana Tommasi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Standard recognition approaches are unable to deal with novel categories at
test time. Their overconfidence on the known classes makes the predictions
unreliable for safety-critical applications such as healthcare or autonomous
driving. Out-Of-Distribution (OOD) detection methods provide a solution by
identifying semantic novelty. Most of these methods leverage a learning stage
on the known data, which means training (or fine-tuning) a model to capture the
concept of normality. This process is clearly sensitive to the amount of
available samples and might be computationally expensive for on-board systems.
A viable alternative is that of evaluating similarities in the embedding space
produced by large pre-trained models without any further learning effort. We
focus exactly on such a fine-tuning-free OOD detection setting. This works
presents an in-depth analysis of the recently introduced relational reasoning
pre-training and investigates the properties of the learned embedding,
highlighting the existence of a correlation between the inter-class feature
distance and the OOD detection accuracy. As the class separation depends on the
chosen pre-training objective, we propose an alternative loss function to
control the inter-class margin, and we show its advantage with thorough
experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at ICIAP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Smart Infrastructure: A Research Junction <span class="chip">SC2</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Hetzel, Hannes Reichert, Konrad Doll, Bernhard Sick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complex inner-city junctions are among the most critical traffic areas for
injury and fatal accidents. The development of highly automated driving (HAD)
systems struggles with the complex and hectic everyday life within those areas.
Sensor-equipped smart infrastructures, which can communicate and cooperate with
vehicles, are essential to enable a holistic scene understanding to resolve
occlusions drivers and vehicle perception systems for themselves can not cover.
We introduce an intelligent research infrastructure equipped with visual sensor
technology, located at a public inner-city junction in Aschaffenburg, Germany.
A multiple-view camera system monitors the traffic situation to perceive road
users' behavior. Both motorized and non-motorized traffic is considered. The
system is used for research in data generation, evaluating new HAD sensors
systems, algorithms, and Artificial Intelligence (AI) training strategies using
real-, synthetic- and augmented data. In addition, the junction features a
highly accurate digital twin. Real-world data can be taken into the digital
twin for simulation purposes and synthetic data generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE International Smart Cities Conference (ISC2) 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Vision-Language Models be a Good Guesser? Exploring VLMs for Times
  and Location Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gengyuan Zhang, Yurui Zhang, Kerui Zhang, Volker Tresp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) are expected to be capable of reasoning with
commonsense knowledge as human beings. One example is that humans can reason
where and when an image is taken based on their knowledge. This makes us wonder
if, based on visual cues, Vision-Language Models that are pre-trained with
large-scale image-text resources can achieve and even outperform human's
capability in reasoning times and location. To address this question, we
propose a two-stage \recognition\space and \reasoning\space probing task,
applied to discriminative and generative VLMs to uncover whether VLMs can
recognize times and location-relevant features and further reason about it. To
facilitate the investigation, we introduce WikiTiLo, a well-curated image
dataset compromising images with rich socio-cultural cues. In the extensive
experimental studies, we find that although VLMs can effectively retain
relevant features in visual encoders, they still fail to make perfect
reasoning. We will release our dataset and codes to facilitate future studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The IMPTC <span class="highlight-title">Dataset</span>: An Infrastructural Multi-Person Trajectory and
  Context <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06165v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06165v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Hetzel, Hannes Reichert, Günther Reitberger, Erich Fuchs, Konrad Doll, Bernhard Sick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inner-city intersections are among the most critical traffic areas for injury
and fatal accidents. Automated vehicles struggle with the complex and hectic
everyday life within those areas. Sensor-equipped smart infrastructures, which
can cooperate with vehicles, can benefit automated traffic by extending the
perception capabilities of drivers and vehicle perception systems.
Additionally, they offer the opportunity to gather reproducible and precise
data of a holistic scene understanding, including context information as a
basis for training algorithms for various applications in automated traffic.
Therefore, we introduce the Infrastructural Multi-Person Trajectory and Context
Dataset (IMPTC). We use an intelligent public inner-city intersection in
Germany with visual sensor technology. A multi-view camera and LiDAR system
perceives traffic situations and road users' behavior. Additional sensors
monitor contextual information like weather, lighting, and traffic light signal
status. The data acquisition system focuses on Vulnerable Road Users (VRUs) and
multi-agent interaction. The resulting dataset consists of eight hours of
measurement data. It contains over 2,500 VRU trajectories, including
pedestrians, cyclists, e-scooter riders, strollers, and wheelchair users, and
over 20,000 vehicle trajectories at different day times, weather conditions,
and seasons. In addition, to enable the entire stack of research capabilities,
the dataset includes all data, starting from the sensor-, calibration- and
detection data until trajectory and context data. The dataset is continuously
expanded and is available online for non-commercial research at
https://github.com/kav-institute/imptc-dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Intelligent Vehicles Conference (IV) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Kernel-Modulated Neural Representation for Efficient Light
  Field Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinglei Shi, Yihong Xu, Christine Guillemot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Light field is a type of image data that captures the 3D scene information by
recording light rays emitted from a scene at various orientations. It offers a
more immersive perception than classic 2D images but at the cost of huge data
volume. In this paper, we draw inspiration from the visual characteristics of
Sub-Aperture Images (SAIs) of light field and design a compact neural network
representation for the light field compression task. The network backbone takes
randomly initialized noise as input and is supervised on the SAIs of the target
light field. It is composed of two types of complementary kernels: descriptive
kernels (descriptors) that store scene description information learned during
training, and modulatory kernels (modulators) that control the rendering of
different SAIs from the queried perspectives. To further enhance compactness of
the network meanwhile retain high quality of the decoded light field, we
accordingly introduce modulator allocation and kernel tensor decomposition
mechanisms, followed by non-uniform quantization and lossless entropy coding
techniques, to finally form an efficient compression pipeline. Extensive
experiments demonstrate that our method outperforms other state-of-the-art
(SOTA) methods by a significant margin in the light field compression task.
Moreover, after aligning descriptors, the modulators learned from one light
field can be transferred to new light fields for rendering dense views,
indicating a potential solution for view synthesis task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recognizing student identification numbers from the matrix templates
  using a modified U-net architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06120v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06120v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Filip Pavičić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an innovative approach to student identification during
exams and knowledge tests, which overcomes the limitations of the traditional
personal information entry method. The proposed method employs a matrix
template on the designated section of the exam, where squares containing
numbers are selectively blackened. The methodology involves the development of
a neural network specifically designed for recognizing students' personal
identification numbers. The neural network utilizes a specially adapted U-Net
architecture, trained on an extensive dataset comprising images of blackened
tables. The network demonstrates proficiency in recognizing the patterns and
arrangement of blackened squares, accurately interpreting the information
inscribed within them. Additionally, the model exhibits high accuracy in
correctly identifying entered student personal numbers and effectively
detecting erroneous entries within the table. This approach offers multiple
advantages. Firstly, it significantly accelerates the exam marking process by
automatically extracting identifying information from the blackened tables,
eliminating the need for manual entry and minimizing the potential for errors.
Secondly, the method automates the identification process, thereby reducing
administrative effort and expediting data processing. The introduction of this
innovative identification system represents a notable advancement in the field
of exams and knowledge tests, replacing the conventional manual entry of
personal data with a streamlined, efficient, and accurate identification
process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TreeFormer: a Semi-Supervised <span class="highlight-title">Transformer</span>-based Framework for Tree
  Counting from a Single High Resolution Image <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamed Amini Amirkolaee, Miaojing Shi, Mark Mulligan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic tree density estimation and counting using single aerial and
satellite images is a challenging task in photogrammetry and remote sensing,
yet has an important role in forest management. In this paper, we propose the
first semisupervised transformer-based framework for tree counting which
reduces the expensive tree annotations for remote sensing images. Our method,
termed as TreeFormer, first develops a pyramid tree representation module based
on transformer blocks to extract multi-scale features during the encoding
stage. Contextual attention-based feature fusion and tree density regressor
modules are further designed to utilize the robust features from the encoder to
estimate tree density maps in the decoder. Moreover, we propose a pyramid
learning strategy that includes local tree density consistency and local tree
count ranking losses to utilize unlabeled images into the training process.
Finally, the tree counter token is introduced to regulate the network by
computing the global tree counts for both labeled and unlabeled images. Our
model was evaluated on two benchmark tree counting datasets, Jiangsu, and
Yosemite, as well as a new dataset, KCL-London, created by ourselves. Our
TreeFormer outperforms the state of the art semi-supervised methods under the
same setting and exceeds the fully-supervised methods using the same number of
labeled images. The codes and datasets are available at
https://github.com/HAAClassic/TreeFormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RFENet: Towards Reciprocal Feature Evolution for Glass Segmentation <span class="chip">IJCAI2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Fan, Changan Wang, Yabiao Wang, Chengjie Wang, Ran Yi, Lizhuang Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Glass-like objects are widespread in daily life but remain intractable to be
segmented for most existing methods. The transparent property makes it
difficult to be distinguished from background, while the tiny separation
boundary further impedes the acquisition of their exact contour. In this paper,
by revealing the key co-evolution demand of semantic and boundary learning, we
propose a Selective Mutual Evolution (SME) module to enable the reciprocal
feature learning between them. Then to exploit the global shape context, we
propose a Structurally Attentive Refinement (SAR) module to conduct a
fine-grained feature refinement for those ambiguous points around the boundary.
Finally, to further utilize the multi-scale representation, we integrate the
above two modules into a cascaded structure and then introduce a Reciprocal
Feature Evolution Network (RFENet) for effective glass-like object
segmentation. Extensive experiments demonstrate that our RFENet achieves
state-of-the-art performance on three popular public datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2023 International Joint Conference on Artificial
  Intelligence (IJCAI2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AICT: An Adaptive Image Compression <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06091v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06091v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Ghorbel, Wassim Hamidouche, Luce Morin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by the efficiency investigation of the Tranformer-based transform
coding framework, namely SwinT-ChARM, we propose to enhance the latter, as
first, with a more straightforward yet effective Tranformer-based channel-wise
auto-regressive prior model, resulting in an absolute image compression
transformer (ICT). Current methods that still rely on ConvNet-based entropy
coding are limited in long-range modeling dependencies due to their local
connectivity and an increasing number of architectural biases and priors. On
the contrary, the proposed ICT can capture both global and local contexts from
the latent representations and better parameterize the distribution of the
quantized latents. Further, we leverage a learnable scaling module with a
sandwich ConvNeXt-based pre/post-processor to accurately extract more compact
latent representation while reconstructing higher-quality images. Extensive
experimental results on benchmark datasets showed that the proposed adaptive
image compression transformer (AICT) framework significantly improves the
trade-off between coding efficiency and decoder complexity over the versatile
video coding (VVC) reference encoder (VTM-18.0) and the neural codec
SwinT-ChARM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2307.02273</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VELMA: Verbalization Embodiment of LLM Agents for Vision and Language
  Navigation in Street View 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06082v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06082v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raphael Schumann, Wanrong Zhu, Weixi Feng, Tsu-Jui Fu, Stefan Riezler, William Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incremental decision making in real-world environments is one of the most
challenging tasks in embodied artificial intelligence. One particularly
demanding scenario is Vision and Language Navigation~(VLN) which requires
visual and natural language understanding as well as spatial and temporal
reasoning capabilities. The embodied agent needs to ground its understanding of
navigation instructions in observations of a real-world environment like Street
View. Despite the impressive results of LLMs in other research areas, it is an
ongoing problem of how to best connect them with an interactive visual
environment. In this work, we propose VELMA, an embodied LLM agent that uses a
verbalization of the trajectory and of visual environment observations as
contextual prompt for the next action. Visual information is verbalized by a
pipeline that extracts landmarks from the human written navigation instructions
and uses CLIP to determine their visibility in the current panorama view. We
show that VELMA is able to successfully follow navigation instructions in
Street View with only two in-context examples. We further finetune the LLM
agent on a few thousand examples and achieve 25%-30% relative improvement in
task completion over the previous state-of-the-art for two datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Operational Support Estimator Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06065v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06065v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mete Ahishali, Mehmet Yamac, Serkan Kiranyaz, Moncef Gabbouj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a novel approach called Operational Support
Estimator Networks (OSENs) for the support estimation task. Support Estimation
(SE) is defined as finding the locations of non-zero elements in a sparse
signal. By its very nature, the mapping between the measurement and sparse
signal is a non-linear operation. Traditional support estimators rely on
computationally expensive iterative signal recovery techniques to achieve such
non-linearity. Contrary to the convolution layers, the proposed OSEN approach
consists of operational layers that can learn such complex non-linearities
without the need for deep networks. In this way, the performance of the
non-iterative support estimation is greatly improved. Moreover, the operational
layers comprise so-called generative \textit{super neurons} with non-local
kernels. The kernel location for each neuron/feature map is optimized jointly
for the SE task during the training. We evaluate the OSENs in three different
applications: i. support estimation from Compressive Sensing (CS) measurements,
ii. representation-based classification, and iii. learning-aided CS
reconstruction where the output of OSENs is used as prior knowledge to the CS
algorithm for an enhanced reconstruction. Experimental results show that the
proposed approach achieves computational efficiency and outperforms competing
methods, especially at low measurement rates by a significant margin. The
software implementation is publicly shared at
https://github.com/meteahishali/OSEN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Visualization for Multivariate Gaussian Anomaly Detection in Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06052v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06052v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joao P C Bertoldo, David Arrustico
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a simplified variation of the PaDiM (Pixel-Wise Anomaly
Detection through Instance Modeling) method for anomaly detection in images,
fitting a single multivariate Gaussian (MVG) distribution to the feature
vectors extracted from a backbone convolutional neural network (CNN) and using
their Mahalanobis distance as the anomaly score. We introduce an intermediate
step in this framework by applying a whitening transformation to the feature
vectors, which enables the generation of heatmaps capable of visually
explaining the features learned by the MVG. The proposed technique is evaluated
on the MVTec-AD dataset, and the results show the importance of visual model
validation, providing insights into issues in this framework that were
otherwise invisible. The visualizations generated for this paper are publicly
available at https://doi.org/10.5281/zenodo.7937978.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 8 figures, accepted to 2023 Twelfth International Conference
  on Image Processing Theory, Tools and Applications (IPTA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pyramid Deep Fusion Network for Two-Hand Reconstruction from RGB-D
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinwei Ren, Jianke Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately recovering the dense 3D mesh of both hands from monocular images
poses considerable challenges due to occlusions and projection ambiguity. Most
of the existing methods extract features from color images to estimate the
root-aligned hand meshes, which neglect the crucial depth and scale information
in the real world. Given the noisy sensor measurements with limited resolution,
depth-based methods predict 3D keypoints rather than a dense mesh. These
limitations motivate us to take advantage of these two complementary inputs to
acquire dense hand meshes on a real-world scale. In this work, we propose an
end-to-end framework for recovering dense meshes for both hands, which employ
single-view RGB-D image pairs as input. The primary challenge lies in
effectively utilizing two different input modalities to mitigate the blurring
effects in RGB images and noises in depth images. Instead of directly treating
depth maps as additional channels for RGB images, we encode the depth
information into the unordered point cloud to preserve more geometric details.
Specifically, our framework employs ResNet50 and PointNet++ to derive features
from RGB and point cloud, respectively. Additionally, we introduce a novel
pyramid deep fusion network (PDFNet) to aggregate features at different scales,
which demonstrates superior efficacy compared to previous fusion strategies.
Furthermore, we employ a GCN-based decoder to process the fused features and
recover the corresponding 3D pose and dense mesh. Through comprehensive
ablation experiments, we have not only demonstrated the effectiveness of our
proposed fusion algorithm but also outperformed the state-of-the-art approaches
on publicly available datasets. To reproduce the results, we will make our
source code and models publicly available at
{\url{https://github.com/zijinxuxu/PDFNet}}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to TCSVT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning from Exemplary Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06026v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06026v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Misgina Tsighe Hagos, Kathleen M. Curran, Brian Mac Namee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  eXplanation Based Learning (XBL) is a form of Interactive Machine Learning
(IML) that provides a model refining approach via user feedback collected on
model explanations. Although the interactivity of XBL promotes model
transparency, XBL requires a huge amount of user interaction and can become
expensive as feedback is in the form of detailed annotation rather than simple
category labelling which is more common in IML. This expense is exacerbated in
high stakes domains such as medical image classification. To reduce the effort
and expense of XBL we introduce a new approach that uses two input instances
and their corresponding Gradient Weighted Class Activation Mapping (GradCAM)
model explanations as exemplary explanations to implement XBL. Using a medical
image classification task, we demonstrate that, using minimal human input, our
approach produces improved explanations (+0.02, +3%) and achieves reduced
classification performance (-0.04, -4%) when compared against a model trained
without interactions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Happens During Finetuning of Vision <span class="highlight-title">Transformer</span>s: An Invariance
  Based Investigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriele Merlin, Vedant Nanda, Ruchit Rawal, Mariya Toneva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pretrain-finetune paradigm usually improves downstream performance over
training a model from scratch on the same task, becoming commonplace across
many areas of machine learning. While pretraining is empirically observed to be
beneficial for a range of tasks, there is not a clear understanding yet of the
reasons for this effect. In this work, we examine the relationship between
pretrained vision transformers and the corresponding finetuned versions on
several benchmark datasets and tasks. We present new metrics that specifically
investigate the degree to which invariances learned by a pretrained model are
retained or forgotten during finetuning. Using these metrics, we present a
suite of empirical findings, including that pretraining induces transferable
invariances in shallow layers and that invariances from deeper pretrained
layers are compressed towards shallower layers during finetuning. Together,
these findings contribute to understanding some of the reasons for the
successes of pretrained models and the changes that a pretrained model
undergoes when finetuned on a downstream task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CoLLAs 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Optical Flow Estimation with Dynamic Timing Representation
  for Spike Camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lujie Xia, Ziluo Ding, Rui Zhao, Jiyuan Zhang, Lei Ma, Zhaofei Yu, Tiejun Huang, Ruiqin Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficiently selecting an appropriate spike stream data length to extract
precise information is the key to the spike vision tasks. To address this
issue, we propose a dynamic timing representation for spike streams. Based on
multi-layers architecture, it applies dilated convolutions on temporal
dimension to extract features on multi-temporal scales with few parameters. And
we design layer attention to dynamically fuse these features. Moreover, we
propose an unsupervised learning method for optical flow estimation in a
spike-based manner to break the dependence on labeled data. In addition, to
verify the robustness, we also build a spike-based synthetic validation dataset
for extreme scenarios in autonomous driving, denoted as SSES dataset. It
consists of various corner cases. Experiments show that our method can predict
optical flow from spike streams in different high-speed scenes, including real
scenes. For instance, our method gets $15\%$ and $19\%$ error reduction from
the best spike-based work, SCFlow, in $\Delta t=10$ and $\Delta t=20$
respectively which are the same settings as the previous works.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flexible and Fully Quantized Ultra-Lightweight TinyissimoYOLO for
  Ultra-Low-Power Edge Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Moosmann, Hanna Mueller, Nicky Zimmerman, Georg Rutishauser, Luca Benini, Michele Magno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper deploys and explores variants of TinyissimoYOLO, a highly flexible
and fully quantized ultra-lightweight object detection network designed for
edge systems with a power envelope of a few milliwatts. With experimental
measurements, we present a comprehensive characterization of the network's
detection performance, exploring the impact of various parameters, including
input resolution, number of object classes, and hidden layer adjustments. We
deploy variants of TinyissimoYOLO on state-of-the-art ultra-low-power extreme
edge platforms, presenting an in-depth a comparison on latency, energy
efficiency, and their ability to efficiently parallelize the workload. In
particular, the paper presents a comparison between a novel parallel RISC-V
processor (GAP9 from Greenwaves) with and without use of its on-chip hardware
accelerator, an ARM Cortex-M7 core (STM32H7 from ST Microelectronics), two ARM
Cortex-M4 cores (STM32L4 from STM and Apollo4b from Ambiq), and a multi-core
platform with a CNN hardware accelerator (Analog Devices MAX78000).
Experimental results show that the GAP9's hardware accelerator achieves the
lowest inference latency and energy at 2.12ms and 150uJ respectively, which is
around 2x faster and 20% more efficient than the next best platform, the
MAX78000. The hardware accelerator of GAP9 can even run an increased resolution
version of TinyissimoYOLO with 112x112 pixels and 10 detection classes within
3.2ms, consuming 245uJ. To showcase the competitiveness of a versatile
general-purpose system we also deployed and profiled a multi-core
implementation on GAP9 at different operating points, achieving 11.3ms with the
lowest-latency and 490uJ with the most energy-efficient configuration. With
this paper, we demonstrate the suitability and flexibility of TinyissimoYOLO on
state-of-the-art detection datasets for real-time ultra-low-power edge
inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>* All three authors contributed equally to this research</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>s in Reinforcement Learning: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05979v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05979v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Agarwal, Aamer Abdul Rahman, Pierre-Luc St-Charles, Simon J. D. Prince, Samira Ebrahimi Kahou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have significantly impacted domains like natural language
processing, computer vision, and robotics, where they improve performance
compared to other neural networks. This survey explores how transformers are
used in reinforcement learning (RL), where they are seen as a promising
solution for addressing challenges such as unstable training, credit
assignment, lack of interpretability, and partial observability. We begin by
providing a brief domain overview of RL, followed by a discussion on the
challenges of classical RL algorithms. Next, we delve into the properties of
the transformer and its variants and discuss the characteristics that make them
well-suited to address the challenges inherent in RL. We examine the
application of transformers to various aspects of RL, including representation
learning, transition and reward function modeling, and policy optimization. We
also discuss recent research that aims to enhance the interpretability and
efficiency of transformers in RL, using visualization techniques and efficient
training strategies. Often, the transformer architecture must be tailored to
the specific needs of a given application. We present a broad overview of how
transformers have been adapted for several applications, including robotics,
medicine, language modeling, cloud computing, and combinatorial optimization.
We conclude by discussing the limitations of using transformers in RL and
assess their potential for catalyzing future breakthroughs in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Safe Self-Distillation of Internet-Scale Text-to-Image Diffusion
  Models <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05977v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05977v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanghyun Kim, Seohyeon Jung, Balhae Kim, Moonseok Choi, Jinwoo Shin, Juho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale image generation models, with impressive quality made possible by
the vast amount of data available on the Internet, raise social concerns that
these models may generate harmful or copyrighted content. The biases and
harmfulness arise throughout the entire training process and are hard to
completely remove, which have become significant hurdles to the safe deployment
of these models. In this paper, we propose a method called SDD to prevent
problematic content generation in text-to-image diffusion models. We
self-distill the diffusion model to guide the noise estimate conditioned on the
target removal concept to match the unconditional one. Compared to the previous
methods, our method eliminates a much greater proportion of harmful content
from the generated images without degrading the overall image quality.
Furthermore, our method allows the removal of multiple concepts at once,
whereas previous works are limited to removing a single concept at a time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 13 figures, ICML 2023 Workshop on Challenges in Deployable
  Generative AI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VoxPoser: Composable 3D Value Maps for Robotic Manipulation with
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, Li Fei-Fei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are shown to possess a wealth of actionable
knowledge that can be extracted for robot manipulation in the form of reasoning
and planning. Despite the progress, most still rely on pre-defined motion
primitives to carry out the physical interactions with the environment, which
remains a major bottleneck. In this work, we aim to synthesize robot
trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a
large variety of manipulation tasks given an open-set of instructions and an
open-set of objects. We achieve this by first observing that LLMs excel at
inferring affordances and constraints given a free-form language instruction.
More importantly, by leveraging their code-writing capabilities, they can
interact with a visual-language model (VLM) to compose 3D value maps to ground
the knowledge into the observation space of the agent. The composed value maps
are then used in a model-based planning framework to zero-shot synthesize
closed-loop robot trajectories with robustness to dynamic perturbations. We
further demonstrate how the proposed framework can benefit from online
experiences by efficiently learning a dynamics model for scenes that involve
contact-rich interactions. We present a large-scale study of the proposed
method in both simulated and real-robot environments, showcasing the ability to
perform a large variety of everyday manipulation tasks specified in free-form
natural language. Project website: https://voxposer.github.io
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GVCCI: Lifelong Learning of Visual Grounding for Language-Guided Robotic
  Manipulation <span class="chip">IROS2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junghyun Kim, Gi-Cheon Kang, Jaein Kim, Suyeon Shin, Byoung-Tak Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language-Guided Robotic Manipulation (LGRM) is a challenging task as it
requires a robot to understand human instructions to manipulate everyday
objects. Recent approaches in LGRM rely on pre-trained Visual Grounding (VG)
models to detect objects without adapting to manipulation environments. This
results in a performance drop due to a substantial domain gap between the
pre-training and real-world data. A straightforward solution is to collect
additional training data, but the cost of human-annotation is extortionate. In
this paper, we propose Grounding Vision to Ceaselessly Created Instructions
(GVCCI), a lifelong learning framework for LGRM, which continuously learns VG
without human supervision. GVCCI iteratively generates synthetic instruction
via object detection and trains the VG model with the generated data. We
validate our framework in offline and online settings across diverse
environments on different VG models. Experimental results show that
accumulating synthetic data from GVCCI leads to a steady improvement in VG by
up to 56.7% and improves resultant LGRM by up to 29.4%. Furthermore, the
qualitative analysis shows that the unadapted VG model often fails to find
correct objects due to a strong bias learned from the pre-training data.
Finally, we introduce a novel VG dataset for LGRM, consisting of nearly 252k
triplets of image-object-instruction from diverse manipulation environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IROS2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ YOGA: Deep Object Detection in the Wild with Lightweight Feature
  Learning and Multiscale Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raja Sunkara, Tie Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce YOGA, a deep learning based yet lightweight object detection
model that can operate on low-end edge devices while still achieving
competitive accuracy. The YOGA architecture consists of a two-phase feature
learning pipeline with a cheap linear transformation, which learns feature maps
using only half of the convolution filters required by conventional
convolutional neural networks. In addition, it performs multi-scale feature
fusion in its neck using an attention mechanism instead of the naive
concatenation used by conventional detectors. YOGA is a flexible model that can
be easily scaled up or down by several orders of magnitude to fit a broad range
of hardware constraints. We evaluate YOGA on COCO-val and COCO-testdev datasets
with other over 10 state-of-the-art object detectors. The results show that
YOGA strikes the best trade-off between model size and accuracy (up to 22%
increase of AP and 23-34% reduction of parameters and FLOPs), making it an
ideal choice for deployment in the wild on low-end edge devices. This is
further affirmed by our hardware implementation and evaluation on NVIDIA Jetson
Nano.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Pattern Recognition (Elsevier), July 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prototypical Contrastive Transfer Learning for Multimodal Language
  Understanding <span class="chip">IROS23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05942v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05942v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seitaro Otsuki, Shintaro Ishikawa, Komei Sugiura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although domestic service robots are expected to assist individuals who
require support, they cannot currently interact smoothly with people through
natural language. For example, given the instruction "Bring me a bottle from
the kitchen," it is difficult for such robots to specify the bottle in an
indoor environment. Most conventional models have been trained on real-world
datasets that are labor-intensive to collect, and they have not fully leveraged
simulation data through a transfer learning framework. In this study, we
propose a novel transfer learning approach for multimodal language
understanding called Prototypical Contrastive Transfer Learning (PCTL), which
uses a new contrastive loss called Dual ProtoNCE. We introduce PCTL to the task
of identifying target objects in domestic environments according to free-form
natural language instructions. To validate PCTL, we built new real-world and
simulation datasets. Our experiment demonstrated that PCTL outperformed
existing methods. Specifically, PCTL achieved an accuracy of 78.1%, whereas
simple fine-tuning achieved an accuracy of 73.4%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at IROS23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sem-CS: Semantic CLIPStyler for Text-Based Image Style Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chanda Grover Kamra, Indra Deep Mastan, Debayan Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  CLIPStyler demonstrated image style transfer with realistic textures using
only a style text description (instead of requiring a reference style image).
However, the ground semantics of objects in the style transfer output is lost
due to style spill-over on salient and background objects (content mismatch) or
over-stylization. To solve this, we propose Semantic CLIPStyler (Sem-CS), that
performs semantic style transfer. Sem-CS first segments the content image into
salient and non-salient objects and then transfers artistic style based on a
given style text description. The semantic style transfer is achieved using
global foreground loss (for salient objects) and global background loss (for
non-salient objects). Our empirical results, including DISTS, NIMA and user
study scores, show that our proposed framework yields superior qualitative and
quantitative performance. Our code is available at
github.com/chandagrover/sem-cs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 Figures, 2 Tables. arXiv admin note: substantial text
  overlap with arXiv:2303.06334</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A New <span class="highlight-title">Dataset</span> and Comparative Study for Aphid Cluster Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianxiao Zhang, Kaidong Li, Xiangyu Chen, Cuncong Zhong, Bo Luo, Ivan Grijalva Teran, Brian McCornack, Daniel Flippo, Ajay Sharda, Guanghui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aphids are one of the main threats to crops, rural families, and global food
security. Chemical pest control is a necessary component of crop production for
maximizing yields, however, it is unnecessary to apply the chemical approaches
to the entire fields in consideration of the environmental pollution and the
cost. Thus, accurately localizing the aphid and estimating the infestation
level is crucial to the precise local application of pesticides. Aphid
detection is very challenging as each individual aphid is really small and all
aphids are crowded together as clusters. In this paper, we propose to estimate
the infection level by detecting aphid clusters. We have taken millions of
images in the sorghum fields, manually selected 5,447 images that contain
aphids, and annotated each aphid cluster in the image. To use these images for
machine learning models, we crop the images into patches and created a labeled
dataset with over 151,000 image patches. Then, we implement and compare the
performance of four state-of-the-art object detection models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reading Radiology Imaging Like The Radiologist 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated radiology report generation aims to generate radiology reports that
contain rich, fine-grained descriptions of radiology imaging. Compared with
image captioning in the natural image domain, medical images are very similar
to each other, with only minor differences in the occurrence of diseases. Given
the importance of these minor differences in the radiology report, it is
crucial to encourage the model to focus more on the subtle regions of disease
occurrence. Secondly, the problem of visual and textual data biases is serious.
Not only do normal cases make up the majority of the dataset, but sentences
describing areas with pathological changes also constitute only a small part of
the paragraph. Lastly, generating medical image reports involves the challenge
of long text generation, which requires more expertise and empirical training
in medical knowledge. As a result, the difficulty of generating such reports is
increased. To address these challenges, we propose a disease-oriented retrieval
framework that utilizes similar reports as prior knowledge references. We
design a factual consistency captioning generator to generate more accurate and
factually consistent disease descriptions. Our framework can find most similar
reports for a given disease from the CXR database by retrieving a
disease-oriented mask consisting of the position and morphological
characteristics. By referencing the disease-oriented similar report and the
visual features, the factual consistency model can generate a more accurate
radiology report.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Medical Image-Text-Label Contrastive Learning With Continuous
  <span class="highlight-title">Prompt</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive language-image Pre-training (CLIP) [13] can leverage large
datasets of unlabeled Image-Text pairs, which have demonstrated impressive
performance in various downstream tasks. Given that annotating medical data is
time-consuming and laborious, Image-Text Pre-training has promising
applications in exploiting large-scale medical image and radiology report
datasets. However, medical Image-Text Pre-training faces several challenges, as
follows: (1) Due to privacy concerns, the amount of available medical data is
relatively small compared to natural data, leading to weaker generalization
ability of the model. (2) Medical images are highly similar with only
fine-grained differences in subtleties, resulting in a large number of
false-negative sample pairs in comparison learning. (3) The hand-crafted Prompt
usually differs from the natural medical image report, Subtle changes in
wording can lead to significant differences in performance. In this paper, we
propose a unified Image-Text-Label contrastive learning framework based on
continuous prompts, with three main contributions. First, We unified the data
of images, text, and labels, which greatly expanded the training data that the
model could utilize. Second, we address the issue of data diversity and the
impact of hand-crafted prompts on model performance by introducing continuous
implicit prompts. Lastly, we propose a ImageText-Label contrastive Training to
mitigate the problem of too many false-negative samples. We demonstrate through
sufficient experiments that the Unified Medical Contrastive Learning (UMCL)
framework exhibits excellent performance on several downstream tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SwiFT: Swin 4D fMRI <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Yongho Kim, Junbeom Kwon, Sunghwan Joo, Sangyoon Bae, Donggyu Lee, Yoonho Jung, Shinjae Yoo, Jiook Cha, Taesup Moon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The modeling of spatiotemporal brain dynamics from high-dimensional data,
such as 4D functional MRI, is a formidable task in neuroscience. To address
this challenge, we present SwiFT (Swin 4D fMRI Transformer), a Swin Transformer
architecture that can learn brain dynamics directly from 4D functional brain
MRI data in a memory and computation-efficient manner. SwiFT achieves this by
implementing a 4D window multi-head self-attention mechanism and absolute
positional embeddings. We evaluate SwiFT using multiple largest-scale human
functional brain imaging datasets in tasks such as predicting sex, age, and
cognitive intelligence. Our experimental outcomes reveal that SwiFT
consistently outperforms recent state-of-the-art models. To the best of our
knowledge, SwiFT is the first Swin Transformer architecture that can process
dimensional spatiotemporal brain functional data in an end-to-end fashion.
Furthermore, due to the end-to-end learning capability, we also show that
contrastive loss-based self-supervised pre-training of SwiFT is also feasible
for achieving improved performance on a downstream task. We believe that our
work holds substantial potential in facilitating scalable learning of
functional brain imaging in neuroscience research by reducing the hurdles
associated with applying Transformer models to high-dimensional fMRI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Close-up View synthesis by Interpolating Optical Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Bai, Ze Wang, Lu Yang, Hong Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The virtual viewpoint is perceived as a new technique in virtual navigation,
as yet not supported due to the lack of depth information and obscure camera
parameters. In this paper, a method for achieving close-up virtual view is
proposed and it only uses optical flow to build parallax effects to realize
pseudo 3D projection without using depth sensor. We develop a bidirectional
optical flow method to obtain any virtual viewpoint by proportional
interpolation of optical flow. Moreover, with the ingenious application of the
optical-flow-value, we achieve clear and visual-fidelity magnified results
through lens stretching in any corner, which overcomes the visual distortion
and image blur through viewpoint magnification and transition in Google Street
View system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Single Domain Generalization via Normalised Cross-correlation Based
  Convolutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05901v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05901v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        WeiQin Chuah, Ruwan Tennakoon, Reza Hoseinnezhad, David Suter, Alireza Bab-Hadiashar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning techniques often perform poorly in the presence of domain
shift, where the test data follows a different distribution than the training
data. The most practically desirable approach to address this issue is Single
Domain Generalization (S-DG), which aims to train robust models using data from
a single source. Prior work on S-DG has primarily focused on using data
augmentation techniques to generate diverse training data. In this paper, we
explore an alternative approach by investigating the robustness of linear
operators, such as convolution and dense layers commonly used in deep learning.
We propose a novel operator called XCNorm that computes the normalized
cross-correlation between weights and an input feature patch. This approach is
invariant to both affine shifts and changes in energy within a local feature
patch and eliminates the need for commonly used non-linear activation
functions. We show that deep neural networks composed of this operator are
robust to common semantic distribution shifts. Furthermore, our empirical
results on single-domain generalization benchmarks demonstrate that our
proposed technique performs comparably to the state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffuseGAE: Controllable and High-fidelity Image Manipulation from
  Disentangled Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yipeng Leng, Qiangjuan Huang, Zhiyuan Wang, Yangyang Liu, Haoyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion probabilistic models (DPMs) have shown remarkable results on
various image synthesis tasks such as text-to-image generation and image
inpainting. However, compared to other generative methods like VAEs and GANs,
DPMs lack a low-dimensional, interpretable, and well-decoupled latent code.
Recently, diffusion autoencoders (Diff-AE) were proposed to explore the
potential of DPMs for representation learning via autoencoding. Diff-AE
provides an accessible latent space that exhibits remarkable interpretability,
allowing us to manipulate image attributes based on latent codes from the
space. However, previous works are not generic as they only operated on a few
limited attributes. To further explore the latent space of Diff-AE and achieve
a generic editing pipeline, we proposed a module called Group-supervised
AutoEncoder(dubbed GAE) for Diff-AE to achieve better disentanglement on the
latent code. Our proposed GAE has trained via an attribute-swap strategy to
acquire the latent codes for multi-attribute image manipulation based on
examples. We empirically demonstrate that our method enables
multiple-attributes manipulation and achieves convincing sample quality and
attribute alignments, while significantly reducing computational requirements
compared to pixel-based approaches for representational decoupling. Code will
be released soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rectifying Noisy Labels with Sequential Prior: Multi-Scale Temporal
  Feature Affinity Learning for Robust Video Segmentation <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05898v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05898v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beilei Cui, Minqing Zhang, Mengya Xu, An Wang, Wu Yuan, Hongliang Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Noisy label problems are inevitably in existence within medical image
segmentation causing severe performance degradation. Previous segmentation
methods for noisy label problems only utilize a single image while the
potential of leveraging the correlation between images has been overlooked.
Especially for video segmentation, adjacent frames contain rich contextual
information beneficial in cognizing noisy labels. Based on two insights, we
propose a Multi-Scale Temporal Feature Affinity Learning (MS-TFAL) framework to
resolve noisy-labeled medical video segmentation issues. First, we argue the
sequential prior of videos is an effective reference, i.e., pixel-level
features from adjacent frames are close in distance for the same class and far
in distance otherwise. Therefore, Temporal Feature Affinity Learning (TFAL) is
devised to indicate possible noisy labels by evaluating the affinity between
pixels in two adjacent frames. We also notice that the noise distribution
exhibits considerable variations across video, image, and pixel levels. In this
way, we introduce Multi-Scale Supervision (MSS) to supervise the network from
three different perspectives by re-weighting and refining the samples. This
design enables the network to concentrate on clean samples in a coarse-to-fine
manner. Experiments with both synthetic and real-world label noise demonstrate
that our method outperforms recent state-of-the-art robust segmentation
approaches. Code is available at https://github.com/BeileiCui/MS-TFAL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by MICCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep learning-based estimation of whole-body kinematics from multi-view
  images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05896v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05896v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kien X. Nguyen, Liying Zheng, Ashley L. Hawke, Robert E. Carey, Scott P. Breloff, Kang Li, Xi Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is necessary to analyze the whole-body kinematics (including joint
locations and joint angles) to assess risks of fatal and musculoskeletal
injuries in occupational tasks. Human pose estimation has gotten more attention
in recent years as a method to minimize the errors in determining joint
locations. However, the joint angles are not often estimated, nor is the
quality of joint angle estimation assessed. In this paper, we presented an
end-to-end approach on direct joint angle estimation from multi-view images.
Our method leveraged the volumetric pose representation and mapped the rotation
representation to a continuous space where each rotation was uniquely
represented. We also presented a new kinematic dataset in the domain of
residential roofing with a data processing pipeline to generate necessary
annotations for the supervised training procedure on direct joint angle
estimation. We achieved a mean angle error of $7.19^\circ$ on the new Roofing
dataset and $8.41^\circ$ on the Human3.6M dataset, paving the way for
employment of on-site kinematic analysis using multi-view images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SC-NeuS: Consistent Neural Surface Reconstruction from Sparse and Noisy
  Views 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi-Sheng Huang, Zi-Xin Zou, Yi-Chi Zhang, Hua Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent neural surface reconstruction by volume rendering approaches have
made much progress by achieving impressive surface reconstruction quality, but
are still limited to dense and highly accurate posed views. To overcome such
drawbacks, this paper pays special attention on the consistent surface
reconstruction from sparse views with noisy camera poses. Unlike previous
approaches, the key difference of this paper is to exploit the multi-view
constraints directly from the explicit geometry of the neural surface, which
can be used as effective regularization to jointly learn the neural surface and
refine the camera poses. To build effective multi-view constraints, we
introduce a fast differentiable on-surface intersection to generate on-surface
points, and propose view-consistent losses based on such differentiable points
to regularize the neural surface learning. Based on this point, we propose a
jointly learning strategy for neural surface and camera poses, named SC-NeuS,
to perform geometry-consistent surface reconstruction in an end-to-end manner.
With extensive evaluation on public datasets, our SC-NeuS can achieve
consistently better surface reconstruction results with fine-grained details
than previous state-of-the-art neural surface reconstruction approaches,
especially from sparse and noisy camera views.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FreeSeed: Frequency-band-aware and Self-guided Network for Sparse-view
  CT Reconstruction <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05890v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05890v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenglong Ma, Zilong Li, Junping Zhang, Yi Zhang, Hongming Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse-view computed tomography (CT) is a promising solution for expediting
the scanning process and mitigating radiation exposure to patients, the
reconstructed images, however, contain severe streak artifacts, compromising
subsequent screening and diagnosis. Recently, deep learning-based image
post-processing methods along with their dual-domain counterparts have shown
promising results. However, existing methods usually produce over-smoothed
images with loss of details due to (1) the difficulty in accurately modeling
the artifact patterns in the image domain, and (2) the equal treatment of each
pixel in the loss function. To address these issues, we concentrate on the
image post-processing and propose a simple yet effective FREquency-band-awarE
and SElf-guidED network, termed FreeSeed, which can effectively remove artifact
and recover missing detail from the contaminated sparse-view CT images.
Specifically, we first propose a frequency-band-aware artifact modeling network
(FreeNet), which learns artifact-related frequency-band attention in Fourier
domain for better modeling the globally distributed streak artifact on the
sparse-view CT images. We then introduce a self-guided artifact refinement
network (SeedNet), which leverages the predicted artifact to assist FreeNet in
continuing to refine the severely corrupted details. Extensive experiments
demonstrate the superior performance of FreeSeed and its dual-domain
counterpart over the state-of-the-art sparse-view CT reconstruction methods.
Source code is made available at https://github.com/Masaaki-75/freeseed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Mitosis Detection: Towards Diverse Data and Feature
  Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05889v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05889v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Wang, Jiatai Lin, Danyi Li, Jing Wang, Bingchao Zhao, Zhenwei Shi, Xipeng Pan, Huadeng Wang, Bingbing Li, Changhong Liang, Guoqiang Han, Li Liang, Chu Han, Zaiyi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitosis detection is one of the fundamental tasks in computational pathology,
which is extremely challenging due to the heterogeneity of mitotic cell. Most
of the current studies solve the heterogeneity in the technical aspect by
increasing the model complexity. However, lacking consideration of the
biological knowledge and the complex model design may lead to the overfitting
problem while limited the generalizability of the detection model. In this
paper, we systematically study the morphological appearances in different
mitotic phases as well as the ambiguous non-mitotic cells and identify that
balancing the data and feature diversity can achieve better generalizability.
Based on this observation, we propose a novel generalizable framework (MitDet)
for mitosis detection. The data diversity is considered by the proposed
diversity-guided sample balancing (DGSB). And the feature diversity is
preserved by inter- and intra- class feature diversity-preserved module
(InCDP). Stain enhancement (SE) module is introduced to enhance the
domain-relevant diversity of both data and features simultaneously. Extensive
experiments have demonstrated that our proposed model outperforms all the SOTA
approaches in several popular mitosis detection datasets in both internal and
external test sets using minimal annotation efforts with point annotations
only. Comprehensive ablation studies have also proven the effectiveness of the
rethinking of data and feature diversity balancing. By analyzing the results
quantitatively and qualitatively, we believe that our proposed model not only
achieves SOTA performance but also might inspire the future studies in new
perspectives. Source code is at https://github.com/Onehour0108/MitDet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Object Tracking as Attention Mechanism <span class="chip">ICIP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05874v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05874v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiroshi Fukui, Taiki Miyagawa, Yusuke Morishita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a conceptually simple and thus fast multi-object tracking (MOT)
model that does not require any attached modules, such as the Kalman filter,
Hungarian algorithm, transformer blocks, or graph networks. Conventional MOT
models are built upon the multi-step modules listed above, and thus the
computational cost is high. Our proposed end-to-end MOT model,
\textit{TicrossNet}, is composed of a base detector and a cross-attention
module only. As a result, the overhead of tracking does not increase
significantly even when the number of instances ($N_t$) increases. We show that
TicrossNet runs \textit{in real-time}; specifically, it achieves 32.6 FPS on
MOT17 and 31.0 FPS on MOT20 (Tesla V100), which includes as many as $>$100
instances per frame. We also demonstrate that TicrossNet is robust to $N_t$;
thus, it does not have to change the size of the base detector, depending on
$N_t$, as is often done by other models for real-time processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE International Conference on Image Processing (IEEE
  ICIP) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OG: Equip vision occupancy with instance segmentation and visual
  grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05873v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05873v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichao Dong, Hang Ji, Weikun Zhang, Xufeng Huang, Junbo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Occupancy prediction tasks focus on the inference of both geometry and
semantic labels for each voxel, which is an important perception mission.
However, it is still a semantic segmentation task without distinguishing
various instances. Further, although some existing works, such as
Open-Vocabulary Occupancy (OVO), have already solved the problem of open
vocabulary detection, visual grounding in occupancy has not been solved to the
best of our knowledge. To tackle the above two limitations, this paper proposes
Occupancy Grounding (OG), a novel method that equips vanilla occupancy instance
segmentation ability and could operate visual grounding in a voxel manner with
the help of grounded-SAM. Keys to our approach are (1) affinity field
prediction for instance clustering and (2) association strategy for aligning 2D
instance masks and 3D occupancy instances. Extensive experiments have been
conducted whose visualization results and analysis are shown below. Our code
will be publicly released soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLA-GCN: Global-local Adaptive Graph Convolutional Network for 3D Human 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruce X. B. Yu, Zhi Zhang, Yongxu Liu, Sheng-hua Zhong, Yan Liu, Chang Wen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D human pose estimation has been researched for decades with promising
fruits. 3D human pose lifting is one of the promising research directions
toward the task where both estimated pose and ground truth pose data are used
for training. Existing pose lifting works mainly focus on improving the
performance of estimated pose, but they usually underperform when testing on
the ground truth pose data. We observe that the performance of the estimated
pose can be easily improved by preparing good quality 2D pose, such as
fine-tuning the 2D pose or using advanced 2D pose detectors. As such, we
concentrate on improving the 3D human pose lifting via ground truth data for
the future improvement of more quality estimated pose data. Towards this goal,
a simple yet effective model called Global-local Adaptive Graph Convolutional
Network (GLA-GCN) is proposed in this work. Our GLA-GCN globally models the
spatiotemporal structure via a graph representation and backtraces local joint
features for 3D human pose estimation via individually connected layers. To
validate our model design, we conduct extensive experiments on three benchmark
datasets: Human3.6M, HumanEva-I, and MPI-INF-3DHP. Experimental results show
that our GLA-GCN implemented with ground truth 2D poses significantly
outperforms state-of-the-art methods (e.g., up to around 3%, 17%, and 13% error
reductions on Human3.6M, HumanEva-I, and MPI-INF-3DHP, respectively).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DreamWaltz: Make a Scene with Complex 3D Animatable Avatars 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12529v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12529v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukun Huang, Jianan Wang, Ailing Zeng, He Cao, Xianbiao Qi, Yukai Shi, Zheng-Jun Zha, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present DreamWaltz, a novel framework for generating and animating complex
3D avatars given text guidance and parametric human body prior. While recent
methods have shown encouraging results for text-to-3D generation of common
objects, creating high-quality and animatable 3D avatars remains challenging.
To create high-quality 3D avatars, DreamWaltz proposes 3D-consistent
occlusion-aware Score Distillation Sampling (SDS) to optimize implicit neural
representations with canonical poses. It provides view-aligned supervision via
3D-aware skeleton conditioning which enables complex avatar generation without
artifacts and multiple faces. For animation, our method learns an animatable
and generalizable avatar representation which could map arbitrary poses to the
canonical pose representation. Extensive evaluations demonstrate that
DreamWaltz is an effective and robust approach for creating 3D avatars that can
take on complex shapes and appearances as well as novel poses for animation.
The proposed framework further enables the creation of complex scenes with
diverse compositions, including avatar-avatar, avatar-object and avatar-scene
interactions. See https://dreamwaltz3d.github.io/ for more vivid 3D avatar and
animation results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page at https://dreamwaltz3d.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synthesizing Artistic Cinemagraphs from Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03190v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03190v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aniruddha Mahapatra, Aliaksandr Siarohin, Hsin-Ying Lee, Sergey Tulyakov, Jun-Yan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Text2Cinemagraph, a fully automated method for creating
cinemagraphs from text descriptions - an especially challenging task when
prompts feature imaginary elements and artistic styles, given the complexity of
interpreting the semantics and motions of these images. Existing single-image
animation methods fall short on artistic inputs, and recent text-based video
methods frequently introduce temporal inconsistencies, struggling to keep
certain regions static. To address these challenges, we propose an idea of
synthesizing image twins from a single text prompt - a pair of an artistic
image and its pixel-aligned corresponding natural-looking twin. While the
artistic image depicts the style and appearance detailed in our text prompt,
the realistic counterpart greatly simplifies layout and motion analysis.
Leveraging existing natural image and video datasets, we can accurately segment
the realistic image and predict plausible motion given the semantic
information. The predicted motion can then be transferred to the artistic image
to create the final cinemagraph. Our method outperforms existing approaches in
creating cinemagraphs for natural landscapes as well as artistic and
other-worldly scenes, as validated by automated metrics and user studies.
Finally, we demonstrate two extensions: animating existing paintings and
controlling motion directions using text.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://text2cinemagraph.github.io/website/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dividing and Conquering a BlackBox to a Mixture of Interpretable Models:
  Route, Interpret, Repeat <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05350v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05350v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shantanu Ghosh, Ke Yu, Forough Arabshahi, Kayhan Batmanghelich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ML model design either starts with an interpretable model or a Blackbox and
explains it post hoc. Blackbox models are flexible but difficult to explain,
while interpretable models are inherently explainable. Yet, interpretable
models require extensive ML knowledge and tend to be less flexible and
underperforming than their Blackbox variants. This paper aims to blur the
distinction between a post hoc explanation of a Blackbox and constructing
interpretable models. Beginning with a Blackbox, we iteratively carve out a
mixture of interpretable experts (MoIE) and a residual network. Each
interpretable model specializes in a subset of samples and explains them using
First Order Logic (FOL), providing basic reasoning on concepts from the
Blackbox. We route the remaining samples through a flexible residual. We repeat
the method on the residual network until all the interpretable models explain
the desired proportion of data. Our extensive experiments show that our route,
interpret, and repeat approach (1) identifies a diverse set of
instance-specific concepts with high concept completeness via MoIE without
compromising in performance, (2) identifies the relatively ``harder'' samples
to explain via residuals, (3) outperforms the interpretable by-design models by
significant margins during test-time interventions, and (4) fixes the shortcut
learned by the original Blackbox. The code for MoIE is publicly available at:
\url{https://github.com/batmanlab/ICML-2023-Route-interpret-repeat}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>appeared as v5 of arXiv:2302.10289 which was replaced in error, which
  drifted into a different work, accepted in ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Graph Attention for Enhanced Spatial Context 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04149v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04149v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Singh, Yash Bhambhu, Himanshu Buckchash, Deepak K. Gupta, Dilip K. Prasad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Global contexts in images are quite valuable in image-to-image translation
problems. Conventional attention-based and graph-based models capture the
global context to a large extent, however, these are computationally expensive.
Moreover, the existing approaches are limited to only learning the pairwise
semantic relation between any two points on the image. In this paper, we
present Latent Graph Attention (LGA) a computationally inexpensive (linear to
the number of nodes) and stable, modular framework for incorporating the global
context in the existing architectures, especially empowering small-scale
architectures to give performance closer to large size architectures, thus
making the light-weight architectures more useful for edge devices with lower
compute power and lower energy needs. LGA propagates information spatially
using a network of locally connected graphs, thereby facilitating to construct
a semantically coherent relation between any two spatially distant points that
also takes into account the influence of the intermediate pixels. Moreover, the
depth of the graph network can be used to adapt the extent of contextual spread
to the target dataset, thereby being able to explicitly control the added
computational cost. To enhance the learning mechanism of LGA, we also introduce
a novel contrastive loss term that helps our LGA module to couple well with the
original architecture at the expense of minimal additional computational load.
We show that incorporating LGA improves the performance on three challenging
applications, namely transparent object segmentation, image restoration for
dehazing and optical flow estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analytical reconstructions of full-scan multiple source-translation
  computed tomography under large field of views 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.19767v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.19767v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhisheng Wang, Yue Liu, Shunli Wang, Xingyuan Bian, Zongfeng Li, Junning Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper is to investigate the high-quality analytical reconstructions of
multiple source-translation computed tomography (mSTCT) under an extended field
of view (FOV). Under the larger FOVs, the previously proposed backprojection
filtration (BPF) algorithms for mSTCT, including D-BPF and S-BPF (their
differences are different derivate directions along the detector and source,
respectively), make some errors and artifacts in the reconstructed images due
to a backprojection weighting factor and the half-scan mode, which deviates
from the intention of mSTCT imaging. In this paper, to achieve reconstruction
with as little error as possible under the extremely extended FOV, we combine
the full-scan mSTCT (F-mSTCT) geometry with the previous BPF algorithms to
study the performance and derive a suitable redundancy-weighted function for
F-mSTCT. The experimental results indicate FS-BPF can get high-quality, stable
images under the extremely extended FOV of imaging a large object, though it
requires more projections than FD-BPF. Finally, for different practical
requirements in extending FOV imaging, we give suggestions on algorithm
selection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weakly-supervised positional contrastive learning: application to
  cirrhosis classification <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04617v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04617v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emma Sarfati, Alexandre Bône, Marc-Michel Rohé, Pietro Gori, Isabelle Bloch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large medical imaging datasets can be cheaply and quickly annotated with
low-confidence, weak labels (e.g., radiological scores). Access to
high-confidence labels, such as histology-based diagnoses, is rare and costly.
Pretraining strategies, like contrastive learning (CL) methods, can leverage
unlabeled or weakly-annotated datasets. These methods typically require large
batch sizes, which poses a difficulty in the case of large 3D images at full
resolution, due to limited GPU memory. Nevertheless, volumetric positional
information about the spatial context of each 2D slice can be very important
for some medical applications. In this work, we propose an efficient
weakly-supervised positional (WSP) contrastive learning strategy where we
integrate both the spatial context of each 2D slice and a weak label via a
generic kernel-based loss function. We illustrate our method on cirrhosis
prediction using a large volume of weakly-labeled images, namely radiological
low-confidence annotations, and small strongly-labeled (i.e., high-confidence)
datasets. The proposed model improves the classification AUC by 5% with respect
to a baseline model on our internal dataset, and by 26% on the public LIHC
dataset from the Cancer Genome Atlas. The code is available at:
https://github.com/Guerbet-AI/wsp-contrastive.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neuromorphic Optical Flow and Real-time Implementation with Event
  Cameras <span class="chip">CVPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.07139v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.07139v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yannick Schnider, Stanislaw Wozniak, Mathias Gehrig, Jules Lecomte, Axel von Arnim, Luca Benini, Davide Scaramuzza, Angeliki Pantazi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical flow provides information on relative motion that is an important
component in many computer vision pipelines. Neural networks provide high
accuracy optical flow, yet their complexity is often prohibitive for
application at the edge or in robots, where efficiency and latency play crucial
role. To address this challenge, we build on the latest developments in
event-based vision and spiking neural networks. We propose a new network
architecture, inspired by Timelens, that improves the state-of-the-art
self-supervised optical flow accuracy when operated both in spiking and
non-spiking mode. To implement a real-time pipeline with a physical event
camera, we propose a methodology for principled model simplification based on
activity and latency analysis. We demonstrate high speed optical flow
prediction with almost two orders of magnitude reduced complexity while
maintaining the accuracy, opening the path for real-time deployments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for IEEE CVPRW, Vancouver 2023. Personal use of this
  material is permitted. Permission from IEEE must be obtained for all other
  uses, in any current or future media. Copyright 2023 IEEE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RAMiT: Reciprocal Attention Mixing <span class="highlight-title">Transformer</span> for Lightweight Image
  Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11474v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11474v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haram Choi, Cheolwoong Na, Jihyeon Oh, Seungjae Lee, Jinseop Kim, Subeen Choe, Jeongmin Lee, Taehoon Kim, Jihoon Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although many recent works have made advancements in the image restoration
(IR) field, they often suffer from an excessive number of parameters. Another
issue is that most Transformer-based IR methods focus only on either local or
global features, leading to limited receptive fields or deficient parameter
issues. To address these problems, we propose a lightweight IR network,
Reciprocal Attention Mixing Transformer (RAMiT). It employs our proposed
dimensional reciprocal attention mixing Transformer (D-RAMiT) blocks, which
compute bi-dimensional (spatial and channel) self-attentions in parallel with
different numbers of multi-heads. The bi-dimensional attentions help each other
to complement their counterpart's drawbacks and are then mixed. Additionally,
we introduce a hierarchical reciprocal attention mixing (H-RAMi) layer that
compensates for pixel-level information losses and utilizes semantic
information while maintaining an efficient hierarchical structure. Furthermore,
we revisit and modify MobileNet V1 and V2 to attach efficient convolutions to
our proposed components. The experimental results demonstrate that RAMiT
achieves state-of-the-art performance on multiple lightweight IR tasks,
including super-resolution, color denoising, grayscale denoising, low-light
enhancement, and deraining. Codes are available at
https://github.com/rami0205/RAMiT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report. 9 pages for main contents + 14 pages for appendix +
  6 pages for references. Codes are available at
  https://github.com/rami0205/RAMiT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint Hierarchical Priors and Adaptive Spatial Resolution for Efficient
  Neural Image Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02273v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02273v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Ghorbel, Wassim Hamidouche, Luce Morin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the performance of neural image compression (NIC) has steadily
improved thanks to the last line of study, reaching or outperforming
state-of-the-art conventional codecs. Despite significant progress, current NIC
methods still rely on ConvNet-based entropy coding, limited in modeling
long-range dependencies due to their local connectivity and the increasing
number of architectural biases and priors, resulting in complex underperforming
models with high decoding latency. Motivated by the efficiency investigation of
the Tranformer-based transform coding framework, namely SwinT-ChARM, we propose
to enhance the latter, as first, with a more straightforward yet effective
Tranformer-based channel-wise auto-regressive prior model, resulting in an
absolute image compression transformer (ICT). Through the proposed ICT, we can
capture both global and local contexts from the latent representations and
better parameterize the distribution of the quantized latents. Further, we
leverage a learnable scaling module with a sandwich ConvNeXt-based
pre-/post-processor to accurately extract more compact latent codes while
reconstructing higher-quality images. Extensive experimental results on
benchmark datasets showed that the proposed framework significantly improves
the trade-off between coding efficiency and decoder complexity over the
versatile video coding (VVC) reference encoder (VTM-18.0) and the neural codec
SwinT-ChARM. Moreover, we provide model scaling studies to verify the
computational efficiency of our approach and conduct several objective and
subjective analyses to bring to the fore the performance gap between the
adaptive image compression transformer (AICT) and the neural codec SwinT-ChARM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reasoning over the Behaviour of Objects in Video-Clips for Adverb-Type
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04132v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04132v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amrit Diggavi Seshadri, Alessandra Russo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, following the intuition that adverbs describing scene-sequences
are best identified by reasoning over high-level concepts of object-behavior,
we propose the design of a new framework that reasons over object-behaviours
extracted from raw-video-clips to recognize the clip's corresponding
adverb-types. Importantly, while previous works for general scene
adverb-recognition assume knowledge of the clips underlying action-types, our
method is directly applicable in the more general problem setting where the
action-type of a video-clip is unknown. Specifically, we propose a novel
pipeline that extracts human-interpretable object-behaviour-facts from raw
video clips and propose novel symbolic and transformer based reasoning methods
that operate over these extracted facts to identify adverb-types. Experiment
results demonstrate that our proposed methods perform favourably against the
previous state-of-the-art. Additionally, to support efforts in symbolic
video-processing, we release two new datasets of object-behaviour-facts
extracted from raw video clips - the MSR-VTT-ASP and ActivityNet-ASP datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BoxVIS: Video Instance Segmentation with Box Annotations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.14618v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.14618v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghan Li, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is expensive and labour-extensive to label the pixel-wise object masks in
a video. As a result, the amount of pixel-wise annotations in existing video
instance segmentation (VIS) datasets is small, limiting the generalization
capability of trained VIS models. An alternative but much cheaper solution is
to use bounding boxes to label instances in videos. Inspired by the recent
success of box-supervised image instance segmentation, we adapt the
state-of-the-art pixel-supervised VIS models to a box-supervised VIS (BoxVIS)
baseline, and observe slight performance degradation. We consequently propose
to improve the BoxVIS performance from two aspects. First, we propose a
box-center guided spatial-temporal pairwise affinity (STPA) loss to predict
instance masks for better spatial and temporal consistency. Second, we collect
a larger scale box-annotated VIS dataset (BVISD) by consolidating the videos
from current VIS benchmarks and converting images from the COCO dataset to
short pseudo video clips. With the proposed BVISD and the STPA loss, our
trained BoxVIS model achieves 43.2\% and 29.0\% mask AP on the YouTube-VIS 2021
and OVIS valid sets, respectively. It exhibits comparable instance mask
prediction performance and better generalization ability than state-of-the-art
pixel-supervised VIS models by using only 16\% of their annotation time and
cost. Codes and data can be found at \url{https://github.com/MinghanLi/BoxVIS}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I See Dead People: Gray-Box Adversarial Attack on Image-To-Text Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07591v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07591v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raz Lapid, Moshe Sipper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern image-to-text systems typically adopt the encoder-decoder framework,
which comprises two main components: an image encoder, responsible for
extracting image features, and a transformer-based decoder, used for generating
captions. Taking inspiration from the analysis of neural networks' robustness
against adversarial perturbations, we propose a novel gray-box algorithm for
creating adversarial examples in image-to-text models. Unlike image
classification tasks that have a finite set of class labels, finding visually
similar adversarial examples in an image-to-text task poses greater challenges
because the captioning system allows for a virtually infinite space of possible
captions. In this paper, we present a gray-box adversarial attack on
image-to-text, both untargeted and targeted. We formulate the process of
discovering adversarial perturbations as an optimization problem that uses only
the image-encoder component, meaning the proposed attack is language-model
agnostic. Through experiments conducted on the ViT-GPT2 model, which is the
most-used image-to-text model in Hugging Face, and the Flickr30k dataset, we
demonstrate that our proposed attack successfully generates visually similar
adversarial examples, both with untargeted and targeted captions. Notably, our
attack operates in a gray-box manner, requiring no knowledge about the decoder
module. We also show that our attacks fool the popular open-source platform
Hugging Face.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Navigating Uncertainty: The Role of Short-Term Trajectory Prediction in
  Autonomous Vehicle Safety 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05288v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05288v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sushil Sharma, Ganesh Sistu, Lucie Yahiaoui, Arindam Das, Mark Halton, Ciarán Eising
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous vehicles require accurate and reliable short-term trajectory
predictions for safe and efficient driving. While most commercial automated
vehicles currently use state machine-based algorithms for trajectory
forecasting, recent efforts have focused on end-to-end data-driven systems.
Often, the design of these models is limited by the availability of datasets,
which are typically restricted to generic scenarios. To address this
limitation, we have developed a synthetic dataset for short-term trajectory
prediction tasks using the CARLA simulator. This dataset is extensive and
incorporates what is considered complex scenarios - pedestrians crossing the
road, vehicles overtaking - and comprises 6000 perspective view images with
corresponding IMU and odometry information for each frame. Furthermore, an
end-to-end short-term trajectory prediction model using convolutional neural
networks (CNN) and long short-term memory (LSTM) networks has also been
developed. This model can handle corner cases, such as slowing down near zebra
crossings and stopping when pedestrians cross the road, without the need for
explicit encoding of the surrounding environment. In an effort to accelerate
this research and assist others, we are releasing our dataset and model to the
research community. Our datasets are publicly available on
https://github.com/sharmasushil/Navigating-Uncertainty-Trajectory-Prediction .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Table Structure Recognition with Dynamic Queries Enhanced
  Detection <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11615v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11615v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Wang, Weihong Lin, Chixiang Ma, Mingze Li, Zheng Sun, Lei Sun, Qiang Huo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a new table structure recognition (TSR) approach, called
TSRFormer, to robustly recognizing the structures of complex tables with
geometrical distortions from various table images. Unlike previous methods, we
formulate table separation line prediction as a line regression problem instead
of an image segmentation problem and propose a new two-stage dynamic queries
enhanced DETR based separation line regression approach, named DQ-DETR, to
predict separation lines from table images directly. Compared to Vallina DETR,
we propose three improvements in DQ-DETR to make the two-stage DETR framework
work efficiently and effectively for the separation line prediction task: 1) A
new query design, named Dynamic Query, to decouple single line query into
separable point queries which could intuitively improve the localization
accuracy for regression tasks; 2) A dynamic queries based progressive line
regression approach to progressively regressing points on the line which
further enhances localization accuracy for distorted tables; 3) A
prior-enhanced matching strategy to solve the slow convergence issue of DETR.
After separation line prediction, a simple relation network based cell merging
module is used to recover spanning cells. With these new techniques, our
TSRFormer achieves state-of-the-art performance on several benchmark datasets,
including SciTSR, PubTabNet, WTW and FinTabNet. Furthermore, we have validated
the robustness and high localization accuracy of our approach to tables with
complex structures, borderless cells, large blank spaces, empty or spanning
cells as well as distorted or even curved shapes on a more challenging
real-world in-house dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 11 figures, PR2023. arXiv admin note: substantial text
  overlap with arXiv:2208.04921</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STGlow: A Flow-based Generative Framework with Dual Graphormer for
  Pedestrian Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.11220v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.11220v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongqin Liang, Yuanman Li, Jiantao Zhou, Xia Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pedestrian trajectory prediction task is an essential component of
intelligent systems. Its applications include but are not limited to autonomous
driving, robot navigation, and anomaly detection of monitoring systems. Due to
the diversity of motion behaviors and the complex social interactions among
pedestrians, accurately forecasting their future trajectory is challenging.
Existing approaches commonly adopt GANs or CVAEs to generate diverse
trajectories. However, GAN-based methods do not directly model data in a latent
space, which may make them fail to have full support over the underlying data
distribution; CVAE-based methods optimize a lower bound on the log-likelihood
of observations, which may cause the learned distribution to deviate from the
underlying distribution. The above limitations make existing approaches often
generate highly biased or inaccurate trajectories. In this paper, we propose a
novel generative flow based framework with dual graphormer for pedestrian
trajectory prediction (STGlow). Different from previous approaches, our method
can more precisely model the underlying data distribution by optimizing the
exact log-likelihood of motion behaviors. Besides, our method has clear
physical meanings for simulating the evolution of human motion behaviors. The
forward process of the flow gradually degrades complex motion behavior into
simple behavior, while its reverse process represents the evolution of simple
behavior into complex motion behavior. Further, we introduce a dual graphormer
combining with the graph structure to more adequately model the temporal
dependencies and the mutual spatial interactions. Experimental results on
several benchmarks demonstrate that our method achieves much better performance
compared to previous state-of-the-art approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UOD: Universal One-shot Detection of Anatomical Landmarks <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07615v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07615v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heqin Zhu, Quan Quan, Qingsong Yao, Zaiyi Liu, S. kevin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One-shot medical landmark detection gains much attention and achieves great
success for its label-efficient training process. However, existing one-shot
learning methods are highly specialized in a single domain and suffer domain
preference heavily in the situation of multi-domain unlabeled data. Moreover,
one-shot learning is not robust that it faces performance drop when annotating
a sub-optimal image. To tackle these issues, we resort to developing a
domain-adaptive one-shot landmark detection framework for handling multi-domain
medical images, named Universal One-shot Detection (UOD). UOD consists of two
stages and two corresponding universal models which are designed as
combinations of domain-specific modules and domain-shared modules. In the first
stage, a domain-adaptive convolution model is self-supervised learned to
generate pseudo landmark labels. In the second stage, we design a
domain-adaptive transformer to eliminate domain preference and build the global
context for multi-domain data. Even though only one annotated sample from each
domain is available for training, the domain-shared modules help UOD aggregate
all one-shot samples to detect more robust and accurate landmarks. We
investigated both qualitatively and quantitatively the proposed UOD on three
widely-used public X-ray datasets in different anatomical domains (i.e., head,
hand, chest) and obtained state-of-the-art performances in each domain. The
code is available at
https://github.com/heqin-zhu/UOD_universal_oneshot_detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Eealy accepted by MICCAI 2023. 11pages, 4 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Effectiveness of Out-of-Distribution Data in <span class="highlight-title">Self-Supervised</span>
  Long-Tail Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04934v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04934v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianhong Bai, Zuozhu Liu, Hualiang Wang, Jin Hao, Yang Feng, Huanpeng Chu, Haoji Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Though Self-supervised learning (SSL) has been widely studied as a promising
technique for representation learning, it doesn't generalize well on
long-tailed datasets due to the majority classes dominating the feature space.
Recent work shows that the long-tailed learning performance could be boosted by
sampling extra in-domain (ID) data for self-supervised training, however,
large-scale ID data which can rebalance the minority classes are expensive to
collect. In this paper, we propose an alternative but easy-to-use and effective
solution, Contrastive with Out-of-distribution (OOD) data for Long-Tail
learning (COLT), which can effectively exploit OOD data to dynamically
re-balance the feature space. We empirically identify the counter-intuitive
usefulness of OOD samples in SSL long-tailed learning and principally design a
novel SSL method. Concretely, we first localize the `head' and `tail' samples
by assigning a tailness score to each OOD sample based on its neighborhoods in
the feature space. Then, we propose an online OOD sampling strategy to
dynamically re-balance the feature space. Finally, we enforce the model to be
capable of distinguishing ID and OOD samples by a distribution-level supervised
contrastive loss. Extensive experiments are conducted on various datasets and
several state-of-the-art SSL frameworks to verify the effectiveness of the
proposed method. The results show that our method significantly improves the
performance of SSL on long-tailed datasets by a large margin, and even
outperforms previous work which uses external ID data. Our code is available at
https://github.com/JianhongBai/COLT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TinyissimoYOLO: A Quantized, Low-Memory Footprint, TinyML Object
  Detection Network for Low Power Microcontrollers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00001v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00001v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Moosmann, Marco Giordano, Christian Vogt, Michele Magno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a highly flexible, quantized, memory-efficient, and
ultra-lightweight object detection network, called TinyissimoYOLO. It aims to
enable object detection on microcontrollers in the power domain of milliwatts,
with less than 0.5MB memory available for storing convolutional neural network
(CNN) weights. The proposed quantized network architecture with 422k
parameters, enables real-time object detection on embedded microcontrollers,
and it has been evaluated to exploit CNN accelerators. In particular, the
proposed network has been deployed on the MAX78000 microcontroller achieving
high frame-rate of up to 180fps and an ultra-low energy consumption of only
196{\mu}J per inference with an inference efficiency of more than 106
MAC/Cycle. TinyissimoYOLO can be trained for any multi-object detection.
However, considering the small network size, adding object detection classes
will increase the size and memory consumption of the network, thus object
detection with up to 3 classes is demonstrated. Furthermore, the network is
trained using quantization-aware training and deployed with 8-bit quantization
on different microcontrollers, such as STM32H7A3, STM32L4R9, Apollo4b and on
the MAX78000's CNN accelerator. Performance evaluations are presented in this
paper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published In: 2023 IEEE 5th International Conference on Artificial
  Intelligence Circuits and Systems (AICAS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unveiling the Invisible: Enhanced Detection and Analysis of Deteriorated
  Areas in Solar PV Modules Using Unsupervised Sensing Algorithms and 3D
  Augmented Reality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05136v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05136v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adel Oulefki, Yassine Himeur, Thaweesak Trongtiraku, Kahina Amara, Sos Agaian, Samir Benbelkacem, Mohamed Amine Guerroudji, Mohamed Zemmouri, Sahla Ferhat, Nadia Zenati, Shadi Atalla, Wathiq Mansoor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solar Photovoltaic (PV) is increasingly being used to address the global
concern of energy security. However, hot spot and snail trails in PV modules
caused mostly by crakes reduce their efficiency and power capacity. This
article presents a groundbreaking methodology for automatically identifying and
analyzing anomalies like hot spots and snail trails in Solar Photovoltaic (PV)
modules, leveraging unsupervised sensing algorithms and 3D Augmented Reality
(AR) visualization. By transforming the traditional methods of diagnosis and
repair, our approach not only enhances efficiency but also substantially cuts
down the cost of PV system maintenance. Validated through computer simulations
and real-world image datasets, the proposed framework accurately identifies
dirty regions, emphasizing the critical role of regular maintenance in
optimizing the power capacity of solar PV modules. Our immediate objective is
to leverage drone technology for real-time, automatic solar panel detection,
significantly boosting the efficacy of PV maintenance. The proposed methodology
could revolutionize solar PV maintenance, enabling swift, precise anomaly
detection without human intervention. This could result in significant cost
savings, heightened energy production, and improved overall performance of
solar PV systems. Moreover, the novel combination of unsupervised sensing
algorithms with 3D AR visualization heralds new opportunities for further
research and development in solar PV maintenance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ My3DGen: Building Lightweight Personalized 3D Generative Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05468v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05468v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luchao Qi, Jiaye Wu, Shengze Wang, Soumyadip Sengupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our paper presents My3DGen, a practical system for creating a personalized
and lightweight 3D generative prior using as few as 10 images. My3DGen can
reconstruct multi-view consistent images from an input test image, and generate
novel appearances by interpolating between any two images of the same
individual. While recent studies have demonstrated the effectiveness of
personalized generative priors in producing high-quality 2D portrait
reconstructions and syntheses, to the best of our knowledge, we are the first
to develop a personalized 3D generative prior. Instead of fine-tuning a large
pre-trained generative model with millions of parameters to achieve
personalization, we propose a parameter-efficient approach. Our method involves
utilizing a pre-trained model with fixed weights as a generic prior, while
training a separate personalized prior through low-rank decomposition of the
weights in each convolution and fully connected layer. However,
parameter-efficient few-shot fine-tuning on its own often leads to overfitting.
To address this, we introduce a regularization technique based on symmetry of
human faces. This regularization enforces that novel view renderings of a
training sample, rendered from symmetric poses, exhibit the same identity. By
incorporating this symmetry prior, we enhance the quality of reconstruction and
synthesis, particularly for non-frontal (profile) faces. Our final system
combines low-rank fine-tuning with symmetry regularization and significantly
surpasses the performance of pre-trained models, e.g. EG3D. It introduces only
approximately 0.6 million additional parameters per identity compared to 31
million for full finetuning of the original model. As a result, our system
achieves a 50-fold reduction in model size without sacrificing the quality of
the generated 3D faces. Code will be available at our project page:
https://luchaoqi.github.io/my3dgen.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Test-Time Training on Video Streams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05014v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05014v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renhao Wang, Yu Sun, Yossi Gandelsman, Xinlei Chen, Alexei A. Efros, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior work has established test-time training (TTT) as a general framework to
further improve a trained model at test time. Before making a prediction on
each test instance, the model is trained on the same instance using a
self-supervised task, such as image reconstruction with masked autoencoders. We
extend TTT to the streaming setting, where multiple test instances - video
frames in our case - arrive in temporal order. Our extension is online TTT: The
current model is initialized from the previous model, then trained on the
current frame and a small window of frames immediately before. Online TTT
significantly outperforms the fixed-model baseline for four tasks, on three
real-world datasets. The relative improvement is 45% and 66% for instance and
panoptic segmentation. Surprisingly, online TTT also outperforms its offline
variant that accesses more information, training on all frames from the entire
test video regardless of temporal order. This differs from previous findings
using synthetic videos. We conceptualize locality as the advantage of online
over offline TTT. We analyze the role of locality with ablations and a theory
based on bias-variance trade-off.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website with videos, dataset and code:
  https://video-ttt.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty-Aware Lidar Place Recognition in Novel Environments <span class="chip">IROS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.01361v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.01361v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keita Mason, Joshua Knights, Milad Ramezani, Peyman Moghadam, Dimity Miller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art lidar place recognition models exhibit unreliable
performance when tested on environments different from their training dataset,
which limits their use in complex and evolving environments. To address this
issue, we investigate the task of uncertainty-aware lidar place recognition,
where each predicted place must have an associated uncertainty that can be used
to identify and reject incorrect predictions. We introduce a novel evaluation
protocol and present the first comprehensive benchmark for this task, testing
across five uncertainty estimation techniques and three large-scale datasets.
Our results show that an Ensembles approach is the highest performing
technique, consistently improving the performance of lidar place recognition
and uncertainty estimation in novel environments, though it incurs a
computational cost. Code is publicly available at
https://github.com/csiro-robotics/Uncertainty-LPR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures. Accepted for publication at IEEE IROS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Low-Earth Satellite Orbit Determination Using Deep Convolutional
  Networks with Satellite Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12286v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12286v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohit Khorana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is increasingly common for satellites to lose connection with the ground
stations on Earth with which they communicate, due to signal interruptions from
the Earth's ionosphere and magnetosphere. Given the important roles that
satellites play in national defense, public safety, and worldwide
communications, finding ways to determine satellite trajectories in such
situations is a crucially important task. In this paper, we demonstrate the
efficacy of a novel computer vision based approach, which relies on earth
imagery taken by the satellite itself, to determine the orbit of a satellite
that has lost contact with its ground stations. We empirically observe
significant improvements by more than an order of magnitude, over the present
state of the art approach, namely, the Gibbs method for an initial orbit
estimate with the Kalman filter for differential error correction. We further
investigate the performance of the approach by comparing various neural
networks, namely, ResNet50, ResNet101, VGG19, VGG16, AlexNet, and CoAtNet4.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards a More Rigorous Science of Blindspot Discovery in Image
  Classification Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.04104v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.04104v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gregory Plumb, Nari Johnson, Ángel Alexander Cabrera, Ameet Talwalkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A growing body of work studies Blindspot Discovery Methods ("BDM"s): methods
that use an image embedding to find semantically meaningful (i.e., united by a
human-understandable concept) subsets of the data where an image classifier
performs significantly worse. Motivated by observed gaps in prior work, we
introduce a new framework for evaluating BDMs, SpotCheck, that uses synthetic
image datasets to train models with known blindspots and a new BDM, PlaneSpot,
that uses a 2D image representation. We use SpotCheck to run controlled
experiments that identify factors that influence BDM performance (e.g., the
number of blindspots in a model, or features used to define the blindspot) and
show that PlaneSpot is competitive with and in many cases outperforms existing
BDMs. Importantly, we validate these findings by designing additional
experiments that use real image data from MS-COCO, a large image benchmark
dataset. Our findings suggest several promising directions for future work on
BDM design and evaluation. Overall, we hope that the methodology and analyses
presented in this work will help facilitate a more rigorous science of
blindspot discovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>reviewed on OpenReview: https://openreview.net/forum?id=MaDvbLaBiF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Matching-based Data Valuation for Generative Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.10701v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.10701v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxi Yang, Wenglong Deng, Benlin Liu, Yangsibo Huang, Xiaoxiao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data valuation is critical in machine learning, as it helps enhance model
transparency and protect data properties. Existing data valuation methods have
primarily focused on discriminative models, neglecting deep generative models
that have recently gained considerable attention. Similar to discriminative
models, there is an urgent need to assess data contributions in deep generative
models as well. However, previous data valuation approaches mainly relied on
discriminative model performance metrics and required model retraining.
Consequently, they cannot be applied directly and efficiently to recent deep
generative models, such as generative adversarial networks and diffusion
models, in practice. To bridge this gap, we formulate the data valuation
problem in generative models from a similarity-matching perspective.
Specifically, we introduce Generative Model Valuator (GMValuator), the first
model-agnostic approach for any generative models, designed to provide data
valuation for generation tasks. We have conducted extensive experiments to
demonstrate the effectiveness of the proposed method. To the best of their
knowledge, GMValuator is the first work that offers a training-free, post-hoc
data valuation strategy for deep generative models.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Testing different Log Bases For Vector Model Weighting Technique 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06213v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06213v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kamel Assaf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information retrieval systems retrieves relevant documents based on a query
submitted by the user. The documents are initially indexed and the words in the
documents are assigned weights using a weighting technique called TFIDF which
is the product of Term Frequency (TF) and Inverse Document Frequency (IDF). TF
represents the number of occurrences of a term in a document. IDF measures
whether the term is common or rare across all documents. It is computed by
dividing the total number of documents in the system by the number of documents
containing the term and then computing the logarithm of the quotient. By
default, we use base 10 to calculate the logarithm. In this paper, we are going
to test this weighting technique by using a range of log bases from 0.1 to
100.0 to calculate the IDF. Testing different log bases for vector model
weighting technique is to highlight the importance of understanding the
performance of the system at different weighting values. We use the documents
of MED, CRAN, NPL, LISA, and CISI test collections that scientists assembled
explicitly for experiments in data information retrieval systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures, vector model, logarithms, tfidf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DDNAS: Discretized Differentiable Neural Architecture Search for Text
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuan-Chun Chen, Cheng-Te Li, Kuo-Jung Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Architecture Search (NAS) has shown promising capability in learning
text representation. However, existing text-based NAS neither performs a
learnable fusion of neural operations to optimize the architecture, nor encodes
the latent hierarchical categorization behind text input. This paper presents a
novel NAS method, Discretized Differentiable Neural Architecture Search
(DDNAS), for text representation learning and classification. With the
continuous relaxation of architecture representation, DDNAS can use gradient
descent to optimize the search. We also propose a novel discretization layer
via mutual information maximization, which is imposed on every search node to
model the latent hierarchical categorization in text representation. Extensive
experiments conducted on eight diverse real datasets exhibit that DDNAS can
consistently outperform the state-of-the-art NAS methods. While DDNAS relies on
only three basic operations, i.e., convolution, pooling, and none, to be the
candidates of NAS building blocks, its promising performance is noticeable and
extensible to obtain further improvement by adding more different operations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM Trans. Intell. Syst. Technol. (TIST) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Learning for Conversion Rate Prediction <span class="chip">SIGIR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Ouyang, Rui Dong, Xiuwu Zhang, Chaofeng Guo, Jinmei Luo, Xiangzheng Liu, Yanlong Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversion rate (CVR) prediction plays an important role in advertising
systems. Recently, supervised deep neural network-based models have shown
promising performance in CVR prediction. However, they are data hungry and
require an enormous amount of training data. In online advertising systems,
although there are millions to billions of ads, users tend to click only a
small set of them and to convert on an even smaller set. This data sparsity
issue restricts the power of these deep models. In this paper, we propose the
Contrastive Learning for CVR prediction (CL4CVR) framework. It associates the
supervised CVR prediction task with a contrastive learning task, which can
learn better data representations exploiting abundant unlabeled data and
improve the CVR prediction performance. To tailor the contrastive learning task
to the CVR prediction problem, we propose embedding masking (EM), rather than
feature masking, to create two views of augmented samples. We also propose a
false negative elimination (FNE) component to eliminate samples with the same
feature as the anchor sample, to account for the natural property in user
behavior data. We further propose a supervised positive inclusion (SPI)
component to include additional positive samples for each anchor sample, in
order to make full use of sparse but precious user conversion events.
Experimental results on two real-world conversion datasets demonstrate the
superior performance of CL4CVR. The source code is available at
https://github.com/DongRuiHust/CL4CVR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGIR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Disentangled Contrastive Collaborative Filtering <span class="chip">SIGIR'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.02759v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.02759v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xubin Ren, Lianghao Xia, Jiashu Zhao, Dawei Yin, Chao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies show that graph neural networks (GNNs) are prevalent to model
high-order relationships for collaborative filtering (CF). Towards this
research line, graph contrastive learning (GCL) has exhibited powerful
performance in addressing the supervision label shortage issue by learning
augmented user and item representations. While many of them show their
effectiveness, two key questions still remain unexplored: i) Most existing
GCL-based CF models are still limited by ignoring the fact that user-item
interaction behaviors are often driven by diverse latent intent factors (e.g.,
shopping for family party, preferred color or brand of products); ii) Their
introduced non-adaptive augmentation techniques are vulnerable to noisy
information, which raises concerns about the model's robustness and the risk of
incorporating misleading self-supervised signals. In light of these
limitations, we propose a Disentangled Contrastive Collaborative Filtering
framework (DCCF) to realize intent disentanglement with self-supervised
augmentation in an adaptive fashion. With the learned disentangled
representations with global context, our DCCF is able to not only distill
finer-grained latent factors from the entangled self-supervision signals but
also alleviate the augmentation-induced noise. Finally, the cross-view
contrastive learning task is introduced to enable adaptive augmentation with
our parameterized interaction mask generator. Experiments on various public
datasets demonstrate the superiority of our method compared to existing
solutions. Our model implementation is released at the link
https://github.com/HKUDS/DCCF.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a SIGIR'23 full paper</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">121</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diagnosis, Feedback, Adaptation: A Human-in-the-Loop Framework for
  Test-Time Policy Adaptation <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andi Peng, Aviv Netanyahu, Mark Ho, Tianmin Shu, Andreea Bobu, Julie Shah, Pulkit Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Policies often fail due to distribution shift -- changes in the state and
reward that occur when a policy is deployed in new environments. Data
augmentation can increase robustness by making the model invariant to
task-irrelevant changes in the agent's observation. However, designers don't
know which concepts are irrelevant a priori, especially when different end
users have different preferences about how the task is performed. We propose an
interactive framework to leverage feedback directly from the user to identify
personalized task-irrelevant concepts. Our key idea is to generate
counterfactual demonstrations that allow users to quickly identify possible
task-relevant and irrelevant concepts. The knowledge of task-irrelevant
concepts is then used to perform data augmentation and thus obtain a policy
adapted to personalized user objectives. We present experiments validating our
framework on discrete and continuous control tasks with real human users. Our
method (1) enables users to better understand agent failure, (2) reduces the
number of demonstrations required for fine-tuning, and (3) aligns the agent to
individual user task preferences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Machine Learning (ICML) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Budgeting Counterfactual for Offline RL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Liu, Pratik Chaudhari, Rasool Fakoor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The main challenge of offline reinforcement learning, where data is limited,
arises from a sequence of counterfactual reasoning dilemmas within the realm of
potential actions: What if we were to choose a different course of action?
These circumstances frequently give rise to extrapolation errors, which tend to
accumulate exponentially with the problem horizon. Hence, it becomes crucial to
acknowledge that not all decision steps are equally important to the final
outcome, and to budget the number of counterfactual decisions a policy make in
order to control the extrapolation. Contrary to existing approaches that use
regularization on either the policy or value function, we propose an approach
to explicitly bound the amount of out-of-distribution actions during training.
Specifically, our method utilizes dynamic programming to decide where to
extrapolate and where not to, with an upper bound on the decisions different
from behavior policy. It balances between the potential for improvement from
taking out-of-distribution actions and the risk of making errors due to
extrapolation. Theoretically, we justify our method by the constrained
optimality of the fixed point solution to our $Q$ updating rules. Empirically,
we show that the overall performance of our method is better than the
state-of-the-art offline RL methods on tasks in the widely-used D4RL
benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provably Faster Gradient Descent via Long Steps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Grimmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work establishes provably faster convergence rates for gradient descent
via a computer-assisted analysis technique. Our theory allows nonconstant
stepsize policies with frequent long steps potentially violating descent by
analyzing the overall effect of many iterations at once rather than the typical
one-iteration inductions used in most first-order method analyses. We show that
long steps, which may increase the objective value in the short term, lead to
provably faster convergence in the long term. A conjecture towards proving a
faster $O(1/T\log T)$ rate for gradient descent is also motivated along with
simple numerical validation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14pages plus references and appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Facial Reenactment Through a Personalized Generator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ariel Elazary, Yotam Nitzan, Daniel Cohen-Or
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the role of image generative models in facial reenactment
has been steadily increasing. Such models are usually subject-agnostic and
trained on domain-wide datasets. The appearance of the reenacted individual is
learned from a single image, and hence, the entire breadth of the individual's
appearance is not entirely captured, leading these methods to resort to
unfaithful hallucination. Thanks to recent advancements, it is now possible to
train a personalized generative model tailored specifically to a given
individual. In this paper, we propose a novel method for facial reenactment
using a personalized generator. We train the generator using frames from a
short, yet varied, self-scan video captured using a simple commodity camera.
Images synthesized by the personalized generator are guaranteed to preserve
identity. The premise of our work is that the task of reenactment is thus
reduced to accurately mimicking head poses and expressions. To this end, we
locate the desired frames in the latent space of the personalized generator
using carefully designed latent optimization. Through extensive evaluation, we
demonstrate state-of-the-art performance for facial reenactment. Furthermore,
we show that since our reenactment takes place in a semantic latent space, it
can be semantically edited and stylized in post-processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage: https://arielazary.github.io/PGR/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Locally Adaptive Federated Learning via Stochastic Polyak Stepsizes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sohom Mukherjee, Nicolas Loizou, Sebastian U. Stich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art federated learning algorithms such as FedAvg require
carefully tuned stepsizes to achieve their best performance. The improvements
proposed by existing adaptive federated methods involve tuning of additional
hyperparameters such as momentum parameters, and consider adaptivity only in
the server aggregation round, but not locally. These methods can be inefficient
in many practical scenarios because they require excessive tuning of
hyperparameters and do not capture local geometric information. In this work,
we extend the recently proposed stochastic Polyak stepsize (SPS) to the
federated learning setting, and propose new locally adaptive and nearly
parameter-free distributed SPS variants (FedSPS and FedDecSPS). We prove that
FedSPS converges linearly in strongly convex and sublinearly in convex settings
when the interpolation condition (overparametrization) is satisfied, and
converges to a neighborhood of the solution in the general case. We extend our
proposed method to a decreasing stepsize version FedDecSPS, that converges also
when the interpolation condition does not hold. We validate our theoretical
claims by performing illustrative convex experiments. Our proposed algorithms
match the optimization performance of FedAvg with the best tuned
hyperparameters in the i.i.d. case, and outperform FedAvg in the non-i.i.d.
case.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Patch n' Pack: NaViT, a Vision <span class="highlight-title">Transformer</span> for any Aspect Ratio and
  Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, Avital Oliver, Piotr Padlewski, Alexey Gritsenko, Mario Lučić, Neil Houlsby
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ubiquitous and demonstrably suboptimal choice of resizing images to a
fixed resolution before processing them with computer vision models has not yet
been successfully challenged. However, models such as the Vision Transformer
(ViT) offer flexible sequence-based modeling, and hence varying input sequence
lengths. We take advantage of this with NaViT (Native Resolution ViT) which
uses sequence packing during training to process inputs of arbitrary
resolutions and aspect ratios. Alongside flexible model usage, we demonstrate
improved training efficiency for large-scale supervised and contrastive
image-text pretraining. NaViT can be efficiently transferred to standard tasks
such as image and video classification, object detection, and semantic
segmentation and leads to improved results on robustness and fairness
benchmarks. At inference time, the input resolution flexibility can be used to
smoothly navigate the test-time cost-performance trade-off. We believe that
NaViT marks a departure from the standard, CNN-designed, input and modelling
pipeline used by most computer vision models, and represents a promising
direction for ViTs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Certified Proof Checker for Deep Neural Network Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06299v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06299v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Remi Desmartin, Omri Isac, Grant Passmore, Kathrin Stark, Guy Katz, Ekaterina Komendantskaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments in deep neural networks (DNNs) have led to their adoption
in safety-critical systems, which in turn has heightened the need for
guaranteeing their safety. These safety properties of DNNs can be proven using
tools developed by the verification community. However, these tools are
themselves prone to implementation bugs and numerical stability problems, which
make their reliability questionable. To overcome this, some verifiers produce
proofs of their results which can be checked by a trusted checker. In this
work, we present a novel implementation of a proof checker for DNN
verification. It improves on existing implementations by offering numerical
stability and greater verifiability. To achieve this, we leverage two key
capabilities of Imandra, an industrial theorem prover: its support of infinite
precision real arithmetic and its formal verification infrastructure. So far,
we have implemented a proof checker in Imandra, specified its correctness
properties and started to verify the checker's compliance with them. Our
ongoing work focuses on completing the formal verification of the checker and
further optimizing its performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instruction Mining: High-Quality Instruction Data Selection for Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06290v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06290v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihan Cao, Yanbin Kang, Lichao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models typically undergo two training stages, pretraining and
finetuning. Despite that large-scale pretraining endows the model with strong
capabilities to generate natural language responses, these pretrained models
can still fail to understand human instructions at times. To enhance language
models' ability of interpreting and responding to instructions, instruction
finetuning has emerged as a critical method in this area. Recent studies found
that large language models can be finetuned to perform well even with a small
amount of high-quality instruction-following data. However, the selection of
high-quality datasets for finetuning language models still lacks clear
guidelines to follow. In this paper, we propose InstructMining, a linear rule
for evaluating instruction-following data quality. We formulate InstructMining
using specific natural language indicators. To investigate the relationship
between data quality and these indicators, we further conduct extensive
finetuning experiments. The experiment results are then applied to estimating
parameters in InstructMining. To further investigate its performance, we use
InstructMining to select high-quality data from unseen datasets. Results
demonstrate that InstructMining can help select relatively high-quality samples
from various instruction-following datasets. Compared to models finetuned on
unfiltered datasets, models finetuned on InstructMining selected datasets
perform better on 42.5% cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress. 12 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rational Neural Network Controllers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Newton, Antonis Papachristodoulou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks have shown great success in many machine learning related
tasks, due to their ability to act as general function approximators. Recent
work has demonstrated the effectiveness of neural networks in control systems
(known as neural feedback loops), most notably by using a neural network as a
controller. However, one of the big challenges of this approach is that neural
networks have been shown to be sensitive to adversarial attacks. This means
that, unless they are designed properly, they are not an ideal candidate for
controllers due to issues with robustness and uncertainty, which are pivotal
aspects of control systems. There has been initial work on robustness to both
analyse and design dynamical systems with neural network controllers. However,
one prominent issue with these methods is that they use existing neural network
architectures tailored for traditional machine learning tasks. These structures
may not be appropriate for neural network controllers and it is important to
consider alternative architectures. This paper considers rational neural
networks and presents novel rational activation functions, which can be used
effectively in robustness problems for neural feedback loops. Rational
activation functions are replaced by a general rational neural network
structure, which is convex in the neural network's parameters. A method is
proposed to recover a stabilising controller from a Sum of Squares feasibility
test. This approach is then applied to a refined rational neural network which
is more compatible with Sum of Squares programming. Numerical examples show
that this method can successfully recover stabilising rational neural network
controllers for neural feedback loops with non-linear plants with noise and
parametric uncertainty.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 Pages, 12 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tackling Computational Heterogeneity in FL: A Few Theoretical Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adnan Ben Mansour, Gaia Carenini, Alexandre Duplessis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The future of machine learning lies in moving data collection along with
training to the edge. Federated Learning, for short FL, has been recently
proposed to achieve this goal. The principle of this approach is to aggregate
models learned over a large number of distributed clients, i.e.,
resource-constrained mobile devices that collect data from their environment,
to obtain a new more general model. The latter is subsequently redistributed to
clients for further training. A key feature that distinguishes federated
learning from data-center-based distributed training is the inherent
heterogeneity. In this work, we introduce and analyse a novel aggregation
framework that allows for formalizing and tackling computational heterogeneity
in federated optimization, in terms of both heterogeneous data and local
updates. Proposed aggregation algorithms are extensively analyzed from a
theoretical, and an experimental prospective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 7 figures. Extended version of the paper "Federated
  Learning Aggregation: New Robust Algorithms with Guarantees"
  (arXiv:2205.10864)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exposing the Fake: Effective Diffusion-Generated Images Detection <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruipeng Ma, Jinhao Duan, Fei Kong, Xiaoshuang Shi, Kaidi Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image synthesis has seen significant advancements with the advent of
diffusion-based generative models like Denoising Diffusion Probabilistic Models
(DDPM) and text-to-image diffusion models. Despite their efficacy, there is a
dearth of research dedicated to detecting diffusion-generated images, which
could pose potential security and privacy risks. This paper addresses this gap
by proposing a novel detection method called Stepwise Error for
Diffusion-generated Image Detection (SeDID). Comprising statistical-based
$\text{SeDID}_{\text{Stat}}$ and neural network-based
$\text{SeDID}_{\text{NNs}}$, SeDID exploits the unique attributes of diffusion
models, namely deterministic reverse and deterministic denoising computation
errors. Our evaluations demonstrate SeDID's superior performance over existing
methods when applied to diffusion models. Thus, our work makes a pivotal
contribution to distinguishing diffusion model-generated images, marking a
significant step in the domain of artificial intelligence security.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AdvML-Frontiers@ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-informed Machine Learning for Calibrating Macroscopic Traffic
  Flow Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Tang, Li Jin, Kaan Ozbay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Well-calibrated traffic flow models are fundamental to understanding traffic
phenomena and designing control strategies. Traditional calibration has been
developed base on optimization methods. In this paper, we propose a novel
physics-informed, learning-based calibration approach that achieves
performances comparable to and even better than those of optimization-based
methods. To this end, we combine the classical deep autoencoder, an
unsupervised machine learning model consisting of one encoder and one decoder,
with traffic flow models. Our approach informs the decoder of the physical
traffic flow models and thus induces the encoder to yield reasonable traffic
parameters given flow and speed measurements. We also introduce the denoising
autoencoder into our method so that it can handles not only with normal data
but also with corrupted data with missing values. We verified our approach with
a case study of I-210 E in California.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the hierarchical Bayesian modelling of frequency response functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06263v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06263v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        T. A. Dardeno, R. S. Mills, N. Dervilis, K. Worden, L. A. Bull
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Population-based structural health monitoring (PBSHM) aims to share valuable
information among members of a population, such as normal- and damage-condition
data, to improve inferences regarding the health states of the members. Even
when the population is comprised of nominally-identical structures, benign
variations among the members will exist as a result of slight differences in
material properties, geometry, boundary conditions, or environmental effects
(e.g., temperature changes). These discrepancies can affect modal properties
and present as changes in the characteristics of the resonance peaks of the
frequency response function (FRF). Many SHM strategies depend on monitoring the
dynamic properties of structures, so benign variations can be challenging for
the practical implementation of these systems. Another common challenge with
vibration-based SHM is data loss, which may result from transmission issues,
sensor failure, a sample-rate mismatch between sensors, and other causes.
Missing data in the time domain will result in decreased resolution in the
frequency domain, which can impair dynamic characterisation. The hierarchical
Bayesian approach provides a useful modelling structure for PBSHM, because
statistical distributions at the population and individual (or domain) level
are learnt simultaneously to bolster statistical strength among the parameters.
As a result, variance is reduced among the parameter estimates, particularly
when data are limited. In this paper, combined probabilistic FRF models are
developed for a small population of nominally-identical helicopter blades under
varying temperature conditions, using a hierarchical Bayesian structure. These
models address critical challenges in SHM, by accommodating benign variations
that present as differences in the underlying dynamics, while also considering
(and utilising), the similarities among the blades.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine learning and Topological data analysis identify unique features
  of human papillae in 3D scans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rayna Andreeva, Anwesha Sarkar, Rik Sarkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The tongue surface houses a range of papillae that are integral to the
mechanics and chemistry of taste and textural sensation. Although gustatory
function of papillae is well investigated, the uniqueness of papillae within
and across individuals remains elusive. Here, we present the first machine
learning framework on 3D microscopic scans of human papillae (n = 2092),
uncovering the uniqueness of geometric and topological features of papillae.
The finer differences in shapes of papillae are investigated computationally
based on a number of features derived from discrete differential geometry and
computational topology. Interpretable machine learning techniques show that
persistent homology features of the papillae shape are the most effective in
predicting the biological variables. Models trained on these features with
small volumes of data samples predict the type of papillae with an accuracy of
85%. The papillae type classification models can map the spatial arrangement of
filiform and fungiform papillae on a surface. Remarkably, the papillae are
found to be distinctive across individuals and an individual can be identified
with an accuracy of 48% among the 15 participants from a single papillae.
Collectively, this is the first unprecedented evidence demonstrating that
tongue papillae can serve as a unique identifier inspiring new research
direction for food preferences and oral diagnostics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifiability Guarantees for Causal Disentanglement from Soft
  Interventions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Zhang, Chandler Squires, Kristjan Greenewald, Akash Srivastava, Karthikeyan Shanmugam, Caroline Uhler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal disentanglement aims to uncover a representation of data using latent
variables that are interrelated through a causal model. Such a representation
is identifiable if the latent model that explains the data is unique. In this
paper, we focus on the scenario where unpaired observational and interventional
data are available, with each intervention changing the mechanism of a latent
variable. When the causal variables are fully observed, statistically
consistent algorithms have been developed to identify the causal model under
faithfulness assumptions. We here show that identifiability can still be
achieved with unobserved causal variables, given a generalized notion of
faithfulness. Our results guarantee that we can recover the latent causal model
up to an equivalence class and predict the effect of unseen combinations of
interventions, in the limit of infinite data. We implement our causal
disentanglement framework by developing an autoencoding variational Bayes
algorithm and apply it to the problem of predicting combinatorial perturbation
effects in genomics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Based Multi-Agent Adversarial Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06244v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06244v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sean Ye, Manisha Natarajan, Zixuan Wu, Matthew Gombolay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Target tracking plays a crucial role in real-world scenarios, particularly in
drug-trafficking interdiction, where the knowledge of an adversarial target's
location is often limited. Improving autonomous tracking systems will enable
unmanned aerial, surface, and underwater vehicles to better assist in
interdicting smugglers that use manned surface, semi-submersible, and aerial
vessels. As unmanned drones proliferate, accurate autonomous target estimation
is even more crucial for security and safety. This paper presents Constrained
Agent-based Diffusion for Enhanced Multi-Agent Tracking (CADENCE), an approach
aimed at generating comprehensive predictions of adversary locations by
leveraging past sparse state information. To assess the effectiveness of this
approach, we evaluate predictions on single-target and multi-target pursuit
environments, employing Monte-Carlo sampling of the diffusion model to estimate
the probability associated with each generated trajectory. We propose a novel
cross-attention based diffusion model that utilizes constraint-based sampling
to generate multimodal track hypotheses. Our single-target model surpasses the
performance of all baseline methods on Average Displacement Error (ADE) for
predictions across all time horizons.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reconstructing Spatiotemporal Data with C-VAEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiago F. R. Ribeiro, Fernando Silva, Rogério Luís de C. Costa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The continuous representation of spatiotemporal data commonly relies on using
abstract data types, such as \textit{moving regions}, to represent entities
whose shape and position continuously change over time. Creating this
representation from discrete snapshots of real-world entities requires using
interpolation methods to compute in-between data representations and estimate
the position and shape of the object of interest at arbitrary temporal points.
Existing region interpolation methods often fail to generate smooth and
realistic representations of a region's evolution. However, recent advancements
in deep learning techniques have revealed the potential of deep models trained
on discrete observations to capture spatiotemporal dependencies through
implicit feature learning.
  In this work, we explore the capabilities of Conditional Variational
Autoencoder (C-VAE) models to generate smooth and realistic representations of
the spatiotemporal evolution of moving regions. We evaluate our proposed
approach on a sparsely annotated dataset on the burnt area of a forest fire. We
apply compression operations to sample from the dataset and use the C-VAE model
and other commonly used interpolation algorithms to generate in-between region
representations. To evaluate the performance of the methods, we compare their
interpolation results with manually annotated data and regions generated by a
U-Net model. We also assess the quality of generated data considering temporal
consistency metrics.
  The proposed C-VAE-based approach demonstrates competitive results in
geometric similarity metrics. It also exhibits superior temporal consistency,
suggesting that C-VAE models may be a viable alternative to modelling the
spatiotemporal evolution of 2D moving regions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DSSE: a drone swarm search environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Castanares, Luis F. S. Carrete, Enrico F. Damiani, Leonardo D. M. de Abreu, José Fernando B. Brancalion, Fabrício J. Barth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Drone Swarm Search project is an environment, based on PettingZoo, that
is to be used in conjunction with multi-agent (or single-agent) reinforcement
learning algorithms. It is an environment in which the agents (drones), have to
find the targets (shipwrecked people). The agents do not know the position of
the target and do not receive rewards related to their own distance to the
target(s). However, the agents receive the probabilities of the target(s) being
in a certain cell of the map. The aim of this project is to aid in the study of
reinforcement learning algorithms that require dynamic probabilities as inputs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Molecular Modeling via Modality Blending 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06235v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06235v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiying Yu, Yudi Zhang, Yuyan Ni, Shikun Feng, Yanyan Lan, Hao Zhou, Jingjing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised molecular representation learning is critical for
molecule-based tasks such as AI-assisted drug discovery. Recent studies
consider leveraging both 2D and 3D information for representation learning,
with straightforward alignment strategies that treat each modality separately.
In this work, we introduce a novel "blend-then-predict" self-supervised
learning method (MoleBLEND), which blends atom relations from different
modalities into one unified relation matrix for encoding, then recovers
modality-specific information for both 2D and 3D structures. By treating atom
relationships as anchors, seemingly dissimilar 2D and 3D manifolds are aligned
and integrated at fine-grained relation-level organically. Extensive
experiments show that MoleBLEND achieves state-of-the-art performance across
major 2D/3D benchmarks. We further provide theoretical insights from the
perspective of mutual-information maximization, demonstrating that our method
unifies contrastive, generative (inter-modal prediction) and mask-then-predict
(intra-modal prediction) objectives into a single cohesive blend-then-predict
framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Local Conditional Neural Fields for Versatile and Generalizable
  Large-Scale Reconstructions in Computational Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06207v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06207v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Wang, Lei Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has transformed computational imaging, but traditional
pixel-based representations limit their ability to capture continuous,
multiscale details of objects. Here we introduce a novel Local Conditional
Neural Fields (LCNF) framework, leveraging a continuous implicit neural
representation to address this limitation. LCNF enables flexible object
representation and facilitates the reconstruction of multiscale information. We
demonstrate the capabilities of LCNF in solving the highly ill-posed inverse
problem in Fourier ptychographic microscopy (FPM) with multiplexed
measurements, achieving robust, scalable, and generalizable large-scale phase
retrieval. Unlike traditional neural fields frameworks, LCNF incorporates a
local conditional representation that promotes model generalization, learning
multiscale information, and efficient processing of large-scale imaging data.
By combining an encoder and a decoder conditioned on a learned latent vector,
LCNF achieves versatile continuous-domain super-resolution image
reconstruction. We demonstrate accurate reconstruction of wide field-of-view,
high-resolution phase images using only a few multiplexed measurements. LCNF
robustly captures the continuous object priors and eliminates various phase
artifacts, even when it is trained on imperfect datasets. The framework
exhibits strong generalization, reconstructing diverse objects even with
limited training data. Furthermore, LCNF can be trained on a physics simulator
using natural images and successfully applied to experimental measurements on
biological samples. Our results highlight the potential of LCNF for solving
large-scale inverse problems in computational imaging, with broad applicability
in various deep-learning-based techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Decentralized Partially Observable Mean Field Control for
  Artificial Collective Behavior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Cui, Sascha Hauck, Christian Fabian, Heinz Koeppl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent reinforcement learning (RL) methods have achieved success in various
domains. However, multi-agent RL (MARL) remains a challenge in terms of
decentralization, partial observability and scalability to many agents.
Meanwhile, collective behavior requires resolution of the aforementioned
challenges, and remains of importance to many state-of-the-art applications
such as active matter physics, self-organizing systems, opinion dynamics, and
biological or robotic swarms. Here, MARL via mean field control (MFC) offers a
potential solution to scalability, but fails to consider decentralized and
partially observable systems. In this paper, we enable decentralized behavior
of agents under partial information by proposing novel models for decentralized
partially observable MFC (Dec-POMFC), a broad class of problems with
permutation-invariant agents allowing for reduction to tractable single-agent
Markov decision processes (MDP) with single-agent RL solution. We provide
rigorous theoretical results, including a dynamic programming principle,
together with optimality guarantees for Dec-POMFC solutions applied to finite
swarms of interest. Algorithmically, we propose Dec-POMFC-based policy gradient
methods for MARL via centralized training and decentralized execution, together
with policy gradient approximation guarantees. In addition, we improve upon
state-of-the-art histogram-based MFC by kernel methods, which is of separate
interest also for fully observable MFC. We evaluate numerically on
representative collective behavior tasks such as adapted Kuramoto and Vicsek
swarming models, being on par with state-of-the-art MARL. Overall, our
framework takes a step towards RL-based engineering of artificial collective
behavior via MFC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Auxiliary-Tasks Learning for Physics-Informed Neural Network-Based
  Partial Differential Equations Solving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjun Yan, Xinhai Chen, Zhichao Wang, Enqiang Zhou, Jie Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-informed neural networks (PINNs) have emerged as promising surrogate
modes for solving partial differential equations (PDEs). Their effectiveness
lies in the ability to capture solution-related features through neural
networks. However, original PINNs often suffer from bottlenecks, such as low
accuracy and non-convergence, limiting their applicability in complex physical
contexts. To alleviate these issues, we proposed auxiliary-task learning-based
physics-informed neural networks (ATL-PINNs), which provide four different
auxiliary-task learning modes and investigate their performance compared with
original PINNs. We also employ the gradient cosine similarity algorithm to
integrate auxiliary problem loss with the primary problem loss in ATL-PINNs,
which aims to enhance the effectiveness of the auxiliary-task learning modes.
To the best of our knowledge, this is the first study to introduce
auxiliary-task learning modes in the context of physics-informed learning. We
conduct experiments on three PDE problems across different fields and
scenarios. Our findings demonstrate that the proposed auxiliary-task learning
modes can significantly improve solution accuracy, achieving a maximum
performance boost of 96.62% (averaging 28.23%) compared to the original
single-task PINNs. The code and dataset are open source at
https://github.com/junjun-yan/ATL-PINN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Generative Models for Physiological Signals: A Systematic
  Literature <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nour Neifar, Afef Mdhaffar, Achraf Ben-Hamadou, Mohamed Jmaiel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a systematic literature review on deep generative
models for physiological signals, particularly electrocardiogram,
electroencephalogram, photoplethysmogram and electromyogram. Compared to the
existing review papers, we present the first review that summarizes the recent
state-of-the-art deep generative models. By analysing the state-of-the-art
research related to deep generative models along with their main applications
and challenges, this review contributes to the overall understanding of these
models applied to physiological signals. Additionally, by highlighting the
employed evaluation protocol and the most used physiological databases, this
review facilitates the assessment and benchmarking of deep generative models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>paper under review, 34 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Maneuver Decision-Making Through Automatic Curriculum Reinforcement
  Learning Without Handcrafted Reward functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhang Hong-Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Maneuver decision-making is the core of unmanned combat aerial vehicle for
autonomous air combat. To solve this problem, we propose an automatic
curriculum reinforcement learning method, which enables agents to learn
effective decisions in air combat from scratch. The range of initial states are
used for distinguishing curricula of different difficulty levels, thereby
maneuver decision is divided into a series of sub-tasks from easy to difficult,
and test results are used to change sub-tasks. As sub-tasks change, agents
gradually learn to complete a series of sub-tasks from easy to difficult,
enabling them to make effective maneuvering decisions to cope with various
states without the need to spend effort designing reward functions. The
ablation studied show that the automatic curriculum learning proposed in this
article is an essential component for training through reinforcement learning,
namely, agents cannot complete effective decisions without curriculum learning.
Simulation experiments show that, after training, agents are able to make
effective decisions given different states, including tracking, attacking and
escaping, which are both rational and interpretable.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Net<span class="highlight-title">GPT</span>: A Native-AI Network Architecture Beyond Provisioning
  Personalized Generative Services 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06148v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06148v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Chen, Rongpeng Li, Zhifeng Zhao, Chenghui Peng, Jianjun Wu, Ekram Hossain, Honggang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have triggered tremendous success to empower
daily life by generative information, and the personalization of LLMs could
further contribute to their applications due to better alignment with human
intents. Towards personalized generative services, a collaborative cloud-edge
methodology sounds promising, as it facilitates the effective orchestration of
heterogeneous distributed communication and computing resources. In this
article, after discussing the pros and cons of several candidate cloud-edge
collaboration techniques, we put forward NetGPT to capably deploy appropriate
LLMs at the edge and the cloud in accordance with their computing capacity. In
addition, edge LLMs could efficiently leverage location-based information for
personalized prompt completion, thus benefiting the interaction with cloud
LLMs. After deploying representative open-source LLMs (e.g., GPT-2-base and
LLaMA model) at the edge and the cloud, we present the feasibility of NetGPT on
the basis of low-rank adaptation-based light-weight fine-tuning. Subsequently,
we highlight substantial essential changes required for a native artificial
intelligence (AI) network architecture towards NetGPT, with special emphasis on
deeper integration of communications and computing resources and careful
calibration of logical AI workflow. Furthermore, we demonstrate several
by-product benefits of NetGPT, given edge LLM's astonishing capability to
predict trends and infer intents, which possibly leads to a unified solution
for intelligent network management \& orchestration. In a nutshell, we argue
that NetGPT is a promising native-AI network architecture beyond provisioning
personalized generative services.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Hierarchical Interactive Multi-Object Search for Mobile
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Schmalstieg, Daniel Honerkamp, Tim Welschehold, Abhinav Valada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing object-search approaches enable robots to search through free
pathways, however, robots operating in unstructured human-centered environments
frequently also have to manipulate the environment to their needs. In this
work, we introduce a novel interactive multi-object search task in which a
robot has to open doors to navigate rooms and search inside cabinets and
drawers to find target objects. These new challenges require combining
manipulation and navigation skills in unexplored environments. We present
HIMOS, a hierarchical reinforcement learning approach that learns to compose
exploration, navigation, and manipulation skills. To achieve this, we design an
abstract high-level action space around a semantic map memory and leverage the
explored environment as instance navigation points. We perform extensive
experiments in simulation and the real-world that demonstrate that HIMOS
effectively transfers to new environments in a zero-shot manner. It shows
robustness to unseen subpolicies, failures in their execution, and different
robot kinematics. These capabilities open the door to a wide range of
downstream tasks across embodied AI and real-world use cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SoK: Comparing Different Membership Inference Attacks with a
  Comprehensive Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Niu, Xiaoyan Zhu, Moxuan Zeng, Ge Zhang, Qingyang Zhao, Chunhui Huang, Yangming Zhang, Suyu An, Yangzhong Wang, Xinghui Yue, Zhipeng He, Weihao Guo, Kuo Shen, Peng Liu, Yulong Shen, Xiaohong Jiang, Jianfeng Ma, Yuqing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Membership inference (MI) attacks threaten user privacy through determining
if a given data example has been used to train a target model. However, it has
been increasingly recognized that the "comparing different MI attacks"
methodology used in the existing works has serious limitations. Due to these
limitations, we found (through the experiments in this work) that some
comparison results reported in the literature are quite misleading. In this
paper, we seek to develop a comprehensive benchmark for comparing different MI
attacks, called MIBench, which consists not only the evaluation metrics, but
also the evaluation scenarios. And we design the evaluation scenarios from four
perspectives: the distance distribution of data samples in the target dataset,
the distance between data samples of the target dataset, the differential
distance between two datasets (i.e., the target dataset and a generated dataset
with only nonmembers), and the ratio of the samples that are made no inferences
by an MI attack. The evaluation metrics consist of ten typical evaluation
metrics. We have identified three principles for the proposed "comparing
different MI attacks" methodology, and we have designed and implemented the
MIBench benchmark with 84 evaluation scenarios for each dataset. In total, we
have used our benchmark to fairly and systematically compare 15
state-of-the-art MI attack algorithms across 588 evaluation scenarios, and
these evaluation scenarios cover 7 widely used datasets and 7 representative
types of models. All codes and evaluations of MIBench are publicly available at
https://github.com/MIBench/MIBench.github.io/blob/main/README.md.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages,15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep learning for dynamic graphs: models and benchmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06104v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06104v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessio Gravina, Davide Bacciu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in research on Deep Graph Networks (DGNs) has led to a
maturation of the domain of learning on graphs. Despite the growth of this
research field, there are still important challenges that are yet unsolved.
Specifically, there is an urge of making DGNs suitable for predictive tasks on
realworld systems of interconnected entities, which evolve over time. With the
aim of fostering research in the domain of dynamic graphs, at first, we survey
recent advantages in learning both temporal and spatial information, providing
a comprehensive overview of the current state-of-the-art in the domain of
representation learning for dynamic graphs. Secondly, we conduct a fair
performance comparison among the most popular proposed approaches, leveraging
rigorous model selection and assessment for all the methods, thus establishing
a sound baseline for evaluating new architectures and approaches
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Stochastic Dynamical Systems as an Implicit Regularization with
  Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06097v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06097v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Guo, Ting Gao, Yufu Lan, Peng Zhang, Sikun Yang, Jinqiao Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic Gumbel graph networks are proposed to learn high-dimensional time
series, where the observed dimensions are often spatially correlated. To that
end, the observed randomness and spatial-correlations are captured by learning
the drift and diffusion terms of the stochastic differential equation with a
Gumble matrix embedding, respectively. In particular, this novel framework
enables us to investigate the implicit regularization effect of the noise terms
in S-GGNs. We provide a theoretical guarantee for the proposed S-GGNs by
deriving the difference between the two corresponding loss functions in a small
neighborhood of weight. Then, we employ Kuramoto's model to generate data for
comparing the spectral density from the Hessian Matrix of the two loss
functions. Experimental results on real-world data, demonstrate that S-GGNs
exhibit superior convergence, robustness, and generalization, compared with
state-of-the-arts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Laplace Model Selection Revisited 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihao Andreas Lin, Javier Antorán, José Miguel Hernández-Lobato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Laplace approximation provides a closed-form model selection objective
for neural networks (NN). Online variants, which optimise NN parameters jointly
with hyperparameters, like weight decay strength, have seen renewed interest in
the Bayesian deep learning community. However, these methods violate Laplace's
method's critical assumption that the approximation is performed around a mode
of the loss, calling into question their soundness. This work re-derives online
Laplace methods, showing them to target a variational bound on a mode-corrected
variant of the Laplace evidence which does not make stationarity assumptions.
Online Laplace and its mode-corrected counterpart share stationary points where
1. the NN parameters are a maximum a posteriori, satisfying the Laplace
method's assumption, and 2. the hyperparameters maximise the Laplace evidence,
motivating online methods. We demonstrate that these optima are roughly
attained in practise by online algorithms using full-batch gradient descent on
UCI regression datasets. The optimised hyperparameters prevent overfitting and
outperform validation-based early stopping.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Advances in Approximate Bayesian Inference 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantitative CLTs in Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano Favaro, Boris Hanin, Domenico Marinucci, Ivan Nourdin, Giovanni Peccati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the distribution of a fully connected neural network with random
Gaussian weights and biases in which the hidden layer widths are proportional
to a large constant $n$. Under mild assumptions on the non-linearity, we obtain
quantitative bounds on normal approximations valid at large but finite $n$ and
any fixed network depth. Our theorems show, both for the finite-dimensional
distributions and the entire process, that the distance between a random fully
connected network (and its derivatives) to the corresponding infinite width
Gaussian process scales like $n^{-\gamma}$ for $\gamma>0,$ with the exponent
depending on the metric used to measure discrepancy. Our bounds are stronger in
terms of their dependence on network width than any previously available in the
literature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpreting deep embeddings for disease progression clustering <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06060v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06060v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Munoz-Farre, Antonios Poulakakis-Daktylidis, Dilini Mahesha Kothalawala, Andrea Rodriguez-Martinez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel approach for interpreting deep embeddings in the context
of patient clustering. We evaluate our approach on a dataset of participants
with type 2 diabetes from the UK Biobank, and demonstrate clinically meaningful
insights into disease progression patterns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Workshop on Interpretable ML in Healthcare at International
  Conference on Machine Learning (ICML), Honolulu, Hawaii, USA. 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Function-Space Regularization for Deep Bayesian Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihao Andreas Lin, Joe Watson, Pascal Klink, Jan Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian deep learning approaches assume model parameters to be latent random
variables and infer posterior distributions to quantify uncertainty, increase
safety and trust, and prevent overconfident and unpredictable behavior.
However, weight-space priors are model-specific, can be difficult to interpret
and are hard to specify. Instead, we apply a Dirichlet prior in predictive
space and perform approximate function-space variational inference. To this
end, we interpret conventional categorical predictions from stochastic neural
network classifiers as samples from an implicit Dirichlet distribution. By
adapting the inference, the same function-space prior can be combined with
different models without affecting model architecture or size. We illustrate
the flexibility and efficacy of such a prior with toy experiments and
demonstrate scalability, improved uncertainty quantification and adversarial
robustness with large-scale image classification experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Advances in Approximate Bayesian Inference 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Inventory Problems: Beyond the i.i.d. Setting with Online Convex
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06048v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06048v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Massil Hihat, Stéphane Gaïffas, Guillaume Garrigos, Simon Bussy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study multi-product inventory control problems where a manager makes
sequential replenishment decisions based on partial historical information in
order to minimize its cumulative losses. Our motivation is to consider general
demands, losses and dynamics to go beyond standard models which usually rely on
newsvendor-type losses, fixed dynamics, and unrealistic i.i.d. demand
assumptions. We propose MaxCOSD, an online algorithm that has provable
guarantees even for problems with non-i.i.d. demands and stateful dynamics,
including for instance perishability. We consider what we call non-degeneracy
assumptions on the demand process, and argue that they are necessary to allow
learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An OOD Multi-Task Perspective for Link Prediction with New Relation
  Types and Nodes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jincheng Zhou, Beatrice Bevilacqua, Bruno Ribeiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of inductive link prediction in (discrete) attributed multigraphs
infers missing attributed links (relations) between nodes in new test
multigraphs. Traditional relational learning methods face the challenge of
limited generalization to OOD test multigraphs containing both novel nodes and
novel relation types not seen in training. Recently, under the only assumption
that all relation types share the same structural predictive patterns (single
task), Gao et al. (2023) proposed an OOD link prediction method using the
theoretical concept of double exchangeability (for nodes & relation types), in
contrast to the (single) exchangeability (only for nodes) used to design Graph
Neural Networks (GNNs). In this work we further extend the double
exchangeability concept to multi-task double exchangeability, where we define
link prediction in attributed multigraphs that can have distinct and
potentially conflicting predictive patterns for different sets of relation
types (multiple tasks). Our empirical results on real-world datasets
demonstrate that our approach can effectively generalize to entirely new
relation types in test, without access to additional information, yielding
significant performance improvements over existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rhythm Modeling for Voice Conversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06040v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06040v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin van Niekerk, Marc-André Carbonneau, Herman Kamper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Voice conversion aims to transform source speech into a different target
voice. However, typical voice conversion systems do not account for rhythm,
which is an important factor in the perception of speaker identity. To bridge
this gap, we introduce Urhythmic-an unsupervised method for rhythm conversion
that does not require parallel data or text transcriptions. Using
self-supervised representations, we first divide source audio into segments
approximating sonorants, obstruents, and silences. Then we model rhythm by
estimating speaking rate or the duration distribution of each segment type.
Finally, we match the target speaking rate or rhythm by time-stretching the
speech segments. Experiments show that Urhythmic outperforms existing
unsupervised methods in terms of quality and prosody. Code and checkpoints:
https://github.com/bshall/urhythmic. Audio demo page:
https://ubisoft-laforge.github.io/speech/urhythmic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, 4 tables, submitted to IEEE Signal Processing
  Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning from Exemplary Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06026v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06026v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Misgina Tsighe Hagos, Kathleen M. Curran, Brian Mac Namee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  eXplanation Based Learning (XBL) is a form of Interactive Machine Learning
(IML) that provides a model refining approach via user feedback collected on
model explanations. Although the interactivity of XBL promotes model
transparency, XBL requires a huge amount of user interaction and can become
expensive as feedback is in the form of detailed annotation rather than simple
category labelling which is more common in IML. This expense is exacerbated in
high stakes domains such as medical image classification. To reduce the effort
and expense of XBL we introduce a new approach that uses two input instances
and their corresponding Gradient Weighted Class Activation Mapping (GradCAM)
model explanations as exemplary explanations to implement XBL. Using a medical
image classification task, we demonstrate that, using minimal human input, our
approach produces improved explanations (+0.02, +3%) and achieves reduced
classification performance (-0.04, -4%) when compared against a model trained
without interactions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Effective and Efficient Time-aware Entity Alignment Framework via
  Two-aspect Three-view Label Propagation <span class="chip">IJCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Cai, Xin Mao, Youshao Xiao, Changxu Wu, Man Lan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity alignment (EA) aims to find the equivalent entity pairs between
different knowledge graphs (KGs), which is crucial to promote knowledge fusion.
With the wide use of temporal knowledge graphs (TKGs), time-aware EA (TEA)
methods appear to enhance EA. Existing TEA models are based on Graph Neural
Networks (GNN) and achieve state-of-the-art (SOTA) performance, but it is
difficult to transfer them to large-scale TKGs due to the scalability issue of
GNN. In this paper, we propose an effective and efficient non-neural EA
framework between TKGs, namely LightTEA, which consists of four essential
components: (1) Two-aspect Three-view Label Propagation, (2) Sparse Similarity
with Temporal Constraints, (3) Sinkhorn Operator, and (4) Temporal Iterative
Learning. All of these modules work together to improve the performance of EA
while reducing the time consumption of the model. Extensive experiments on
public datasets indicate that our proposed model significantly outperforms the
SOTA methods for EA between TKGs, and the time consumed by LightTEA is only
dozens of seconds at most, no more than 10% of the most efficient TEA method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IJCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Happens During Finetuning of Vision <span class="highlight-title">Transformer</span>s: An Invariance
  Based Investigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriele Merlin, Vedant Nanda, Ruchit Rawal, Mariya Toneva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pretrain-finetune paradigm usually improves downstream performance over
training a model from scratch on the same task, becoming commonplace across
many areas of machine learning. While pretraining is empirically observed to be
beneficial for a range of tasks, there is not a clear understanding yet of the
reasons for this effect. In this work, we examine the relationship between
pretrained vision transformers and the corresponding finetuned versions on
several benchmark datasets and tasks. We present new metrics that specifically
investigate the degree to which invariances learned by a pretrained model are
retained or forgotten during finetuning. Using these metrics, we present a
suite of empirical findings, including that pretraining induces transferable
invariances in shallow layers and that invariances from deeper pretrained
layers are compressed towards shallower layers during finetuning. Together,
these findings contribute to understanding some of the reasons for the
successes of pretrained models and the changes that a pretrained model
undergoes when finetuned on a downstream task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CoLLAs 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DDNAS: Discretized Differentiable Neural Architecture Search for Text
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuan-Chun Chen, Cheng-Te Li, Kuo-Jung Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Architecture Search (NAS) has shown promising capability in learning
text representation. However, existing text-based NAS neither performs a
learnable fusion of neural operations to optimize the architecture, nor encodes
the latent hierarchical categorization behind text input. This paper presents a
novel NAS method, Discretized Differentiable Neural Architecture Search
(DDNAS), for text representation learning and classification. With the
continuous relaxation of architecture representation, DDNAS can use gradient
descent to optimize the search. We also propose a novel discretization layer
via mutual information maximization, which is imposed on every search node to
model the latent hierarchical categorization in text representation. Extensive
experiments conducted on eight diverse real datasets exhibit that DDNAS can
consistently outperform the state-of-the-art NAS methods. While DDNAS relies on
only three basic operations, i.e., convolution, pooling, and none, to be the
candidates of NAS building blocks, its promising performance is noticeable and
extensible to obtain further improvement by adding more different operations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM Trans. Intell. Syst. Technol. (TIST) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive <span class="highlight-title">Review</span> of Automated Data Annotation Techniques in Human
  Activity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florenc Demrozi, Cristian Turetta, Fadi Al Machot, Graziano Pravadelli, Philipp H. Kindt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human Activity Recognition (HAR) has become one of the leading research
topics of the last decade. As sensing technologies have matured and their
economic costs have declined, a host of novel applications, e.g., in
healthcare, industry, sports, and daily life activities have become popular.
The design of HAR systems requires different time-consuming processing steps,
such as data collection, annotation, and model training and optimization. In
particular, data annotation represents the most labor-intensive and cumbersome
step in HAR, since it requires extensive and detailed manual work from human
annotators. Therefore, different methodologies concerning the automation of the
annotation procedure in HAR have been proposed. The annotation problem occurs
in different notions and scenarios, which all require individual solutions. In
this paper, we provide the first systematic review on data annotation
techniques for HAR. By grouping existing approaches into classes and providing
a taxonomy, our goal is to support the decision on which techniques can be
beneficially used in a given scenario.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 5 figures, 20 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>s in Reinforcement Learning: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05979v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05979v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Agarwal, Aamer Abdul Rahman, Pierre-Luc St-Charles, Simon J. D. Prince, Samira Ebrahimi Kahou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have significantly impacted domains like natural language
processing, computer vision, and robotics, where they improve performance
compared to other neural networks. This survey explores how transformers are
used in reinforcement learning (RL), where they are seen as a promising
solution for addressing challenges such as unstable training, credit
assignment, lack of interpretability, and partial observability. We begin by
providing a brief domain overview of RL, followed by a discussion on the
challenges of classical RL algorithms. Next, we delve into the properties of
the transformer and its variants and discuss the characteristics that make them
well-suited to address the challenges inherent in RL. We examine the
application of transformers to various aspects of RL, including representation
learning, transition and reward function modeling, and policy optimization. We
also discuss recent research that aims to enhance the interpretability and
efficiency of transformers in RL, using visualization techniques and efficient
training strategies. Often, the transformer architecture must be tailored to
the specific needs of a given application. We present a broad overview of how
transformers have been adapted for several applications, including robotics,
medicine, language modeling, cloud computing, and combinatorial optimization.
We conclude by discussing the limitations of using transformers in RL and
assess their potential for catalyzing future breakthroughs in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Safe Self-Distillation of Internet-Scale Text-to-Image Diffusion
  Models <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05977v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05977v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanghyun Kim, Seohyeon Jung, Balhae Kim, Moonseok Choi, Jinwoo Shin, Juho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale image generation models, with impressive quality made possible by
the vast amount of data available on the Internet, raise social concerns that
these models may generate harmful or copyrighted content. The biases and
harmfulness arise throughout the entire training process and are hard to
completely remove, which have become significant hurdles to the safe deployment
of these models. In this paper, we propose a method called SDD to prevent
problematic content generation in text-to-image diffusion models. We
self-distill the diffusion model to guide the noise estimate conditioned on the
target removal concept to match the unconditional one. Compared to the previous
methods, our method eliminates a much greater proportion of harmful content
from the generated images without degrading the overall image quality.
Furthermore, our method allows the removal of multiple concepts at once,
whereas previous works are limited to removing a single concept at a time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 13 figures, ICML 2023 Workshop on Challenges in Deployable
  Generative AI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Outlier detection in regression: conic quadratic formulations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrés Gómez, José Neto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many applications, when building linear regression models, it is important
to account for the presence of outliers, i.e., corrupted input data points.
Such problems can be formulated as mixed-integer optimization problems
involving cubic terms, each given by the product of a binary variable and a
quadratic term of the continuous variables. Existing approaches in the
literature, typically relying on the linearization of the cubic terms using
big-M constraints, suffer from weak relaxation and poor performance in
practice. In this work we derive stronger second-order conic relaxations that
do not involve big-M constraints. Our computational experiments indicate that
the proposed formulations are several orders-of-magnitude faster than existing
big-M formulations in the literature for this problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Learning for Conversion Rate Prediction <span class="chip">SIGIR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Ouyang, Rui Dong, Xiuwu Zhang, Chaofeng Guo, Jinmei Luo, Xiangzheng Liu, Yanlong Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversion rate (CVR) prediction plays an important role in advertising
systems. Recently, supervised deep neural network-based models have shown
promising performance in CVR prediction. However, they are data hungry and
require an enormous amount of training data. In online advertising systems,
although there are millions to billions of ads, users tend to click only a
small set of them and to convert on an even smaller set. This data sparsity
issue restricts the power of these deep models. In this paper, we propose the
Contrastive Learning for CVR prediction (CL4CVR) framework. It associates the
supervised CVR prediction task with a contrastive learning task, which can
learn better data representations exploiting abundant unlabeled data and
improve the CVR prediction performance. To tailor the contrastive learning task
to the CVR prediction problem, we propose embedding masking (EM), rather than
feature masking, to create two views of augmented samples. We also propose a
false negative elimination (FNE) component to eliminate samples with the same
feature as the anchor sample, to account for the natural property in user
behavior data. We further propose a supervised positive inclusion (SPI)
component to include additional positive samples for each anchor sample, in
order to make full use of sparse but precious user conversion events.
Experimental results on two real-world conversion datasets demonstrate the
superior performance of CL4CVR. The source code is available at
https://github.com/DongRuiHust/CL4CVR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGIR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VoxPoser: Composable 3D Value Maps for Robotic Manipulation with
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, Li Fei-Fei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are shown to possess a wealth of actionable
knowledge that can be extracted for robot manipulation in the form of reasoning
and planning. Despite the progress, most still rely on pre-defined motion
primitives to carry out the physical interactions with the environment, which
remains a major bottleneck. In this work, we aim to synthesize robot
trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a
large variety of manipulation tasks given an open-set of instructions and an
open-set of objects. We achieve this by first observing that LLMs excel at
inferring affordances and constraints given a free-form language instruction.
More importantly, by leveraging their code-writing capabilities, they can
interact with a visual-language model (VLM) to compose 3D value maps to ground
the knowledge into the observation space of the agent. The composed value maps
are then used in a model-based planning framework to zero-shot synthesize
closed-loop robot trajectories with robustness to dynamic perturbations. We
further demonstrate how the proposed framework can benefit from online
experiences by efficiently learning a dynamics model for scenes that involve
contact-rich interactions. We present a large-scale study of the proposed
method in both simulated and real-robot environments, showcasing the ability to
perform a large variety of everyday manipulation tasks specified in free-form
natural language. Project website: https://voxposer.github.io
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Distilled Quantization: Achieving High Compression Rates in
  <span class="highlight-title">Transformer</span>-Based Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05972v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05972v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James O' Neill, Sourav Dutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the effects of post-training quantization and
quantization-aware training on the generalization of Transformer language
models. We present a new method called self-distilled quantization (SDQ) that
minimizes accumulative quantization errors and outperforms baselines. We apply
SDQ to multilingual models XLM-R-Base and InfoXLM-Base and demonstrate that
both models can be reduced from 32-bit floating point weights to 8-bit integer
weights while maintaining a high level of performance on the XGLUE benchmark.
Our results also highlight the challenges of quantizing multilingual models,
which must generalize to languages they were not fine-tuned on.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Giving Robots a Hand: Learning Generalizable Manipulation with
  Eye-in-Hand Human Video Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moo Jin Kim, Jiajun Wu, Chelsea Finn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Eye-in-hand cameras have shown promise in enabling greater sample efficiency
and generalization in vision-based robotic manipulation. However, for robotic
imitation, it is still expensive to have a human teleoperator collect large
amounts of expert demonstrations with a real robot. Videos of humans performing
tasks, on the other hand, are much cheaper to collect since they eliminate the
need for expertise in robotic teleoperation and can be quickly captured in a
wide range of scenarios. Therefore, human video demonstrations are a promising
data source for learning generalizable robotic manipulation policies at scale.
In this work, we augment narrow robotic imitation datasets with broad unlabeled
human video demonstrations to greatly enhance the generalization of eye-in-hand
visuomotor policies. Although a clear visual domain gap exists between human
and robot data, our framework does not need to employ any explicit domain
adaptation method, as we leverage the partial observability of eye-in-hand
cameras as well as a simple fixed image masking scheme. On a suite of eight
real-world tasks involving both 3-DoF and 6-DoF robot arm control, our method
improves the success rates of eye-in-hand manipulation policies by 58%
(absolute) on average, enabling robots to generalize to both new environment
configurations and new tasks that are unseen in the robot demonstration data.
See video results at https://giving-robots-a-hand.github.io/ .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 7 figures, project webpage at
  https://giving-robots-a-hand.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Newell's theory based feature transformations for spatio-temporal
  traffic prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Agnimitra Sengupta, S. Ilgin Guler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning (DL) models for spatio-temporal traffic flow forecasting employ
convolutional or graph-convolutional filters along with recurrent neural
networks to capture spatial and temporal dependencies in traffic data. These
models, such as CNN-LSTM, utilize traffic flows from neighboring detector
stations to predict flows at a specific location of interest. However, these
models are limited in their ability to capture the broader dynamics of the
traffic system, as they primarily learn features specific to the detector
configuration and traffic characteristics at the target location. Hence, the
transferability of these models to different locations becomes challenging,
particularly when data is unavailable at the new location for model training.
To address this limitation, we propose a traffic flow physics-based feature
transformation for spatio-temporal DL models. This transformation incorporates
Newell's uncongested and congested-state estimators of traffic flows at the
target locations, enabling the models to learn broader dynamics of the system.
Our methodology is empirically validated using traffic data from two different
locations. The results demonstrate that the proposed feature transformation
improves the models' performance in predicting traffic flows over different
prediction horizons, as indicated by better goodness-of-fit statistics. An
important advantage of our framework is its ability to be transferred to new
locations where data is unavailable. This is achieved by appropriately
accounting for spatial dependencies based on station distances and various
traffic parameters. In contrast, regular DL models are not easily transferable
as their inputs remain fixed. It should be noted that due to data limitations,
we were unable to perform spatial sensitivity analysis, which calls for further
research using simulated data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diversity-enhancing Generative Network for Few-shot Hypothesis
  Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05948v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05948v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruijiang Dong, Feng Liu, Haoang Chi, Tongliang Liu, Mingming Gong, Gang Niu, Masashi Sugiyama, Bo Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating unlabeled data has been recently shown to help address the
few-shot hypothesis adaptation (FHA) problem, where we aim to train a
classifier for the target domain with a few labeled target-domain data and a
well-trained source-domain classifier (i.e., a source hypothesis), for the
additional information of the highly-compatible unlabeled data. However, the
generated data of the existing methods are extremely similar or even the same.
The strong dependency among the generated data will lead the learning to fail.
In this paper, we propose a diversity-enhancing generative network (DEG-Net)
for the FHA problem, which can generate diverse unlabeled data with the help of
a kernel independence measure: the Hilbert-Schmidt independence criterion
(HSIC). Specifically, DEG-Net will generate data via minimizing the HSIC value
(i.e., maximizing the independence) among the semantic features of the
generated data. By DEG-Net, the generated unlabeled data are more diverse and
more effective for addressing the FHA problem. Experimental results show that
the DEG-Net outperforms existing FHA baselines and further verifies that
generating diverse data plays a vital role in addressing the FHA problem
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Bayesian approach to quantifying uncertainties and improving
  generalizability in traffic prediction models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Agnimitra Sengupta, Sudeepta Mondal, Adway Das, S. Ilgin Guler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep-learning models for traffic data prediction can have superior
performance in modeling complex functions using a multi-layer architecture.
However, a major drawback of these approaches is that most of these approaches
do not offer forecasts with uncertainty estimates, which are essential for
traffic operations and control. Without uncertainty estimates, it is difficult
to place any level of trust to the model predictions, and operational
strategies relying on overconfident predictions can lead to worsening traffic
conditions. In this study, we propose a Bayesian recurrent neural network
framework for uncertainty quantification in traffic prediction with higher
generalizability by introducing spectral normalization to its hidden layers. In
our paper, we have shown that normalization alters the training process of deep
neural networks by controlling the model's complexity and reducing the risk of
overfitting to the training data. This, in turn, helps improve the
generalization performance of the model on out-of-distribution datasets.
Results demonstrate that spectral normalization improves uncertainty estimates
and significantly outperforms both the layer normalization and model without
normalization in single-step prediction horizons. This improved performance can
be attributed to the ability of spectral normalization to better localize the
feature space of the data under perturbations. Our findings are especially
relevant to traffic management applications, where predicting traffic
conditions across multiple locations is the goal, but the availability of
training data from multiple locations is limited. Spectral normalization,
therefore, provides a more generalizable approach that can effectively capture
the underlying patterns in traffic data without requiring location-specific
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ YOGA: Deep Object Detection in the Wild with Lightweight Feature
  Learning and Multiscale Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raja Sunkara, Tie Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce YOGA, a deep learning based yet lightweight object detection
model that can operate on low-end edge devices while still achieving
competitive accuracy. The YOGA architecture consists of a two-phase feature
learning pipeline with a cheap linear transformation, which learns feature maps
using only half of the convolution filters required by conventional
convolutional neural networks. In addition, it performs multi-scale feature
fusion in its neck using an attention mechanism instead of the naive
concatenation used by conventional detectors. YOGA is a flexible model that can
be easily scaled up or down by several orders of magnitude to fit a broad range
of hardware constraints. We evaluate YOGA on COCO-val and COCO-testdev datasets
with other over 10 state-of-the-art object detectors. The results show that
YOGA strikes the best trade-off between model size and accuracy (up to 22%
increase of AP and 23-34% reduction of parameters and FLOPs), making it an
ideal choice for deployment in the wild on low-end edge devices. This is
further affirmed by our hardware implementation and evaluation on NVIDIA Jetson
Nano.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Pattern Recognition (Elsevier), July 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Filling time-series gaps using image techniques: Multidimensional
  context autoencoder approach for building energy data imputation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05926v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05926v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chun Fu, Matias Quintana, Zoltan Nagy, Clayton Miller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building energy prediction and management has become increasingly important
in recent decades, driven by the growth of Internet of Things (IoT) devices and
the availability of more energy data. However, energy data is often collected
from multiple sources and can be incomplete or inconsistent, which can hinder
accurate predictions and management of energy systems and limit the usefulness
of the data for decision-making and research. To address this issue, past
studies have focused on imputing missing gaps in energy data, including random
and continuous gaps. One of the main challenges in this area is the lack of
validation on a benchmark dataset with various building and meter types, making
it difficult to accurately evaluate the performance of different imputation
methods. Another challenge is the lack of application of state-of-the-art
imputation methods for missing gaps in energy data. Contemporary
image-inpainting methods, such as Partial Convolution (PConv), have been widely
used in the computer vision domain and have demonstrated their effectiveness in
dealing with complex missing patterns. To study whether energy data imputation
can benefit from the image-based deep learning method, this study compared
PConv, Convolutional neural networks (CNNs), and weekly persistence method
using one of the biggest publicly available whole building energy datasets,
consisting of 1479 power meters worldwide, as the benchmark. The results show
that, compared to the CNN with the raw time series (1D-CNN) and the weekly
persistence method, neural network models with reshaped energy data with two
dimensions reduced the Mean Squared Error (MSE) by 10% to 30%. The advanced
deep learning method, Partial convolution (PConv), has further reduced the MSE
by 20-30% than 2D-CNN and stands out among all models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Medical Image-Text-Label Contrastive Learning With Continuous
  <span class="highlight-title">Prompt</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive language-image Pre-training (CLIP) [13] can leverage large
datasets of unlabeled Image-Text pairs, which have demonstrated impressive
performance in various downstream tasks. Given that annotating medical data is
time-consuming and laborious, Image-Text Pre-training has promising
applications in exploiting large-scale medical image and radiology report
datasets. However, medical Image-Text Pre-training faces several challenges, as
follows: (1) Due to privacy concerns, the amount of available medical data is
relatively small compared to natural data, leading to weaker generalization
ability of the model. (2) Medical images are highly similar with only
fine-grained differences in subtleties, resulting in a large number of
false-negative sample pairs in comparison learning. (3) The hand-crafted Prompt
usually differs from the natural medical image report, Subtle changes in
wording can lead to significant differences in performance. In this paper, we
propose a unified Image-Text-Label contrastive learning framework based on
continuous prompts, with three main contributions. First, We unified the data
of images, text, and labels, which greatly expanded the training data that the
model could utilize. Second, we address the issue of data diversity and the
impact of hand-crafted prompts on model performance by introducing continuous
implicit prompts. Lastly, we propose a ImageText-Label contrastive Training to
mitigate the problem of too many false-negative samples. We demonstrate through
sufficient experiments that the Unified Medical Contrastive Learning (UMCL)
framework exhibits excellent performance on several downstream tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span> Generate Train (PGT): A framework for few-shot domain adaptation,
  alignment, and uncertainty calibration of a retriever augmented generation
  (RAG) model for domain specific open book question-answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05915v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05915v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        C. S. Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a framework - Prompt, Generate, Train (PGT) - to efficiently
develop a generative question-answering model for open-book question-answering
over a proprietary collection of text documents. The framework adapts a
retriever augmented generation model to the target domain using supervised
finetuning and reinforcement learning with synthetic feedback in a few-shot
setting. This yields an aligned, uncertainty calibrated model that is
competitive with GPT-4 based in-context retrieval augmented generation in
generating relevant answers at lower serving costs. The synthetic generation
pipeline generates high quality synthetic training data musing a medium sized
LLM, Flan-T5 XXL, and a novel consistency filtering scheme. The pipeline is
designed to generate both abstractive and extractive questions that span the
entire corpus. Using samples from this dataset, the framework fine-tunes a
smaller RAG model comprising a dense retriever and a smaller sized LLM on
samples from the dataset. In parallel, the framework trains a Reward model to
score domain grounded answers higher than hallucinated answers. In the next
phase, the framework aligns to the RAG model with the target domain using
reinforcement learning. This step improves the RAG model's ability to generate
grounded answers and ignore out of domain questions. In the final phase, the
framework calibrates the model uncertainty for extractive question-answers.
This is a desirable feature since the model can be integrated into a cascading
system where the RAG model's answer is surfaced only when the model is
confident of its answer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FIS-ONE: Floor Identification System with One Label for Crowdsourced RF
  Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05914v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05914v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weipeng Zhuo, Ka Ho Chiu, Jierun Chen, Ziqi Zhao, S. -H. Gary Chan, Sangtae Ha, Chul-Ho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Floor labels of crowdsourced RF signals are crucial for many smart-city
applications, such as multi-floor indoor localization, geofencing, and robot
surveillance. To build a prediction model to identify the floor number of a new
RF signal upon its measurement, conventional approaches using the crowdsourced
RF signals assume that at least few labeled signal samples are available on
each floor. In this work, we push the envelope further and demonstrate that it
is technically feasible to enable such floor identification with only one
floor-labeled signal sample on the bottom floor while having the rest of signal
samples unlabeled.
  We propose FIS-ONE, a novel floor identification system with only one labeled
sample. FIS-ONE consists of two steps, namely signal clustering and cluster
indexing. We first build a bipartite graph to model the RF signal samples and
obtain a latent representation of each node (each signal sample) using our
attention-based graph neural network model so that the RF signal samples can be
clustered more accurately. Then, we tackle the problem of indexing the clusters
with proper floor labels, by leveraging the observation that signals from an
access point can be detected on different floors, i.e., signal spillover.
Specifically, we formulate a cluster indexing problem as a combinatorial
optimization problem and show that it is equivalent to solving a traveling
salesman problem, whose (near-)optimal solution can be found efficiently. We
have implemented FIS-ONE and validated its effectiveness on the Microsoft
dataset and in three large shopping malls. Our results show that FIS-ONE
outperforms other baseline algorithms significantly, with up to 23% improvement
in adjusted rand index and 25% improvement in normalized mutual information
using only one floor-labeled signal sample.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE ICDCS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grain and Grain Boundary Segmentation using Machine Learning with Real
  and Generated <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05911v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05911v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Warren, Nandhini Raju, Abhilash Prasad, Shajahan Hossain, Ramesh Subramanian, Jayanta Kapat, Navin Manjooran, Ranajay Ghosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We report significantly improved accuracy of grain boundary segmentation
using Convolutional Neural Networks (CNN) trained on a combination of real and
generated data. Manual segmentation is accurate but time-consuming, and
existing computational methods are faster but often inaccurate. To combat this
dilemma, machine learning models can be used to achieve the accuracy of manual
segmentation and have the efficiency of a computational method. An extensive
dataset of from 316L stainless steel samples is additively manufactured,
prepared, polished, etched, and then microstructure grain images were
systematically collected. Grain segmentation via existing computational methods
and manual (by-hand) were conducted, to create "real" training data. A Voronoi
tessellation pattern combined with random synthetic noise and simulated
defects, is developed to create a novel artificial grain image fabrication
method. This provided training data supplementation for data-intensive machine
learning methods. The accuracy of the grain measurements from microstructure
images segmented via computational methods and machine learning methods
proposed in this work are calculated and compared to provide much benchmarks in
grain segmentation. Over 400 images of the microstructure of stainless steel
samples were manually segmented for machine learning training applications.
This data and the artificial data is available on Kaggle.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM
  Decoding <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05908v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05908v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seongjun Yang, Gibbeum Lee, Jaewoong Cho, Dimitris Papailiopoulos, Kangwook Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents "Predictive Pipelined Decoding (PPD)," an approach that
speeds up greedy decoding in Large Language Models (LLMs) while maintaining the
exact same output as the original decoding. Unlike conventional strategies, PPD
employs additional compute resources to parallelize the initiation of
subsequent token decoding during the current token decoding. This innovative
method reduces decoding latency and reshapes the understanding of trade-offs in
LLM decoding strategies. We have developed a theoretical framework that allows
us to analyze the trade-off between computation and latency. Using this
framework, we can analytically estimate the potential reduction in latency
associated with our proposed method, achieved through the assessment of the
match rate, represented as p_correct. The results demonstrate that the use of
extra computational resources has the potential to accelerate LLM greedy
decoding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ES-FoMo Workshop at ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mini-Batch Optimization of Contrastive Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaewoong Cho, Kartik Sreenivasan, Keon Lee, Kyunghoo Mun, Soheun Yi, Jeong-Gwan Lee, Anna Lee, Jy-yong Sohn, Dimitris Papailiopoulos, Kangwook Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has gained significant attention as a method for
self-supervised learning. The contrastive loss function ensures that embeddings
of positive sample pairs (e.g., different samples from the same class or
different views of the same object) are similar, while embeddings of negative
pairs are dissimilar. Practical constraints such as large memory requirements
make it challenging to consider all possible positive and negative pairs,
leading to the use of mini-batch optimization. In this paper, we investigate
the theoretical aspects of mini-batch optimization in contrastive learning. We
show that mini-batch optimization is equivalent to full-batch optimization if
and only if all $\binom{N}{B}$ mini-batches are selected, while sub-optimality
may arise when examining only a subset. We then demonstrate that utilizing
high-loss mini-batches can speed up SGD convergence and propose a spectral
clustering-based approach for identifying these high-loss mini-batches. Our
experimental results validate our theoretical findings and demonstrate that our
proposed algorithm outperforms vanilla SGD in practically relevant settings,
providing a better understanding of mini-batch optimization in contrastive
learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stability Guarantees for Feature Attributions with Multiplicative
  Smoothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton Xue, Rajeev Alur, Eric Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explanation methods for machine learning models tend to not provide any
formal guarantees and may not reflect the underlying decision-making process.
In this work, we analyze stability as a property for reliable feature
attribution methods. We prove that relaxed variants of stability are guaranteed
if the model is sufficiently Lipschitz with respect to the masking of features.
To achieve such a model, we develop a smoothing method called Multiplicative
Smoothing (MuS). We show that MuS overcomes theoretical limitations of standard
smoothing techniques and can be integrated with any classifier and feature
attribution method. We evaluate MuS on vision and language models with a
variety of feature attribution methods, such as LIME and SHAP, and demonstrate
that MuS endows feature attributions with non-trivial stability guarantees.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Unrolling for Nonconvex Robust Principal Component Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elizabeth Z. C. Tan, Caroline Chaux, Emmanuel Soubies, Vincent Y. F. Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We design algorithms for Robust Principal Component Analysis (RPCA) which
consists in decomposing a matrix into the sum of a low rank matrix and a sparse
matrix. We propose a deep unrolled algorithm based on an accelerated
alternating projection algorithm which aims to solve RPCA in its nonconvex
form. The proposed procedure combines benefits of deep neural networks and the
interpretability of the original algorithm and it automatically learns
hyperparameters. We demonstrate the unrolled algorithm's effectiveness on
synthetic datasets and also on a face modeling problem, where it leads to both
better numerical and visual performances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures; Accepted to the 2023 IEEE International Workshop
  on Machine Learning for Signal Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PID-Inspired Inductive Biases for Deep Reinforcement Learning in
  Partially Observable Control Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ian Char, Jeff Schneider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep reinforcement learning (RL) has shown immense potential for learning to
control systems through data alone. However, one challenge deep RL faces is
that the full state of the system is often not observable. When this is the
case, the policy needs to leverage the history of observations to infer the
current state. At the same time, differences between the training and testing
environments makes it critical for the policy not to overfit to the sequence of
observations it sees at training time. As such, there is an important balancing
act between having the history encoder be flexible enough to extract relevant
information, yet be robust to changes in the environment. To strike this
balance, we look to the PID controller for inspiration. We assert the PID
controller's success shows that only summing and differencing are needed to
accumulate information over time for many control tasks. Following this
principle, we propose two architectures for encoding history: one that directly
uses PID features and another that extends these core ideas and can be used in
arbitrary control tasks. When compared with prior approaches, our encoders
produce policies that are often more robust and achieve better performance on a
variety of tracking tasks. Going beyond tracking tasks, our policies achieve
1.7x better performance on average over previous state-of-the-art methods on a
suite of high dimensional control tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Task Offloading Algorithm for Digital Twin in Edge/Cloud
  Computing Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05888v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05888v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziru Zhang, Xuling Zhang, Guangzhi Zhu, Yuyang Wang, Pan Hui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of Internet of Things (IoT), Digital Twin (DT) is envisioned to
empower various areas as a bridge between physical objects and the digital
world. Through virtualization and simulation techniques, multiple functions can
be achieved by leveraging computing resources. In this process, Mobile Cloud
Computing (MCC) and Mobile Edge Computing (MEC) have become two of the key
factors to achieve real-time feedback. However, current works only considered
edge servers or cloud servers in the DT system models. Besides, The models
ignore the DT with not only one data resource. In this paper, we propose a new
DT system model considering a heterogeneous MEC/MCC environment. Each DT in the
model is maintained in one of the servers via multiple data collection devices.
The offloading decision-making problem is also considered and a new offloading
scheme is proposed based on Distributed Deep Learning (DDL). Simulation results
demonstrate that our proposed algorithm can effectively and efficiently
decrease the system's average latency and energy consumption. Significant
improvement is achieved compared with the baselines under the dynamic
environment of DTs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Prediction using Time-Dependent Cox Survival Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lang Zeng, Jipeng Zhang, Wei Chen, Ying Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The target of dynamic prediction is to provide individualized risk
predictions over time which can be updated as new data become available.
Motivated by establishing a dynamic prediction model for the progressive eye
disease, age-related macular degeneration (AMD), we proposed a time-dependent
Cox model-based survival neural network (tdCoxSNN) to predict its progression
on a continuous time scale using longitudinal fundus images. tdCoxSNN extends
the time-dependent Cox model by utilizing a neural network to model the
non-linear effect of the time-dependent covariates on the survival outcome.
Additionally, by incorporating the convolutional neural network (CNN), tdCoxSNN
can take the longitudinal raw images as input. We evaluate and compare our
proposed method with joint modeling and landmarking approaches through
comprehensive simulations using two time-dependent accuracy metrics, the Brier
Score and dynamic AUC. We applied the proposed approach to two real datasets.
One is a large AMD study, the Age-Related Eye Disease Study (AREDS), in which
more than 50,000 fundus images were captured over a period of 12 years for more
than 4,000 participants. Another is a public dataset of the primary biliary
cirrhosis (PBC) disease, in which multiple lab tests were longitudinally
collected to predict the time-to-liver transplant. Our approach achieves
satisfactory prediction performance in both simulation studies and the two real
data analyses. tdCoxSNN was implemented in PyTorch, Tensorflow, and
R-Tensorflow.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ecosystem-level Analysis of Deployed Machine Learning Reveals
  Homogeneous Outcomes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Connor Toups, Rishi Bommasani, Kathleen A. Creel, Sarah H. Bana, Dan Jurafsky, Percy Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning is traditionally studied at the model level: researchers
measure and improve the accuracy, robustness, bias, efficiency, and other
dimensions of specific models. In practice, the societal impact of machine
learning is determined by the surrounding context of machine learning
deployments. To capture this, we introduce ecosystem-level analysis: rather
than analyzing a single model, we consider the collection of models that are
deployed in a given context. For example, ecosystem-level analysis in hiring
recognizes that a job candidate's outcomes are not only determined by a single
hiring algorithm or firm but instead by the collective decisions of all the
firms they applied to. Across three modalities (text, images, speech) and 11
datasets, we establish a clear trend: deployed machine learning is prone to
systemic failure, meaning some users are exclusively misclassified by all
models available. Even when individual models improve at the population level
over time, we find these improvements rarely reduce the prevalence of systemic
failure. Instead, the benefits of these improvements predominantly accrue to
individuals who are already correctly classified by other models. In light of
these trends, we consider medical imaging for dermatology where the costs of
systemic failure are especially high. While traditional analyses reveal racial
performance disparities for both models and humans, ecosystem-level analysis
reveals new forms of racial disparity in model predictions that do not present
in human predictions. These examples demonstrate ecosystem-level analysis has
unique strengths for characterizing the societal impact of machine learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>All code is available at
  https://github.com/rishibommasani/EcosystemLevelAnalysis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FAIRO: Fairness-aware Adaptation in Sequential-Decision Making for
  Human-in-the-Loop Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05857v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05857v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyu Zhao, Mojtaba Taherisadr, Salma Elmalaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving fairness in sequential-decision making systems within
Human-in-the-Loop (HITL) environments is a critical concern, especially when
multiple humans with different behavior and expectations are affected by the
same adaptation decisions in the system. This human variability factor adds
more complexity since policies deemed fair at one point in time may become
discriminatory over time due to variations in human preferences resulting from
inter- and intra-human variability. This paper addresses the fairness problem
from an equity lens, considering human behavior variability, and the changes in
human preferences over time. We propose FAIRO, a novel algorithm for
fairness-aware sequential-decision making in HITL adaptation, which
incorporates these notions into the decision-making process. In particular,
FAIRO decomposes this complex fairness task into adaptive sub-tasks based on
individual human preferences through leveraging the Options reinforcement
learning framework. We design FAIRO to generalize to three types of HITL
application setups that have the shared adaptation decision problem.
Furthermore, we recognize that fairness-aware policies can sometimes conflict
with the application's utility. To address this challenge, we provide a
fairness-utility tradeoff in FAIRO, allowing system designers to balance the
objectives of fairness and utility based on specific application requirements.
Extensive evaluations of FAIRO on the three HITL applications demonstrate its
generalizability and effectiveness in promoting fairness while accounting for
human variability. On average, FAIRO can improve fairness compared with other
methods across all three applications by 35.36%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DreamWaltz: Make a Scene with Complex 3D Animatable Avatars 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12529v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12529v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukun Huang, Jianan Wang, Ailing Zeng, He Cao, Xianbiao Qi, Yukai Shi, Zheng-Jun Zha, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present DreamWaltz, a novel framework for generating and animating complex
3D avatars given text guidance and parametric human body prior. While recent
methods have shown encouraging results for text-to-3D generation of common
objects, creating high-quality and animatable 3D avatars remains challenging.
To create high-quality 3D avatars, DreamWaltz proposes 3D-consistent
occlusion-aware Score Distillation Sampling (SDS) to optimize implicit neural
representations with canonical poses. It provides view-aligned supervision via
3D-aware skeleton conditioning which enables complex avatar generation without
artifacts and multiple faces. For animation, our method learns an animatable
and generalizable avatar representation which could map arbitrary poses to the
canonical pose representation. Extensive evaluations demonstrate that
DreamWaltz is an effective and robust approach for creating 3D avatars that can
take on complex shapes and appearances as well as novel poses for animation.
The proposed framework further enables the creation of complex scenes with
diverse compositions, including avatar-avatar, avatar-object and avatar-scene
interactions. See https://dreamwaltz3d.github.io/ for more vivid 3D avatar and
animation results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page at https://dreamwaltz3d.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synthesizing Artistic Cinemagraphs from Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03190v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03190v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aniruddha Mahapatra, Aliaksandr Siarohin, Hsin-Ying Lee, Sergey Tulyakov, Jun-Yan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Text2Cinemagraph, a fully automated method for creating
cinemagraphs from text descriptions - an especially challenging task when
prompts feature imaginary elements and artistic styles, given the complexity of
interpreting the semantics and motions of these images. Existing single-image
animation methods fall short on artistic inputs, and recent text-based video
methods frequently introduce temporal inconsistencies, struggling to keep
certain regions static. To address these challenges, we propose an idea of
synthesizing image twins from a single text prompt - a pair of an artistic
image and its pixel-aligned corresponding natural-looking twin. While the
artistic image depicts the style and appearance detailed in our text prompt,
the realistic counterpart greatly simplifies layout and motion analysis.
Leveraging existing natural image and video datasets, we can accurately segment
the realistic image and predict plausible motion given the semantic
information. The predicted motion can then be transferred to the artistic image
to create the final cinemagraph. Our method outperforms existing approaches in
creating cinemagraphs for natural landscapes as well as artistic and
other-worldly scenes, as validated by automated metrics and user studies.
Finally, we demonstrate two extensions: animating existing paintings and
controlling motion directions using text.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://text2cinemagraph.github.io/website/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probabilistic Counterexample Guidance for Safer Reinforcement Learning
  (Extended Version) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04927v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04927v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaotong Ji, Antonio Filieri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe exploration aims at addressing the limitations of Reinforcement Learning
(RL) in safety-critical scenarios, where failures during trial-and-error
learning may incur high costs. Several methods exist to incorporate external
knowledge or to use proximal sensor data to limit the exploration of unsafe
states. However, reducing exploration risks in unknown environments, where an
agent must discover safety threats during exploration, remains challenging. In
this paper, we target the problem of safe exploration by guiding the training
with counterexamples of the safety requirement. Our method abstracts both
continuous and discrete state-space systems into compact abstract models
representing the safety-relevant knowledge acquired by the agent during
exploration. We then exploit probabilistic counterexample generation to
construct minimal simulation submodels eliciting safety requirement violations,
where the agent can efficiently train offline to refine its policy towards
minimising the risk of safety violations during the subsequent online
exploration. We demonstrate our method's effectiveness in reducing safety
violations during online exploration in preliminary experiments by an average
of 40.3% compared with QL and DQN standard algorithms and 29.1% compared with
previous related work, while achieving comparable cumulative rewards with
respect to unrestricted exploration and alternative approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted and Evaluated by the 20th International Conference on
  Quantitative Evaluation of Systems 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CAR-DESPOT: Causally-Informed Online POMDP Planning for Robots in
  Confounded Environments <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06848v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06848v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ricardo Cannizzaro, Lars Kunze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robots operating in real-world environments must reason about possible
outcomes of stochastic actions and make decisions based on partial observations
of the true world state. A major challenge for making accurate and robust
action predictions is the problem of confounding, which if left untreated can
lead to prediction errors. The partially observable Markov decision process
(POMDP) is a widely-used framework to model these stochastic and
partially-observable decision-making problems. However, due to a lack of
explicit causal semantics, POMDP planning methods are prone to confounding bias
and thus in the presence of unobserved confounders may produce underperforming
policies. This paper presents a novel causally-informed extension of "anytime
regularized determinized sparse partially observable tree" (AR-DESPOT), a
modern anytime online POMDP planner, using causal modelling and inference to
eliminate errors caused by unmeasured confounder variables. We further propose
a method to learn offline the partial parameterisation of the causal model for
planning, from ground truth model data. We evaluate our methods on a toy
problem with an unobserved confounder and show that the learned causal model is
highly accurate, while our planning method is more robust to confounding and
produces overall higher performing policies than AR-DESPOT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures, accepted to 2023 IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep learning and MCMC with aggVAE for shifting administrative
  boundaries: mapping malaria prevalence in Kenya 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.19779v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.19779v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elizaveta Semenova, Swapnil Mishra, Samir Bhatt, Seth Flaxman, H Juliette T Unwin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model-based disease mapping remains a fundamental policy-informing tool in
public health and disease surveillance. Hierarchical Bayesian models have
become the state-of-the-art approach for disease mapping since they are able to
capture structure in the data, as well as to characterise uncertainty. When
working with areal data, e.g.~aggregates at the administrative unit level such
as district or province, routinely used models rely on the adjacency structure
of areal units to account for spatial correlations. The goal of disease
surveillance systems is to track disease outcomes over time. This task provides
challenging in situations of crises, such as political changes, leading to
changes of administrative boundaries. Kenya is an example of a country where
change of boundaries took place in 2010. Moreover, the adjacency-based approach
ignores the continuous nature of spatial processes and cannot solve the
change-of-support problem, i.e.~when administrative boundaries change or when
estimates must be produced at a different administrative level. We present a
novel, practical, and easy to implement solution relying on a methodology
combining deep generative modelling and fully Bayesian inference: we build on
the recently proposed PriorVAE method able to encode spatial priors over small
areas with variational autoencoders, to map malaria prevalence in Kenya.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Noisy Fixed-Point Iterations to Private ADMM for Centralized and
  Federated Learning <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.12559v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.12559v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edwige Cyffers, Aurélien Bellet, Debabrota Basu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study differentially private (DP) machine learning algorithms as instances
of noisy fixed-point iterations, in order to derive privacy and utility results
from this well-studied framework. We show that this new perspective recovers
popular private gradient-based methods like DP-SGD and provides a principled
way to design and analyze new private optimization algorithms in a flexible
manner. Focusing on the widely-used Alternating Directions Method of
Multipliers (ADMM) method, we use our general framework to derive novel private
ADMM algorithms for centralized, federated and fully decentralized learning.
For these three algorithms, we establish strong privacy guarantees leveraging
privacy amplification by iteration and by subsampling. Finally, we provide
utility guarantees using a unified analysis that exploits a recent linear
convergence result for noisy fixed-point iterations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2023. v3: added references to the first papers that
  introduced block-wise fixed-point iterations (Iutzeler et al., 2013; Bianchi
  et al., 2016)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asymptotically Optimal Fixed-Budget Best Arm Identification with
  Variance-Dependent Bounds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02988v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02988v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masahiro Kato, Masaaki Imaizumi, Takuya Ishihara, Toru Kitagawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the problem of fixed-budget best arm identification (BAI) for
minimizing expected simple regret. In an adaptive experiment, a decision maker
draws one of multiple treatment arms based on past observations and observes
the outcome of the drawn arm. After the experiment, the decision maker
recommends the treatment arm with the highest expected outcome. We evaluate the
decision based on the expected simple regret, which is the difference between
the expected outcomes of the best arm and the recommended arm. Due to inherent
uncertainty, we evaluate the regret using the minimax criterion. First, we
derive asymptotic lower bounds for the worst-case expected simple regret, which
are characterized by the variances of potential outcomes (leading factor).
Based on the lower bounds, we propose the Two-Stage (TS)-Hirano-Imbens-Ridder
(HIR) strategy, which utilizes the HIR estimator (Hirano et al., 2003) in
recommending the best arm. Our theoretical analysis shows that the TS-HIR
strategy is asymptotically minimax optimal, meaning that the leading factor of
its worst-case expected simple regret matches our derived worst-case lower
bound. Additionally, we consider extensions of our method, such as the
asymptotic optimality for the probability of misidentification. Finally, we
validate the proposed method's effectiveness through simulations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepGD: A Multi-Objective Black-Box Test Selection Approach for Deep
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04878v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04878v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zohreh Aghababaeyan, Manel Abdellatif, Mahboubeh Dadkhah, Lionel Briand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) are widely used in various application domains
such as image processing, speech recognition, and natural language processing.
However, testing DNN models may be challenging due to the complexity and size
of their input domain. Particularly, testing DNN models often requires
generating or exploring large unlabeled datasets. In practice, DNN test
oracles, which identify the correct outputs for inputs, often require expensive
manual effort to label test data, possibly involving multiple experts to ensure
labeling correctness. In this paper, we propose DeepGD, a black-box
multi-objective test selection approach for DNN models. It reduces the cost of
labeling by prioritizing the selection of test inputs with high fault revealing
power from large unlabeled datasets. DeepGD not only selects test inputs with
high uncertainty scores to trigger as many mispredicted inputs as possible but
also maximizes the probability of revealing distinct faults in the DNN model by
selecting diverse mispredicted inputs. The experimental results conducted on
four widely used datasets and five DNN models show that in terms of
fault-revealing ability: (1) White-box, coverage-based approaches fare poorly,
(2) DeepGD outperforms existing black-box test selection approaches in terms of
fault detection, and (3) DeepGD also leads to better guidance for DNN model
retraining when using selected inputs to augment the training set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dividing and Conquering a BlackBox to a Mixture of Interpretable Models:
  Route, Interpret, Repeat <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05350v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05350v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shantanu Ghosh, Ke Yu, Forough Arabshahi, Kayhan Batmanghelich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ML model design either starts with an interpretable model or a Blackbox and
explains it post hoc. Blackbox models are flexible but difficult to explain,
while interpretable models are inherently explainable. Yet, interpretable
models require extensive ML knowledge and tend to be less flexible and
underperforming than their Blackbox variants. This paper aims to blur the
distinction between a post hoc explanation of a Blackbox and constructing
interpretable models. Beginning with a Blackbox, we iteratively carve out a
mixture of interpretable experts (MoIE) and a residual network. Each
interpretable model specializes in a subset of samples and explains them using
First Order Logic (FOL), providing basic reasoning on concepts from the
Blackbox. We route the remaining samples through a flexible residual. We repeat
the method on the residual network until all the interpretable models explain
the desired proportion of data. Our extensive experiments show that our route,
interpret, and repeat approach (1) identifies a diverse set of
instance-specific concepts with high concept completeness via MoIE without
compromising in performance, (2) identifies the relatively ``harder'' samples
to explain via residuals, (3) outperforms the interpretable by-design models by
significant margins during test-time interventions, and (4) fixes the shortcut
learned by the original Blackbox. The code for MoIE is publicly available at:
\url{https://github.com/batmanlab/ICML-2023-Route-interpret-repeat}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>appeared as v5 of arXiv:2302.10289 which was replaced in error, which
  drifted into a different work, accepted in ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Graph Attention for Enhanced Spatial Context 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04149v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04149v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Singh, Yash Bhambhu, Himanshu Buckchash, Deepak K. Gupta, Dilip K. Prasad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Global contexts in images are quite valuable in image-to-image translation
problems. Conventional attention-based and graph-based models capture the
global context to a large extent, however, these are computationally expensive.
Moreover, the existing approaches are limited to only learning the pairwise
semantic relation between any two points on the image. In this paper, we
present Latent Graph Attention (LGA) a computationally inexpensive (linear to
the number of nodes) and stable, modular framework for incorporating the global
context in the existing architectures, especially empowering small-scale
architectures to give performance closer to large size architectures, thus
making the light-weight architectures more useful for edge devices with lower
compute power and lower energy needs. LGA propagates information spatially
using a network of locally connected graphs, thereby facilitating to construct
a semantically coherent relation between any two spatially distant points that
also takes into account the influence of the intermediate pixels. Moreover, the
depth of the graph network can be used to adapt the extent of contextual spread
to the target dataset, thereby being able to explicitly control the added
computational cost. To enhance the learning mechanism of LGA, we also introduce
a novel contrastive loss term that helps our LGA module to couple well with the
original architecture at the expense of minimal additional computational load.
We show that incorporating LGA improves the performance on three challenging
applications, namely transparent object segmentation, image restoration for
dehazing and optical flow estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Dynamics Modeling in Interactive Environments with Koopman
  Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11941v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11941v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arnab Kumar Mondal, Siba Smarak Panigrahi, Sai Rajeswar, Kaleem Siddiqi, Siamak Ravanbakhsh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accurate modeling of dynamics in interactive environments is critical for
successful long-range prediction. Such a capability could advance Reinforcement
Learning (RL) and Planning algorithms, but achieving it is challenging.
Inaccuracies in model estimates can compound, resulting in increased errors
over long horizons. We approach this problem from the lens of Koopman theory,
where the nonlinear dynamics of the environment can be linearized in a
high-dimensional latent space. This allows us to efficiently parallelize the
sequential problem of long-range prediction using convolution, while accounting
for the agent's action at every time step. Our approach also enables stability
analysis and better control over gradients through time. Taken together, these
advantages result in significant improvement over the existing approaches, both
in the efficiency and the accuracy of modeling dynamics over extended horizons.
We also report promising experimental results in dynamics modeling for the
scenarios of both model-based planning and model-free RL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MARBLE: Music Audio Representation Benchmark for Universal Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.10548v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.10548v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruibin Yuan, Yinghao Ma, Yizhi Li, Ge Zhang, Xingran Chen, Hanzhi Yin, Le Zhuo, Yiqi Liu, Jiawen Huang, Zeyue Tian, Binyue Deng, Ningzhi Wang, Chenghua Lin, Emmanouil Benetos, Anton Ragni, Norbert Gyenge, Roger Dannenberg, Wenhu Chen, Gus Xia, Wei Xue, Si Liu, Shi Wang, Ruibo Liu, Yike Guo, Jie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of extensive intersection between art and Artificial Intelligence
(AI), such as image generation and fiction co-creation, AI for music remains
relatively nascent, particularly in music understanding. This is evident in the
limited work on deep music representations, the scarcity of large-scale
datasets, and the absence of a universal and community-driven benchmark. To
address this issue, we introduce the Music Audio Representation Benchmark for
universaL Evaluation, termed MARBLE. It aims to provide a benchmark for various
Music Information Retrieval (MIR) tasks by defining a comprehensive taxonomy
with four hierarchy levels, including acoustic, performance, score, and
high-level description. We then establish a unified protocol based on 14 tasks
on 8 public-available datasets, providing a fair and standard assessment of
representations of all open-sourced pre-trained models developed on music
recordings as baselines. Besides, MARBLE offers an easy-to-use, extendable, and
reproducible suite for the community, with a clear statement on copyright
issues on datasets. Results suggest recently proposed large-scale pre-trained
musical language models perform the best in most tasks, with room for further
improvement. The leaderboard and toolkit repository are published at
https://marble-bm.shef.ac.uk to promote future music AI research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ spred: Solving $L_1$ Penalty with SGD <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.01212v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.01212v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liu Ziyin, Zihao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose to minimize a generic differentiable objective with $L_1$
constraint using a simple reparametrization and straightforward stochastic
gradient descent. Our proposal is the direct generalization of previous ideas
that the $L_1$ penalty may be equivalent to a differentiable reparametrization
with weight decay. We prove that the proposed method, \textit{spred}, is an
exact differentiable solver of $L_1$ and that the reparametrization trick is
completely ``benign" for a generic nonconvex function. Practically, we
demonstrate the usefulness of the method in (1) training sparse neural networks
to perform gene selection tasks, which involves finding relevant features in a
very high dimensional space, and (2) neural network compression task, to which
previous attempts at applying the $L_1$-penalty have been unsuccessful.
Conceptually, our result bridges the gap between the sparsity in deep learning
and conventional statistical learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023, 16 pages, 10 figures, and 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weakly-supervised positional contrastive learning: application to
  cirrhosis classification <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04617v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04617v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emma Sarfati, Alexandre Bône, Marc-Michel Rohé, Pietro Gori, Isabelle Bloch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large medical imaging datasets can be cheaply and quickly annotated with
low-confidence, weak labels (e.g., radiological scores). Access to
high-confidence labels, such as histology-based diagnoses, is rare and costly.
Pretraining strategies, like contrastive learning (CL) methods, can leverage
unlabeled or weakly-annotated datasets. These methods typically require large
batch sizes, which poses a difficulty in the case of large 3D images at full
resolution, due to limited GPU memory. Nevertheless, volumetric positional
information about the spatial context of each 2D slice can be very important
for some medical applications. In this work, we propose an efficient
weakly-supervised positional (WSP) contrastive learning strategy where we
integrate both the spatial context of each 2D slice and a weak label via a
generic kernel-based loss function. We illustrate our method on cirrhosis
prediction using a large volume of weakly-labeled images, namely radiological
low-confidence annotations, and small strongly-labeled (i.e., high-confidence)
datasets. The proposed model improves the classification AUC by 5% with respect
to a baseline model on our internal dataset, and by 26% on the public LIHC
dataset from the Cancer Genome Atlas. The code is available at:
https://github.com/Guerbet-AI/wsp-contrastive.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Closing the gap between SVRG and TD-SVRG with Gradient Splitting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.16237v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.16237v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arsenii Mustafin, Alex Olshevsky, Ioannis Ch. Paschalidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal difference (TD) learning is a policy evaluation in reinforcement
learning whose performance can be enhanced by variance reduction techniques.
Recently, multiple works have sought to fuse TD learning with SVRG to obtain a
policy evaluation method with a geometric rate of convergence. However, the
resulting convergence rate is significantly weaker than what is achieved by
SVRG in the setting of convex optimization. In this work we utilize a recent
interpretation of TD-learning as the splitting of the gradient of an
appropriately chosen function, thus simplifying the algorithm and fusing TD
with SVRG. Our main result is a geometric convergence bound with predetermined
learning rate of $1/8$, which is identical to the convergence bound available
for SVRG in the convex setting. Our theoretical findings are supported by a set
of experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 9 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VAE-Loco: Versatile Quadruped Locomotion by Learning a Disentangled Gait
  Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.01179v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.01179v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander L. Mitchell, Wolfgang Merkt, Mathieu Geisert, Siddhant Gangapurwala, Martin Engelcke, Oiwi Parker Jones, Ioannis Havoutis, Ingmar Posner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quadruped locomotion is rapidly maturing to a degree where robots are able to
realise highly dynamic manoeuvres. However, current planners are unable to vary
key gait parameters of the in-swing feet midair. In this work we address this
limitation and show that it is pivotal in increasing controller robustness by
learning a latent space capturing the key stance phases constituting a
particular gait. This is achieved via a generative model trained on a single
trot style, which encourages disentanglement such that application of a drive
signal to a single dimension of the latent state induces holistic plans
synthesising a continuous variety of trot styles. We demonstrate that specific
properties of the drive signal map directly to gait parameters such as cadence,
footstep height and full stance duration. Due to the nature of our approach
these synthesised gaits are continuously variable online during robot
operation. The use of a generative model facilitates the detection and
mitigation of disturbances to provide a versatile and robust planning
framework. We evaluate our approach on two versions of the real ANYmal
quadruped robots and demonstrate that our method achieves a continuous blend of
dynamic trot styles whilst being robust and reactive to external perturbations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 13 figures, 1 table, accepted by IEEE Transactions on
  Robotics (T-RO) as an extended paper. arXiv admin note: substantial text
  overlap with arXiv:2112.04809</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Deep Generative Decoder: MAP estimation of representations improves
  modeling of single-cell RNA data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.06672v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.06672v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viktoria Schuster, Anders Krogh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning low-dimensional representations of single-cell transcriptomics has
become instrumental to its downstream analysis. The state of the art is
currently represented by neural network models such as variational autoencoders
(VAEs) which use a variational approximation of the likelihood for inference.
We here present the Deep Generative Decoder (DGD), a simple generative model
that computes model parameters and representations directly via maximum a
posteriori (MAP) estimation. The DGD handles complex parameterized latent
distributions naturally unlike VAEs which typically use a fixed Gaussian
distribution, because of the complexity of adding other types. We first show
its general functionality on a commonly used benchmark set, Fashion-MNIST.
Secondly, we apply the model to multiple single-cell data sets. Here the DGD
learns low-dimensional, meaningful and well-structured latent representations
with sub-clustering beyond the provided labels. The advantages of this approach
are its simplicity and its capability to provide representations of much
smaller dimensionality than a comparable VAE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linearization Algorithms for Fully Composite Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.12808v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.12808v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria-Luiza Vladarean, Nikita Doikov, Martin Jaggi, Nicolas Flammarion
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies first-order algorithms for solving fully composite
optimization problems over convex and compact sets. We leverage the structure
of the objective by handling its differentiable and non-differentiable
components separately, linearizing only the smooth parts. This provides us with
new generalizations of the classical Frank-Wolfe method and the Conditional
Gradient Sliding algorithm, that cater to a subclass of non-differentiable
problems. Our algorithms rely on a stronger version of the linear minimization
oracle, which can be efficiently implemented in several practical applications.
We provide the basic version of our method with an affine-invariant analysis
and prove global convergence rates for both convex and non-convex objectives.
Furthermore, in the convex case, we propose an accelerated method with
correspondingly improved complexity. Finally, we provide illustrative
experiments to support our theoretical results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03042v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03042v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aryo Pradipta Gema, Luke Daines, Pasquale Minervini, Beatrice Alex
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adapting pretrained language models to novel domains, such as clinical
applications, traditionally involves retraining their entire set of parameters.
However, this approach is increasingly proven to be impractical owing to the
substantial computational requirements associated with training such large
language models. To address this issue, Parameter-Efficient Fine-Tuning (PEFT)
techniques offer a viable solution by selectively fine-tuning a small subset of
additional parameters, significantly reducing the computational requirements
for domain adaptation. In this study, we propose Clinical LLaMA-LoRA, a PEFT
adapter layer built upon the open-sourced LLaMA model. Clinical LLaMA-LoRA is
trained using clinical notes obtained from the MIMIC-IV database, thereby
creating a specialised adapter designed for the clinical domain. Additionally,
we propose a two-step PEFT framework which fuses Clinical LLaMA-LoRA with
Downstream LLaMA-LoRA, another PEFT adapter specialised for downstream tasks.
We evaluate this framework on multiple clinical outcome prediction datasets,
comparing it to clinically trained language models. Our proposed framework
achieves a state-of-the-art AUROC score averaged across all clinical downstream
tasks. We observe substantial improvements of 6-9% AUROC score in the
large-scale multilabel classification tasks, such as diagnoses and procedures
classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One <span class="highlight-title">Transformer</span> for All Time Series: Representing and Training with
  Time-Dependent Heterogeneous Tabular Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06375v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06375v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simone Luetto, Fabrizio Garuti, Enver Sangineto, Lorenzo Forni, Rita Cucchiara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a recent growing interest in applying Deep Learning techniques to
tabular data, in order to replicate the success of other Artificial
Intelligence areas in this structured domain. Specifically interesting is the
case in which tabular data have a time dependence, such as, for instance
financial transactions. However, the heterogeneity of the tabular values, in
which categorical elements are mixed with numerical items, makes this
adaptation difficult. In this paper we propose a Transformer architecture to
represent heterogeneous time-dependent tabular data, in which numerical
features are represented using a set of frequency functions and the whole
network is uniformly trained with a unique loss function.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reinforced Labels: Multi-Agent Deep Reinforcement Learning for
  Point-Feature Label Placement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01388v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01388v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Petr Bobák, Ladislav Čmolík, Martin Čadík
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over recent years, Reinforcement Learning combined with Deep Learning
techniques has successfully proven to solve complex problems in various
domains, including robotics, self-driving cars, and finance. In this paper, we
are introducing Reinforcement Learning (RL) to label placement, a complex task
in data visualization that seeks optimal positioning for labels to avoid
overlap and ensure legibility. Our novel point-feature label placement method
utilizes Multi-Agent Deep Reinforcement Learning (MADRL) to learn the label
placement strategy, which is the first machine-learning-driven labeling method
in contrast to existing hand-crafted algorithms designed by human experts. To
facilitate RL learning, we developed an environment where an agent acts as a
proxy for a label, a short textual annotation that augments visualization. Our
results show that the strategy trained by our method significantly outperforms
the random strategy of an untrained agent and compared methods designed by
human experts in terms of completeness (i.e., the number of placed labels). The
trade-off is increased computation time, making the proposed method slower than
compared methods. Nevertheless, our method is ideal for scenarios where the
labeling can be computed in advance, and completeness is essential, such as
cartographic maps, technical drawings, and medical atlases. Additionally, we
conducted a user study to assess the perceived performance. The outcomes
revealed that the participants considered the proposed method to be
significantly better than the other examined methods. This indicates that the
improved completeness is not just reflected in the quantitative metrics but
also in the subjective evaluation of the participants.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning for Survival Analysis: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14961v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14961v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Wiegrebe, Philipp Kopper, Raphael Sonabend, Bernd Bischl, Andreas Bender
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The influx of deep learning (DL) techniques into the field of survival
analysis in recent years has led to substantial methodological progress; for
instance, learning from unstructured or high-dimensional data such as images,
text or omics data. In this work, we conduct a comprehensive systematic review
of DL-based methods for time-to-event analysis, characterizing them according
to both survival- and DL-related attributes. In summary, the reviewed methods
often address only a small subset of tasks relevant to time-to-event data -
e.g., single-risk right-censored data - and neglect to incorporate more complex
settings. Our findings are summarized in an editable, open-source, interactive
table: https://survival-org.github.io/DL4Survival. As this research area is
advancing rapidly, we encourage community contribution in order to keep this
database up to date.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures, 2 tables, 1 interactive table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Fleet-wide Sharing of Wind Turbine Condition Information through
  Privacy-preserving Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.03529v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.03529v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorin Jenkel, Stefan Jonas, Angela Meyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Terabytes of data are collected by wind turbine manufacturers from their
fleets every day. And yet, a lack of data access and sharing impedes exploiting
the full potential of the data. We present a distributed machine learning
approach that preserves the data privacy by leaving the data on the wind
turbines while still enabling fleet-wide learning on those local data. We show
that through federated fleet-wide learning, turbines with little or no
representative training data can benefit from more accurate normal behavior
models. Customizing the global federated model to individual turbines yields
the highest fault detection accuracy in cases where the monitored target
variable is distributed heterogeneously across the fleet. We demonstrate this
for bearing temperatures, a target variable whose normal behavior can vary
widely depending on the turbine. We show that no turbine experiences a loss in
model performance from participating in the federated learning process,
resulting in superior performance of the federated learning strategy in our
case studies. The distributed learning increases the normal behavior model
training times by about a factor of ten due to increased communication overhead
and slower model convergence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added: case study results for data from a different fleet;
  distribution shift discussion; formatting and presentation changes. Original
  results remain unchanged</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MOPO-LSI: A User Guide 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.01719v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.01719v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yong Zheng, Kumar Neelotpal Shukla, Jasmine Xu,  David,  Wang, Michael O'Leary
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  MOPO-LSI is an open-source Multi-Objective Portfolio Optimization Library for
Sustainable Investments. This document provides a user guide for MOPO-LSI
version 1.0, including problem setup, workflow and the hyper-parameters in
configurations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Supervised topological data analysis for MALDI mass spectrometry imaging
  applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13948v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13948v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gideon Klaila, Vladimir Vutov, Anastasios Stefanou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: Matrix-assisted laser desorption/ionization mass spectrometry
imaging (MALDI MSI) displays significant potential for applications in cancer
research, especially in tumor typing and subtyping. Lung cancer is the primary
cause of tumor-related deaths, where the most lethal entities are
adenocarcinoma (ADC) and squamous cell carcinoma (SqCC). Distinguishing between
these two common subtypes is crucial for therapy decisions and successful
patient management.
  Results: We propose a new algebraic topological framework, which obtains
intrinsic information from MALDI data and transforms it to reflect topological
persistence. Our framework offers two main advantages. Firstly, topological
persistence aids in distinguishing the signal from noise. Secondly, it
compresses the MALDI data, saving storage space and optimizes computational
time for subsequent classification tasks. We present an algorithm that
efficiently implements our topological framework, relying on a single tuning
parameter. Afterwards, logistic regression and random forest classifiers are
employed on the extracted persistence features, thereby accomplishing an
automated tumor (sub-)typing process. To demonstrate the competitiveness of our
proposed framework, we conduct experiments on a real-world MALDI dataset using
cross-validation. Furthermore, we showcase the effectiveness of the single
denoising parameter by evaluating its performance on synthetic MALDI images
with varying levels of noise.
  Conclusion: Our empirical experiments demonstrate that the proposed algebraic
topological framework successfully captures and leverages the intrinsic
spectral information from MALDI data, leading to competitive results in
classifying lung cancer subtypes. Moreover, the frameworks ability to be
fine-tuned for denoising highlights its versatility and potential for enhancing
data analysis in MALDI applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Roman Numeral Analysis with Graph Neural Networks: Onset-wise
  Predictions from Note-wise Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03544v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03544v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emmanouil Karystinaios, Gerhard Widmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Roman Numeral analysis is the important task of identifying chords and their
functional context in pieces of tonal music. This paper presents a new approach
to automatic Roman Numeral analysis in symbolic music. While existing
techniques rely on an intermediate lossy representation of the score, we
propose a new method based on Graph Neural Networks (GNNs) that enable the
direct description and processing of each individual note in the score. The
proposed architecture can leverage notewise features and interdependencies
between notes but yield onset-wise representation by virtue of our novel edge
contraction algorithm. Our results demonstrate that ChordGNN outperforms
existing state-of-the-art models, achieving higher accuracy in Roman Numeral
analysis on the reference datasets. In addition, we investigate variants of our
model using proposed techniques such as NADE, and post-processing of the chord
predictions. The full source code for this work is available at
https://github.com/manoskary/chordgnn
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the 24th Conference of the International Society
  for Music Information Retrieval (ISMIR 2023), Milan, Italy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Deep Learning Method for Comparing Bayesian Hierarchical Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11873v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11873v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lasse Elsemüller, Martin Schnuerch, Paul-Christian Bürkner, Stefan T. Radev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian model comparison (BMC) offers a principled approach for assessing
the relative merits of competing computational models and propagating
uncertainty into model selection decisions. However, BMC is often intractable
for the popular class of hierarchical models due to their high-dimensional
nested parameter structure. To address this intractability, we propose a deep
learning method for performing BMC on any set of hierarchical models which can
be instantiated as probabilistic programs. Since our method enables amortized
inference, it allows efficient re-estimation of posterior model probabilities
and fast performance validation prior to any real-data application. In a series
of extensive validation studies, we benchmark the performance of our method
against the state-of-the-art bridge sampling method and demonstrate excellent
amortized inference across all BMC settings. We then showcase our method by
comparing four hierarchical evidence accumulation models that have previously
been deemed intractable for BMC due to partly implicit likelihoods. In this
application, we corroborate evidence for the recently proposed L\'evy flight
model of decision-making and show how transfer learning can be leveraged to
enhance training efficiency. We provide reproducible code for all analyses and
an open-source implementation of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diagrammatization: Rationalizing with diagrammatic AI explanations for
  abductive-deductive reasoning on hypotheses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01241v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01241v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian Y. Lim, Joseph P. Cahaly, Chester Y. F. Sng, Adam Chew
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many visualizations have been developed for explainable AI (XAI), but they
often require further reasoning by users to interpret. We argue that XAI should
support diagrammatic and abductive reasoning for the AI to perform hypothesis
generation and evaluation to reduce the interpretability gap. We propose
Diagrammatization to i) perform Peircean abductive-deductive reasoning, ii)
follow domain conventions, and iii) explain with diagrams visually or verbally.
We implemented DiagramNet for a clinical application to predict cardiac
diagnoses from heart auscultation, and explain with shape-based murmur
diagrams. In modeling studies, we found that DiagramNet not only provides
faithful murmur shape explanations, but also has better prediction performance
than baseline models. We further demonstrate the interpretability and
trustworthiness of diagrammatic explanations in a qualitative user study with
medical students, showing that clinically-relevant, diagrammatic explanations
are preferred over technical saliency map explanations. This work contributes
insights into providing domain-conventional abductive explanations for
user-centric XAI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Bellman's principle of optimality and Reinforcement learning for
  safety-constrained Markov decision process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13152v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13152v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahul Misra, Rafał Wisniewski, Carsten Skovmose Kallesøe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study optimality for the safety-constrained Markov decision process which
is the underlying framework for safe reinforcement learning. Specifically, we
consider a constrained Markov decision process (with finite states and finite
actions) where the goal of the decision maker is to reach a target set while
avoiding an unsafe set(s) with certain probabilistic guarantees. Therefore the
underlying Markov chain for any control policy will be multichain since by
definition there exists a target set and an unsafe set. The decision maker also
has to be optimal (with respect to a cost function) while navigating to the
target set. This gives rise to a multi-objective optimization problem. We
highlight the fact that Bellman's principle of optimality may not hold for
constrained Markov decision problems with an underlying multichain structure
(as shown by the counterexample due to Haviv. We resolve the counterexample by
formulating the aforementioned multi-objective optimization problem as a
zero-sum game and thereafter construct an asynchronous value iteration scheme
for the Lagrangian (similar to Shapley's algorithm). Finally, we consider the
reinforcement learning problem for the same and construct a modified
$Q$-learning algorithm for learning the Lagrangian from data. We also provide a
lower bound on the number of iterations required for learning the Lagrangian
and corresponding error bounds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning based Uncertainty Decomposition for Real-time Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2010.02613v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2010.02613v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neha Das, Jonas Umlauft, Armin Lederer, Thomas Beckers, Sandra Hirche
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven control in unknown environments requires a clear understanding of
the involved uncertainties for ensuring safety and efficient exploration. While
aleatoric uncertainty that arises from measurement noise can often be
explicitly modeled given a parametric description, it can be harder to model
epistemic uncertainty, which describes the presence or absence of training
data. The latter can be particularly useful for implementing exploratory
control strategies when system dynamics are unknown. We propose a novel method
for detecting the absence of training data using deep learning, which gives a
continuous valued scalar output between $0$ (indicating low uncertainty) and
$1$ (indicating high uncertainty). We utilize this detector as a proxy for
epistemic uncertainty and show its advantages over existing approaches on
synthetic and real-world datasets. Our approach can be directly combined with
aleatoric uncertainty estimates and allows for uncertainty estimation in
real-time as the inference is sample-free unlike existing approaches for
uncertainty modeling. We further demonstrate the practicality of this
uncertainty estimate in deploying online data-efficient control on a simulated
quadcopter acted upon by an unknown disturbance model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IFAC World Congress 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Rates for the Regret of Offline Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.00479v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.00479v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichun Hu, Nathan Kallus, Masatoshi Uehara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the regret of reinforcement learning from offline data generated by
a fixed behavior policy in an infinite-horizon discounted Markov decision
process (MDP). While existing analyses of common approaches, such as fitted
$Q$-iteration (FQI), suggest a $O(1/\sqrt{n})$ convergence for regret,
empirical behavior exhibits \emph{much} faster convergence. In this paper, we
present a finer regret analysis that exactly characterizes this phenomenon by
providing fast rates for the regret convergence. First, we show that given any
estimate for the optimal quality function $Q^*$, the regret of the policy it
defines converges at a rate given by the exponentiation of the $Q^*$-estimate's
pointwise convergence rate, thus speeding it up. The level of exponentiation
depends on the level of noise in the \emph{decision-making} problem, rather
than the estimation problem. We establish such noise levels for linear and
tabular MDPs as examples. Second, we provide new analyses of FQI and Bellman
residual minimization to establish the correct pointwise convergence
guarantees. As specific cases, our results imply $O(1/n)$ regret rates in
linear cases and $\exp(-\Omega(n))$ regret rates in tabular cases. We extend
our findings to general function approximation by extending our results to
regret guarantees based on $L_p$-convergence rates for estimating $Q^*$ rather
than pointwise rates, where $L_2$ guarantees for nonparametric $Q^*$-estimation
can be ensured under mild conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Value Functions are Control Barrier Functions: Verification of Safe
  Policies using Control Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04026v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04026v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel C. H. Tan, Fernando Acero, Robert McCarthy, Dimitrios Kanoulas, Zhibin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Guaranteeing safe behaviour of reinforcement learning (RL) policies poses
significant challenges for safety-critical applications, despite RL's
generality and scalability. To address this, we propose a new approach to apply
verification methods from control theory to learned value functions. By
analyzing task structures for safety preservation, we formalize original
theorems that establish links between value functions and control barrier
functions. Further, we propose novel metrics for verifying value functions in
safe control tasks and practical implementation details to improve learning.
Our work presents a novel method for certificate learning, which unlocks a
diversity of verification techniques from control theory for RL policies, and
marks a significant step towards a formal framework for the general, scalable,
and verifiable design of RL-based control systems. Code and videos are
available at this https url: https://rl-cbf.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ANTONIO: Towards a Systematic Method of Generating NLP Benchmarks for
  Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.04003v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.04003v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Casadio, Luca Arnaboldi, Matthew L. Daggitt, Omri Isac, Tanvi Dinkar, Daniel Kienitz, Verena Rieser, Ekaterina Komendantskaya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Verification of machine learning models used in Natural Language Processing
(NLP) is known to be a hard problem. In particular, many known neural network
verification methods that work for computer vision and other numeric datasets
do not work for NLP. Here, we study technical reasons that underlie this
problem. Based on this analysis, we propose practical methods and heuristics
for preparing NLP datasets and models in a way that renders them amenable to
known verification methods based on abstract interpretation. We implement these
methods as a Python library called ANTONIO that links to the neural network
verifiers ERAN and Marabou. We perform evaluation of the tool using an NLP
dataset R-U-A-Robot suggested as a benchmark for verifying legally critical NLP
applications. We hope that, thanks to its general applicability, this work will
open novel possibilities for including NLP verification problems into neural
network verification competitions, and will popularise NLP problems within this
community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in proceedings of 6th Workshop on Formal Methods for
  ML-Enabled Autonomous Systems (Affiliated with CAV 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collaborative Robotic Biopsy with Trajectory Guidance and Needle Tip
  Force Feedback <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07129v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07129v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robin Mieling, Maximilian Neidhardt, Sarah Latus, Carolin Stapper, Stefan Gerlach, Inga Kniep, Axel Heinemann, Benjamin Ondruschka, Alexander Schlaefer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The diagnostic value of biopsies is highly dependent on the placement of
needles. Robotic trajectory guidance has been shown to improve needle
positioning, but feedback for real-time navigation is limited. Haptic display
of needle tip forces can provide rich feedback for needle navigation by
enabling localization of tissue structures along the insertion path. We present
a collaborative robotic biopsy system that combines trajectory guidance with
kinesthetic feedback to assist the physician in needle placement. The robot
aligns the needle while the insertion is performed in collaboration with a
medical expert who controls the needle position on site. We present a needle
design that senses forces at the needle tip based on optical coherence
tomography and machine learning for real-time data processing. Our robotic
setup allows operators to sense deep tissue interfaces independent of
frictional forces to improve needle placement relative to a desired target
structure. We first evaluate needle tip force sensing in ex-vivo tissue in a
phantom study. We characterize the tip forces during insertions with constant
velocity and demonstrate the ability to detect tissue interfaces in a
collaborative user study. Participants are able to detect 91% of ex-vivo tissue
interfaces based on needle tip force feedback alone. Finally, we demonstrate
that even smaller, deep target structures can be accurately sampled by
performing post-mortem in situ biopsies of the pancreas.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at ICRA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ESGCN: Edge Squeeze Attention Graph Convolutional Network for Traffic
  Flow Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.01227v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.01227v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangrok Lee, Ha Young Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic forecasting is a highly challenging task owing to the dynamical
spatio-temporal dependencies of traffic flows. To handle this, we focus on
modeling the spatio-temporal dynamics and propose a network termed Edge Squeeze
Graph Convolutional Network (ESGCN) to forecast traffic flow in multiple
regions. ESGCN consists of two modules: W-module and ES module. W-module is a
fully node-wise convolutional network. It encodes the time-series of each
traffic region separately and decomposes the time-series at various scales to
capture fine and coarse features. The ES module models the spatio-temporal
dynamics using Graph Convolutional Network (GCN) and generates an Adaptive
Adjacency Matrix (AAM) with temporal features. To improve the accuracy of AAM,
we introduce three key concepts. 1) Using edge features to directly capture the
spatiotemporal flow representation among regions. 2) Applying an edge attention
mechanism to GCN to extract the AAM from the edge features. Here, the attention
mechanism can effectively determine important spatio-temporal adjacency
relations. 3) Proposing a novel node contrastive loss to suppress obstructed
connections and emphasize related connections. Experimental results show that
ESGCN achieves state-of-the-art performance by a large margin on four
real-world datasets (PEMS03, 04, 07, and 08) with a low computational cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 Pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Empowering General-purpose User Representation with Full-life Cycle
  Behavior Modeling <span class="chip">KDD 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.11337v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.11337v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bei Yang, Jie Gu, Ke Liu, Xiaoxiao Xu, Renjun Xu, Qinghui Sun, Hong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User Modeling plays an essential role in industry. In this field,
task-agnostic approaches, which generate general-purpose representation
applicable to diverse downstream user cognition tasks, is a promising direction
being more valuable and economical than task-specific representation learning.
With the rapid development of Internet service platforms, user behaviors have
been accumulated continuously. However, existing general-purpose user
representation researches have little ability for full-life cycle modeling on
extremely long behavior sequences since user registration. In this study, we
propose a novel framework called full- Life cycle User Representation Model
(LURM) to tackle this challenge. Specifically, LURM consists of two cascaded
sub-models: (I) Bag-of-Interests (BoI) encodes user behaviors in any time
period into a sparse vector with super-high dimension (e.g., 10^5); (II)
Self-supervised Multi-anchor Encoder Network (SMEN) maps sequences of BoI
features to multiple low-dimensional user representations. Specially, SMEN
achieves almost lossless dimensionality reduction, benefiting from a novel
multi-anchor module which can learn different aspects of user interests.
Experiments on several benchmark datasets show that our approach outperforms
state-of-the-art general-purpose representation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by KDD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics-Informed Neural Networks for Discovering Localised Eigenstates
  in Disordered Media 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.06802v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.06802v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liam Harcombe, Quanling Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Schr\"{o}dinger equation with random potentials is a fundamental model
for understanding the behaviour of particles in disordered systems. Disordered
media are characterised by complex potentials that lead to the localisation of
wavefunctions, also called Anderson localisation. These wavefunctions may have
similar scales of eigenenergies which poses difficulty in their discovery. It
has been a longstanding challenge due to the high computational cost and
complexity of solving the Schr\"{o}dinger equation. Recently, machine-learning
tools have been adopted to tackle these challenges. In this paper, based upon
recent advances in machine learning, we present a novel approach for
discovering localised eigenstates in disordered media using physics-informed
neural networks (PINNs). We focus on the spectral approximation of Hamiltonians
in one dimension with potentials that are randomly generated according to the
Bernoulli, normal, and uniform distributions. We introduce a novel feature to
the loss function that exploits known physical phenomena occurring in these
regions to scan across the domain and successfully discover these eigenstates,
regardless of the similarity of their eigenenergies. We present various
examples to demonstrate the performance of the proposed approach and compare it
with isogeometric analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High Dimensional Quantum Machine Learning With Small Quantum Computers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13739v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13739v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon C. Marshall, Casper Gyurik, Vedran Dunjko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum computers hold great promise to enhance machine learning, but their
current qubit counts restrict the realisation of this promise. In an attempt to
placate this limitation techniques can be applied for evaluating a quantum
circuit using a machine with fewer qubits than the circuit naively requires.
These techniques work by evaluating many smaller circuits on the smaller
machine, that are then combined in a polynomial to replicate the output of the
larger machine. This scheme requires more circuit evaluations than are
practical for general circuits. However, we investigate the possibility that
for certain applications many of these subcircuits are superfluous, and that a
much smaller sum is sufficient to estimate the full circuit. We construct a
machine learning model that may be capable of approximating the outputs of the
larger circuit with much fewer circuit evaluations. We successfully apply our
model to the task of digit recognition, using simulated quantum computers much
smaller than the data dimension. The model is also applied to the task of
approximating a random 10 qubit PQC with simulated access to a 5 qubit
computer, even with only relatively modest number of circuits our model
provides an accurate approximation of the 10 qubit PQCs output, superior to a
neural network attempt. The developed method might be useful for implementing
quantum models on larger data throughout the NISQ era.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reduce Computational Complexity for Convolutional Layers by Skipping
  Zeros 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15951v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15951v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyi Zhang, Pengfei Zhang, Zhuopin Xu, Qi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks rely on parallel processors for acceleration. To design
operators for them, it requires not only good algorithm to reduce complexity,
but also sufficient utilization of hardwares. Convolutional layers mainly
contain 3 kinds of operators: convolution in forward propagation, deconvolution
and dilated-convolution in backward propagation. When executing these
operators, 0s are always added to tensors, causing redundant calculations. This
paper gives C-K-S algorithm (ConvV2, KS-deconv, Sk-dilated), which skips these
0s in two ways: trim the filters to exclude padded 0s; transform sparse tensors
to dense tensors, to avoid inserted 0s in deconvolution and
dilated-convolution. In contrast to regular convolution, deconvolution is hard
to accelerate due to its complicacy. This paper provides high-performance GPU
implementations of C-K-S, and verifies their effectiveness with comparison to
PyTorch. According to the experiments, C-K-S has advantages over PyTorch in
certain cases, especially in deconvolution on small feature-maps. Further
enhancement of C-K-S can be done by making full optimizations oriented at
specific GPU architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To download the code of Dragon-Alpha and experimental datas, please
  go to https://github.com/GilgameshXYZ123/Dragon-Alpha</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ B-HAR: an open-source baseline framework for in depth study of human
  activity recognition <span class="highlight-title">dataset</span>s and workflows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2101.10870v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2101.10870v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florenc Demrozi, Cristian Turetta, Graziano Pravadelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human Activity Recognition (HAR), based on machine and deep learning
algorithms is considered one of the most promising technologies to monitor
professional and daily life activities for different categories of people
(e.g., athletes, elderly, kids, employers) in order to provide a variety of
services related, for example to well-being, empowering of technical
performances, prevention of risky situation, and educational purposes. However,
the analysis of the effectiveness and the efficiency of HAR methodologies
suffers from the lack of a standard workflow, which might represent the
baseline for the estimation of the quality of the developed pattern recognition
models. This makes the comparison among different approaches a challenging
task. In addition, researchers can make mistakes that, when not detected,
definitely affect the achieved results. To mitigate such issues, this paper
proposes an open-source automatic and highly configurable framework, named
B-HAR, for the definition, standardization, and development of a baseline
framework in order to evaluate and compare HAR methodologies. It implements the
most popular data processing methods for data preparation and the most commonly
used machine and deep learning pattern recognition models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 Pages, 3 Figures, 3 Tables, Link to B-HAR Library:
  https://github.com/B-HAR-HumanActivityRecognition/B-HAR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Bayesian Take on Gaussian Process Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11380v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11380v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enrico Giudice, Jack Kuipers, Giusi Moffa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian Process Networks (GPNs) are a class of directed graphical models
which employ Gaussian processes as priors for the conditional expectation of
each variable given its parents in the network. The model allows describing
continuous joint distributions in a compact but flexible manner with minimal
parametric assumptions on the dependencies between variables. Bayesian
structure learning of GPNs requires computing the posterior over graphs of the
network and is computationally infeasible even in low dimensions. This work
implements Monte Carlo and Markov Chain Monte Carlo methods to sample from the
posterior distribution of network structures. As such, the approach follows the
Bayesian paradigm, comparing models via their marginal likelihood and computing
the posterior probability of the GPN features. Simulation studies show that our
method outperforms state-of-the-art algorithms in recovering the graphical
structure of the network and provides an accurate approximation of its
posterior distribution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improved uncertainty quantification for neural networks with Bayesian
  last layer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10975v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10975v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Fiedler, Sergio Lucia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainty quantification is an essential task in machine learning - a task
in which neural networks (NNs) have traditionally not excelled. This can be a
limitation for safety-critical applications, where uncertainty-aware methods
like Gaussian processes or Bayesian linear regression are often preferred.
Bayesian neural networks are an approach to address this limitation. They
assume probability distributions for all parameters and yield distributed
predictions. However, training and inference are typically intractable and
approximations must be employed. A promising approximation is NNs with Bayesian
last layer (BLL). They assume distributed weights only in the last linear layer
and yield a normally distributed prediction. NNs with BLL can be seen as a
Bayesian linear regression model with learned nonlinear features. To
approximate the intractable Bayesian neural network, point estimates of the
distributed weights in all but the last layer should be obtained by maximizing
the marginal likelihood. This has previously been challenging, as the marginal
likelihood is expensive to evaluate in this setting and prohibits direct
training through backpropagation. We present a reformulation of the
log-marginal likelihood of a NN with BLL which allows for efficient training
using backpropagation. Furthermore, we address the challenge of quantifying
uncertainty for extrapolation points. We provide a metric to quantify the
degree of extrapolation and derive a method to improve the uncertainty
quantification for these points. Our methods are derived for the multivariate
case and demonstrated in a simulation study, where we compare Bayesian linear
regression applied to a previously trained neural network with our proposed
algorithm
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, 1 table. This work has been submitted to the
  IEEE for possible publication. Copyright may be transferred without notice,
  after which this version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A large sample theory for infinitesimal gradient boosting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.00736v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.00736v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clement Dombry, Jean-Jil Duchamps
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Infinitesimal gradient boosting (Dombry and Duchamps, 2021) is defined as the
vanishing-learning-rate limit of the popular tree-based gradient boosting
algorithm from machine learning. It is characterized as the solution of a
nonlinear ordinary differential equation in a infinite-dimensional function
space where the infinitesimal boosting operator driving the dynamics depends on
the training sample. We consider the asymptotic behavior of the model in the
large sample limit and prove its convergence to a deterministic process. This
population limit is again characterized by a differential equation that depends
on the population distribution. We explore some properties of this population
limit: we prove that the dynamics makes the test error decrease and we consider
its long time behavior.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic mean field programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.05200v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.05200v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        George Stamatescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A dynamic mean field theory is developed for finite state and action Bayesian
reinforcement learning in the large state space limit. In an analogy with
statistical physics, the Bellman equation is studied as a disordered dynamical
system; the Markov decision process transition probabilities are interpreted as
couplings and the value functions as deterministic spins that evolve
dynamically. Thus, the mean-rewards and transition probabilities are considered
to be quenched random variables. The theory reveals that, under certain
assumptions, the state-action values are statistically independent across
state-action pairs in the asymptotic state space limit, and provides the form
of the distribution exactly. The results hold in the finite and discounted
infinite horizon settings, for both value iteration and policy evaluation. The
state-action value statistics can be computed from a set of mean field
equations, which we call dynamic mean field programming (DMFP). For policy
evaluation the equations are exact. For value iteration, approximate equations
are obtained by appealing to extreme value theory or bounds. The result
provides analytic insight into the statistical structure of tabular
reinforcement learning, for example revealing the conditions under which
reinforcement learning is equivalent to a set of independent multi-armed bandit
problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solvent: A Framework for Protein Folding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04603v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04603v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaemyung Lee, Kyeongtak Han, Jaehoon Kim, Hasun Yu, Youhan Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Consistency and reliability are crucial for conducting AI research. Many
famous research fields, such as object detection, have been compared and
validated with solid benchmark frameworks. After AlphaFold2, the protein
folding task has entered a new phase, and many methods are proposed based on
the component of AlphaFold2. The importance of a unified research framework in
protein folding contains implementations and benchmarks to consistently and
fairly compare various approaches. To achieve this, we present Solvent, an
protein folding framework that supports significant components of
state-of-th-arts models in the manner of off-the-shelf interface Solvent
contains different models implemented in a unified codebase and supports
training and evaluation for defined models on the same dataset. We benchmark
well-known algorithms and their components and provide experiments that give
helpful insights into the protein structure modeling field. We hope that
Solvent will increase the reliability and consistency of proposed models and
gives efficiency in both speed and costs, resulting in acceleration on protein
folding modeling research. The code is available at
https://github.com/kakaobrain/solvent, and the project will continue to be
developed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint, 8pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bidirectional Generation of Structure and Properties Through a Single
  Molecular Foundation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10590v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10590v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinho Chang, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent success of large foundation models in artificial intelligence has
prompted the emergence of chemical pre-trained models. Despite the growing
interest in large molecular pre-trained models that provide informative
representations for downstream tasks, attempts for multimodal pre-training
approaches on the molecule domain were limited. To address this, we present a
novel multimodal molecular pre-trained model that incorporates the modalities
of structure and biochemical properties, drawing inspiration from recent
advances in multimodal learning techniques. Our proposed model pipeline of data
handling and training objectives aligns the structure/property features in a
common embedding space, which enables the model to regard bidirectional
information between the molecules' structure and properties. These
contributions emerge synergistic knowledge, allowing us to tackle both
multimodal and unimodal downstream tasks through a single model. Through
extensive experiments, we demonstrate that our model shows remarkable
capabilities in solving various meaningful chemical challenges, including
conditional molecule generation, property prediction, molecule classification,
and reaction prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Test-Time Training on Video Streams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05014v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05014v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renhao Wang, Yu Sun, Yossi Gandelsman, Xinlei Chen, Alexei A. Efros, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior work has established test-time training (TTT) as a general framework to
further improve a trained model at test time. Before making a prediction on
each test instance, the model is trained on the same instance using a
self-supervised task, such as image reconstruction with masked autoencoders. We
extend TTT to the streaming setting, where multiple test instances - video
frames in our case - arrive in temporal order. Our extension is online TTT: The
current model is initialized from the previous model, then trained on the
current frame and a small window of frames immediately before. Online TTT
significantly outperforms the fixed-model baseline for four tasks, on three
real-world datasets. The relative improvement is 45% and 66% for instance and
panoptic segmentation. Surprisingly, online TTT also outperforms its offline
variant that accesses more information, training on all frames from the entire
test video regardless of temporal order. This differs from previous findings
using synthetic videos. We conceptualize locality as the advantage of online
over offline TTT. We analyze the role of locality with ablations and a theory
based on bias-variance trade-off.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website with videos, dataset and code:
  https://video-ttt.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recurrent Equilibrium Networks: Flexible Dynamic Models with Guaranteed
  Stability and Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.05942v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.05942v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Revay, Ruigang Wang, Ian R. Manchester
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces recurrent equilibrium networks (RENs), a new class of
nonlinear dynamical models} for applications in machine learning, system
identification and control. The new model class admits ``built in'' behavioural
guarantees of stability and robustness. All models in the proposed class are
contracting -- a strong form of nonlinear stability -- and models can satisfy
prescribed incremental integral quadratic constraints (IQC), including
Lipschitz bounds and incremental passivity. RENs are otherwise very flexible:
they can represent all stable linear systems, all previously-known sets of
contracting recurrent neural networks and echo state networks, all deep
feedforward neural networks, and all stable Wiener/Hammerstein models, and can
approximate all fading-memory and contracting nonlinear systems. RENs are
parameterized directly by a vector in R^N, i.e. stability and robustness are
ensured without parameter constraints, which simplifies learning since
\HL{generic methods for unconstrained optimization such as stochastic gradient
descent and its variants can be used}. The performance and robustness of the
new model set is evaluated on benchmark nonlinear system identification
problems, and the paper also presents applications in data-driven nonlinear
observer design and control with stability guarantees.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in IEEE Transactions on Automatic Control</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fundamental Limits for Sensor-Based Robot Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.00129v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.00129v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anirudha Majumdar, Zhiting Mei, Vincent Pacelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our goal is to develop theory and algorithms for establishing fundamental
limits on performance imposed by a robot's sensors for a given task. In order
to achieve this, we define a quantity that captures the amount of task-relevant
information provided by a sensor. Using a novel version of the generalized Fano
inequality from information theory, we demonstrate that this quantity provides
an upper bound on the highest achievable expected reward for one-step decision
making tasks. We then extend this bound to multi-step problems via a dynamic
programming approach. We present algorithms for numerically computing the
resulting bounds, and demonstrate our approach on three examples: (i) the lava
problem from the literature on partially observable Markov decision processes,
(ii) an example with continuous state and observation spaces corresponding to a
robot catching a freely-falling object, and (iii) obstacle avoidance using a
depth sensor with non-Gaussian noise. We demonstrate the ability of our
approach to establish strong limits on achievable performance for these
problems by comparing our upper bounds with achievable lower bounds (computed
by synthesizing or learning concrete control policies).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of paper presented at the 2022 Robotics: Science and
  Systems (RSS) conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating the Effectiveness of Chat<span class="highlight-title">GPT</span> in Mathematical Reasoning and
  Problem Solving: Evidence from the Vietnamese National High School Graduation
  Examination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06331v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06331v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan-Quy Dao, Ngoc-Bich Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study offers a complete analysis of ChatGPT's mathematics abilities in
responding to multiple-choice questions for the Vietnamese National High School
Graduation Examination (VNHSGE) on a range of subjects and difficulty levels.
The dataset included 250 questions divided into four levels: knowledge (K),
comprehension (C), application (A), and high application (H), and it included
ten themes that covered diverse mathematical concepts. The outcomes demonstrate
that ChatGPT's performance varies depending on the difficulty level and
subject. It performed best on questions at Level (K), with an accuracy rate of
$83\%$; but, as the difficulty level rose, it scored poorly, with an accuracy
rate of $10\%$. The study has also shown that ChatGPT significantly succeeds in
providing responses to questions on subjects including exponential and
logarithmic functions, geometric progression, and arithmetic progression. The
study found that ChatGPT had difficulty correctly answering questions on topics
including derivatives and applications, spatial geometry, and Oxyz spatial
calculus. Additionally, this study contrasted ChatGPT outcomes with Vietnamese
students in VNHSGE and in other math competitions. ChatGPT dominated in the SAT
Math competition with a success rate of $70\%$, followed by VNHSGE mathematics
($58.8\%)$. However, its success rates were lower on other exams, such as AP
Statistics, the GRE Quantitative, AMC 10, AMC 12, and AP Calculus BC. These
results suggest that ChatGPT has the potential to be an effective teaching tool
for mathematics, but more work is needed to enhance its handling of graphical
data and address the challenges presented by questions that are getting more
challenging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 13 images</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards a More Rigorous Science of Blindspot Discovery in Image
  Classification Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.04104v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.04104v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gregory Plumb, Nari Johnson, Ángel Alexander Cabrera, Ameet Talwalkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A growing body of work studies Blindspot Discovery Methods ("BDM"s): methods
that use an image embedding to find semantically meaningful (i.e., united by a
human-understandable concept) subsets of the data where an image classifier
performs significantly worse. Motivated by observed gaps in prior work, we
introduce a new framework for evaluating BDMs, SpotCheck, that uses synthetic
image datasets to train models with known blindspots and a new BDM, PlaneSpot,
that uses a 2D image representation. We use SpotCheck to run controlled
experiments that identify factors that influence BDM performance (e.g., the
number of blindspots in a model, or features used to define the blindspot) and
show that PlaneSpot is competitive with and in many cases outperforms existing
BDMs. Importantly, we validate these findings by designing additional
experiments that use real image data from MS-COCO, a large image benchmark
dataset. Our findings suggest several promising directions for future work on
BDM design and evaluation. Overall, we hope that the methodology and analyses
presented in this work will help facilitate a more rigorous science of
blindspot discovery.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>reviewed on OpenReview: https://openreview.net/forum?id=MaDvbLaBiF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Matching-based Data Valuation for Generative Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.10701v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.10701v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxi Yang, Wenglong Deng, Benlin Liu, Yangsibo Huang, Xiaoxiao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data valuation is critical in machine learning, as it helps enhance model
transparency and protect data properties. Existing data valuation methods have
primarily focused on discriminative models, neglecting deep generative models
that have recently gained considerable attention. Similar to discriminative
models, there is an urgent need to assess data contributions in deep generative
models as well. However, previous data valuation approaches mainly relied on
discriminative model performance metrics and required model retraining.
Consequently, they cannot be applied directly and efficiently to recent deep
generative models, such as generative adversarial networks and diffusion
models, in practice. To bridge this gap, we formulate the data valuation
problem in generative models from a similarity-matching perspective.
Specifically, we introduce Generative Model Valuator (GMValuator), the first
model-agnostic approach for any generative models, designed to provide data
valuation for generation tasks. We have conducted extensive experiments to
demonstrate the effectiveness of the proposed method. To the best of their
knowledge, GMValuator is the first work that offers a training-free, post-hoc
data valuation strategy for deep generative models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MGR: Multi-generator Based Rationalization <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.04492v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.04492v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Liu, Haozhao Wang, Jun Wang, Ruixuan Li, Xinyang Li, Yuankai Zhang, Yang Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rationalization is to employ a generator and a predictor to construct a
self-explaining NLP model in which the generator selects a subset of
human-intelligible pieces of the input text to the following predictor.
However, rationalization suffers from two key challenges, i.e., spurious
correlation and degeneration, where the predictor overfits the spurious or
meaningless pieces solely selected by the not-yet well-trained generator and in
turn deteriorates the generator. Although many studies have been proposed to
address the two challenges, they are usually designed separately and do not
take both of them into account. In this paper, we propose a simple yet
effective method named MGR to simultaneously solve the two problems. The key
idea of MGR is to employ multiple generators such that the occurrence stability
of real pieces is improved and more meaningful pieces are delivered to the
predictor. Empirically, we show that MGR improves the F1 score by up to 20.9%
as compared to state-of-the-art methods. Codes are available at
https://github.com/jugechengzi/Rationalization-MGR .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023, oral presentation. arXiv admin note: text overlap with
  arXiv:2209.08285</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Polysemanticity and Capacity in Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.01892v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.01892v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Scherlis, Kshitij Sachan, Adam S. Jermyn, Joe Benton, Buck Shlegeris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Individual neurons in neural networks often represent a mixture of unrelated
features. This phenomenon, called polysemanticity, can make interpreting neural
networks more difficult and so we aim to understand its causes. We propose
doing so through the lens of feature \emph{capacity}, which is the fractional
dimension each feature consumes in the embedding space. We show that in a toy
model the optimal capacity allocation tends to monosemantically represent the
most important features, polysemantically represent less important features (in
proportion to their impact on the loss), and entirely ignore the least
important features. Polysemanticity is more prevalent when the inputs have
higher kurtosis or sparsity and more prevalent in some architectures than
others. Given an optimal allocation of capacity, we go on to study the geometry
of the embedding space. We find a block-semi-orthogonal structure, with
differing block sizes in different models, highlighting the impact of model
architecture on the interpretability of its neurons.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 7 figures. Corrected typos in Figure 7, improved notation
  to distinguish column and row vectors, corrected proof in Appendix A, and
  other misc changes</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pointwise convergence theorem of gradient descent in sparse deep neural
  network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.08172v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.08172v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tsuyoshi Yoneda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The theoretical structure of deep neural network (DNN) has been clarified
gradually. Imaizumi-Fukumizu (2019) and Suzuki (2019) clarified that the
learning ability of DNN is superior to the previous theories when the target
function is non-smooth functions. However, as far as the author is aware, none
of the numerous works to date attempted to mathematically investigate what kind
of DNN architectures really induce pointwise convergence of gradient descent
(without any statistical argument), and this attempt seems to be closer to the
practical DNNs. In this paper we restrict target functions to non-smooth
indicator functions, and construct a deep neural network inducing pointwise
convergence provided by gradient descent process in ReLU-DNN. The DNN has a
sparse and a special shape, with certain variable transformations.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic Communications System with Model Division Multiple Access and
  Controllable Coding Rate for Point Cloud 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyi Liu, Haotai Liang, Zhicheng Bao, Chen Dong, Xiaodong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point cloud, as a 3D representation, is widely used in autonomous driving,
virtual reality (VR), and augmented reality (AR). However, traditional
communication systems think that the point cloud's semantic information is
irrelevant to communication, which hinders the efficient transmission of point
clouds in the era of artificial intelligence (AI). This paper proposes a point
cloud based semantic communication system (PCSC), which uses AI-based encoding
techniques to extract the semantic information of the point cloud and joint
source-channel coding (JSCC) technology to overcome the distortion caused by
noise channels and solve the "cliff effect" in traditional communication. In
addition, the system realizes the controllable coding rate without fine-tuning
the network. The method analyzes the coded semantic vector's importance and
discards semantically-unimportant information, thereby improving the
transmission efficiency. Besides, PCSC and the recently proposed non-orthogonal
model division multiple access (MDMA) technology are combined to design a point
cloud MDMA transmission system (M-PCSC) for multi-user transmission. Relevant
experimental results show that the proposed method outperforms the traditional
method 10dB in the same channel bandwidth ratio under the PSNR D1 and PSNR D2
metrics. In terms of transmission, the proposed method can effectively solve
the "cliff effect" in the traditional methods.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-07-11T00:00:00Z">2023-07-11</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">47</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Relational Extraction on Wikipedia Tables using Convolutional and Memory
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arif Shahriar, Rohan Saha, Denilson Barbosa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation extraction (RE) is the task of extracting relations between entities
in text. Most RE methods extract relations from free-form running text and
leave out other rich data sources, such as tables. We explore RE from the
perspective of applying neural methods on tabularly organized data. We
introduce a new model consisting of Convolutional Neural Network (CNN) and
Bidirectional-Long Short Term Memory (BiLSTM) network to encode entities and
learn dependencies among them, respectively. We evaluate our model on a large
and recent dataset and compare results with previous neural methods.
Experimental results show that our model consistently outperforms the previous
model for the task of relation extraction on tabular data. We perform
comprehensive error analyses and ablation study to show the contribution of
various components of our model. Finally, we discuss the usefulness and
trade-offs of our approach, and provide suggestions for fostering further
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved POS tagging for spontaneous, clinical speech using data
  augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seth Kulick, Neville Ryant, David J. Irwin, Naomi Nevler, Sunghye Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of improving POS tagging of transcripts of
speech from clinical populations. In contrast to prior work on parsing and POS
tagging of transcribed speech, we do not make use of an in domain treebank for
training. Instead, we train on an out of domain treebank of newswire using data
augmentation techniques to make these structures resemble natural, spontaneous
speech. We trained a parser with and without the augmented data and tested its
performance using manually validated POS tags in clinical speech produced by
patients with various types of neurodegenerative conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05782v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05782v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael R. Douglas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence is making spectacular progress, and one of the best
examples is the development of large language models (LLMs) such as OpenAI's
GPT series. In these lectures, written for readers with a background in
mathematics or physics, we give a brief history and survey of the state of the
art, and describe the underlying transformer architecture in detail. We then
explore some current ideas on how LLMs work and how models trained to predict
the next word in a text are able to perform other tasks displaying
intelligence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>46 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Machine Translation Data Generation and Augmentation using
  Chat<span class="highlight-title">GPT</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wayne Yang, Garrett Nicolai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural models have revolutionized the field of machine translation, but
creating parallel corpora is expensive and time-consuming. We investigate an
alternative to manual parallel corpora - hallucinated parallel corpora created
by generative language models. Although these models are themselves trained on
parallel data, they can leverage a multilingual vector space to create data,
and may be able to supplement small manually-procured corpora. Our experiments
highlight two key findings - despite a lack of diversity in their output, the
hallucinated data improves the translation signal, even when the domain clashes
with the original dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 Pages, 4 Figures, 4 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Robust and Efficient Continual Language Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05741v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05741v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Fisch, Amal Rannen-Triki, Razvan Pascanu, Jörg Bornschein, Angeliki Lazaridou, Elena Gribovskaya, Marc'Aurelio Ranzato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the application space of language models continues to evolve, a natural
question to ask is how we can quickly adapt models to new tasks. We approach
this classic question from a continual learning perspective, in which we aim to
continue fine-tuning models trained on past tasks on new tasks, with the goal
of "transferring" relevant knowledge. However, this strategy also runs the risk
of doing more harm than good, i.e., negative transfer. In this paper, we
construct a new benchmark of task sequences that target different possible
transfer scenarios one might face, such as a sequence of tasks with high
potential of positive transfer, high potential for negative transfer, no
expected effect, or a mixture of each. An ideal learner should be able to
maximally exploit information from all tasks that have any potential for
positive transfer, while also avoiding the negative effects of any distracting
tasks that may confuse it. We then propose a simple, yet effective, learner
that satisfies many of our desiderata simply by leveraging a selective strategy
for initializing new models from past task checkpoints. Still, limitations
remain, and we hope this benchmark can help the community to further build and
analyze such learners.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stack More Layers Differently: High-Rank Training Through Low-Rank
  Updates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladislav Lialin, Namrata Shivagunde, Sherin Muckatira, Anna Rumshisky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the dominance and effectiveness of scaling, resulting in large
networks with hundreds of billions of parameters, the necessity to train
overparametrized models remains poorly understood, and alternative approaches
do not necessarily make it cheaper to train high-performance models. In this
paper, we explore low-rank training techniques as an alternative approach to
training large neural networks. We introduce a novel method called ReLoRA,
which utilizes low-rank updates to train high-rank networks. We apply ReLoRA to
pre-training transformer language models with up to 350M parameters and
demonstrate comparable performance to regular neural network training.
Furthermore, we observe that the efficiency of ReLoRA increases with model
size, making it a promising approach for training multi-billion-parameter
networks efficiently. Our findings shed light on the potential of low-rank
training techniques and their implications for scaling laws.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Empowering Cross-lingual Behavioral Testing of NLP Models with
  Typological Features <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ester Hlavnova, Sebastian Ruder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A challenge towards developing NLP systems for the world's languages is
understanding how they generalize to typological differences relevant for
real-world applications. To this end, we propose M2C, a morphologically-aware
framework for behavioral testing of NLP models. We use M2C to generate tests
that probe models' behavior in light of specific linguistic features in 12
typologically diverse languages. We evaluate state-of-the-art language models
on the generated tests. While models excel at most tests in English, we
highlight generalization failures to specific typological characteristics such
as temporal expressions in Swahili and compounding possessives in Finish. Our
findings motivate the development of models that address these blind spots.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings of the 61st Annual Meeting of the Association for
  Computational Linguistics(ACL). July 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ISLTranslate: <span class="highlight-title">Dataset</span> for Translating Indian Sign Language <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Joshi, Susmit Agrawal, Ashutosh Modi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sign languages are the primary means of communication for many
hard-of-hearing people worldwide. Recently, to bridge the communication gap
between the hard-of-hearing community and the rest of the population, several
sign language translation datasets have been proposed to enable the development
of statistical sign language translation systems. However, there is a dearth of
sign language resources for the Indian sign language. This resource paper
introduces ISLTranslate, a translation dataset for continuous Indian Sign
Language (ISL) consisting of 31k ISL-English sentence/phrase pairs. To the best
of our knowledge, it is the largest translation dataset for continuous Indian
Sign Language. We provide a detailed analysis of the dataset. To validate the
performance of existing end-to-end Sign language to spoken language translation
systems, we benchmark the created dataset with a transformer-based model for
ISL translation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2023 Findings, 8 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Duncode Characters Shorter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changshang Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the employment of various encoders in text
transformation, converting characters into bytes. It discusses local encoders
such as ASCII and GB-2312, which encode specific characters into shorter bytes,
and universal encoders like UTF-8 and UTF-16, which can encode the complete
Unicode set with greater space requirements and are gaining widespread
acceptance. Other encoders, including SCSU, BOCU-1, and binary encoders,
however, lack self-synchronizing capabilities. Duncode is introduced as an
innovative encoding method that aims to encode the entire Unicode character set
with high space efficiency, akin to local encoders. It has the potential to
compress multiple characters of a string into a Duncode unit using fewer bytes.
Despite offering less self-synchronizing identification information, Duncode
surpasses UTF8 in terms of space efficiency. The application is available at
\url{https://github.com/laohur/duncode}. Additionally, we have developed a
benchmark for evaluating character encoders across different languages. It
encompasses 179 languages and can be accessed at
\url{https://github.com/laohur/wiki2txt}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BLUEX: A benchmark based on Brazilian Leading Universities Entrance
  eXams 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05410v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05410v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thales Sales Almeida, Thiago Laitz, Giovana K. Bonás, Rodrigo Nogueira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One common trend in recent studies of language models (LMs) is the use of
standardized tests for evaluation. However, despite being the fifth most spoken
language worldwide, few such evaluations have been conducted in Portuguese.
This is mainly due to the lack of high-quality datasets available to the
community for carrying out evaluations in Portuguese. To address this gap, we
introduce the Brazilian Leading Universities Entrance eXams (BLUEX), a dataset
of entrance exams from the two leading universities in Brazil: UNICAMP and USP.
The dataset includes annotated metadata for evaluating the performance of NLP
models on a variety of subjects. Furthermore, BLUEX includes a collection of
recently administered exams that are unlikely to be included in the training
data of many popular LMs as of 2023. The dataset is also annotated to indicate
the position of images in each question, providing a valuable resource for
advancing the state-of-the-art in multimodal language understanding and
reasoning. We describe the creation and characteristics of BLUEX and establish
a benchmark through experiments with state-of-the-art LMs, demonstrating its
potential for advancing the state-of-the-art in natural language understanding
and reasoning in Portuguese. The data and relevant code can be found at
https://github.com/Portuguese-Benchmark-Datasets/BLUEX
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guji<span class="highlight-title">BERT</span> and Guji<span class="highlight-title">GPT</span>: Construction of Intelligent Information Processing
  Foundation Language Models for Ancient Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongbo Wang, Chang Liu, Zhixiao Zhao, Si Shen, Liu Liu, Bin Li, Haotian Hu, Mengcheng Wu, Litao Lin, Xue Zhao, Xiyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of the rapid development of large language models, we have
meticulously trained and introduced the GujiBERT and GujiGPT language models,
which are foundational models specifically designed for intelligent information
processing of ancient texts. These models have been trained on an extensive
dataset that encompasses both simplified and traditional Chinese characters,
allowing them to effectively handle various natural language processing tasks
related to ancient books, including but not limited to automatic sentence
segmentation, punctuation, word segmentation, part-of-speech tagging, entity
recognition, and automatic translation. Notably, these models have exhibited
exceptional performance across a range of validation tasks using publicly
available datasets. Our research findings highlight the efficacy of employing
self-supervised methods to further train the models using classical text
corpora, thus enhancing their capability to tackle downstream tasks. Moreover,
it is worth emphasizing that the choice of font, the scale of the corpus, and
the initial model selection all exert significant influence over the ultimate
experimental outcomes. To cater to the diverse text processing preferences of
researchers in digital humanities and linguistics, we have developed three
distinct categories comprising a total of nine model variations. We believe
that by sharing these foundational language models specialized in the domain of
ancient texts, we can facilitate the intelligent processing and scholarly
exploration of ancient literary works and, consequently, contribute to the
global dissemination of China's rich and esteemed traditional culture in this
new era.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22pages,0 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explaining Competitive-Level Programming Solutions using LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jierui Li, Szymon Tworkowski, Yingying Wu, Raymond Mooney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we approach competitive-level programming problem-solving as a
composite task of reasoning and code generation. We propose a novel method to
automatically annotate natural language explanations to \textit{<problem,
solution>} pairs. We show that despite poor performance in solving
competitive-level programming problems, state-of-the-art LLMs exhibit a strong
capacity in describing and explaining solutions. Our explanation generation
methodology can generate a structured solution explanation for the problem
containing descriptions and analysis. To evaluate the quality of the annotated
explanations, we examine their effectiveness in two aspects: 1) satisfying the
human programming expert who authored the oracle solution, and 2) aiding LLMs
in solving problems more effectively. The experimental results on the
CodeContests dataset demonstrate that while LLM GPT3.5's and GPT-4's abilities
in describing the solution are comparable, GPT-4 shows a better understanding
of the key idea behind the solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, presented at the 1st NLRSE workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unleashing Cognitive Synergy in Large Language Models: A Task-Solving
  Agent through Multi-Persona Self-Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human intelligence thrives on the concept of cognitive synergy, where
collaboration and information integration among different cognitive processes
yield superior outcomes compared to individual cognitive processes in
isolation. Although Large Language Models (LLMs) have demonstrated promising
performance as general task-solving agents, they still struggle with tasks that
require intensive domain knowledge and complex reasoning. In this work, we
propose Solo Performance Prompting (SPP), which transforms a single LLM into a
cognitive synergist by engaging in multi-turn self-collaboration with multiple
personas. A cognitive synergist refers to an intelligent agent that
collaborates with multiple minds, combining their individual strengths and
knowledge, to enhance problem-solving and overall performance in complex tasks.
By dynamically identifying and simulating different personas based on task
inputs, SPP unleashes the potential of cognitive synergy in LLMs. We have
discovered that assigning multiple, fine-grained personas in LLMs elicits
better problem-solving abilities compared to using a single or fixed number of
personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing,
Codenames Collaborative, and Logic Grid Puzzle, encompassing both
knowledge-intensive and reasoning-intensive types. Unlike previous works, such
as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, SPP
effectively elicits internal knowledge acquisition abilities, reduces
hallucination, and maintains strong reasoning capabilities. Code, data, and
prompts can be found at:
https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ U-CREAT: Unsupervised Case Retrieval using Events extrAcTion <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Joshi, Akshat Sharma, Sai Kiran Tanikella, Ashutosh Modi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of Prior Case Retrieval (PCR) in the legal domain is about
automatically citing relevant (based on facts and precedence) prior legal cases
in a given query case. To further promote research in PCR, in this paper, we
propose a new large benchmark (in English) for the PCR task: IL-PCR (Indian
Legal Prior Case Retrieval) corpus. Given the complex nature of case relevance
and the long size of legal documents, BM25 remains a strong baseline for
ranking the cited prior documents. In this work, we explore the role of events
in legal case retrieval and propose an unsupervised retrieval method-based
pipeline U-CREAT (Unsupervised Case Retrieval using Events Extraction). We find
that the proposed unsupervised retrieval method significantly increases
performance compared to BM25 and makes retrieval faster by a considerable
margin, making it applicable to real-time case retrieval systems. Our proposed
system is generic, we show that it generalizes across two different legal
systems (Indian and Canadian), and it shows state-of-the-art performance on the
benchmarks for both the legal systems (IL-PCR and COLIEE corpora).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2023, 15 pages (12 main + 3 Appendix)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attribute Controlled Dialogue <span class="highlight-title">Prompt</span>ing <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runcheng Liu, Ahmad Rashid, Ivan Kobyzev, Mehdi Rezagholizadeh, Pascal Poupart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt-tuning has become an increasingly popular parameter-efficient method
for adapting large pretrained language models to downstream tasks. However,
both discrete prompting and continuous prompting assume fixed prompts for all
data samples within a task, neglecting the fact that inputs vary greatly in
some tasks such as open-domain dialogue generation. In this paper, we present a
novel, instance-specific prompt-tuning algorithm for dialogue generation.
Specifically, we generate prompts based on instance-level control code, rather
than the conversation history, to explore their impact on controlled dialogue
generation. Experiments on popular open-domain dialogue datasets, evaluated on
both automated metrics and human evaluation, demonstrate that our method is
superior to prompting baselines and comparable to fine-tuning with only 5%-6%
of total parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2023 In Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Better Handling Coreference Resolution in Aspect Level Sentiment
  Classification by Fine-Tuning Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhruv Mullick, Bilal Ghanem, Alona Fyshe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Customer feedback is invaluable to companies as they refine their products.
Monitoring customer feedback can be automated with Aspect Level Sentiment
Classification (ALSC) which allows us to analyse specific aspects of the
products in reviews. Large Language Models (LLMs) are the heart of many
state-of-the-art ALSC solutions, but they perform poorly in some scenarios
requiring Coreference Resolution (CR). In this work, we propose a framework to
improve an LLM's performance on CR-containing reviews by fine tuning on highly
inferential tasks. We show that the performance improvement is likely
attributed to the improved model CR ability. We also release a new dataset that
focuses on CR in ALSC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work done up till December 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mao-Zedong At SemEval-2023 Task 4: Label Represention Multi-Head
  Attention Model With Contrastive Learning-Enhanced Nearest Neighbor Mechanism
  For Multi-Label Text Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05174v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05174v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Che Zhang, Ping'an Liu, Zhenyang Xiao, Haojun Fei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of human values is essential in both practical and theoretical
domains. With the development of computational linguistics, the creation of
large-scale datasets has made it possible to automatically recognize human
values accurately. SemEval 2023 Task 4\cite{kiesel:2023} provides a set of
arguments and 20 types of human values that are implicitly expressed in each
argument. In this paper, we present our team's solution. We use the
Roberta\cite{liu_roberta_2019} model to obtain the word vector encoding of the
document and propose a multi-head attention mechanism to establish connections
between specific labels and semantic components. Furthermore, we use a
contrastive learning-enhanced K-nearest neighbor
mechanism\cite{su_contrastive_2022} to leverage existing instance information
for prediction. Our approach achieved an F1 score of 0.533 on the test set and
ranked fourth on the leaderboard.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SuryaKiran at MEDIQA-Sum 2023: Leveraging LoRA for Clinical Dialogue
  Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunal Suri, Prakhar Mishra, Saumajit Saha, Atul Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finetuning Large Language Models helps improve the results for
domain-specific use cases. End-to-end finetuning of large language models is
time and resource intensive and has high storage requirements to store the
finetuned version of the large language model. Parameter Efficient Fine Tuning
(PEFT) methods address the time and resource challenges by keeping the large
language model as a fixed base and add additional layers, which the PEFT
methods finetune. This paper demonstrates the evaluation results for one such
PEFT method Low Rank Adaptation (LoRA), for Clinical Dialogue Summarization.
The evaluation results show that LoRA works at par with end-to-end finetuning
for a large language model. The paper presents the evaluations done for solving
both the Subtask A and B from ImageCLEFmedical
{https://www.imageclef.org/2023/medical}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Grimal, Hervé Le Borgne, Olivier Ferret, Julien Tourille
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The progress in the generation of synthetic images has made it crucial to
assess their quality. While several metrics have been proposed to assess the
rendering of images, it is crucial for Text-to-Image (T2I) models, which
generate images based on a prompt, to consider additional aspects such as to
which extent the generated image matches the important content of the prompt.
Moreover, although the generated images usually result from a random starting
point, the influence of this one is generally not considered. In this article,
we propose a new metric based on prompt templates to study the alignment
between the content specified in the prompt and the corresponding generated
images. It allows us to better characterize the alignment in terms of the type
of the specified objects, their number, and their color. We conducted a study
on several recent T2I models about various aspects. An additional interesting
result we obtained with our approach is that image quality can vary drastically
depending on the latent noise used as a seed for the images. We also quantify
the influence of the number of concepts in the prompt, their order as well as
their (color) attributes. Finally, our method allows us to identify some latent
seeds that produce better images than others, opening novel directions of
research on this understudied topic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Overview</span> of BioASQ 2023: The eleventh BioASQ challenge on Large-Scale
  Biomedical Semantic Indexing and Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasios Nentidis, Georgios Katsimpras, Anastasia Krithara, Salvador Lima López, Eulália Farré-Maduell, Luis Gasco, Martin Krallinger, Georgios Paliouras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This is an overview of the eleventh edition of the BioASQ challenge in the
context of the Conference and Labs of the Evaluation Forum (CLEF) 2023. BioASQ
is a series of international challenges promoting advances in large-scale
biomedical semantic indexing and question answering. This year, BioASQ
consisted of new editions of the two established tasks b and Synergy, and a new
task (MedProcNER) on semantic annotation of clinical content in Spanish with
medical procedures, which have a critical role in medical practice. In this
edition of BioASQ, 28 competing teams submitted the results of more than 150
distinct systems in total for the three different shared tasks of the
challenge. Similarly to previous editions, most of the participating systems
achieved competitive performance, suggesting the continuous advancement of the
state-of-the-art in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 12 tables, 3 figures. CLEF2023. arXiv admin note: text
  overlap with arXiv:2210.06852</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond the Obvious: Evaluating the Reasoning Ability In Real-life
  Scenarios of Language Models on Life Scapes Reasoning
  Benchmark~(LSR-Benchmark) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhouhong Gu, Zihan Li, Lin Zhang, Zhuozhi Xiong, Sihang Jiang, Xiaoxuan Zhu, Shusen Wang, Zili Wang, Jianchen Wang, Haoning Ye, Wenhao Huang, Yikai Zhang, Hongwei Feng, Yanghua Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the Life Scapes Reasoning Benchmark (LSR-Benchmark), a
novel dataset targeting real-life scenario reasoning, aiming to close the gap
in artificial neural networks' ability to reason in everyday contexts. In
contrast to domain knowledge reasoning datasets, LSR-Benchmark comprises
free-text formatted questions with rich information on real-life scenarios,
human behaviors, and character roles. The dataset consists of 2,162 questions
collected from open-source online sources and is manually annotated to improve
its quality. Experiments are conducted using state-of-the-art language models,
such as gpt3.5-turbo and instruction fine-tuned llama models, to test the
performance in LSR-Benchmark. The results reveal that humans outperform these
models significantly, indicating a persisting challenge for machine learning
models in comprehending daily human life.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vacaspati: A Diverse Corpus of Bangla Literature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pramit Bhattacharyya, Joydeep Mondal, Subhadip Maji, Arnab Bhattacharya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bangla (or Bengali) is the fifth most spoken language globally; yet, the
state-of-the-art NLP in Bangla is lagging for even simple tasks such as
lemmatization, POS tagging, etc. This is partly due to lack of a varied quality
corpus. To alleviate this need, we build Vacaspati, a diverse corpus of Bangla
literature. The literary works are collected from various websites; only those
works that are publicly available without copyright violations or restrictions
are collected. We believe that published literature captures the features of a
language much better than newspapers, blogs or social media posts which tend to
follow only a certain literary pattern and, therefore, miss out on language
variety. Our corpus Vacaspati is varied from multiple aspects, including type
of composition, topic, author, time, space, etc. It contains more than 11
million sentences and 115 million words. We also built a word embedding model,
Vac-FT, using FastText from Vacaspati as well as trained an Electra model,
Vac-BERT, using the corpus. Vac-BERT has far fewer parameters and requires only
a fraction of resources compared to other state-of-the-art transformer models
and yet performs either better or similar on various downstream tasks. On
multiple downstream tasks, Vac-FT outperforms other FastText-based models. We
also demonstrate the efficacy of Vacaspati as a corpus by showing that similar
models built from other corpora are not as effective. The models are available
at https://bangla.iitk.ac.in/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OntoChat<span class="highlight-title">GPT</span> Information System: Ontology-Driven Structured <span class="highlight-title">Prompt</span>s for
  Chat<span class="highlight-title">GPT</span> Meta-Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05082v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05082v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oleksandr Palagin, Vladislav Kaverinskiy, Anna Litvin, Kyrylo Malakhov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research presents a comprehensive methodology for utilizing an
ontology-driven structured prompts system in interplay with ChatGPT, a widely
used large language model (LLM). The study develops formal models, both
information and functional, and establishes the methodological foundations for
integrating ontology-driven prompts with ChatGPT's meta-learning capabilities.
The resulting productive triad comprises the methodological foundations,
advanced information technology, and the OntoChatGPT system, which collectively
enhance the effectiveness and performance of chatbot systems. The
implementation of this technology is demonstrated using the Ukrainian language
within the domain of rehabilitation. By applying the proposed methodology, the
OntoChatGPT system effectively extracts entities from contexts, classifies
them, and generates relevant responses. The study highlights the versatility of
the methodology, emphasizing its applicability not only to ChatGPT but also to
other chatbot systems based on LLMs, such as Google's Bard utilizing the PaLM 2
LLM. The underlying principles of meta-learning, structured prompts, and
ontology-driven information retrieval form the core of the proposed
methodology, enabling their adaptation and utilization in various LLM-based
systems. This versatile approach opens up new possibilities for NLP and
dialogue systems, empowering developers to enhance the performance and
functionality of chatbot systems across different domains and languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 1 figure. Published. International Journal of Computing,
  22(2), 170-183. https://doi.org/10.47839/ijc.22.2.3086</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Argumentative Segmentation Enhancement for Legal Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huihui Xu, Kevin Ashley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We use the combination of argumentative zoning [1] and a legal argumentative
scheme to create legal argumentative segments. Based on the argumentative
segmentation, we propose a novel task of classifying argumentative segments of
legal case decisions. GPT-3.5 is used to generate summaries based on
argumentative segments. In terms of automatic evaluation metrics, our method
generates higher quality argumentative summaries while leaving out less
relevant context as compared to GPT-4 and non-GPT models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Understanding In-Context Learning with Contrastive
  Demonstrations and Saliency Maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05052v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05052v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongxia Li, Paiheng Xu, Fuxiao Liu, Hyemi Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the role of various demonstration components in the in-context
learning (ICL) performance of large language models (LLMs). Specifically, we
explore the impacts of ground-truth labels, input distribution, and
complementary explanations, particularly when these are altered or perturbed.
We build on previous work, which offers mixed findings on how these elements
influence ICL. To probe these questions, we employ explainable NLP (XNLP)
methods and utilize saliency maps of contrastive demonstrations for both
qualitative and quantitative analysis. Our findings reveal that flipping
ground-truth labels significantly affects the saliency, though it's more
noticeable in larger LLMs. Our analysis of the input distribution at a granular
level reveals that changing sentiment-indicative terms in a sentiment analysis
task to neutral ones does not have as substantial an impact as altering
ground-truth labels. Finally, we find that the effectiveness of complementary
explanations in boosting ICL performance is task-dependent, with limited
benefits seen in sentiment analysis tasks compared to symbolic reasoning tasks.
These insights are critical for understanding the functionality of LLMs and
guiding the development of effective demonstrations, which is increasingly
relevant in light of the growing use of LLMs in applications such as ChatGPT.
Our research code is publicly available at https://github.com/paihengxu/XICL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Separate-and-Aggregate: A <span class="highlight-title">Transformer</span>-based Patch Refinement Model for
  Knowledge Graph Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Chen, Yufei Wang, Yang Zhang, Quan Z. Sheng, Kwok-Yan Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graph completion (KGC) is the task of inferencing missing facts
from any given knowledge graphs (KG). Previous KGC methods typically represent
knowledge graph entities and relations as trainable continuous embeddings and
fuse the embeddings of the entity $h$ (or $t$) and relation $r$ into hidden
representations of query $(h, r, ?)$ (or $(?, r, t$)) to approximate the
missing entities. To achieve this, they either use shallow linear
transformations or deep convolutional modules. However, the linear
transformations suffer from the expressiveness issue while the deep
convolutional modules introduce unnecessary inductive bias, which could
potentially degrade the model performance. Thus, we propose a novel
Transformer-based Patch Refinement Model (PatReFormer) for KGC. PatReFormer
first segments the embedding into a sequence of patches and then employs
cross-attention modules to allow bi-directional embedding feature interaction
between the entities and relations, leading to a better understanding of the
underlying KG. We conduct experiments on four popular KGC benchmarks, WN18RR,
FB15k-237, YAGO37 and DB100K. The experimental results show significant
performance improvement from existing KGC methods on standard KGC evaluation
metrics, e.g., MRR and H@n. Our analysis first verifies the effectiveness of
our model design choices in PatReFormer. We then find that PatReFormer can
better capture KG information from a large relation embedding dimension.
Finally, we demonstrate that the strength of PatReFormer is at complex relation
types, compared to other KGC models
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ADMA 2023, oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving RNN-Transducers with Acoustic LookAhead 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05006v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05006v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinit S. Unni, Ashish Mittal, Preethi Jyothi, Sunita Sarawagi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  RNN-Transducers (RNN-Ts) have gained widespread acceptance as an end-to-end
model for speech to text conversion because of their high accuracy and
streaming capabilities. A typical RNN-T independently encodes the input audio
and the text context, and combines the two encodings by a thin joint network.
While this architecture provides SOTA streaming accuracy, it also makes the
model vulnerable to strong LM biasing which manifests as multi-step
hallucination of text without acoustic evidence. In this paper we propose
LookAhead that makes text representations more acoustically grounded by looking
ahead into the future within the audio input. This technique yields a
significant 5%-20% relative reduction in word error rate on both in-domain and
out-of-domain evaluation sets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 fig, 7 tables, Proceedings of Interspeech 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Secrets of RLHF in Large Language Models Part I: PPO 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04964v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04964v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Zheng, Shihan Dou, Songyang Gao, Wei Shen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu, Limao Xiong, Lu Chen, Zhiheng Xi, Yuhao Zhou, Nuo Xu, Wenbin Lai, Minghao Zhu, Rongxiang Weng, Wensen Cheng, Cheng Chang, Zhangyue Yin, Yuan Hua, Haoran Huang, Tianxiang Sun, Hang Yan, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have formulated a blueprint for the advancement
of artificial general intelligence. Its primary objective is to function as a
human-centric (helpful, honest, and harmless) assistant. Alignment with humans
assumes paramount significance, and reinforcement learning with human feedback
(RLHF) emerges as the pivotal technological paradigm underpinning this pursuit.
Current technical routes usually include \textbf{reward models} to measure
human preferences, \textbf{Proximal Policy Optimization} (PPO) to optimize
policy model outputs, and \textbf{process supervision} to improve step-by-step
reasoning capabilities. However, due to the challenges of reward design,
environment interaction, and agent training, coupled with huge trial and error
cost of large language models, there is a significant barrier for AI
researchers to motivate the development of technical alignment and safe landing
of LLMs. The stable training of RLHF has still been a puzzle. In the first
report, we dissect the framework of RLHF, re-evaluate the inner workings of
PPO, and explore how the parts comprising PPO algorithms impact policy agent
training. We identify policy constraints being the key factor for the effective
implementation of the PPO algorithm. Therefore, we explore the PPO-max, an
advanced version of PPO algorithm, to efficiently improve the training
stability of the policy model. Based on our main results, we perform a
comprehensive analysis of RLHF abilities compared with SFT models and ChatGPT.
The absence of open-source implementations has posed significant challenges to
the investigation of LLMs alignment. Therefore, we are eager to release
technical reports, reward models and PPO codes
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DyCL: Dynamic Neural Network Compilation Via Program Rewriting and Graph
  Optimization <span class="chip">ISSTA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simin Chen, Shiyi Wei, Cong Liu, Wei Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DL compiler's primary function is to translate DNN programs written in
high-level DL frameworks such as PyTorch and TensorFlow into portable
executables. These executables can then be flexibly executed by the deployed
host programs. However, existing DL compilers rely on a tracing mechanism,
which involves feeding a runtime input to a neural network program and tracing
the program execution paths to generate the computational graph necessary for
compilation. Unfortunately, this mechanism falls short when dealing with modern
dynamic neural networks (DyNNs) that possess varying computational graphs
depending on the inputs. Consequently, conventional DL compilers struggle to
accurately compile DyNNs into executable code. To address this limitation, we
propose \tool, a general approach that enables any existing DL compiler to
successfully compile DyNNs. \tool tackles the dynamic nature of DyNNs by
introducing a compilation mechanism that redistributes the control and data
flow of the original DNN programs during the compilation process. Specifically,
\tool develops program analysis and program transformation techniques to
convert a dynamic neural network into multiple sub-neural networks. Each
sub-neural network is devoid of conditional statements and is compiled
independently. Furthermore, \tool synthesizes a host module that models the
control flow of the DyNNs and facilitates the invocation of the sub-neural
networks. Our evaluation demonstrates the effectiveness of \tool, achieving a
100\% success rate in compiling all dynamic neural networks. Moreover, the
compiled executables generated by \tool exhibit significantly improved
performance, running between $1.12\times$ and $20.21\times$ faster than the
original DyNNs executed on general-purpose DL frameworks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to ISSTA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sparse Modular Activation for Efficient Sequence Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11197v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11197v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liliang Ren, Yang Liu, Shuohang Wang, Yichong Xu, Chenguang Zhu, ChengXiang Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Linear State Space Models (SSMs) have demonstrated strong performance in a
variety of sequence modeling tasks due to their efficient encoding of the
recurrent structure. However, in more comprehensive tasks like language
modeling and machine translation, self-attention-based models still outperform
SSMs. Hybrid models employing both SSM and self-attention generally show
promising performance, but current approaches apply attention modules
statically and uniformly to all elements in the input sequences, leading to
sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse
Modular Activation (SMA), a general mechanism enabling neural networks to
sparsely and dynamically activate sub-modules for sequence elements in a
differentiable manner. Through allowing each element to skip non-activated
sub-modules, SMA reduces computation and memory consumption at both training
and inference stages of sequence modeling. As a specific instantiation of SMA,
we design a novel neural architecture, SeqBoat, which employs SMA to sparsely
activate a Gated Attention Unit (GAU) based on the state representations
learned from an SSM. By constraining the GAU to only conduct local attention on
the activated inputs, SeqBoat can achieve linear inference complexity with
theoretically infinite attention span, and provide substantially better
quality-efficiency trade-off than the chunking-based models. With experiments
on a wide range of tasks, including language modeling, speech classification
and long-range arena, SeqBoat brings new state-of-the-art results among hybrid
models with linear complexity and reveals the amount of attention needed for
each task through the learned sparse activation patterns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In and Out-of-Domain Text Adversarial Robustness via Label Smoothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yahan Yang, Soham Dan, Dan Roth, Insup Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently it has been shown that state-of-the-art NLP models are vulnerable to
adversarial attacks, where the predictions of a model can be drastically
altered by slight modifications to the input (such as synonym substitutions).
While several defense techniques have been proposed, and adapted, to the
discrete nature of text adversarial attacks, the benefits of general-purpose
regularization methods such as label smoothing for language models, have not
been studied. In this paper, we study the adversarial robustness provided by
various label smoothing strategies in foundational models for diverse NLP tasks
in both in-domain and out-of-domain settings. Our experiments show that label
smoothing significantly improves adversarial robustness in pre-trained models
like BERT, against various popular attacks. We also analyze the relationship
between prediction confidence and robustness, showing that label smoothing
reduces over-confident errors on adversarial examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain Specialization as the Key to Make Large Language Models
  Disruptive: A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18703v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18703v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Ling, Xujiang Zhao, Jiaying Lu, Chengyuan Deng, Can Zheng, Junxiang Wang, Tanmoy Chowdhury, Yun Li, Hejie Cui, Xuchao Zhang, Tianjiao Zhao, Amit Panalkar, Wei Cheng, Haoyu Wang, Yanchi Liu, Zhengzhang Chen, Haifeng Chen, Chris White, Quanquan Gu, Jian Pei, Carl Yang, Liang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have significantly advanced the field of natural
language processing (NLP), providing a highly useful, task-agnostic foundation
for a wide range of applications. However, directly applying LLMs to solve
sophisticated problems in specific domains meets many hurdles, caused by the
heterogeneity of domain data, the sophistication of domain knowledge, the
uniqueness of domain objectives, and the diversity of the constraints (e.g.,
various social norms, cultural conformity, religious beliefs, and ethical
standards in the domain applications). Domain specification techniques are key
to make large language models disruptive in many applications. Specifically, to
solve these hurdles, there has been a notable increase in research and
practices conducted in recent years on the domain specialization of LLMs. This
emerging field of study, with its substantial potential for impact,
necessitates a comprehensive and systematic review to better summarize and
guide ongoing work in this area. In this article, we present a comprehensive
survey on domain specification techniques for large language models, an
emerging direction critical for large language model applications. First, we
propose a systematic taxonomy that categorizes the LLM domain-specialization
techniques based on the accessibility to LLMs and summarizes the framework for
all the subcategories as well as their relations and differences to each other.
Second, we present an extensive taxonomy of critical application domains that
can benefit dramatically from specialized LLMs, discussing their practical
significance and open challenges. Last, we offer our insights into the current
research status and future trends in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RL4F: Generating Natural Language Feedback with Reinforcement Learning
  for Repairing Model Outputs <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.08844v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.08844v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Afra Feyza Akyürek, Ekin Akyürek, Aman Madaan, Ashwin Kalyan, Peter Clark, Derry Wijaya, Niket Tandon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their unprecedented success, even the largest language models make
mistakes. Similar to how humans learn and improve using feedback, previous work
proposed providing language models with natural language feedback to guide them
in repairing their outputs. Because human-generated critiques are expensive to
obtain, researchers have devised learned critique generators in lieu of human
critics while assuming one can train downstream models to utilize generated
feedback. However, this approach does not apply to black-box or limited access
models such as ChatGPT, as they cannot be fine-tuned. Moreover, in the era of
large general-purpose language agents, fine-tuning is neither computationally
nor spatially efficient as it results in multiple copies of the network. In
this work, we introduce RL4F (Reinforcement Learning for Feedback), a
multi-agent collaborative framework where the critique generator is trained to
maximize end-task performance of GPT-3, a fixed model more than 200 times its
size. RL4F produces critiques that help GPT-3 revise its outputs. We study
three datasets for action planning, summarization and alphabetization and show
relative improvements up to 10% in multiple text similarity metrics over other
learned, retrieval-augmented or prompting-based critique generators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TOAST: Transfer Learning via Attention Steering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15542v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15542v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baifeng Shi, Siyu Gai, Trevor Darrell, Xin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transfer learning involves adapting a pre-trained model to novel downstream
tasks. However, we observe that current transfer learning methods often fail to
focus on task-relevant features. In this work, we explore refocusing model
attention for transfer learning. We introduce Top-Down Attention Steering
(TOAST), a novel transfer learning algorithm that keeps the pre-trained
backbone frozen, selects task-relevant features in the output, and feeds those
features back to the model to steer the attention to the task-specific
features. By refocusing the attention only, TOAST achieves state-of-the-art
results on a number of transfer learning benchmarks, while having a small
number of tunable parameters. Compared to fully fine-tuning, LoRA, and prompt
tuning, TOAST substantially improves performance across a range of fine-grained
visual classification datasets (e.g., 81.1% -> 86.2% on FGVC). TOAST also
outperforms the fully fine-tuned Alpaca and Vicuna models on
instruction-following language generation. Code is available at
https://github.com/bfshi/TOAST.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/bfshi/TOAST</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LegoNN: Building Modular Encoder-Decoder Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.03318v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.03318v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Dalmia, Dmytro Okhonko, Mike Lewis, Sergey Edunov, Shinji Watanabe, Florian Metze, Luke Zettlemoyer, Abdelrahman Mohamed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art encoder-decoder models (e.g. for machine translation (MT) or
automatic speech recognition (ASR)) are constructed and trained end-to-end as
an atomic unit. No component of the model can be (re-)used without the others,
making it impossible to share parts, e.g. a high resourced decoder, across
tasks. We describe LegoNN, a procedure for building encoder-decoder
architectures in a way so that its parts can be applied to other tasks without
the need for any fine-tuning. To achieve this reusability, the interface
between encoder and decoder modules is grounded to a sequence of marginal
distributions over a pre-defined discrete vocabulary. We present two approaches
for ingesting these marginals; one is differentiable, allowing the flow of
gradients across the entire network, and the other is gradient-isolating. To
enable the portability of decoder modules between MT tasks for different source
languages and across other tasks like ASR, we introduce a modality agnostic
encoder which consists of a length control mechanism to dynamically adapt
encoders' output lengths in order to match the expected input length range of
pre-trained decoders. We present several experiments to demonstrate the
effectiveness of LegoNN models: a trained language generation LegoNN decoder
module from German-English (De-En) MT task can be reused without any
fine-tuning for the Europarl English ASR and the Romanian-English (Ro-En) MT
tasks, matching or beating the performance of baseline. After fine-tuning,
LegoNN models improve the Ro-En MT task by 1.5 BLEU points and achieve 12.5%
relative WER reduction on the Europarl ASR task. To show how the approach
generalizes, we compose a LegoNN ASR model from three modules -- each has been
learned within different end-to-end trained models on three different datasets
-- achieving an overall WER reduction of 19.5%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE/ACM Transactions on Audio, Speech, and Language Processing
  (TASLP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive Error Attribution for Finetuned Language Models <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10722v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10722v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Faisal Ladhak, Esin Durmus, Tatsunori Hashimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has identified noisy and misannotated data as a core cause of
hallucinations and unfaithful outputs in Natural Language Generation (NLG)
tasks. Consequently, identifying and removing these examples is a key open
challenge in creating reliable NLG systems. In this work, we introduce a
framework to identify and remove low-quality training instances that lead to
undesirable outputs, such as faithfulness errors in text summarization. We show
that existing approaches for error tracing, such as gradient-based influence
measures, do not perform reliably for detecting faithfulness errors in NLG
datasets. We overcome the drawbacks of existing error tracing methods through a
new, contrast-based estimate that compares undesired generations to
human-corrected outputs. Our proposed method can achieve a mean average
precision of 0.93 at detecting known data errors across synthetic tasks with
known ground truth, substantially outperforming existing approaches. Using this
approach and re-training models on cleaned data leads to a 70% reduction in
entity hallucinations on the NYT dataset and a 55% reduction in semantic errors
on the E2E dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Faithful Low-Resource Data-to-Text Generation through Cycle Training <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14793v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14793v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoer Wang, Marcus Collins, Nikhita Vedula, Simone Filice, Shervin Malmasi, Oleg Rokhlenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Methods to generate text from structured data have advanced significantly in
recent years, primarily due to fine-tuning of pre-trained language models on
large datasets. However, such models can fail to produce output faithful to the
input data, particularly on out-of-domain data. Sufficient annotated data is
often not available for specific domains, leading us to seek an unsupervised
approach to improve the faithfulness of output text. Since the problem is
fundamentally one of consistency between the representations of the structured
data and text, we evaluate the effectiveness of cycle training in this work.
Cycle training uses two models which are inverses of each other: one that
generates text from structured data, and one which generates the structured
data from natural language text. We show that cycle training, when initialized
with a small amount of supervised data (100 samples in our case), achieves
nearly the same performance as fully supervised approaches for the data-to-text
generation task on the WebNLG, E2E, WTQ, and WSQL datasets. We perform
extensive empirical analysis with automated evaluation metrics and a newly
designed human evaluation schema to reveal different cycle training strategies'
effectiveness of reducing various types of generation errors. Our code is
publicly available at https://github.com/Edillower/CycleNLG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 4 figures, ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">GPT</span>4Graph: Can Large Language Models Understand Graph Structured Data ?
  An Empirical Evaluation and Benchmarking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15066v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15066v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayan Guo, Lun Du, Hengyu Liu, Mengyu Zhou, Xinyi He, Shi Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models~(LLM) like ChatGPT have become indispensable to
artificial general intelligence~(AGI), demonstrating excellent performance in
various natural language processing tasks. In the real world, graph data is
ubiquitous and an essential part of AGI and prevails in domains like social
network analysis, bioinformatics and recommender systems. The training corpus
of large language models often includes some algorithmic components, which
allows them to achieve certain effects on some graph data-related problems.
However, there is still little research on their performance on a broader range
of graph-structured data. In this study, we conduct an extensive investigation
to assess the proficiency of LLMs in comprehending graph data, employing a
diverse range of structural and semantic-related tasks. Our analysis
encompasses 10 distinct tasks that evaluate the LLMs' capabilities in graph
understanding. Through our study, we not only uncover the current limitations
of language models in comprehending graph structures and performing associated
reasoning tasks but also emphasize the necessity for further advancements and
novel approaches to enhance their graph processing capabilities. Our findings
contribute valuable insights towards bridging the gap between language models
and graph understanding, paving the way for more effective graph mining and
knowledge extraction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A data science and machine learning approach to continuous analysis of
  Shakespeare's plays 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06024v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06024v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles Swisher, Lior Shamir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The availability of quantitative text analysis methods has provided new ways
of analyzing literature in a manner that was not available in the
pre-information era. Here we apply comprehensive machine learning analysis to
the work of William Shakespeare. The analysis shows clear changes in the style
of writing over time, with the most significant changes in the sentence length,
frequency of adjectives and adverbs, and the sentiments expressed in the text.
Applying machine learning to make a stylometric prediction of the year of the
play shows a Pearson correlation of 0.71 between the actual and predicted year,
indicating that Shakespeare's writing style as reflected by the quantitative
measurements changed over time. Additionally, it shows that the stylometrics of
some of the plays is more similar to plays written either before or after the
year they were written. For instance, Romeo and Juliet is dated 1596, but is
more similar in stylometrics to plays written by Shakespeare after 1600. The
source code for the analysis is available for free download.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Journal of Data Mining and Digital Humanities, accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are Large Language Models Really Good Logical Reasoners? A Comprehensive
  Evaluation and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, Erik Cambria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Logical reasoning consistently plays a fundamental and significant role in
the domains of knowledge engineering and artificial intelligence. Recently,
Large Language Models (LLMs) have emerged as a noteworthy innovation in natural
language processing (NLP), exhibiting impressive achievements across various
classic NLP tasks. However, the question of whether LLMs can effectively
address the task of logical reasoning, which requires gradual cognitive
inference similar to human intelligence, remains unanswered. To this end, we
aim to bridge this gap and provide comprehensive evaluations in this paper.
Firstly, to offer systematic evaluations, we select fifteen typical logical
reasoning datasets and organize them into deductive, inductive, abductive and
mixed-form reasoning settings. Considering the comprehensiveness of
evaluations, we include three representative LLMs (i.e., text-davinci-003,
ChatGPT and BARD) and evaluate them on all selected datasets under zero-shot,
one-shot and three-shot settings. Secondly, different from previous evaluations
relying only on simple metrics (e.g., accuracy), we propose fine-level
evaluations from objective and subjective manners, covering both answers and
explanations. Additionally, to uncover the logical flaws of LLMs, problematic
cases will be attributed to five error types from two dimensions, i.e.,
evidence selection process and reasoning process. Thirdly, to avoid the
influences of knowledge bias and purely focus on benchmarking the logical
reasoning capability of LLMs, we propose a new dataset with neutral content. It
contains 3,000 samples and covers deductive, inductive and abductive settings.
Based on the in-depth evaluations, this paper finally forms a general
evaluation scheme of logical reasoning capability from six dimensions. It
reflects the pros and cons of LLMs and gives guiding directions for future
works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Does m<span class="highlight-title">BERT</span> understand Romansh? Evaluating word embeddings using word
  alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.08702v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.08702v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eyal Liron Dolev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We test similarity-based word alignment models (SimAlign and awesome-align)
in combination with word embeddings from mBERT and XLM-R on parallel sentences
in German and Romansh. Since Romansh is an unseen language, we are dealing with
a zero-shot setting. Using embeddings from mBERT, both models reach an
alignment error rate of 0.22, which outperforms fast_align, a statistical
model, and is on par with similarity-based word alignment for seen languages.
We interpret these results as evidence that mBERT contains information that can
be meaningful and applicable to Romansh.
  To evaluate performance, we also present a new trilingual corpus, which we
call the DERMIT (DE-RM-IT) corpus, containing press releases made by the Canton
of Grisons in German, Romansh and Italian in the past 25 years. The corpus
contains 4 547 parallel documents and approximately 100 000 sentence pairs in
each language combination. We additionally present a gold standard for
German-Romansh word alignment. The data is available at
https://github.com/eyldlv/DERMIT-Corpus.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discriminative Models Can Still Outperform Generative Models in Aspect
  Based Sentiment Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.02892v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.02892v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhruv Mullick, Alona Fyshe, Bilal Ghanem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aspect-based Sentiment Analysis (ABSA) helps to explain customers' opinions
towards products and services. In the past, ABSA models were discriminative,
but more recently generative models have been used to generate aspects and
polarities directly from text. In contrast, discriminative models commonly
first select aspects from the text, and then classify the aspect's polarity.
Previous results showed that generative models outperform discriminative models
on several English ABSA datasets. Here, we evaluate and contrast two
state-of-the-art discriminative and generative models in several settings:
cross-lingual, cross-domain, and cross-lingual and domain, to understand
generalizability in settings other than English mono-lingual in-domain. Our
more thorough evaluation shows that, contrary to previous studies,
discriminative models can still outperform generative models in almost all
settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submission for work done up till December 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tencent<span class="highlight-title">Pretrain</span>: A Scalable and Flexible Toolkit for <span class="highlight-title">Pre-train</span>ing Models
  of Different Modalities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06385v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06385v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Zhao, Yudong Li, Cheng Hou, Jing Zhao, Rong Tian, Weijie Liu, Yiren Chen, Ningyuan Sun, Haoyan Liu, Weiquan Mao, Han Guo, Weigang Guo, Taiqiang Wu, Tao Zhu, Wenhang Shi, Chen Chen, Shan Huang, Sihong Chen, Liqun Liu, Feifei Li, Xiaoshuai Chen, Xingwu Sun, Zhanhui Kang, Xiaoyong Du, Linlin Shen, Kimmo Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the success of pre-training in text domain has been fully extended
to vision, audio, and cross-modal scenarios. The proposed pre-training models
of different modalities are showing a rising trend of homogeneity in their
model structures, which brings the opportunity to implement different
pre-training models within a uniform framework. In this paper, we present
TencentPretrain, a toolkit supporting pre-training models of different
modalities. The core feature of TencentPretrain is the modular design. The
toolkit uniformly divides pre-training models into 5 components: embedding,
encoder, target embedding, decoder, and target. As almost all of common modules
are provided in each component, users can choose the desired modules from
different components to build a complete pre-training model. The modular design
enables users to efficiently reproduce existing pre-training models or build
brand-new one. We test the toolkit on text, vision, and audio benchmarks and
show that it can match the performance of the original implementations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Algorithms for Acyclic Weighted Finite-State Automata with Failure Arcs <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06862v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06862v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anej Svete, Benjamin Dayan, Tim Vieira, Ryan Cotterell, Jason Eisner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weighted finite-state automata (WSFAs) are commonly used in NLP. Failure
transitions are a useful extension for compactly representing backoffs or
interpolation in $n$-gram models and CRFs, which are special cases of WFSAs.
The pathsum in ordinary acyclic WFSAs is efficiently computed by the backward
algorithm in time $O(|E|)$, where $E$ is the set of transitions. However, this
does not allow failure transitions, and preprocessing the WFSA to eliminate
failure transitions could greatly increase $|E|$. We extend the backward
algorithm to handle failure transitions directly. Our approach is efficient
when the average state has outgoing arcs for only a small fraction $s \ll 1$ of
the alphabet $\Sigma$. We propose an algorithm for general acyclic WFSAs which
runs in $O{\left(|E| + s |\Sigma| |Q| T_\text{max} \log{|\Sigma|}\right)}$,
where $Q$ is the set of states and $T_\text{max}$ is the size of the largest
connected component of failure transitions. When the failure transition
topology satisfies a condition exemplified by CRFs, the $T_\text{max}$ factor
can be dropped, and when the weight semiring is a ring, the $\log{|\Sigma|}$
factor can be dropped. In the latter case (ring-weighted acyclic WFSAs), we
also give an alternative algorithm with complexity $\displaystyle O{\left(|E| +
|\Sigma| |Q| \min(1,s\pi_\text{max}) \right)}$, where $\pi_\text{max}$ is the
size of the longest failure path.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, Proceedings of EMNLP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explanation Regeneration via Information Bottleneck <span class="chip">ACL2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09603v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09603v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qintong Li, Zhiyong Wu, Lingpeng Kong, Wei Bi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explaining the black-box predictions of NLP models naturally and accurately
is an important open problem in natural language generation. These free-text
explanations are expected to contain sufficient and carefully-selected evidence
to form supportive arguments for predictions. Due to the superior generative
capacity of large pretrained language models, recent work built on prompt
engineering enables explanation generation without specific training. However,
explanation generated through single-pass prompting often lacks sufficiency and
conciseness. To address this problem, we develop an information bottleneck
method EIB to produce refined explanations that are sufficient and concise. Our
approach regenerates the free-text explanation by polishing the single-pass
output from the pretrained language model but retaining the information that
supports the contents being explained. Experiments on two out-of-domain tasks
verify the effectiveness of EIB through automatic evaluation and
thoroughly-conducted human evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ACL2023 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-Shot Dense Video Captioning by Jointly Optimizing Text and Moment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02682v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02682v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongrae Jo, Seongyun Lee, Aiden SJ Lee, Hyunji Lee, Hanseok Oh, Minjoon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense video captioning, a task of localizing meaningful moments and
generating relevant captions for videos, often requires a large, expensive
corpus of annotated video segments paired with text. In an effort to minimize
the annotation cost, we propose ZeroTA, a novel method for dense video
captioning in a zero-shot manner. Our method does not require any videos or
annotations for training; instead, it localizes and describes events within
each input video at test time by optimizing solely on the input. This is
accomplished by introducing a soft moment mask that represents a temporal
segment in the video and jointly optimizing it with the prefix parameters of a
language model. This joint optimization aligns a frozen language generation
model (i.e., GPT-2) with a frozen vision-language contrastive model (i.e.,
CLIP) by maximizing the matching score between the generated text and a moment
within the video. We also introduce a pairwise temporal IoU loss to let a set
of soft moment masks capture multiple distinct events within the video. Our
method effectively discovers diverse significant events within the video, with
the resulting captions appropriately describing these events. The empirical
results demonstrate that ZeroTA surpasses zero-shot baselines and even
outperforms the state-of-the-art few-shot method on the widely-used benchmark
ActivityNet Captions. Moreover, our method shows greater robustness compared to
supervised methods when evaluated in out-of-domain scenarios. This research
provides insight into the potential of aligning widely-used models, such as
language generation models and vision-language models, to unlock a new
capability: understanding temporal aspects of videos.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BTPK-based interpretable method for NER tasks based on Talmudic Public
  Announcement Logic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.09523v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.09523v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulin Chen, Beishui Liao, Bruno Bentzen, Bo Yuan, Zelai Yao, Haixiao Chi, Dov Gabbay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As one of the basic tasks in natural language processing (NLP), named entity
recognition (NER) is an important basic tool for downstream tasks of NLP, such
as information extraction, syntactic analysis, machine translation and so on.
The internal operation logic of current name entity recognition model is
black-box to the user, so the user has no basis to determine which name entity
makes more sense. Therefore, a user-friendly explainable recognition process
would be very useful for many people. In this paper, we propose a novel
interpretable method, BTPK (Binary Talmudic Public Announcement Logic model),
to help users understand the internal recognition logic of the name entity
recognition tasks based on Talmudic Public Announcement Logic. BTPK model can
also capture the semantic information in the input sentences, that is, the
context dependency of the sentence. We observed the public announcement of BTPK
presents the inner decision logic of BRNNs, and the explanations obtained from
a BTPK model show us how BRNNs essentially handle NER tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">73</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PIGEON: Predicting Image Geolocations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Haas, Silas Alberti, Michal Skreta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce PIGEON, a multi-task end-to-end system for planet-scale image
geolocalization that achieves state-of-the-art performance on both external
benchmarks and in human evaluation. Our work incorporates semantic geocell
creation with label smoothing, conducts pretraining of a vision transformer on
images with geographic information, and refines location predictions with
ProtoNets across a candidate set of geocells. The contributions of PIGEON are
three-fold: first, we design a semantic geocells creation and splitting
algorithm based on open-source data which can be adapted to any geospatial
dataset. Second, we show the effectiveness of intra-geocell refinement and the
applicability of unsupervised clustering and ProtNets to the task. Finally, we
make our pre-trained CLIP transformer model, StreetCLIP, publicly available for
use in adjacent domains with applications to fighting climate change and urban
and rural scene understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bag of Views: An Appearance-based Approach to Next-Best-View Planning
  for 3D Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sara Hatami Gazani, Matthew Tucsok, Iraj Mantegh, Homayoun Najjaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  UAV-based intelligent data acquisition for 3D reconstruction and monitoring
of infrastructure has been experiencing an increasing surge of interest due to
the recent advancements in image processing and deep learning-based techniques.
View planning is an essential part of this task that dictates the information
capture strategy and heavily impacts the quality of the 3D model generated from
the captured data. Recent methods have used prior knowledge or partial
reconstruction of the target to accomplish view planning for active
reconstruction; the former approach poses a challenge for complex or newly
identified targets while the latter is computationally expensive. In this work,
we present Bag-of-Views (BoV), a fully appearance-based model used to assign
utility to the captured views for both offline dataset refinement and online
next-best-view (NBV) planning applications targeting the task of 3D
reconstruction. With this contribution, we also developed the View Planning
Toolbox (VPT), a lightweight package for training and testing machine
learning-based view planning frameworks, custom view dataset generation of
arbitrary 3D scenes, and 3D reconstruction. Through experiments which pair a
BoV-based reinforcement learning model with VPT, we demonstrate the efficacy of
our model in reducing the number of required views for high-quality
reconstructions in dataset refinement and NBV planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Robotics and Automation Letters (RA-L)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Segmentation and Detection of Lesions in CT Scans Using
  Intensity Distribution Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seung Yeon Shin, Thomas C. Shen, Ronald M. Summers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a method to incorporate the intensity information of a target
lesion on CT scans in training segmentation and detection networks. We first
build an intensity-based lesion probability (ILP) function from an intensity
histogram of the target lesion. It is used to compute the probability of being
the lesion for each voxel based on its intensity. Finally, the computed ILP map
of each input CT scan is provided as additional supervision for network
training, which aims to inform the network about possible lesion locations in
terms of intensity values at no additional labeling cost. The method was
applied to improve the segmentation of three different lesion types, namely,
small bowel carcinoid tumor, kidney tumor, and lung nodule. The effectiveness
of the proposed method on a detection task was also investigated. We observed
improvements of 41.3% -> 47.8%, 74.2% -> 76.0%, and 26.4% -> 32.7% in
segmenting small bowel carcinoid tumor, kidney tumor, and lung nodule,
respectively, in terms of per case Dice scores. An improvement of 64.6% ->
75.5% was achieved in detecting kidney tumors in terms of average precision.
The results of different usages of the ILP map and the effect of varied amount
of training data are also presented.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Computerized Medical Imaging and Graphics 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentiable Forward Projector for X-ray Computed Tomography <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyojin Kim, Kyle Champley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven deep learning has been successfully applied to various computed
tomographic reconstruction problems. The deep inference models may outperform
existing analytical and iterative algorithms, especially in ill-posed CT
reconstruction. However, those methods often predict images that do not agree
with the measured projection data. This paper presents an accurate
differentiable forward and back projection software library to ensure the
consistency between the predicted images and the original measurements. The
software library efficiently supports various projection geometry types while
minimizing the GPU memory footprint requirement, which facilitates seamless
integration with existing deep learning training and inference pipelines. The
proposed software is available as open source: https://github.com/LLNL/LEAP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023 Workshop: Differentiable Almost Everything: Differentiable
  Relaxations, Algorithms, Operators, and Simulators</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Hierarchical <span class="highlight-title">Transformer</span> Encoder to Improve Entire Neoplasm
  Segmentation on Whole Slide Image of Hepatocellular Carcinoma 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuxian Guo, Qitong Wang, Henning Müller, Themis Palpanas, Nicolas Loménie, Camille Kurtz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In digital histopathology, entire neoplasm segmentation on Whole Slide Image
(WSI) of Hepatocellular Carcinoma (HCC) plays an important role, especially as
a preprocessing filter to automatically exclude healthy tissue, in histological
molecular correlations mining and other downstream histopathological tasks. The
segmentation task remains challenging due to HCC's inherent high-heterogeneity
and the lack of dependency learning in large field of view. In this article, we
propose a novel deep learning architecture with a hierarchical Transformer
encoder, HiTrans, to learn the global dependencies within expanded
4096$\times$4096 WSI patches. HiTrans is designed to encode and decode the
patches with larger reception fields and the learned global dependencies,
compared to the state-of-the-art Fully Convolutional Neural networks (FCNN).
Empirical evaluations verified that HiTrans leads to better segmentation
performance by taking into account regional and global dependency information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Medical Image Segmentation based on multi-scale MPU-Net 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeqiu. Yu, Shuo. Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The high cure rate of cancer is inextricably linked to physicians' accuracy
in diagnosis and treatment, therefore a model that can accomplish
high-precision tumor segmentation has become a necessity in many applications
of the medical industry. It can effectively lower the rate of misdiagnosis
while considerably lessening the burden on clinicians. However, fully automated
target organ segmentation is problematic due to the irregular stereo structure
of 3D volume organs. As a basic model for this class of real applications,
U-Net excels. It can learn certain global and local features, but still lacks
the capacity to grasp spatial long-range relationships and contextual
information at multiple scales. This paper proposes a tumor segmentation model
MPU-Net for patient volume CT images, which is inspired by Transformer with a
global attention mechanism. By combining image serialization with the Position
Attention Module, the model attempts to comprehend deeper contextual
dependencies and accomplish precise positioning. Each layer of the decoder is
also equipped with a multi-scale module and a cross-attention mechanism. The
capability of feature extraction and integration at different levels has been
enhanced, and the hybrid loss function developed in this study can better
exploit high-resolution characteristic information. Moreover, the suggested
architecture is tested and evaluated on the Liver Tumor Segmentation Challenge
2017 (LiTS 2017) dataset. Compared with the benchmark model U-Net, MPU-Net
shows excellent segmentation results. The dice, accuracy, precision,
specificity, IOU, and MCC metrics for the best model segmentation results are
92.17%, 99.08%, 91.91%, 99.52%, 85.91%, and 91.74%, respectively. Outstanding
indicators in various aspects illustrate the exceptional performance of this
framework in automatic medical image segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Merging multiple input descriptors and supervisors in a deep neural
  network for tractogram filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Jörgens, Pierre-Marc Jodoin, Maxime Descoteaux, Rodrigo Moreno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the main issues of the current tractography methods is their high
false-positive rate. Tractogram filtering is an option to remove false-positive
streamlines from tractography data in a post-processing step. In this paper, we
train a deep neural network for filtering tractography data in which every
streamline of a tractogram is classified as {\em plausible, implausible}, or
{\em inconclusive}. For this, we use four different tractogram filtering
strategies as supervisors: TractQuerier, RecobundlesX, TractSeg, and an
anatomy-inspired filter. Their outputs are combined to obtain the
classification labels for the streamlines. We assessed the importance of
different types of information along the streamlines for performing this
classification task, including the coordinates of the streamlines, diffusion
data, landmarks, T1-weighted information, and a brain parcellation. We found
that the streamline coordinates are the most relevant followed by the diffusion
data in this particular classification task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EgoAdapt: A multi-stream evaluation study of adaptation to real-world
  egocentric user video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias De Lange, Hamid Eghbalzadeh, Reuben Tan, Michael Iuzzolino, Franziska Meier, Karl Ridgeway
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In egocentric action recognition a single population model is typically
trained and subsequently embodied on a head-mounted device, such as an
augmented reality headset. While this model remains static for new users and
environments, we introduce an adaptive paradigm of two phases, where after
pretraining a population model, the model adapts on-device and online to the
user's experience. This setting is highly challenging due to the change from
population to user domain and the distribution shifts in the user's data
stream. Coping with the latter in-stream distribution shifts is the focus of
continual learning, where progress has been rooted in controlled benchmarks but
challenges faced in real-world applications often remain unaddressed. We
introduce EgoAdapt, a benchmark for real-world egocentric action recognition
that facilitates our two-phased adaptive paradigm, and real-world challenges
naturally occur in the egocentric video streams from Ego4d, such as long-tailed
action distributions and large-scale classification over 2740 actions. We
introduce an evaluation framework that directly exploits the user's data stream
with new metrics to measure the adaptation gain over the population model,
online generalization, and hindsight performance. In contrast to single-stream
evaluation in existing works, our framework proposes a meta-evaluation that
aggregates the results from 50 independent user streams. We provide an
extensive empirical study for finetuning and experience replay.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Artifact Detection in Ultra-widefield Fundus Photography of
  Patients with Sickle Cell Disease 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anqi Feng, Dimitri Johnson, Grace R. Reilly, Loka Thangamathesvaran, Ann Nampomba, Mathias Unberath, Adrienne W. Scott, Craig Jones
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Importance: Ultra-widefield fundus photography (UWF-FP) has shown utility in
sickle cell retinopathy screening; however, image artifact may diminish quality
and gradeability of images. Objective: To create an automated algorithm for
UWF-FP artifact classification. Design: A neural network based automated
artifact detection algorithm was designed to identify commonly encountered
UWF-FP artifacts in a cross section of patient UWF-FP. A pre-trained ResNet-50
neural network was trained on a subset of the images and the classification
accuracy, sensitivity, and specificity were quantified on the hold out test
set. Setting: The study is based on patients from a tertiary care hospital
site. Participants: There were 243 UWF-FP acquired from patients with sickle
cell disease (SCD), and artifact labelling in the following categories was
performed: Eyelash Present, Lower Eyelid Obstructing, Upper Eyelid Obstructing,
Image Too Dark, Dark Artifact, and Image Not Centered. Results: Overall, the
accuracy for each class was Eyelash Present at 83.7%, Lower Eyelid Obstructing
at 83.7%, Upper Eyelid Obstructing at 98.0%, Image Too Dark at 77.6%, Dark
Artifact at 93.9%, and Image Not Centered at 91.8%. Conclusions and Relevance:
This automated algorithm shows promise in identifying common imaging artifacts
on a subset of Optos UWF-FP in SCD patients. Further refinement is ongoing with
the goal of improving efficiency of tele-retinal screening in sickle cell
retinopathy (SCR) by providing a photographer real-time feedback as to the
types of artifacts present, and the need for image re-acquisition. This
algorithm also may have potential future applicability in other retinal
diseases by improving quality and efficiency of image acquisition of UWF-FP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology
  Reporting <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chantal Pellegrini, Matthias Keicher, Ege Özsoy, Nassir Navab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiology reporting is a crucial part of the communication between
radiologists and other medical professionals, but it can be time-consuming and
error-prone. One approach to alleviate this is structured reporting, which
saves time and enables a more accurate evaluation than free-text reports.
However, there is limited research on automating structured reporting, and no
public benchmark is available for evaluating and comparing different methods.
To close this gap, we introduce Rad-ReStruct, a new benchmark dataset that
provides fine-grained, hierarchically ordered annotations in the form of
structured reports for X-Ray images. We model the structured reporting task as
hierarchical visual question answering (VQA) and propose hi-VQA, a novel method
that considers prior context in the form of previously asked questions and
answers for populating a structured radiology report. Our experiments show that
hi-VQA achieves competitive performance to the state-of-the-art on the medical
VQA benchmark VQARad while performing best among methods without
domain-specific vision-language pretraining and provides a strong baseline on
Rad-ReStruct. Our work represents a significant step towards the automated
population of structured radiology reports and provides a valuable first
benchmark for future research in this area. We will make all annotations and
our code for annotation generation, model evaluation, and training publicly
available upon acceptance. Our dataset and code is available at
https://github.com/ChantalMP/Rad-ReStruct.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at MICCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Line Art Colorization of Fakemon using Generative Adversarial Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erick Oliveira Rodrigues, Esteban Clua, Giovani Bernardes Vitor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a complete methodology to colorize images of Fakemon,
anime-style monster-like creatures. In addition, we propose algorithms to
extract the line art from colorized images as well as to extract color hints.
Our work is the first in the literature to use automatic color hint extraction,
to train the networks specifically with anime-styled creatures and to combine
the Pix2Pix and CycleGAN approaches, two different generative adversarial
networks that create a single final result. Visual results of the colorizations
are feasible but there is still room for improvement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>art generation, lineart colorization, image colorization, generative
  adversarial networks, stable diffusion, pokemon, fakemon, digimon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoP-CLIP: A Mixture of <span class="highlight-title">Prompt</span>-Tuned CLIP Models for Domain Incremental
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julien Nicolas, Florent Chiaroni, Imtiaz Ziko, Ola Ahmad, Christian Desrosiers, Jose Dolz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the recent progress in incremental learning, addressing catastrophic
forgetting under distributional drift is still an open and important problem.
Indeed, while state-of-the-art domain incremental learning (DIL) methods
perform satisfactorily within known domains, their performance largely degrades
in the presence of novel domains. This limitation hampers their
generalizability, and restricts their scalability to more realistic settings
where train and test data are drawn from different distributions. To address
these limitations, we present a novel DIL approach based on a mixture of
prompt-tuned CLIP models (MoP-CLIP), which generalizes the paradigm of
S-Prompting to handle both in-distribution and out-of-distribution data at
inference. In particular, at the training stage we model the features
distribution of every class in each domain, learning individual text and visual
prompts to adapt to a given domain. At inference, the learned distributions
allow us to identify whether a given test sample belongs to a known domain,
selecting the correct prompt for the classification task, or from an unseen
domain, leveraging a mixture of the prompt-tuned CLIP models. Our empirical
evaluation reveals the poor performance of existing DIL methods under domain
shift, and suggests that the proposed MoP-CLIP performs competitively in the
standard DIL settings while outperforming state-of-the-art methods in OOD
scenarios. These results demonstrate the superiority of MoP-CLIP, offering a
robust and general solution to the problem of domain incremental learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Causal Ordering Prior for Unsupervised Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avinash Kori, Pedro Sanchez, Konstantinos Vilouras, Ben Glocker, Sotirios A. Tsaftaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised representation learning with variational inference relies
heavily on independence assumptions over latent variables. Causal
representation learning (CRL), however, argues that factors of variation in a
dataset are, in fact, causally related. Allowing latent variables to be
correlated, as a consequence of causal relationships, is more realistic and
generalisable. So far, provably identifiable methods rely on: auxiliary
information, weak labels, and interventional or even counterfactual data.
Inspired by causal discovery with functional causal models, we propose a fully
unsupervised representation learning method that considers a data generation
process with a latent additive noise model (ANM). We encourage the latent space
to follow a causal ordering via loss function based on the Hessian of the
latent distribution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SepHRNet: Generating High-Resolution Crop Maps from Remote Sensing
  imagery using HRNet with Separable Convolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Priyanka Goyal, Sohan Patnaik, Adway Mitra, Manjira Sinha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accurate mapping of crop production is crucial for ensuring food
security, effective resource management, and sustainable agricultural
practices. One way to achieve this is by analyzing high-resolution satellite
imagery. Deep Learning has been successful in analyzing images, including
remote sensing imagery. However, capturing intricate crop patterns is
challenging due to their complexity and variability. In this paper, we propose
a novel Deep learning approach that integrates HRNet with Separable
Convolutional layers to capture spatial patterns and Self-attention to capture
temporal patterns of the data. The HRNet model acts as a backbone and extracts
high-resolution features from crop images. Spatially separable convolution in
the shallow layers of the HRNet model captures intricate crop patterns more
effectively while reducing the computational cost. The multi-head attention
mechanism captures long-term temporal dependencies from the encoded vector
representation of the images. Finally, a CNN decoder generates a crop map from
the aggregated representation. Adaboost is used on top of this to further
improve accuracy. The proposed algorithm achieves a high classification
accuracy of 97.5\% and IoU of 55.2\% in generating crop maps. We evaluate the
performance of our pipeline on the Zuericrop dataset and demonstrate that our
results outperform state-of-the-art models such as U-Net++, ResNet50, VGG19,
InceptionV3, DenseNet, and EfficientNet. This research showcases the potential
of Deep Learning for Earth Observation Systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentiable Blocks World: Qualitative 3D Decomposition by Rendering
  Primitives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Monnier, Jake Austin, Angjoo Kanazawa, Alexei A. Efros, Mathieu Aubry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a set of calibrated images of a scene, we present an approach that
produces a simple, compact, and actionable 3D world representation by means of
3D primitives. While many approaches focus on recovering high-fidelity 3D
scenes, we focus on parsing a scene into mid-level 3D representations made of a
small set of textured primitives. Such representations are interpretable, easy
to manipulate and suited for physics-based simulations. Moreover, unlike
existing primitive decomposition methods that rely on 3D input data, our
approach operates directly on images through differentiable rendering.
Specifically, we model primitives as textured superquadric meshes and optimize
their parameters from scratch with an image rendering loss. We highlight the
importance of modeling transparency for each primitive, which is critical for
optimization and also enables handling varying numbers of primitives. We show
that the resulting textured primitives faithfully reconstruct the input images
and accurately model the visible 3D points, while providing amodal shape
completions of unseen object regions. We compare our approach to the state of
the art on diverse scenes from DTU, and demonstrate its robustness on real-life
captures from BlendedMVS and Nerfstudio. We also showcase how our results can
be used to effortlessly edit a scene or perform physical simulations. Code and
video results are available at https://www.tmonnier.com/DBW .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage with code and videos: https://www.tmonnier.com/DBW</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Objaverse-XL: A Universe of 10M+ 3D Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05663v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05663v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, Ali Farhadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language processing and 2D vision models have attained remarkable
proficiency on many tasks primarily by escalating the scale of training data.
However, 3D vision tasks have not seen the same progress, in part due to the
challenges of acquiring high-quality 3D data. In this work, we present
Objaverse-XL, a dataset of over 10 million 3D objects. Our dataset comprises
deduplicated 3D objects from a diverse set of sources, including manually
designed objects, photogrammetry scans of landmarks and everyday items, and
professional scans of historic and antique artifacts. Representing the largest
scale and diversity in the realm of 3D datasets, Objaverse-XL enables
significant new possibilities for 3D vision. Our experiments demonstrate the
improvements enabled with the scale provided by Objaverse-XL. We show that by
training Zero123 on novel view synthesis, utilizing over 100 million multi-view
rendered images, we achieve strong zero-shot generalization abilities. We hope
that releasing Objaverse-XL will enable further innovations in the field of 3D
vision at scale.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scale Alone Does not Improve Mechanistic Interpretability in Vision
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05471v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05471v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roland S. Zimmermann, Thomas Klein, Wieland Brendel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In light of the recent widespread adoption of AI systems, understanding the
internal information processing of neural networks has become increasingly
critical. Most recently, machine vision has seen remarkable progress by scaling
neural networks to unprecedented levels in dataset and model size. We here ask
whether this extraordinary increase in scale also positively impacts the field
of mechanistic interpretability. In other words, has our understanding of the
inner workings of scaled neural networks improved as well? We here use a
psychophysical paradigm to quantify mechanistic interpretability for a diverse
suite of models and find no scaling effect for interpretability - neither for
model nor dataset size. Specifically, none of the nine investigated
state-of-the-art models are easier to interpret than the GoogLeNet model from
almost a decade ago. Latest-generation vision models appear even less
interpretable than older architectures, hinting at a regression rather than
improvement, with modern models sacrificing interpretability for accuracy.
These results highlight the need for models explicitly designed to be
mechanistically interpretable and the need for more helpful interpretability
methods to increase our understanding of networks at an atomic level. We
release a dataset containing more than 120'000 human responses from our
psychophysical evaluation of 767 units across nine models. This dataset is
meant to facilitate research on automated instead of human-based
interpretability evaluations that can ultimately be leveraged to directly
optimize the mechanistic interpretability of models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally. Code available at
  https://brendel-group.github.io/imi/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EgoVLPv2: Egocentric Video-Language <span class="highlight-title">Pre-train</span>ing with Fusion in the
  Backbone 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05463v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05463v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shraman Pramanick, Yale Song, Sayan Nag, Kevin Qinghong Lin, Hardik Shah, Mike Zheng Shou, Rama Chellappa, Pengchuan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video-language pre-training (VLP) has become increasingly important due to
its ability to generalize to various vision and language tasks. However,
existing egocentric VLP frameworks utilize separate video and language encoders
and learn task-specific cross-modal information only during fine-tuning,
limiting the development of a unified system. In this work, we introduce the
second generation of egocentric video-language pre-training (EgoVLPv2), a
significant improvement from the previous generation, by incorporating
cross-modal fusion directly into the video and language backbones. EgoVLPv2
learns strong video-text representation during pre-training and reuses the
cross-modal attention modules to support different downstream tasks in a
flexible and efficient manner, reducing fine-tuning costs. Moreover, our
proposed fusion in the backbone strategy is more lightweight and
compute-efficient than stacking additional fusion-specific layers. Extensive
experiments on a wide range of VL tasks demonstrate the effectiveness of
EgoVLPv2 by achieving consistent state-of-the-art performance over strong
baselines across all downstream. Our project page can be found at
https://shramanpramanick.github.io/EgoVLPv2/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient 3D Articulated Human Generation with Layered Surface Volumes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05462v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05462v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinghao Xu, Wang Yifan, Alexander W. Bergman, Menglei Chai, Bolei Zhou, Gordon Wetzstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Access to high-quality and diverse 3D articulated digital human assets is
crucial in various applications, ranging from virtual reality to social
platforms. Generative approaches, such as 3D generative adversarial networks
(GANs), are rapidly replacing laborious manual content creation tools. However,
existing 3D GAN frameworks typically rely on scene representations that
leverage either template meshes, which are fast but offer limited quality, or
volumes, which offer high capacity but are slow to render, thereby limiting the
3D fidelity in GAN settings. In this work, we introduce layered surface volumes
(LSVs) as a new 3D object representation for articulated digital humans. LSVs
represent a human body using multiple textured mesh layers around a
conventional template. These layers are rendered using alpha compositing with
fast differentiable rasterization, and they can be interpreted as a volumetric
representation that allocates its capacity to a manifold of finite thickness
around the template. Unlike conventional single-layer templates that struggle
with representing fine off-surface details like hair or accessories, our
surface volumes naturally capture such details. LSVs can be articulated, and
they exhibit exceptional efficiency in GAN settings, where a 2D generator
learns to synthesize the RGBA textures for the individual layers. Trained on
unstructured, single-view 2D image datasets, our LSV-GAN generates high-quality
and view-consistent 3D articulated digital humans without the need for
view-inconsistent 2D upsampling networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://www.computationalimaging.org/publications/lsv/
  Demo: https://www.youtube.com/watch?v=vahgMFCM3j4</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bio-Inspired Night Image Enhancement Based on Contrast Enhancement and
  Denoising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05447v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05447v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Bai, Steffi Agino Priyanka, Hsiao-Jung Tung, Yuankai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the low accuracy of object detection and recognition in many
intelligent surveillance systems at nighttime, the quality of night images is
crucial. Compared with the corresponding daytime image, nighttime image is
characterized as low brightness, low contrast and high noise. In this paper, a
bio-inspired image enhancement algorithm is proposed to convert a low
illuminance image to a brighter and clear one. Different from existing
bio-inspired algorithm, the proposed method doesn't use any training sequences,
we depend on a novel chain of contrast enhancement and denoising algorithms
without using any forms of recursive functions. Our method can largely improve
the brightness and contrast of night images, besides, suppress noise. Then we
implement on real experiment, and simulation experiment to test our algorithms.
Both results show the advantages of proposed algorithm over contrast pair,
Meylan and Retinex.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Cognitive Systems and Signal Processing
  (2016)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D detection of roof sections from a single satellite image and
  application to LOD2-building reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johann Lussange, Mulin Yu, Yuliya Tarabalka, Florent Lafarge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing urban areas in 3D out of satellite raster images has been a
long-standing and challenging goal of both academical and industrial research.
The rare methods today achieving this objective at a Level Of Details $2$ rely
on procedural approaches based on geometry, and need stereo images and/or LIDAR
data as input. We here propose a method for urban 3D reconstruction named
KIBS(\textit{Keypoints Inference By Segmentation}), which comprises two novel
features: i) a full deep learning approach for the 3D detection of the roof
sections, and ii) only one single (non-orthogonal) satellite raster image as
model input. This is achieved in two steps: i) by a Mask R-CNN model performing
a 2D segmentation of the buildings' roof sections, and after blending these
latter segmented pixels within the RGB satellite raster image, ii) by another
identical Mask R-CNN model inferring the heights-to-ground of the roof
sections' corners via panoptic segmentation, unto full 3D reconstruction of the
buildings and city. We demonstrate the potential of the KIBS method by
reconstructing different urban areas in a few minutes, with a Jaccard index for
the 2D segmentation of individual roof sections of $88.55\%$ and $75.21\%$ on
our two data sets resp., and a height's mean error of such correctly segmented
pixels for the 3D reconstruction of $1.60$ m and $2.06$ m on our two data sets
resp., hence within the LOD2 precision range.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Vulnerability of DeepFake Detectors to Attacks Generated by
  Denoising Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marija Ivanovska, Vitomir Štruc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The detection of malicious Deepfakes is a constantly evolving problem, that
requires continuous monitoring of detectors, to ensure they are able to detect
image manipulations generated by the latest emerging models. In this paper, we
present a preliminary study that investigates the vulnerability of single-image
Deepfake detectors to attacks created by a representative of the newest
generation of generative methods, i.e. Denoising Diffusion Models (DDMs). Our
experiments are run on FaceForensics++, a commonly used benchmark dataset,
consisting of Deepfakes generated with various techniques for face swapping and
face reenactment. The analysis shows, that reconstructing existing Deepfakes
with only one denoising diffusion step significantly decreases the accuracy of
all tested detectors, without introducing visually perceptible image changes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Handwritten Text Recognition Using Convolutional Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atman Mishra, A. Sharath Ram, Kavyashree C
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  OCR (Optical Character Recognition) is a technology that offers comprehensive
alphanumeric recognition of handwritten and printed characters at electronic
speed by merely scanning the document. Recently, the understanding of visual
data has been termed Intelligent Character Recognition (ICR). Intelligent
Character Recognition (ICR) is the OCR module that can convert scans of
handwritten or printed characters into ASCII text. ASCII data is the standard
format for data encoding in electronic communication. ASCII assigns standard
numeric values to letters, numeral, symbols, white-spaces and other characters.
In more technical terms, OCR is the process of using an electronic device to
transform 2-Dimensional textual information into machine-encoded text. Anything
that contains text both machine written or handwritten can be scanned either
through a scanner or just simply a picture of the text is enough for the
recognition system to distinguish the text. The goal of this papers is to show
the results of a Convolutional Neural Network model which has been trained on
National Institute of Science and Technology (NIST) dataset containing over a
100,000 images. The network learns from the features extracted from the images
and use it to generate the probability of each class to which the picture
belongs to. We have achieved an accuracy of 90.54% with a loss of 2.53%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-supervised</span> adversarial masking for 3D point cloud representation
  learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michał Szachniewicz, Wojciech Kozłowski, Michał Stypułkowski, Maciej Zięba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised methods have been proven effective for learning deep
representations of 3D point cloud data. Although recent methods in this domain
often rely on random masking of inputs, the results of this approach can be
improved. We introduce PointCAM, a novel adversarial method for learning a
masking function for point clouds. Our model utilizes a self-distillation
framework with an online tokenizer for 3D point clouds. Compared to previous
techniques that optimize patch-level and object-level objectives, we postulate
applying an auxiliary network that learns how to select masks instead of
choosing them randomly. Our results show that the learned masking function
achieves state-of-the-art or competitive performance on various downstream
tasks. The source code is available at https://github.com/szacho/pointcam.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Class Instance Balanced Learning for Long-Tailed Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc-Antoine Lavoie, Steven Waslander
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The long-tailed image classification task remains important in the
development of deep neural networks as it explicitly deals with large
imbalances in the class frequencies of the training data. While uncommon in
engineered datasets, this imbalance is almost always present in real-world
data. Previous approaches have shown that combining cross-entropy and
contrastive learning can improve performance on the long-tailed task, but they
do not explore the tradeoff between head and tail classes. We propose a novel
class instance balanced loss (CIBL), which reweights the relative contributions
of a cross-entropy and a contrastive loss as a function of the frequency of
class instances in the training batch. This balancing favours the contrastive
loss for more common classes, leading to a learned classifier with a more
balanced performance across all class frequencies. Furthermore, increasing the
relative weight on the contrastive head shifts performance from common (head)
to rare (tail) classes, allowing the user to skew the performance towards these
classes if desired. We also show that changing the linear classifier head with
a cosine classifier yields a network that can be trained to similar performance
in substantially fewer epochs. We obtain competitive results on both
CIFAR-100-LT and ImageNet-LT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures, presented at the Conference on Robots and Vision
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Generation of Semantic Parts for Face Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomaso Fontanini, Claudio Ferrari, Massimo Bertozzi, Andrea Prati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic image synthesis (SIS) refers to the problem of generating realistic
imagery given a semantic segmentation mask that defines the spatial layout of
object classes. Most of the approaches in the literature, other than the
quality of the generated images, put effort in finding solutions to increase
the generation diversity in terms of style i.e. texture. However, they all
neglect a different feature, which is the possibility of manipulating the
layout provided by the mask. Currently, the only way to do so is manually by
means of graphical users interfaces. In this paper, we describe a network
architecture to address the problem of automatically manipulating or generating
the shape of object classes in semantic segmentation masks, with specific focus
on human faces. Our proposed model allows embedding the mask class-wise into a
latent space where each class embedding can be independently edited. Then, a
bi-directional LSTM block and a convolutional decoder output a new, locally
manipulated mask. We report quantitative and qualitative results on the
CelebMask-HQ dataset, which show our model can both faithfully reconstruct and
modify a segmentation mask at the class level. Also, we show our model can be
put before a SIS generator, opening the way to a fully automatic generation
control of both shape and texture. Code available at
https://github.com/TFonta/Semantic-VAE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, accepted for publication at ICIAP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Masked Vision and Language <span class="highlight-title">Pre-train</span>ing with Unimodal and Multimodal
  Contrastive Losses for Medical Visual Question Answering <span class="chip">MICCAI2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Li, Gang Liu, Jinlong He, Zixu Zhao, Shenjun Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical visual question answering (VQA) is a challenging task that requires
answering clinical questions of a given medical image, by taking consider of
both visual and language information. However, due to the small scale of
training data for medical VQA, pre-training fine-tuning paradigms have been a
commonly used solution to improve model generalization performance. In this
paper, we present a novel self-supervised approach that learns unimodal and
multimodal feature representations of input images and text using medical image
caption datasets, by leveraging both unimodal and multimodal contrastive
losses, along with masked language modeling and image text matching as
pretraining objectives. The pre-trained model is then transferred to downstream
medical VQA tasks. The proposed approach achieves state-of-the-art (SOTA)
performance on three publicly available medical VQA datasets with significant
accuracy improvements of 2.2%, 14.7%, and 1.7% respectively. Besides, we
conduct a comprehensive analysis to validate the effectiveness of different
components of the approach and study different pre-training settings. Our codes
and models are available at https://github.com/pengfeiliHEU/MUMC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by MICCAI2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unbiased Scene Graph Generation via Two-stage Causal Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuzhou Sun, Shuaifeng Zhi, Qing Liao, Janne Heikkilä, Li Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the impressive performance of recent unbiased Scene Graph Generation
(SGG) methods, the current debiasing literature mainly focuses on the
long-tailed distribution problem, whereas it overlooks another source of bias,
i.e., semantic confusion, which makes the SGG model prone to yield false
predictions for similar relationships. In this paper, we explore a debiasing
procedure for the SGG task leveraging causal inference. Our central insight is
that the Sparse Mechanism Shift (SMS) in causality allows independent
intervention on multiple biases, thereby potentially preserving head category
performance while pursuing the prediction of high-informative tail
relationships. However, the noisy datasets lead to unobserved confounders for
the SGG task, and thus the constructed causal models are always
causal-insufficient to benefit from SMS. To remedy this, we propose Two-stage
Causal Modeling (TsCM) for the SGG task, which takes the long-tailed
distribution and semantic confusion as confounders to the Structural Causal
Model (SCM) and then decouples the causal intervention into two stages. The
first stage is causal representation learning, where we use a novel Population
Loss (P-Loss) to intervene in the semantic confusion confounder. The second
stage introduces the Adaptive Logit Adjustment (AL-Adjustment) to eliminate the
long-tailed distribution confounder to complete causal calibration learning.
These two stages are model agnostic and thus can be used in any SGG model that
seeks unbiased predictions. Comprehensive experiments conducted on the popular
SGG backbones and benchmarks show that our TsCM can achieve state-of-the-art
performance in terms of mean recall rate. Furthermore, TsCM can maintain a
higher recall rate than other debiasing methods, which indicates that our
method can achieve a better tradeoff between head and tail relationships.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 9 figures. Accepted by IEEE Transactions on Pattern
  Analysis and Machine Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ APRF: Anti-Aliasing Projection Representation Field for Inverse Problem
  in Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Chen, Lingxiao Yang, Jianhuang Lai, Xiaohua Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse-view Computed Tomography (SVCT) reconstruction is an ill-posed inverse
problem in imaging that aims to acquire high-quality CT images based on
sparsely-sampled measurements. Recent works use Implicit Neural Representations
(INRs) to build the coordinate-based mapping between sinograms and CT images.
However, these methods have not considered the correlation between adjacent
projection views, resulting in aliasing artifacts on SV sinograms. To address
this issue, we propose a self-supervised SVCT reconstruction method --
Anti-Aliasing Projection Representation Field (APRF), which can build the
continuous representation between adjacent projection views via the spatial
constraints. Specifically, APRF only needs SV sinograms for training, which
first employs a line-segment sampling module to estimate the distribution of
projection views in a local region, and then synthesizes the corresponding
sinogram values using center-based line integral module. After training APRF on
a single SV sinogram itself, it can synthesize the corresponding dense-view
(DV) sinogram with consistent continuity. High-quality CT images can be
obtained by applying re-projection techniques on the predicted DV sinograms.
Extensive experiments on CT images demonstrate that APRF outperforms
state-of-the-art methods, yielding more accurate details and fewer artifacts.
Our code will be publicly available soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenAL: An Efficient Deep Active Learning Framework for Open-Set
  Pathology Image Classification <span class="chip">MICCAI2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linhao Qu, Yingfan Ma, Zhiwei Yang, Manning Wang, Zhijian Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active learning (AL) is an effective approach to select the most informative
samples to label so as to reduce the annotation cost. Existing AL methods
typically work under the closed-set assumption, i.e., all classes existing in
the unlabeled sample pool need to be classified by the target model. However,
in some practical clinical tasks, the unlabeled pool may contain not only the
target classes that need to be fine-grainedly classified, but also non-target
classes that are irrelevant to the clinical tasks. Existing AL methods cannot
work well in this scenario because they tend to select a large number of
non-target samples. In this paper, we formulate this scenario as an open-set AL
problem and propose an efficient framework, OpenAL, to address the challenge of
querying samples from an unlabeled pool with both target class and non-target
class samples. Experiments on fine-grained classification of pathology images
show that OpenAL can significantly improve the query quality of target class
samples and achieve higher performance than current state-of-the-art AL
methods. Code is available at https://github.com/miccaiif/OpenAL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by MICCAI2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evidence-based Hand Hygiene. Can You Trust the Fluorescent-based
  Assessment Methods? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Száva Bánsághi, Viola Sári, Péter Szerémy, Ákos Lehotsky, Bence Takács, Brigitta K. Tóth, Tamás Haidegger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Healthcare-Associated Infections present a major threat to patient safety
globally. According to studies, more than 50% of HAI could be prevented by
proper hand hygiene. Effectiveness of hand hygiene is regularly evaluated with
the fluorescent method: performing hand hygiene with a handrub containing an
ultra violet (UV) fluorescent marker. Typically, human experts evaluate the
hands under UV-A light, and decide whether the applied handrub covered the
whole hand surface. The aim of this study was to investigate how different
experts judge the same UV-pattern, and compare that to microbiology for
objective validation. Hands of volunteer participants were contaminated with
high concentration of a Staphylococcus epidermidis suspension. Hands were
incompletely disinfected with UV-labeled handrub. Four different UV-box type
devices were used to take CCD pictures of the hands under UV light. Size of
inadequately disinfected areas on the hands were determined in two different
ways. First, based on microbiology; the areas where colonies were grown were
measured. Second, four independent senior infection control specialists were
asked to mark the missed areas on printed image, captured under UV light. 8
hands of healthy volunteers were examined. Expert evaluations were highly
uncorrelated (regarding interrater reliability) and inconsistent. Microbiology
results weakly correlated with the expert evaluations. In half of the cases,
there were more than 10% difference in the size of properly disinfected area,
as measured by microbiology versus human experts. Considering the result of the
expert evaluations, variability was disconcertingly high. Evaluating the
fluorescent method is challenging, even for highly experienced professionals. A
patient safety quality assurance system cannot be built on these data quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DRMC: A Generalist Model with Dynamic Routing for Multi-Center PET Image
  Synthesis <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwen Yang, Yang Zhou, Hui Zhang, Bingzheng Wei, Yubo Fan, Yan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-center positron emission tomography (PET) image synthesis aims at
recovering low-dose PET images from multiple different centers. The
generalizability of existing methods can still be suboptimal for a multi-center
study due to domain shifts, which result from non-identical data distribution
among centers with different imaging systems/protocols. While some approaches
address domain shifts by training specialized models for each center, they are
parameter inefficient and do not well exploit the shared knowledge across
centers. To address this, we develop a generalist model that shares
architecture and parameters across centers to utilize the shared knowledge.
However, the generalist model can suffer from the center interference issue,
\textit{i.e.} the gradient directions of different centers can be inconsistent
or even opposite owing to the non-identical data distribution. To mitigate such
interference, we introduce a novel dynamic routing strategy with cross-layer
connections that routes data from different centers to different experts.
Experiments show that our generalist model with dynamic routing (DRMC) exhibits
excellent generalizability across centers. Code and data are available at:
https://github.com/Yaziwel/Multi-Center-PET-Image-Synthesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This article has been early accepted by MICCAI 2023,but has not been
  fully edited. Content may change prior to final publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does <span class="highlight-title">pre-train</span>ing on brain-related tasks results in better
  deep-learning-based brain age biomarkers? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruno Machado Pacheco, Victor Hugo Rocha de Oliveira, Augusto Braga Fernandes Antunes, Saulo Domingos de Souza Pedro, Danilo Silva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain age prediction using neuroimaging data has shown great potential as an
indicator of overall brain health and successful aging, as well as a disease
biomarker. Deep learning models have been established as reliable and efficient
brain age estimators, being trained to predict the chronological age of healthy
subjects. In this paper, we investigate the impact of a pre-training step on
deep learning models for brain age prediction. More precisely, instead of the
common approach of pre-training on natural imaging classification, we propose
pre-training the models on brain-related tasks, which led to state-of-the-art
results in our experiments on ADNI data. Furthermore, we validate the resulting
brain age biomarker on images of patients with mild cognitive impairment and
Alzheimer's disease. Interestingly, our results indicate that better-performing
deep learning models in terms of brain age prediction on healthy patients do
not result in more reliable biomarkers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at BRACIS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative <span class="highlight-title">Pretrain</span>ing in Multimodality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, Xinlong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Emu, a Transformer-based multimodal foundation model, which can
seamlessly generate images and texts in multimodal context. This omnivore model
can take in any single-modality or multimodal data input indiscriminately
(e.g., interleaved image, text and video) through a one-model-for-all
autoregressive training process. First, visual signals are encoded into
embeddings, and together with text tokens form an interleaved input sequence.
Emu is then end-to-end trained with a unified objective of classifying the next
text token or regressing the next visual embedding in the multimodal sequence.
This versatile multimodality empowers the exploration of diverse pretraining
data sources at scale, such as videos with interleaved frames and text,
webpages with interleaved images and text, as well as web-scale image-text
pairs and video-text pairs. Emu can serve as a generalist multimodal interface
for both image-to-text and text-to-image tasks, and supports in-context image
and text generation. Across a broad range of zero-shot/few-shot tasks including
image captioning, visual question answering, video question answering and
text-to-image generation, Emu demonstrates superb performance compared to
state-of-the-art large multimodal models. Extended capabilities such as
multimodal assistants via instruction tuning are also demonstrated with
impressive performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and Demo: https://github.com/baaivision/Emu</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Staged Knowledge Distillation in Video Classification: Harmonizing
  Student Progress by a Complementary Weakly Supervised Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Wang, Zheng Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of label-efficient learning on video data, the distillation
method and the structural design of the teacher-student architecture have a
significant impact on knowledge distillation. However, the relationship between
these factors has been overlooked in previous research. To address this gap, we
propose a new weakly supervised learning framework for knowledge distillation
in video classification that is designed to improve the efficiency and accuracy
of the student model. Our approach leverages the concept of substage-based
learning to distill knowledge based on the combination of student substages and
the correlation of corresponding substages. We also employ the progressive
cascade training method to address the accuracy loss caused by the large
capacity gap between the teacher and the student. Additionally, we propose a
pseudo-label optimization strategy to improve the initial data label. To
optimize the loss functions of different distillation substages during the
training process, we introduce a new loss method based on feature distribution.
We conduct extensive experiments on both real and simulated data sets,
demonstrating that our proposed approach outperforms existing distillation
methods in terms of knowledge distillation for video classification tasks. Our
proposed substage-based distillation approach has the potential to inform
future research on label-efficient learning for video data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Co-Attention Gated Vision-Language Embedding for Visual Question
  Localized-Answering in Robotic Surgery <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Bai, Mobarakol Islam, Hongliang Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical students and junior surgeons often rely on senior surgeons and
specialists to answer their questions when learning surgery. However, experts
are often busy with clinical and academic work, and have little time to give
guidance. Meanwhile, existing deep learning (DL)-based surgical Visual Question
Answering (VQA) systems can only provide simple answers without the location of
the answers. In addition, vision-language (ViL) embedding is still a less
explored research in these kinds of tasks. Therefore, a surgical Visual
Question Localized-Answering (VQLA) system would be helpful for medical
students and junior surgeons to learn and understand from recorded surgical
videos. We propose an end-to-end Transformer with Co-Attention gaTed
Vision-Language (CAT-ViL) for VQLA in surgical scenarios, which does not
require feature extraction through detection models. The CAT-ViL embedding
module is designed to fuse heterogeneous features from visual and textual
sources. The fused embedding will feed a standard Data-Efficient Image
Transformer (DeiT) module, before the parallel classifier and detector for
joint prediction. We conduct the experimental validation on public surgical
videos from MICCAI EndoVis Challenge 2017 and 2018. The experimental results
highlight the superior performance and robustness of our proposed model
compared to the state-of-the-art approaches. Ablation studies further prove the
outstanding performance of all the proposed components. The proposed method
provides a promising solution for surgical scene understanding, and opens up a
primary step in the Artificial Intelligence (AI)-based VQLA system for surgical
training. Our code is publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in MICCAI 2023. Code availability:
  https://github.com/longbai1006/CAT-ViL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ResMatch: Residual Attention Learning for Local Feature Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Deng, Jiayi Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attention-based graph neural networks have made great progress in feature
matching learning. However, insight of how attention mechanism works for
feature matching is lacked in the literature. In this paper, we rethink cross-
and self-attention from the viewpoint of traditional feature matching and
filtering. In order to facilitate the learning of matching and filtering, we
inject the similarity of descriptors and relative positions into cross- and
self-attention score, respectively. In this way, the attention can focus on
learning residual matching and filtering functions with reference to the basic
functions of measuring visual and spatial correlation. Moreover, we mine intra-
and inter-neighbors according to the similarity of descriptors and relative
positions. Then sparse attention for each point can be performed only within
its neighborhoods to acquire higher computation efficiency. Feature matching
networks equipped with our full and sparse residual attention learning
strategies are termed ResMatch and sResMatch respectively. Extensive
experiments, including feature matching, pose estimation and visual
localization, confirm the superiority of our networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Modular Multimodal Architecture for Gaze Target Prediction:
  Application to Privacy-Sensitive Settings <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anshul Gupta, Samy Tafasca, Jean-Marc Odobez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting where a person is looking is a complex task, requiring to
understand not only the person's gaze and scene content, but also the 3D scene
structure and the person's situation (are they manipulating? interacting or
observing others? attentive?) to detect obstructions in the line of sight or
apply attention priors that humans typically have when observing others. In
this paper, we hypothesize that identifying and leveraging such priors can be
better achieved through the exploitation of explicitly derived multimodal cues
such as depth and pose. We thus propose a modular multimodal architecture
allowing to combine these cues using an attention mechanism. The architecture
can naturally be exploited in privacy-sensitive situations such as surveillance
and health, where personally identifiable information cannot be released. We
perform extensive experiments on the GazeFollow and VideoAttentionTarget public
datasets, obtaining state-of-the-art performance and demonstrating very
competitive results in the privacy setting case.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In the proceedings of the GAZE workshop at CVPR 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ExFaceGAN: Exploring Identity Directions in GAN's Learned Latent Space
  for Synthetic Identity Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fadi Boutros, Marcel Klemt, Meiling Fang, Arjan Kuijper, Naser Damer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep generative models have recently presented impressive results in
generating realistic face images of random synthetic identities. To generate
multiple samples of a certain synthetic identity, several previous works
proposed to disentangle the latent space of GANs by incorporating additional
supervision or regularization, enabling the manipulation of certain attributes,
e.g. identity, hairstyle, pose, or expression. Most of these works require
designing special loss functions and training dedicated network architectures.
Others proposed to disentangle specific factors in unconditional pretrained
GANs latent spaces to control their output, which also requires supervision by
attribute classifiers. Moreover, these attributes are entangled in GAN's latent
space, making it difficult to manipulate them without affecting the identity
information. We propose in this work a framework, ExFaceGAN, to disentangle
identity information in state-of-the-art pretrained GANs latent spaces,
enabling the generation of multiple samples of any synthetic identity. The
variations in our generated images are not limited to specific attributes as
ExFaceGAN explicitly aims at disentangling identity information, while other
visual attributes are randomly drawn from a learned GAN latent space. As an
example of the practical benefit of our ExFaceGAN, we empirically prove that
data generated by ExFaceGAN can be successfully used to train face recognition
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IJCB 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Grimal, Hervé Le Borgne, Olivier Ferret, Julien Tourille
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The progress in the generation of synthetic images has made it crucial to
assess their quality. While several metrics have been proposed to assess the
rendering of images, it is crucial for Text-to-Image (T2I) models, which
generate images based on a prompt, to consider additional aspects such as to
which extent the generated image matches the important content of the prompt.
Moreover, although the generated images usually result from a random starting
point, the influence of this one is generally not considered. In this article,
we propose a new metric based on prompt templates to study the alignment
between the content specified in the prompt and the corresponding generated
images. It allows us to better characterize the alignment in terms of the type
of the specified objects, their number, and their color. We conducted a study
on several recent T2I models about various aspects. An additional interesting
result we obtained with our approach is that image quality can vary drastically
depending on the latent noise used as a seed for the images. We also quantify
the influence of the number of concepts in the prompt, their order as well as
their (color) attributes. Finally, our method allows us to identify some latent
seeds that produce better images than others, opening novel directions of
research on this understudied topic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DFR: Depth from Rotation by Uncalibrated Image Rectification with
  Latitudinal Motion Assumption 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongcong Zhang, Yifei Xue, Ming Liao, Huiqing Zhang, Yizhen Lao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the increasing prevalence of rotating-style capture (e.g.,
surveillance cameras), conventional stereo rectification techniques frequently
fail due to the rotation-dominant motion and small baseline between views. In
this paper, we tackle the challenge of performing stereo rectification for
uncalibrated rotating cameras. To that end, we propose Depth-from-Rotation
(DfR), a novel image rectification solution that analytically rectifies two
images with two-point correspondences and serves for further depth estimation.
Specifically, we model the motion of a rotating camera as the camera rotates on
a sphere with fixed latitude. The camera's optical axis lies perpendicular to
the sphere's surface. We call this latitudinal motion assumption. Then we
derive a 2-point analytical solver from directly computing the rectified
transformations on the two images. We also present a self-adaptive strategy to
reduce the geometric distortion after rectification. Extensive synthetic and
real data experiments demonstrate that the proposed method outperforms existing
works in effectiveness and efficiency by a significant margin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One-Shot Learning for Periocular Recognition: Exploring the Effect of
  Domain Adaptation and Data Bias on Deep Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05128v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05128v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Hernandez-Diaz, Fernando Alonso-Fernandez, Josef Bigun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One weakness of machine-learning algorithms is the need to train the models
for a new task. This presents a specific challenge for biometric recognition
due to the dynamic nature of databases and, in some instances, the reliance on
subject collaboration for data collection. In this paper, we investigate the
behavior of deep representations in widely used CNN models under extreme data
scarcity for One-Shot periocular recognition, a biometric recognition task. We
analyze the outputs of CNN layers as identity-representing feature vectors. We
examine the impact of Domain Adaptation on the network layers' output for
unseen data and evaluate the method's robustness concerning data normalization
and generalization of the best-performing layer. We improved state-of-the-art
results that made use of networks trained with biometric datasets with millions
of images and fine-tuned for the target periocular dataset by utilizing
out-of-the-box CNNs trained for the ImageNet Recognition Challenge and standard
computer vision algorithms. For example, for the Cross-Eyed dataset, we could
reduce the EER by 67% and 79% (from 1.70% and 3.41% to 0.56% and 0.71%) in the
Close-World and Open-World protocols, respectively, for the periocular case. We
also demonstrate that traditional algorithms like SIFT can outperform CNNs in
situations with limited data or scenarios where the network has not been
trained with the test classes like the Open-World mode. SIFT alone was able to
reduce the EER by 64% and 71.6% (from 1.7% and 3.41% to 0.6% and 0.97%) for
Cross-Eyed in the Close-World and Open-World protocols, respectively, and a
reduction of 4.6% (from 3.94% to 3.76%) in the PolyU database for the
Open-World and single biometric case.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted preprint to IEE Access</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyperspherical Embedding for Point Cloud Completion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junming Zhang, Haomeng Zhang, Ram Vasudevan, Matthew Johnson-Roberson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most real-world 3D measurements from depth sensors are incomplete, and to
address this issue the point cloud completion task aims to predict the complete
shapes of objects from partial observations. Previous works often adapt an
encoder-decoder architecture, where the encoder is trained to extract
embeddings that are used as inputs to generate predictions from the decoder.
However, the learned embeddings have sparse distribution in the feature space,
which leads to worse generalization results during testing. To address these
problems, this paper proposes a hyperspherical module, which transforms and
normalizes embeddings from the encoder to be on a unit hypersphere. With the
proposed module, the magnitude and direction of the output hyperspherical
embedding are decoupled and only the directional information is optimized. We
theoretically analyze the hyperspherical embedding and show that it enables
more stable training with a wider range of learning rates and more compact
embedding distributions. Experiment results show consistent improvement of
point cloud completion in both single-task and multi-task learning, which
demonstrates the effectiveness of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Offline and Online Optical Flow Enhancement for Deep Video Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanbo Tang, Xihua Sheng, Zhuoyuan Li, Haotian Zhang, Li Li, Dong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video compression relies heavily on exploiting the temporal redundancy
between video frames, which is usually achieved by estimating and using the
motion information. The motion information is represented as optical flows in
most of the existing deep video compression networks. Indeed, these networks
often adopt pre-trained optical flow estimation networks for motion estimation.
The optical flows, however, may be less suitable for video compression due to
the following two factors. First, the optical flow estimation networks were
trained to perform inter-frame prediction as accurately as possible, but the
optical flows themselves may cost too many bits to encode. Second, the optical
flow estimation networks were trained on synthetic data, and may not generalize
well enough to real-world videos. We address the twofold limitations by
enhancing the optical flows in two stages: offline and online. In the offline
stage, we fine-tune a trained optical flow estimation network with the motion
information provided by a traditional (non-deep) video compression scheme, e.g.
H.266/VVC, as we believe the motion information of H.266/VVC achieves a better
rate-distortion trade-off. In the online stage, we further optimize the latent
features of the optical flows with a gradient descent-based algorithm for the
video to be compressed, so as to enhance the adaptivity of the optical flows.
We conduct experiments on a state-of-the-art deep video compression scheme,
DCVC. Experimental results demonstrate that the proposed offline and online
enhancement together achieves on average 12.8% bitrate saving on the tested
videos, without increasing the model or computational complexity of the decoder
side.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAR-NeRF: Neural Radiance Fields for Synthetic Aperture Radar Multi-View
  Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengxin Lei, Feng Xu, Jiangtao Wei, Feng Cai, Feng Wang, Ya-Qiu Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SAR images are highly sensitive to observation configurations, and they
exhibit significant variations across different viewing angles, making it
challenging to represent and learn their anisotropic features. As a result,
deep learning methods often generalize poorly across different view angles.
Inspired by the concept of neural radiance fields (NeRF), this study combines
SAR imaging mechanisms with neural networks to propose a novel NeRF model for
SAR image generation. Following the mapping and projection pinciples, a set of
SAR images is modeled implicitly as a function of attenuation coefficients and
scattering intensities in the 3D imaging space through a differentiable
rendering equation. SAR-NeRF is then constructed to learn the distribution of
attenuation coefficients and scattering intensities of voxels, where the
vectorized form of 3D voxel SAR rendering equation and the sampling
relationship between the 3D space voxels and the 2D view ray grids are
analytically derived. Through quantitative experiments on various datasets, we
thoroughly assess the multi-view representation and generalization capabilities
of SAR-NeRF. Additionally, it is found that SAR-NeRF augumented dataset can
significantly improve SAR target classification performance under few-shot
learning setup, where a 10-type classification accuracy of 91.6\% can be
achieved by using only 12 images per class.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Parametric Depth Based Feature Representation Learning for Object
  Detection and Segmentation in Bird's Eye View 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04106v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04106v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayu Yang, Enze Xie, Miaomiao Liu, Jose M. Alvarez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent vision-only perception models for autonomous driving achieved
promising results by encoding multi-view image features into Bird's-Eye-View
(BEV) space. A critical step and the main bottleneck of these methods is
transforming image features into the BEV coordinate frame. This paper focuses
on leveraging geometry information, such as depth, to model such feature
transformation. Existing works rely on non-parametric depth distribution
modeling leading to significant memory consumption, or ignore the geometry
information to address this problem. In contrast, we propose to use parametric
depth distribution modeling for feature transformation. We first lift the 2D
image features to the 3D space defined for the ego vehicle via a predicted
parametric depth distribution for each pixel in each view. Then, we aggregate
the 3D feature volume based on the 3D space occupancy derived from depth to the
BEV frame. Finally, we use the transformed features for downstream tasks such
as object detection and semantic segmentation. Existing semantic segmentation
methods do also suffer from an hallucination problem as they do not take
visibility information into account. This hallucination can be particularly
problematic for subsequent modules such as control and planning. To mitigate
the issue, our method provides depth uncertainty and reliable visibility-aware
estimations. We further leverage our parametric depth modeling to present a
novel visibility-aware evaluation metric that, when taken into account, can
mitigate the hallucination problem. Extensive experiments on object detection
and semantic segmentation on the nuScenes datasets demonstrate that our method
outperforms existing methods on both tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the dice loss gradient and the ways to mimic it 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.04319v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.04319v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoel Kervadec, Marleen de Bruijne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the past few years, in the context of fully-supervised semantic
segmentation, several losses -- such as cross-entropy and dice -- have emerged
as de facto standards to supervise neural networks. The Dice loss is an
interesting case, as it comes from the relaxation of the popular Dice
coefficient; one of the main evaluation metric in medical imaging applications.
In this paper, we first study theoretically the gradient of the dice loss,
showing that concretely it is a weighted negative of the ground truth, with a
very small dynamic range. This enables us, in the second part of this paper, to
mimic the supervision of the dice loss, through a simple element-wise
multiplication of the network output with a negative of the ground truth. This
rather surprising result sheds light on the practical supervision performed by
the dice loss during gradient descent. This can help the practitioner to
understand and interpret results while guiding researchers when designing new
losses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Algonauts Project 2023 Challenge: How the Human Brain Makes Sense of
  Natural Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.03198v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.03198v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A. T. Gifford, B. Lahner, S. Saba-Sadiya, M. G. Vilas, A. Lascelles, A. Oliva, K. Kay, G. Roig, R. M. Cichy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sciences of biological and artificial intelligence are ever more
intertwined. Neural computational principles inspire new intelligent machines,
which are in turn used to advance theoretical understanding of the brain. To
promote further exchange of ideas and collaboration between biological and
artificial intelligence researchers, we introduce the 2023 installment of the
Algonauts Project challenge: How the Human Brain Makes Sense of Natural Scenes
(http://algonauts.csail.mit.edu). This installment prompts the fields of
artificial and biological intelligence to come together towards building
computational models of the visual brain using the largest and richest dataset
of fMRI responses to visual scenes, the Natural Scenes Dataset (NSD). NSD
provides high-quality fMRI responses to ~73,000 different naturalistic colored
scenes, making it the ideal candidate for data-driven model building approaches
promoted by the 2023 challenge. The challenge is open to all and makes results
directly comparable and transparent through a public leaderboard automatically
updated after each submission, thus allowing for rapid model development. We
believe that the 2023 installment will spark symbiotic collaborations between
biological and artificial intelligence scientists, leading to a deeper
understanding of the brain through cutting-edge computational models and to
novel ways of engineering artificial intelligent agents through inductive
biases from biological systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Development and evaluation of automated localisation and reconstruction
  of all fruits on tomato plants in a greenhouse based on multi-view perception
  and 3D multi-object tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.02760v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.02760v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Rapado Rincon, Eldert J. van Henten, Gert Kootstra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to accurately represent and localise relevant objects is
essential for robots to carry out tasks effectively. Traditional approaches,
where robots simply capture an image, process that image to take an action, and
then forget the information, have proven to struggle in the presence of
occlusions. Methods using multi-view perception, which have the potential to
address some of these problems, require a world model that guides the
collection, integration and extraction of information from multiple viewpoints.
Furthermore, constructing a generic representation that can be applied in
various environments and tasks is a difficult challenge. In this paper, a novel
approach for building generic representations in occluded agro-food
environments using multi-view perception and 3D multi-object tracking is
introduced. The method is based on a detection algorithm that generates partial
point clouds for each detected object, followed by a 3D multi-object tracking
algorithm that updates the representation over time. The accuracy of the
representation was evaluated in a real-world environment, where successful
representation and localisation of tomatoes in tomato plants were achieved,
despite high levels of occlusion, with the total count of tomatoes estimated
with a maximum error of 5.08% and the tomatoes tracked with an accuracy up to
71.47%. Novel tracking metrics were introduced, demonstrating that valuable
insight into the errors in localising and representing the fruits can be
provided by their use. This approach presents a novel solution for building
representations in occluded agro-food environments, demonstrating potential to
enable robots to perform tasks effectively in these challenging environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multiple Instance Learning via Iterative Self-Paced Supervised
  Contrastive Learning <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.09452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.09452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangning Liu, Weicheng Zhu, Yiqiu Shen, Sheng Liu, Narges Razavian, Krzysztof J. Geras, Carlos Fernandez-Granda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning representations for individual instances when only bag-level labels
are available is a fundamental challenge in multiple instance learning (MIL).
Recent works have shown promising results using contrastive self-supervised
learning (CSSL), which learns to push apart representations corresponding to
two different randomly-selected instances. Unfortunately, in real-world
applications such as medical image classification, there is often class
imbalance, so randomly-selected instances mostly belong to the same majority
class, which precludes CSSL from learning inter-class differences. To address
this issue, we propose a novel framework, Iterative Self-paced Supervised
Contrastive Learning for MIL Representations (ItS2CLR), which improves the
learned representation by exploiting instance-level pseudo labels derived from
the bag-level labels. The framework employs a novel self-paced sampling
strategy to ensure the accuracy of pseudo labels. We evaluate ItS2CLR on three
medical datasets, showing that it improves the quality of instance-level pseudo
labels and representations, and outperforms existing MIL methods in terms of
both bag and instance level accuracy. Code is available at
https://github.com/Kangningthu/ItS2CLR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023 camera-ready version. The first two authors contribute
  equally. The last two authors are joint last authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Context-Aware Timewise VAEs for Real-Time Vehicle Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10873v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10873v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pei Xu, Jean-Bernard Hayet, Ioannis Karamouzas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time, accurate prediction of human steering behaviors has wide
applications, from developing intelligent traffic systems to deploying
autonomous driving systems in both real and simulated worlds. In this paper, we
present ContextVAE, a context-aware approach for multi-modal vehicle trajectory
prediction. Built upon the backbone architecture of a timewise variational
autoencoder, ContextVAE observation encoding employs a dual attention mechanism
that accounts for the environmental context and the dynamic agents' states, in
a unified way. By utilizing features extracted from semantic maps during agent
state encoding, our approach takes into account both the social features
exhibited by agents on the scene and the physical environment constraints to
generate map-compliant and socially-aware trajectories. We perform extensive
testing on the nuScenes prediction challenge, Lyft Level 5 dataset and Waymo
Open Motion Dataset to show the effectiveness of our approach and its
state-of-the-art performance. In all tested datasets, ContextVAE models are
fast to train and provide high-quality multi-modal predictions in real-time.
Our code is available at: https://github.com/xupei0610/ContextVAE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Segment Everything Everywhere All at Once 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06718v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06718v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Wang, Lijuan Wang, Jianfeng Gao, Yong Jae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present SEEM, a promptable and interactive model for
segmenting everything everywhere all at once in an image, as shown in Fig.1. In
SEEM, we propose a novel decoding mechanism that enables diverse prompting for
all types of segmentation tasks, aiming at a universal segmentation interface
that behaves like large language models (LLMs). More specifically, SEEM is
designed with four desiderata: i) Versatility. We introduce a new visual prompt
to unify different spatial queries including points, boxes, scribbles and
masks, which can further generalize to a different referring image; ii)
Compositionality. We learn a joint visual-semantic space between text and
visual prompts, which facilitates the dynamic composition of two prompt types
required for various segmentation tasks; iii) Interactivity. We further
incorporate learnable memory prompts into the decoder to retain segmentation
history through mask-guided cross-attention from decoder to image features; and
iv) Semantic-awareness. We use a text encoder to encode text queries and mask
labels into the same semantic space for open-vocabulary segmentation. We
conduct a comprehensive empirical study to validate the effectiveness of SEEM
across diverse segmentation tasks. Notably, our single SEEM model achieves
competitive performance across interactive segmentation, generic segmentation,
referring segmentation, and video object segmentation on 9 datasets with
minimum 1/100 supervision. Furthermore, SEEM showcases a remarkable capacity
for generalization to novel prompts or their combinations, rendering it a
readily universal image segmentation interface.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TOAST: Transfer Learning via Attention Steering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15542v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15542v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baifeng Shi, Siyu Gai, Trevor Darrell, Xin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transfer learning involves adapting a pre-trained model to novel downstream
tasks. However, we observe that current transfer learning methods often fail to
focus on task-relevant features. In this work, we explore refocusing model
attention for transfer learning. We introduce Top-Down Attention Steering
(TOAST), a novel transfer learning algorithm that keeps the pre-trained
backbone frozen, selects task-relevant features in the output, and feeds those
features back to the model to steer the attention to the task-specific
features. By refocusing the attention only, TOAST achieves state-of-the-art
results on a number of transfer learning benchmarks, while having a small
number of tunable parameters. Compared to fully fine-tuning, LoRA, and prompt
tuning, TOAST substantially improves performance across a range of fine-grained
visual classification datasets (e.g., 81.1% -> 86.2% on FGVC). TOAST also
outperforms the fully fine-tuned Alpaca and Vicuna models on
instruction-following language generation. Code is available at
https://github.com/bfshi/TOAST.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/bfshi/TOAST</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CCTV-Gun: Benchmarking Handgun Detection in CCTV Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10703v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10703v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Srikar Yellapragada, Zhenghong Li, Kevin Bhadresh Doshi, Purva Makarand Mhasakar, Heng Fan, Jie Wei, Erik Blasch, Bin Zhang, Haibin Ling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gun violence is a critical security problem, and it is imperative for the
computer vision community to develop effective gun detection algorithms for
real-world scenarios, particularly in Closed Circuit Television (CCTV)
surveillance data. Despite significant progress in visual object detection,
detecting guns in real-world CCTV images remains a challenging and
under-explored task. Firearms, especially handguns, are typically very small in
size, non-salient in appearance, and often severely occluded or
indistinguishable from other small objects. Additionally, the lack of
principled benchmarks and difficulty collecting relevant datasets further
hinder algorithmic development. In this paper, we present a meticulously
crafted and annotated benchmark, called \textbf{CCTV-Gun}, which addresses the
challenges of detecting handguns in real-world CCTV images. Our contribution is
three-fold. Firstly, we carefully select and analyze real-world CCTV images
from three datasets, manually annotate handguns and their holders, and assign
each image with relevant challenge factors such as blur and occlusion.
Secondly, we propose a new cross-dataset evaluation protocol in addition to the
standard intra-dataset protocol, which is vital for gun detection in practical
settings. Finally, we comprehensively evaluate both classical and
state-of-the-art object detection algorithms, providing an in-depth analysis of
their generalizing abilities. The benchmark will facilitate further research
and development on this topic and ultimately enhance security. Code,
annotations, and trained models are available at
https://github.com/srikarym/CCTV-Gun.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What Images are More Memorable to Machines? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.07625v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.07625v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junlin Han, Huangying Zhan, Jie Hong, Pengfei Fang, Hongdong Li, Lars Petersson, Ian Reid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the problem of measuring and predicting how memorable an
image is to pattern recognition machines, as a path to explore machine
intelligence. Firstly, we propose a self-supervised machine memory
quantification pipeline, dubbed ``MachineMem measurer'', to collect machine
memorability scores of images. Similar to humans, machines also tend to
memorize certain kinds of images, whereas the types of images that machines and
humans memorize are different. Through in-depth analysis and comprehensive
visualizations, we gradually unveil that``complex" images are usually more
memorable to machines. We further conduct extensive experiments across 11
different machines (from linear classifiers to modern ViTs) and 9 pre-training
methods to analyze and understand machine memory. This work proposes the
concept of machine memorability and opens a new research direction at the
interface between machine memory and visual data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/JunlinHan/MachineMem Project page:
  https://junlinhan.github.io/projects/machinemem.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spectral Sensitivity Estimation Without a Camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.11549v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.11549v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grigory Solomatov, Derya Akkaynak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A number of problems in computer vision and related fields would be mitigated
if camera spectral sensitivities were known. As consumer cameras are not
designed for high-precision visual tasks, manufacturers do not disclose
spectral sensitivities. Their estimation requires a costly optical setup, which
triggered researchers to come up with numerous indirect methods that aim to
lower cost and complexity by using color targets. However, the use of color
targets gives rise to new complications that make the estimation more
difficult, and consequently, there currently exists no simple, low-cost, robust
go-to method for spectral sensitivity estimation. Furthermore, even if not
limited by hardware or cost, researchers frequently work with imagery from
multiple cameras that they do not have in their possession. To provide a
practical solution to this problem, we propose a framework for spectral
sensitivity estimation that not only does not require any hardware, but also
does not require physical access to the camera itself. Similar to other work,
we formulate an optimization problem that minimizes a two-term objective
function: a camera-specific term from a system of equations, and a universal
term that bounds the solution space. Different than other work, we use publicly
available high-quality calibration data to construct both terms. We use the
colorimetric mapping matrices provided by the Adobe DNG Converter to formulate
the camera-specific system of equations, and constrain the solutions using an
autoencoder trained on a database of ground-truth curves. On average, we
achieve reconstruction errors as low as those that can arise due to
manufacturing imperfections between two copies of the same camera. We provide
our code and predicted sensitivities for 1,000+ cameras, and discuss which
tasks can become trivial when camera responses are available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures, conference: ICCP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting the Trade-off between Accuracy and Robustness via Weight
  Distribution of Filters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03430v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03430v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingxing Wei, Shiji Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks have been proven to be potential threats to Deep Neural
Networks (DNNs), and many methods are proposed to defend against adversarial
attacks. However, while enhancing the robustness, the clean accuracy will
decline to a certain extent, implying a trade-off existed between the accuracy
and robustness. In this paper, we firstly empirically find an obvious
distinction between standard and robust models in the filters' weight
distribution of the same architecture, and then theoretically explain this
phenomenon in terms of the gradient regularization, which shows this difference
is an intrinsic property for DNNs, and thus a static network architecture is
difficult to improve the accuracy and robustness at the same time. Secondly,
based on this observation, we propose a sample-wise dynamic network
architecture named Adversarial Weight-Varied Network (AW-Net), which focuses on
dealing with clean and adversarial examples with a ``divide and rule" weight
strategy. The AW-Net dynamically adjusts network's weights based on regulation
signals generated by an adversarial detector, which is directly influenced by
the input sample. Benefiting from the dynamic network architecture, clean and
adversarial examples can be processed with different network weights, which
provides the potentiality to enhance the accuracy and robustness
simultaneously. A series of experiments demonstrate that our AW-Net is
architecture-friendly to handle both clean and adversarial examples and can
achieve better trade-off performance than state-of-the-art robust models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating the Accuracy-Robustness Trade-off via Multi-Teacher
  Adversarial Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.16170v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.16170v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiji Zhao, Xizhe Wang, Xingxing Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial training is a practical approach for improving the robustness of
deep neural networks against adversarial attacks. Although bringing reliable
robustness, the performance toward clean examples is negatively affected after
adversarial training, which means a trade-off exists between accuracy and
robustness. Recently, some studies have tried to use knowledge distillation
methods in adversarial training, achieving competitive performance in improving
the robustness but the accuracy for clean samples is still limited. In this
paper, to mitigate the accuracy-robustness trade-off, we introduce the
Multi-Teacher Adversarial Robustness Distillation (MTARD) to guide the model's
adversarial training process by applying a strong clean teacher and a strong
robust teacher to handle the clean examples and adversarial examples,
respectively. During the optimization process, to ensure that different
teachers show similar knowledge scales, we design the Entropy-Based Balance
algorithm to adjust the teacher's temperature and keep the teachers'
information entropy consistent. Besides, to ensure that the student has a
relatively consistent learning speed from multiple teachers, we propose the
Normalization Loss Balance algorithm to adjust the learning weights of
different types of knowledge. A series of experiments conducted on public
datasets demonstrate that MTARD outperforms the state-of-the-art adversarial
training and distillation methods against various adversarial attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FAC: 3D Representation Learning via Foreground Aware Feature Contrast <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06388v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06388v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangcheng Liu, Aoran Xiao, Xiaoqin Zhang, Shijian Lu, Ling Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has recently demonstrated great potential for
unsupervised pre-training in 3D scene understanding tasks. However, most
existing work randomly selects point features as anchors while building
contrast, leading to a clear bias toward background points that often dominate
in 3D scenes. Also, object awareness and foreground-to-background
discrimination are neglected, making contrastive learning less effective. To
tackle these issues, we propose a general foreground-aware feature contrast
(FAC) framework to learn more effective point cloud representations in
pre-training. FAC consists of two novel contrast designs to construct more
effective and informative contrast pairs. The first is building positive pairs
within the same foreground segment where points tend to have the same
semantics. The second is that we prevent over-discrimination between 3D
segments/objects and encourage foreground-to-background distinctions at the
segment level with adaptive feature learning in a Siamese correspondence
network, which adaptively learns feature correlations within and across point
cloud views effectively. Visualization with point activation maps shows that
our contrast pairs capture clear correspondences among foreground regions
during pre-training. Quantitative experiments also show that FAC achieves
superior knowledge transfer and data efficiency in various downstream 3D
semantic segmentation and object detection tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, IEEE/CVF Conference on Computer Vision and Pattern
  Recognition 2023 (CVPR 2023), the work is mainly supported by the Natural
  Science Foundation Project of Fujian Province (2020J01826)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Conditioning Spiking Latent Variable Models of the Neural
  Response to Natural Visual Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.12045v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.12045v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gehua Ma, Runhao Jiang, Rui Yan, Huajin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing computational models of neural response is crucial for
understanding sensory processing and neural computations. Current
state-of-the-art neural network methods use temporal filters to handle temporal
dependencies, resulting in an unrealistic and inflexible processing flow.
Meanwhile, these methods target trial-averaged firing rates and fail to capture
important features in spike trains. This work presents the temporal
conditioning spiking latent variable models (TeCoS-LVM) to simulate the neural
response to natural visual stimuli. We use spiking neurons to produce spike
outputs that directly match the recorded trains. This approach helps to avoid
losing information embedded in the original spike trains. We exclude the
temporal dimension from the model parameter space and introduce a temporal
conditioning operation to allow the model to adaptively explore and exploit
temporal dependencies in stimuli sequences in a natural paradigm. We show that
TeCoS-LVM models can produce more realistic spike activities and accurately fit
spike statistics than powerful alternatives. Additionally, learned TeCoS-LVM
models can generalize well to longer time scales. Overall, while remaining
computationally tractable, our model effectively captures key features of
neural coding systems. It thus provides a useful tool for building accurate
predictive computational accounts for various sensory perception circuits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>spiking neural networks, neural coding, visual coding, latent
  variable models, variational information bottleneck, noisy spiking neural
  networks</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distortion-Disentangled Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05066v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05066v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinfeng Wang, Sifan Song, Jionglong Su, S. Kevin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning is well known for its remarkable performance in
representation learning and various downstream computer vision tasks. Recently,
Positive-pair-Only Contrastive Learning (POCL) has achieved reliable
performance without the need to construct positive-negative training sets. It
reduces memory requirements by lessening the dependency on the batch size. The
POCL method typically uses a single loss function to extract the distortion
invariant representation (DIR) which describes the proximity of positive-pair
representations affected by different distortions. This loss function
implicitly enables the model to filter out or ignore the distortion variant
representation (DVR) affected by different distortions. However, existing POCL
methods do not explicitly enforce the disentanglement and exploitation of the
actually valuable DVR. In addition, these POCL methods have been observed to be
sensitive to augmentation strategies. To address these limitations, we propose
a novel POCL framework named Distortion-Disentangled Contrastive Learning
(DDCL) and a Distortion-Disentangled Loss (DDL). Our approach is the first to
explicitly disentangle and exploit the DVR inside the model and feature stream
to improve the overall representation utilization efficiency, robustness and
representation ability. Experiments carried out demonstrate the superiority of
our framework to Barlow Twins and Simsiam in terms of convergence,
representation quality, and robustness on several benchmark datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive Video Question Answering via Video Graph <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13668v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13668v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junbin Xiao, Pan Zhou, Angela Yao, Yicong Li, Richang Hong, Shuicheng Yan, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose to perform video question answering (VideoQA) in a Contrastive
manner via a Video Graph Transformer model (CoVGT). CoVGT's uniqueness and
superiority are three-fold: 1) It proposes a dynamic graph transformer module
which encodes video by explicitly capturing the visual objects, their relations
and dynamics, for complex spatio-temporal reasoning. 2) It designs separate
video and text transformers for contrastive learning between the video and text
to perform QA, instead of multi-modal transformer for answer classification.
Fine-grained video-text communication is done by additional cross-modal
interaction modules. 3) It is optimized by the joint fully- and self-supervised
contrastive objectives between the correct and incorrect answers, as well as
the relevant and irrelevant questions respectively. With superior video
encoding and QA solution, we show that CoVGT can achieve much better
performances than previous arts on video reasoning tasks. Its performances even
surpass those models that are pretrained with millions of external data. We
further show that CoVGT can also benefit from cross-modal pretraining, yet with
orders of magnitude smaller data. The results demonstrate the effectiveness and
superiority of CoVGT, and additionally reveal its potential for more
data-efficient pretraining. We hope our success can advance VideoQA beyond
coarse recognition/description towards fine-grained relation reasoning of video
contents. Our code is available at https://github.com/doc-doc/CoVGT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE T-PAMI'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Novel Pipeline for Diagnosing Acute Lymphoblastic Leukemia Sensitive to
  Related Biomarkers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04014v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04014v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirhossein Askari-Farsangi, Ali Sharifi-Zarchi, Mohammad Hossein Rohban
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acute Lymphoblastic Leukemia (ALL) is one of the most common types of
childhood blood cancer. The quick start of the treatment process is critical to
saving the patient's life, and for this reason, early diagnosis of this disease
is essential. Examining the blood smear images of these patients is one of the
methods used by expert doctors to diagnose this disease. Deep learning-based
methods have numerous applications in medical fields, as they have
significantly advanced in recent years. ALL diagnosis is not an exception in
this field, and several machine learning-based methods for this problem have
been proposed. In previous methods, high diagnostic accuracy was reported, but
our work showed that this alone is not sufficient, as it can lead to models
taking shortcuts and not making meaningful decisions. This issue arises due to
the small size of medical training datasets. To address this, we constrained
our model to follow a pipeline inspired by experts' work. We also demonstrated
that, since a judgement based on only one image is insufficient, redefining the
problem as a multiple-instance learning problem is necessary for achieving a
practical result. Our model is the first to provide a solution to this problem
in a multiple-instance learning setup. We introduced a novel pipeline for
diagnosing ALL that approximates the process used by hematologists, is
sensitive to disease biomarkers, and achieves an accuracy of 96.15%, an
F1-score of 94.24%, a sensitivity of 97.56%, and a specificity of 90.91% on ALL
IDB 1. Our method was further evaluated on an out-of-distribution dataset,
which posed a challenging test and had acceptable performance. Notably, our
model was trained on a relatively small dataset, highlighting the potential for
our approach to be applied to other medical datasets with limited data
availability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhanced Multi-level Features for Very High Resolution Remote Sensing
  Scene Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.00679v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.00679v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chiranjibi Sitaula, Sumesh KC, Jagannath Aryal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Very high-resolution (VHR) remote sensing (RS) scene classification is a
challenging task due to the higher inter-class similarity and intra-class
variability problems. Recently, the existing deep learning (DL)-based methods
have shown great promise in VHR RS scene classification. However, they still
provide an unstable classification performance. To address such a problem, we,
in this letter, propose a novel DL-based approach. For this, we devise an
enhanced VHR attention module (EAM), followed by the atrous spatial pyramid
pooling (ASPP) and global average pooling (GAP). This procedure imparts the
enhanced features from the corresponding level. Then, the multi-level feature
fusion is performed. Experimental results on two widely-used VHR RS datasets
show that the proposed approach yields a competitive and stable/robust
classification performance with the least standard deviation of 0.001. Further,
the highest overall accuracies on the AID and the NWPU datasets are 95.39% and
93.04%, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is under consideration in the International Journal of
  Intelligent Systems (Wiley) journal. Based on the journal's policy and
  restrictions, this version may be updated or deleted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RAPHAEL: Text-to-Image Generation via Large Mixture of Diffusion Paths 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18295v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18295v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu Liu, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image generation has recently witnessed remarkable achievements. We
introduce a text-conditional image diffusion model, termed RAPHAEL, to generate
highly artistic images, which accurately portray the text prompts, encompassing
multiple nouns, adjectives, and verbs. This is achieved by stacking tens of
mixture-of-experts (MoEs) layers, i.e., space-MoE and time-MoE layers, enabling
billions of diffusion paths (routes) from the network input to the output. Each
path intuitively functions as a "painter" for depicting a particular textual
concept onto a specified image region at a diffusion timestep. Comprehensive
experiments reveal that RAPHAEL outperforms recent cutting-edge models, such as
Stable Diffusion, ERNIE-ViLG 2.0, DeepFloyd, and DALL-E 2, in terms of both
image quality and aesthetic appeal. Firstly, RAPHAEL exhibits superior
performance in switching images across diverse styles, such as Japanese comics,
realism, cyberpunk, and ink illustration. Secondly, a single model with three
billion parameters, trained on 1,000 A100 GPUs for two months, achieves a
state-of-the-art zero-shot FID score of 6.61 on the COCO dataset. Furthermore,
RAPHAEL significantly surpasses its counterparts in human evaluation on the
ViLG-300 benchmark. We believe that RAPHAEL holds the potential to propel the
frontiers of image generation research in both academia and industry, paving
the way for future breakthroughs in this rapidly evolving field. More details
can be found on a webpage: https://miaohua.sensetime.com/en.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://miaohua.sensetime.com/en</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DIFF-NST: Diffusion Interleaving For deFormable Neural Style Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04157v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04157v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dan Ruta, Gemma Canet Tarrés, Andrew Gilbert, Eli Shechtman, Nicholas Kolkin, John Collomosse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Style Transfer (NST) is the field of study applying neural techniques
to modify the artistic appearance of a content image to match the style of a
reference style image. Traditionally, NST methods have focused on texture-based
image edits, affecting mostly low level information and keeping most image
structures the same. However, style-based deformation of the content is
desirable for some styles, especially in cases where the style is abstract or
the primary concept of the style is in its deformed rendition of some content.
With the recent introduction of diffusion models, such as Stable Diffusion, we
can access far more powerful image generation techniques, enabling new
possibilities. In our work, we propose using this new class of models to
perform style transfer while enabling deformable style transfer, an elusive
capability in previous models. We show how leveraging the priors of these
models can expose new artistic controls at inference time, and we document our
findings in exploring this new direction for the field of style transfer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RGB-D And Thermal Sensor Fusion: A Systematic Literature <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Brenner, Napoleon H. Reyes, Teo Susnjak, Andre L. C. Barczak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the last decade, the computer vision field has seen significant progress
in multimodal data fusion and learning, where multiple sensors, including
depth, infrared, and visual, are used to capture the environment across diverse
spectral ranges. Despite these advancements, there has been no systematic and
comprehensive evaluation of fusing RGB-D and thermal modalities to date. While
autonomous driving using LiDAR, radar, RGB, and other sensors has garnered
substantial research interest, along with the fusion of RGB and depth
modalities, the integration of thermal cameras and, specifically, the fusion of
RGB-D and thermal data, has received comparatively less attention. This might
be partly due to the limited number of publicly available datasets for such
applications. This paper provides a comprehensive review of both,
state-of-the-art and traditional methods used in fusing RGB-D and thermal
camera data for various applications, such as site inspection, human tracking,
fault detection, and others. The reviewed literature has been categorised into
technical areas, such as 3D reconstruction, segmentation, object detection,
available datasets, and other related topics. Following a brief introduction
and an overview of the methodology, the study delves into calibration and
registration techniques, then examines thermal visualisation and 3D
reconstruction, before discussing the application of classic feature-based
techniques as well as modern deep learning approaches. The paper concludes with
a discourse on current limitations and potential future research directions. It
is hoped that this survey will serve as a valuable reference for researchers
looking to familiarise themselves with the latest advancements and contribute
to the RGB-DT research field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34 pages, 21 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Inter-observer consistent deep adversarial training for visual
  scanpath prediction <span class="chip">ICIP2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.07336v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.07336v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Amine Kerkouri, Marouane Tliba, Aladine Chetouani, Alessandro Bruno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The visual scanpath is a sequence of points through which the human gaze
moves while exploring a scene. It represents the fundamental concepts upon
which visual attention research is based. As a result, the ability to predict
them has emerged as an important task in recent years. In this paper, we
propose an inter-observer consistent adversarial training approach for scanpath
prediction through a lightweight deep neural network. The adversarial method
employs a discriminative neural network as a dynamic loss that is better suited
to model the natural stochastic phenomenon while maintaining consistency
between the distributions related to the subjective nature of scanpaths
traversed by different observers. Through extensive testing, we show the
competitiveness of our approach in regard to state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICIP2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tamed Warping Network for High-Resolution Semantic Video Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2005.01344v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2005.01344v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songyuan Li, Junyi Feng, Xi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent approaches for fast semantic video segmentation have reduced
redundancy by warping feature maps across adjacent frames, greatly speeding up
the inference phase. However, the accuracy drops seriously owing to the errors
incurred by warping. In this paper, we propose a novel framework and design a
simple and effective correction stage after warping. Specifically, we build a
non-key-frame CNN, fusing warped context features with current spatial details.
Based on the feature fusion, our Context Feature Rectification~(CFR) module
learns the model's difference from a per-frame model to correct the warped
features. Furthermore, our Residual-Guided Attention~(RGA) module utilizes the
residual maps in the compressed domain to help CRF focus on error-prone
regions. Results on Cityscapes show that the accuracy significantly increases
from $67.3\%$ to $71.6\%$, and the speed edges down from $65.5$ FPS to $61.8$
FPS at a resolution of $1024\times 2048$. For non-rigid categories, e.g.,
``human'' and ``object'', the improvements are even higher than 18 percentage
points.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ResNeRF: Geometry-Guided Residual Neural Radiance Field for Indoor Scene
  Novel View Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.16211v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.16211v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuting Xiao, Yiqun Zhao, Yanyu Xu, Shenghua Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We represent the ResNeRF, a novel geometry-guided two-stage framework for
indoor scene novel view synthesis. Be aware of that a good geometry would
greatly boost the performance of novel view synthesis, and to avoid the
geometry ambiguity issue, we propose to characterize the density distribution
of the scene based on a base density estimated from scene geometry and a
residual density parameterized by the geometry. In the first stage, we focus on
geometry reconstruction based on SDF representation, which would lead to a good
geometry surface of the scene and also a sharp density. In the second stage,
the residual density is learned based on the SDF learned in the first stage for
encoding more details about the appearance. In this way, our method can better
learn the density distribution with the geometry prior for high-fidelity novel
view synthesis while preserving the 3D structures. Experiments on large-scale
indoor scenes with many less-observed and textureless areas show that with the
good 3D surface, our method achieves state-of-the-art performance for novel
view synthesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an incomplete paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Single-Model Attribution of Generative Models Through Final-Layer
  Inversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06210v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06210v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mike Laszkiewicz, Jonas Ricker, Johannes Lederer, Asja Fischer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent groundbreaking developments on generative modeling have sparked
interest in practical single-model attribution. Such methods predict whether a
sample was generated by a specific generator or not, for instance, to prove
intellectual property theft. However, previous works are either limited to the
closed-world setting or require undesirable changes of the generative model. We
address these shortcomings by proposing FLIPAD, a new approach for single-model
attribution in the open-world setting based on final-layer inversion and
anomaly detection. We show that the utilized final-layer inversion can be
reduced to a convex lasso optimization problem, making our approach
theoretically sound and computationally efficient. The theoretical findings are
accompanied by an experimental study demonstrating the effectiveness of our
approach, outperforming the existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CT-based Subchondral Bone Microstructural Analysis in Knee
  Osteoarthritis via MR-Guided Distillation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04390v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04390v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Hu, Xiangyu Zhao, Gaowei Qing, Kai Xie, Chenglei Liu, Lichi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: MR-based subchondral bone effectively predicts knee
osteoarthritis. However, its clinical application is limited by the cost and
time of MR. Purpose: We aim to develop a novel distillation-learning-based
method named SRRD for subchondral bone microstructural analysis using
easily-acquired CT images, which leverages paired MR images to enhance the
CT-based analysis model during training. Materials and Methods: Knee joint
images of both CT and MR modalities were collected from October 2020 to May
2021. Firstly, we developed a GAN-based generative model to transform MR images
into CT images, which was used to establish the anatomical correspondence
between the two modalities. Next, we obtained numerous patches of subchondral
bone regions of MR images, together with their trabecular parameters (BV / TV,
Tb. Th, Tb. Sp, Tb. N) from the corresponding CT image patches via regression.
The distillation-learning technique was used to train the regression model and
transfer MR structural information to the CT-based model. The regressed
trabecular parameters were further used for knee osteoarthritis classification.
Results: A total of 80 participants were evaluated. CT-based regression results
of trabecular parameters achieved intra-class correlation coefficients (ICCs)
of 0.804, 0.773, 0.711, and 0.622 for BV / TV, Tb. Th, Tb. Sp, and Tb. N,
respectively. The use of distillation learning significantly improved the
performance of the CT-based knee osteoarthritis classification method using the
CNN approach, yielding an AUC score of 0.767 (95% CI, 0.681-0.853) instead of
0.658 (95% CI, 0.574-0.742) (p<.001). Conclusions: The proposed SRRD method
showed high reliability and validity in MR-CT registration, regression, and
knee osteoarthritis classification, indicating the feasibility of subchondral
bone microstructural analysis based on CT images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Structured Semantic Prior for Multi Label Recognition with
  Incomplete Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13223v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13223v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Ding, Ao Wang, Hui Chen, Qiang Zhang, Pengzhang Liu, Yongjun Bao, Weipeng Yan, Jungong Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-label recognition (MLR) with incomplete labels is very challenging.
Recent works strive to explore the image-to-label correspondence in the
vision-language model, \ie, CLIP, to compensate for insufficient annotations.
In spite of promising performance, they generally overlook the valuable prior
about the label-to-label correspondence. In this paper, we advocate remedying
the deficiency of label supervision for the MLR with incomplete labels by
deriving a structured semantic prior about the label-to-label correspondence
via a semantic prior prompter. We then present a novel Semantic Correspondence
Prompt Network (SCPNet), which can thoroughly explore the structured semantic
prior. A Prior-Enhanced Self-Supervised Learning method is further introduced
to enhance the use of the prior. Comprehensive experiments and analyses on
several widely used benchmark datasets show that our method significantly
outperforms existing methods on all datasets, well demonstrating the
effectiveness and the superiority of our method. Our code will be available at
https://github.com/jameslahm/SCPNet.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Relational Extraction on Wikipedia Tables using Convolutional and Memory
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arif Shahriar, Rohan Saha, Denilson Barbosa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation extraction (RE) is the task of extracting relations between entities
in text. Most RE methods extract relations from free-form running text and
leave out other rich data sources, such as tables. We explore RE from the
perspective of applying neural methods on tabularly organized data. We
introduce a new model consisting of Convolutional Neural Network (CNN) and
Bidirectional-Long Short Term Memory (BiLSTM) network to encode entities and
learn dependencies among them, respectively. We evaluate our model on a large
and recent dataset and compare results with previous neural methods.
Experimental results show that our model consistently outperforms the previous
model for the task of relation extraction on tabular data. We perform
comprehensive error analyses and ablation study to show the contribution of
various components of our model. Finally, we discuss the usefulness and
trade-offs of our approach, and provide suggestions for fostering further
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Duncode Characters Shorter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changshang Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the employment of various encoders in text
transformation, converting characters into bytes. It discusses local encoders
such as ASCII and GB-2312, which encode specific characters into shorter bytes,
and universal encoders like UTF-8 and UTF-16, which can encode the complete
Unicode set with greater space requirements and are gaining widespread
acceptance. Other encoders, including SCSU, BOCU-1, and binary encoders,
however, lack self-synchronizing capabilities. Duncode is introduced as an
innovative encoding method that aims to encode the entire Unicode character set
with high space efficiency, akin to local encoders. It has the potential to
compress multiple characters of a string into a Duncode unit using fewer bytes.
Despite offering less self-synchronizing identification information, Duncode
surpasses UTF8 in terms of space efficiency. The application is available at
\url{https://github.com/laohur/duncode}. Additionally, we have developed a
benchmark for evaluating character encoders across different languages. It
encompasses 179 languages and can be accessed at
\url{https://github.com/laohur/wiki2txt}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal Graphs Anomaly Emergence Detection: Benchmarking For Social
  Media Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teddy Lazebnik, Or Iny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal graphs have become an essential tool for analyzing complex dynamic
systems with multiple agents. Detecting anomalies in temporal graphs is crucial
for various applications, including identifying emerging trends, monitoring
network security, understanding social dynamics, tracking disease outbreaks,
and understanding financial dynamics. In this paper, we present a comprehensive
benchmarking study that compares 12 data-driven methods for anomaly detection
in temporal graphs. We conduct experiments on two temporal graphs extracted
from Twitter and Facebook, aiming to identify anomalies in group interactions.
Surprisingly, our study reveals an unclear pattern regarding the best method
for such tasks, highlighting the complexity and challenges involved in anomaly
emergence detection in large and dynamic systems. The results underscore the
need for further research and innovative approaches to effectively detect
emerging anomalies in dynamic systems represented as temporal graphs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ U-CREAT: Unsupervised Case Retrieval using Events extrAcTion <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Joshi, Akshat Sharma, Sai Kiran Tanikella, Ashutosh Modi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of Prior Case Retrieval (PCR) in the legal domain is about
automatically citing relevant (based on facts and precedence) prior legal cases
in a given query case. To further promote research in PCR, in this paper, we
propose a new large benchmark (in English) for the PCR task: IL-PCR (Indian
Legal Prior Case Retrieval) corpus. Given the complex nature of case relevance
and the long size of legal documents, BM25 remains a strong baseline for
ranking the cited prior documents. In this work, we explore the role of events
in legal case retrieval and propose an unsupervised retrieval method-based
pipeline U-CREAT (Unsupervised Case Retrieval using Events Extraction). We find
that the proposed unsupervised retrieval method significantly increases
performance compared to BM25 and makes retrieval faster by a considerable
margin, making it applicable to real-time case retrieval systems. Our proposed
system is generic, we show that it generalizes across two different legal
systems (Indian and Canadian), and it shows state-of-the-art performance on the
benchmarks for both the legal systems (IL-PCR and COLIEE corpora).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2023, 15 pages (12 main + 3 Appendix)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Contrastive Graph Learning for Recommendation <span class="chip">SIGIR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05100v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05100v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghui Yang, Zhengwei Wu, Le Wu, Kun Zhang, Richang Hong, Zhiqiang Zhang, Jun Zhou, Meng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By treating users' interactions as a user-item graph, graph learning models
have been widely deployed in Collaborative Filtering(CF) based recommendation.
Recently, researchers have introduced Graph Contrastive Learning(GCL)
techniques into CF to alleviate the sparse supervision issue, which first
constructs contrastive views by data augmentations and then provides
self-supervised signals by maximizing the mutual information between
contrastive views. Despite the effectiveness, we argue that current GCL-based
recommendation models are still limited as current data augmentation
techniques, either structure augmentation or feature augmentation. First,
structure augmentation randomly dropout nodes or edges, which is easy to
destroy the intrinsic nature of the user-item graph. Second, feature
augmentation imposes the same scale noise augmentation on each node, which
neglects the unique characteristics of nodes on the graph. To tackle the above
limitations, we propose a novel Variational Graph Generative-Contrastive
Learning(VGCL) framework for recommendation. Specifically, we leverage
variational graph reconstruction to estimate a Gaussian distribution of each
node, then generate multiple contrastive views through multiple samplings from
the estimated distributions, which builds a bridge between generative and
contrastive learning. Besides, the estimated variances are tailored to each
node, which regulates the scale of contrastive loss for each node on
optimization. Considering the similarity of the estimated distributions, we
propose a cluster-aware twofold contrastive learning, a node-level to encourage
consistency of a node's contrastive views and a cluster-level to encourage
consistency of nodes in a cluster. Finally, extensive experimental results on
three public datasets clearly demonstrate the effectiveness of the proposed
model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted to SIGIR 2023. Code is avaliable:
  https://github.com/yimutianyang/SIGIR23-VGCL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retrieval-augmented <span class="highlight-title">GPT</span>-3.5-based Text-to-SQL Framework with
  Sample-aware <span class="highlight-title">Prompt</span>ing and Dynamic Revision Chain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunxi Guo, Zhiliang Tian, Jintao Tang, Shasha Li, Zhihua Wen, Kaixuan Wang, Ting Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-SQL aims at generating SQL queries for the given natural language
questions and thus helping users to query databases. Prompt learning with large
language models (LLMs) has emerged as a recent approach, which designs prompts
to lead LLMs to understand the input question and generate the corresponding
SQL. However, it faces challenges with strict SQL syntax requirements. Existing
work prompts the LLMs with a list of demonstration examples (i.e. question-SQL
pairs) to generate SQL, but the fixed prompts can hardly handle the scenario
where the semantic gap between the retrieved demonstration and the input
question is large. In this paper, we propose a retrieval-augmented prompting
method for a LLM-based Text-to-SQL framework, involving sample-aware prompting
and a dynamic revision chain. Our approach incorporates sample-aware
demonstrations, which include the composition of SQL operators and fine-grained
information related to the given question. To retrieve questions sharing
similar intents with input questions, we propose two strategies for assisting
retrieval. Firstly, we leverage LLMs to simplify the original questions,
unifying the syntax and thereby clarifying the users' intentions. To generate
executable and accurate SQLs without human intervention, we design a dynamic
revision chain which iteratively adapts fine-grained feedback from the
previously generated SQL. Experimental results on three Text-to-SQL benchmarks
demonstrate the superiority of our method over strong baseline models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mining for Unknown Unknowns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bernard Sinclair-Desgagné
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unknown unknowns are future relevant contingencies that lack an ex ante
description. While there are numerous retrospective accounts showing that
significant gains or losses might have been achieved or avoided had such
contingencies been previously uncovered, getting hold of unknown unknowns still
remains elusive, both in practice and conceptually. Using Formal Concept
Analysis (FCA) - a subfield of lattice theory which is increasingly applied for
mining and organizing data - this paper introduces a simple framework to
systematically think out of the box and direct the search for unknown unknowns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proceedings TARK 2023, arXiv:2307.04005</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural-Symbolic Recommendation with Graph-Enhanced Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05036v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05036v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bang Chen, Wei Peng, Maonian Wu, Bo Zheng, Shaojun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recommendation system is not only a problem of inductive statistics from
data but also a cognitive task that requires reasoning ability. The most
advanced graph neural networks have been widely used in recommendation systems
because they can capture implicit structured information from graph-structured
data. However, like most neural network algorithms, they only learn matching
patterns from a perception perspective. Some researchers use user behavior for
logic reasoning to achieve recommendation prediction from the perspective of
cognitive reasoning, but this kind of reasoning is a local one and ignores
implicit information on a global scale. In this work, we combine the advantages
of graph neural networks and propositional logic operations to construct a
neuro-symbolic recommendation model with both global implicit reasoning ability
and local explicit logic reasoning ability. We first build an item-item graph
based on the principle of adjacent interaction and use graph neural networks to
capture implicit information in global data. Then we transform user behavior
into propositional logic expressions to achieve recommendations from the
perspective of cognitive reasoning. Extensive experiments on five public
datasets show that our proposed model outperforms several state-of-the-art
methods, source code is avaliable at [https://github.com/hanzo2020/GNNLR].
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Empowering recommender systems using automatically generated Knowledge
  Graphs and Reinforcement Learning <span class="chip">KDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ghanshyam Verma, Shovon Sengupta, Simon Simanta, Huan Chen, Janos A. Perge, Devishree Pillai, John P. McCrae, Paul Buitelaar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized recommendations have a growing importance in direct marketing,
which motivates research to enhance customer experiences by knowledge graph
(KG) applications. For example, in financial services, companies may benefit
from providing relevant financial articles to their customers to cultivate
relationships, foster client engagement and promote informed financial
decisions. While several approaches center on KG-based recommender systems for
improved content, in this study we focus on interpretable KG-based recommender
systems for decision making.To this end, we present two knowledge graph-based
approaches for personalized article recommendations for a set of customers of a
large multinational financial services company. The first approach employs
Reinforcement Learning and the second approach uses the XGBoost algorithm for
recommending articles to the customers. Both approaches make use of a KG
generated from both structured (tabular data) and unstructured data (a large
body of text data).Using the Reinforcement Learning-based recommender system we
could leverage the graph traversal path leading to the recommendation as a way
to generate interpretations (Path Directed Reasoning (PDR)). In the
XGBoost-based approach, one can also provide explainable results using post-hoc
methods such as SHAP (SHapley Additive exPlanations) and ELI5 (Explain Like I
am Five).Importantly, our approach offers explainable results, promoting better
decision-making. This study underscores the potential of combining advanced
machine learning techniques with KG-driven insights to bolster experience in
customer relationship management.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at KDD (OARS) 2023 [https://oars-workshop.github.io/]</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One-Shot Labeling for Automatic Relevance Estimation <span class="chip">SIGIR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.11266v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.11266v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sean MacAvaney, Luca Soldaini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dealing with unjudged documents ("holes") in relevance assessments is a
perennial problem when evaluating search systems with offline experiments.
Holes can reduce the apparent effectiveness of retrieval systems during
evaluation and introduce biases in models trained with incomplete data. In this
work, we explore whether large language models can help us fill such holes to
improve offline evaluations. We examine an extreme, albeit common, evaluation
setting wherein only a single known relevant document per query is available
for evaluation. We then explore various approaches for predicting the relevance
of unjudged documents with respect to a query and the known relevant document,
including nearest neighbor, supervised, and prompting techniques. We find that
although the predictions of these One-Shot Labelers (1SL) frequently disagree
with human assessments, the labels they produce yield a far more reliable
ranking of systems than the single labels do alone. Specifically, the strongest
approaches can consistently reach system ranking correlations of over 0.86 with
the full rankings over a variety of measures. Meanwhile, the approach
substantially increases the reliability of t-tests due to filling holes in
relevance assessments, giving researchers more confidence in results they find
to be significant. Alongside this work, we release an easy-to-use software
package to enable the use of 1SL for evaluation of other ad-hoc collections or
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGIR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Learning on Dynamic Graphs via Parameter Isolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13825v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13825v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiyan Zhang, Yuchen Yan, Chaozhuo Li, Senzhang Wang, Xing Xie, Guojie Song, Sunghun Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many real-world graph learning tasks require handling dynamic graphs where
new nodes and edges emerge. Dynamic graph learning methods commonly suffer from
the catastrophic forgetting problem, where knowledge learned for previous
graphs is overwritten by updates for new graphs. To alleviate the problem,
continual graph learning methods are proposed. However, existing continual
graph learning methods aim to learn new patterns and maintain old ones with the
same set of parameters of fixed size, and thus face a fundamental tradeoff
between both goals. In this paper, we propose Parameter Isolation GNN (PI-GNN)
for continual learning on dynamic graphs that circumvents the tradeoff via
parameter isolation and expansion. Our motivation lies in that different
parameters contribute to learning different graph patterns. Based on the idea,
we expand model parameters to continually learn emerging graph patterns.
Meanwhile, to effectively preserve knowledge for unaffected patterns, we find
parameters that correspond to them via optimization and freeze them to prevent
them from being rewritten. Experiments on eight real-world datasets corroborate
the effectiveness of PI-GNN compared to state-of-the-art baselines.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">29</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PIGEON: Predicting Image Geolocations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Haas, Silas Alberti, Michal Skreta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce PIGEON, a multi-task end-to-end system for planet-scale image
geolocalization that achieves state-of-the-art performance on both external
benchmarks and in human evaluation. Our work incorporates semantic geocell
creation with label smoothing, conducts pretraining of a vision transformer on
images with geographic information, and refines location predictions with
ProtoNets across a candidate set of geocells. The contributions of PIGEON are
three-fold: first, we design a semantic geocells creation and splitting
algorithm based on open-source data which can be adapted to any geospatial
dataset. Second, we show the effectiveness of intra-geocell refinement and the
applicability of unsupervised clustering and ProtNets to the task. Finally, we
make our pre-trained CLIP transformer model, StreetCLIP, publicly available for
use in adjacent domains with applications to fighting climate change and urban
and rural scene understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Distributed Multi-task Reinforcement Learning with Experience
  Sharing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanae Amani, Khushbu Pahwa, Vladimir Braverman, Lin F. Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, DARPA launched the ShELL program, which aims to explore how
experience sharing can benefit distributed lifelong learning agents in adapting
to new challenges. In this paper, we address this issue by conducting both
theoretical and empirical research on distributed multi-task reinforcement
learning (RL), where a group of $N$ agents collaboratively solves $M$ tasks
without prior knowledge of their identities. We approach the problem by
formulating it as linearly parameterized contextual Markov decision processes
(MDPs), where each task is represented by a context that specifies the
transition dynamics and rewards. To tackle this problem, we propose an
algorithm called DistMT-LSVI. First, the agents identify the tasks, and then
they exchange information through a central server to derive $\epsilon$-optimal
policies for the tasks. Our research demonstrates that to achieve
$\epsilon$-optimal policies for all $M$ tasks, a single agent using DistMT-LSVI
needs to run a total number of episodes that is at most
$\tilde{\mathcal{O}}({d^3H^6(\epsilon^{-2}+c_{\rm sep}^{-2})}\cdot M/N)$, where
$c_{\rm sep}>0$ is a constant representing task separability, $H$ is the
horizon of each episode, and $d$ is the feature dimension of the dynamics and
rewards. Notably, DistMT-LSVI improves the sample complexity of non-distributed
settings by a factor of $1/N$, as each agent independently learns
$\epsilon$-optimal policies for all $M$ tasks using
$\tilde{\mathcal{O}}(d^3H^6M\epsilon^{-2})$ episodes. Additionally, we provide
numerical experiments conducted on OpenAI Gym Atari environments that validate
our theoretical findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Memorization Through the Lens of Curvature of Loss Function Around
  Samples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isha Garg, Kaushik Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks are overparametrized and easily overfit the datasets they
train on. In the extreme case, it is shown that they can memorize a training
set with fully randomized labels. We propose using the curvature of loss
function around the training sample as a measure of its memorization, averaged
over all training epochs. We use this to study the generalization versus
memorization properties of different samples in popular image datasets. We
visualize samples with the highest curvature of loss around them, and show that
these visually correspond to long-tailed, mislabeled or conflicting samples.
This analysis helps us find a, to the best of our knowledge, novel failure
model on the CIFAR100 dataset, that of duplicated images with different labels.
We also synthetically mislabel a proportion of the dataset by randomly
corrupting the labels of a few samples, and show that sorting by curvature
yields high AUROC values for identifying the mislabeled samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Relational Extraction on Wikipedia Tables using Convolutional and Memory
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arif Shahriar, Rohan Saha, Denilson Barbosa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation extraction (RE) is the task of extracting relations between entities
in text. Most RE methods extract relations from free-form running text and
leave out other rich data sources, such as tables. We explore RE from the
perspective of applying neural methods on tabularly organized data. We
introduce a new model consisting of Convolutional Neural Network (CNN) and
Bidirectional-Long Short Term Memory (BiLSTM) network to encode entities and
learn dependencies among them, respectively. We evaluate our model on a large
and recent dataset and compare results with previous neural methods.
Experimental results show that our model consistently outperforms the previous
model for the task of relation extraction on tabular data. We perform
comprehensive error analyses and ablation study to show the contribution of
various components of our model. Finally, we discuss the usefulness and
trade-offs of our approach, and provide suggestions for fostering further
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian taut splines for estimating the number of modes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        José E. Chacón, Javier Fernández Serrano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The number of modes in a probability density function is representative of
the model's complexity and can also be viewed as the number of existing
subpopulations. Despite its relevance, little research has been devoted to its
estimation. Focusing on the univariate setting, we propose a novel approach
targeting prediction accuracy inspired by some overlooked aspects of the
problem. We argue for the need for structure in the solutions, the subjective
and uncertain nature of modes, and the convenience of a holistic view blending
global and local density properties. Our method builds upon a combination of
flexible kernel estimators and parsimonious compositional splines. Feature
exploration, model selection and mode testing are implemented in the Bayesian
inference paradigm, providing soft solutions and allowing to incorporate expert
judgement in the process. The usefulness of our proposal is illustrated through
a case study in sports analytics, showcasing multiple companion visualisation
tools. A thorough simulation study demonstrates that traditional
modality-driven approaches paradoxically struggle to provide accurate results.
In this context, our method emerges as a top-tier alternative offering
innovative solutions for analysts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 15 figures (manuscript) + 11 pages, 10 figures
  (supplementary material)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe Reinforcement Learning for Strategic Bidding of Virtual Power
  Plants in Day-Ahead Markets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ognjen Stanojev, Lesia Mitridati, Riccardo de Nardis di Prata, Gabriela Hug
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel safe reinforcement learning algorithm for
strategic bidding of Virtual Power Plants (VPPs) in day-ahead electricity
markets. The proposed algorithm utilizes the Deep Deterministic Policy Gradient
(DDPG) method to learn competitive bidding policies without requiring an
accurate market model. Furthermore, to account for the complex internal
physical constraints of VPPs we introduce two enhancements to the DDPG method.
Firstly, a projection-based safety shield that restricts the agent's actions to
the feasible space defined by the non-linear power flow equations and operating
constraints of distributed energy resources is derived. Secondly, a penalty for
the shield activation in the reward function that incentivizes the agent to
learn a safer policy is introduced. A case study based on the IEEE 13-bus
network demonstrates the effectiveness of the proposed approach in enabling the
agent to learn a highly competitive, safe strategic policy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentiable Forward Projector for X-ray Computed Tomography <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyojin Kim, Kyle Champley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-driven deep learning has been successfully applied to various computed
tomographic reconstruction problems. The deep inference models may outperform
existing analytical and iterative algorithms, especially in ill-posed CT
reconstruction. However, those methods often predict images that do not agree
with the measured projection data. This paper presents an accurate
differentiable forward and back projection software library to ensure the
consistency between the predicted images and the original measurements. The
software library efficiently supports various projection geometry types while
minimizing the GPU memory footprint requirement, which facilitates seamless
integration with existing deep learning training and inference pipelines. The
proposed software is available as open source: https://github.com/LLNL/LEAP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023 Workshop: Differentiable Almost Everything: Differentiable
  Relaxations, Algorithms, Operators, and Simulators</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Learning Study of the Extended Drug-target Interaction Network
  informed by Pain Related Voltage-Gated Sodium Channels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Chen, Jian Jiang, Bozheng Dou, Hongsong Feng, Jie Liu, Yueying Zhu, Bengong Zhang, Tianshou Zhou, Guo-Wei Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pain is a significant global health issue, and the current treatment options
for pain management have limitations in terms of effectiveness, side effects,
and potential for addiction. There is a pressing need for improved pain
treatments and the development of new drugs. Voltage-gated sodium channels,
particularly Nav1.3, Nav1.7, Nav1.8, and Nav1.9, play a crucial role in
neuronal excitability and are predominantly expressed in the peripheral nervous
system. Targeting these channels may provide a means to treat pain while
minimizing central and cardiac adverse effects. In this study, we construct
protein-protein interaction (PPI) networks based on pain-related sodium
channels and develop a corresponding drug-target interaction (DTI) network to
identify potential lead compounds for pain management. To ensure reliable
machine learning predictions, we carefully select 111 inhibitor datasets from a
pool of over 1,000 targets in the PPI network. We employ three distinct machine
learning algorithms combined with advanced natural language processing
(NLP)-based embeddings, specifically pre-trained transformer and autoencoder
representations. Through a systematic screening process, we evaluate the side
effects and repurposing potential of over 150,000 drug candidates targeting
Nav1.7 and Nav1.8 sodium channels. Additionally, we assess the ADMET
(absorption, distribution, metabolism, excretion, and toxicity) properties of
these candidates to identify leads with near-optimal characteristics. Our
strategy provides an innovative platform for the pharmacological development of
pain treatments, offering the potential for improved efficacy and reduced side
effects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implicit regularisation in stochastic gradient descent: from
  single-objective to two-player games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihaela Rosca, Marc Peter Deisenroth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have seen many insights on deep learning optimisation being
brought forward by finding implicit regularisation effects of commonly used
gradient-based optimisers. Understanding implicit regularisation can not only
shed light on optimisation dynamics, but it can also be used to improve
performance and stability across problem domains, from supervised learning to
two-player games such as Generative Adversarial Networks. An avenue for finding
such implicit regularisation effects has been quantifying the discretisation
errors of discrete optimisers via continuous-time flows constructed by backward
error analysis (BEA). The current usage of BEA is not without limitations,
since not all the vector fields of continuous-time flows obtained using BEA can
be written as a gradient, hindering the construction of modified losses
revealing implicit regularisers. In this work, we provide a novel approach to
use BEA, and show how our approach can be used to construct continuous-time
flows with vector fields that can be written as gradients. We then use this to
find previously unknown implicit regularisation effects, such as those induced
by multiple stochastic gradient descent steps while accounting for the exact
data batches used in the updates, and in generally differentiable two-player
games.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Making the Nyström method highly accurate for low-rank approximations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianlin Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Nystr\"om method is a convenient heuristic method to obtain low-rank
approximations to kernel matrices in nearly linear complexity. Existing studies
typically use the method to approximate positive semidefinite matrices with low
or modest accuracies. In this work, we propose a series of heuristic strategies
to make the Nystr\"om method reach high accuracies for nonsymmetric and/or
rectangular matrices. The resulting methods (called high-accuracy Nystr\"om
methods) treat the Nystr\"om method and a skinny rank-revealing factorization
as a fast pivoting strategy in a progressive alternating direction refinement
process. Two refinement mechanisms are used: alternating the row and column
pivoting starting from a small set of randomly chosen columns, and adaptively
increasing the number of samples until a desired rank or accuracy is reached. A
fast subset update strategy based on the progressive sampling of Schur
complements is further proposed to accelerate the refinement process. Efficient
randomized accuracy control is also provided. Relevant accuracy and singular
value analysis is given to support some of the heuristics. Extensive tests with
various kernel functions and data sets show how the methods can quickly reach
prespecified high accuracies in practice, sometimes with quality close to SVDs,
using only small numbers of progressive sampling steps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weisfeiler and Lehman Go Measurement Modeling: Probing the Validity of
  the WL Test 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arjun Subramonian, Adina Williams, Maximilian Nickel, Yizhou Sun, Levent Sagun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The expressive power of graph neural networks is usually measured by
comparing how many pairs of graphs or nodes an architecture can possibly
distinguish as non-isomorphic to those distinguishable by the $k$-dimensional
Weisfeiler-Lehman ($k$-WL) test. In this paper, we uncover misalignments
between practitioners' conceptualizations of expressive power and $k$-WL
through a systematic analysis of the reliability and validity of $k$-WL. We
further conduct a survey ($n = 18$) of practitioners to surface their
conceptualizations of expressive power and their assumptions about $k$-WL. In
contrast to practitioners' opinions, our analysis (which draws from graph
theory and benchmark auditing) reveals that $k$-WL does not guarantee isometry,
can be irrelevant to real-world graph tasks, and may not promote generalization
or trustworthiness. We argue for extensional definitions and measurement of
expressive power based on benchmarks; we further contribute guiding questions
for constructing such benchmarks, which is critical for progress in graph
machine learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Random-Set Convolutional Neural Network (RS-CNN) for Epistemic Deep
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shireen Kudukkil Manchingal, Muhammad Mubashar, Kaizheng Wang, Keivan Shariatmadar, Fabio Cuzzolin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning is increasingly deployed in safety-critical domains where
robustness against adversarial attacks is crucial and erroneous predictions
could lead to potentially catastrophic consequences. This highlights the need
for learning systems to be equipped with the means to determine a model's
confidence in its prediction and the epistemic uncertainty associated with it,
'to know when a model does not know'. In this paper, we propose a novel
Random-Set Convolutional Neural Network (RS-CNN) for classification which
predicts belief functions rather than probability vectors over the set of
classes, using the mathematics of random sets, i.e., distributions over the
power set of the sample space. Based on the epistemic deep learning approach,
random-set models are capable of representing the 'epistemic' uncertainty
induced in machine learning by limited training sets. We estimate epistemic
uncertainty by approximating the size of credal sets associated with the
predicted belief functions, and experimentally demonstrate how our approach
outperforms competing uncertainty-aware approaches in a classical evaluation
setting. The performance of RS-CNN is best demonstrated on OOD samples where it
manages to capture the true prediction while standard CNNs fail.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Realtime Spectrum Monitoring via Reinforcement Learning -- A Comparison
  Between Q-Learning and Heuristic Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Braun, Tobias Korzyzkowske, Larissa Putzar, Jan Mietzner, Peter A. Hoeher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to technological advances in the field of radio technology and its
availability, the number of interference signals in the radio spectrum is
continuously increasing. Interference signals must be detected in a timely
fashion, in order to maintain standards and keep emergency frequencies open. To
this end, specialized (multi-channel) receivers are used for spectrum
monitoring. In this paper, the performances of two different approaches for
controlling the available receiver resources are compared. The methods used for
resource management (ReMa) are linear frequency tuning as a heuristic approach
and a Q-learning algorithm from the field of reinforcement learning. To test
the methods to be investigated, a simplified scenario was designed with two
receiver channels monitoring ten non-overlapping frequency bands with
non-uniform signal activity. For this setting, it is shown that the Q-learning
algorithm used has a significantly higher detection rate than the heuristic
approach at the expense of a smaller exploration rate. In particular, the
Q-learning approach can be parameterized to allow for a suitable trade-off
between detection and exploration rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GOKU-UI: Ubiquitous Inference through Attention and Multiple Shooting
  for Continuous-time Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Germán Abrevaya, Mahta Ramezanian-Panahi, Jean-Christophe Gagnon-Audet, Irina Rish, Pablo Polosecki, Silvina Ponce Dawson, Guillermo Cecchi, Guillaume Dumas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific Machine Learning (SciML) is a burgeoning field that
synergistically combines domain-aware and interpretable models with agnostic
machine learning techniques. In this work, we introduce GOKU-UI, an evolution
of the SciML generative model GOKU-nets. The GOKU-UI broadens the original
model's spectrum to incorporate other classes of differential equations, such
as Stochastic Differential Equations (SDEs), and integrates a distributed, i.e.
ubiquitous, inference through attention mechanisms and a novel multiple
shooting training strategy in the latent space. These enhancements have led to
a significant increase in its performance in both reconstruction and forecast
tasks, as demonstrated by our evaluation of simulated and empirical data.
Specifically, GOKU-UI outperformed all baseline models on synthetic datasets
even with a training set 32-fold smaller, underscoring its remarkable data
efficiency. Furthermore, when applied to empirical human brain data, while
incorporating stochastic Stuart-Landau oscillators into its dynamical core, it
not only surpassed state-of-the-art baseline methods in the reconstruction
task, but also demonstrated better prediction of future brain activity up to 12
seconds ahead. By training GOKU-UI on resting-state fMRI data, we encoded
whole-brain dynamics into a latent representation, learning an effective
low-dimensional dynamical system model that could offer insights into brain
functionality and open avenues for practical applications such as mental state
or psychiatric condition classification. Ultimately, our research provides
further impetus for the field of Scientific Machine Learning, showcasing the
potential for advancements when established scientific insights are interwoven
with modern machine learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards A Scalable Solution for Improving Multi-Group Fairness in
  Compositional Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Atwood, Tina Tian, Ben Packer, Meghana Deodhar, Jilin Chen, Alex Beutel, Flavien Prost, Ahmad Beirami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the rich literature on machine learning fairness, relatively little
attention has been paid to remediating complex systems, where the final
prediction is the combination of multiple classifiers and where multiple groups
are present. In this paper, we first show that natural baseline approaches for
improving equal opportunity fairness scale linearly with the product of the
number of remediated groups and the number of remediated prediction labels,
rendering them impractical. We then introduce two simple techniques, called
{\em task-overconditioning} and {\em group-interleaving}, to achieve a constant
scaling in this multi-group multi-label setup. Our experimental results in
academic and real-world environments demonstrate the effectiveness of our
proposal at mitigation within this environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoP-CLIP: A Mixture of <span class="highlight-title">Prompt</span>-Tuned CLIP Models for Domain Incremental
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julien Nicolas, Florent Chiaroni, Imtiaz Ziko, Ola Ahmad, Christian Desrosiers, Jose Dolz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the recent progress in incremental learning, addressing catastrophic
forgetting under distributional drift is still an open and important problem.
Indeed, while state-of-the-art domain incremental learning (DIL) methods
perform satisfactorily within known domains, their performance largely degrades
in the presence of novel domains. This limitation hampers their
generalizability, and restricts their scalability to more realistic settings
where train and test data are drawn from different distributions. To address
these limitations, we present a novel DIL approach based on a mixture of
prompt-tuned CLIP models (MoP-CLIP), which generalizes the paradigm of
S-Prompting to handle both in-distribution and out-of-distribution data at
inference. In particular, at the training stage we model the features
distribution of every class in each domain, learning individual text and visual
prompts to adapt to a given domain. At inference, the learned distributions
allow us to identify whether a given test sample belongs to a known domain,
selecting the correct prompt for the classification task, or from an unseen
domain, leveraging a mixture of the prompt-tuned CLIP models. Our empirical
evaluation reveals the poor performance of existing DIL methods under domain
shift, and suggests that the proposed MoP-CLIP performs competitively in the
standard DIL settings while outperforming state-of-the-art methods in OOD
scenarios. These results demonstrate the superiority of MoP-CLIP, offering a
robust and general solution to the problem of domain incremental learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Looking Beyond IoCs: Automatically Extracting Attack Patterns from
  External CTI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01753v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01753v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Tanvirul Alam, Dipkamal Bhusal, Youngja Park, Nidhi Rastogi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Public and commercial organizations extensively share cyberthreat
intelligence (CTI) to prepare systems to defend against existing and emerging
cyberattacks. However, traditional CTI has primarily focused on tracking known
threat indicators such as IP addresses and domain names, which may not provide
long-term value in defending against evolving attacks. To address this
challenge, we propose to use more robust threat intelligence signals called
attack patterns. LADDER is a knowledge extraction framework that can extract
text-based attack patterns from CTI reports at scale. The framework
characterizes attack patterns by capturing the phases of an attack in Android
and enterprise networks and systematically maps them to the MITRE ATT\&CK
pattern framework. LADDER can be used by security analysts to determine the
presence of attack vectors related to existing and emerging threats, enabling
them to prepare defenses proactively. We also present several use cases to
demonstrate the application of LADDER in real-world scenarios. Finally, we
provide a new, open-access benchmark malware dataset to train future
cyberthreat intelligence models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privacy Risk for anisotropic Langevin dynamics using relative entropy
  bounds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.00766v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.00766v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasia Borovykh, Nikolas Kantas, Panos Parpas, Greg Pavliotis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The privacy preserving properties of Langevin dynamics with additive
isotropic noise have been extensively studied. However, the isotropic noise
assumption is very restrictive: (a) when adding noise to existing learning
algorithms to preserve privacy and maintain the best possible accuracy one
should take into account the relative magnitude of the outputs and their
correlations; (b) popular algorithms such as stochastic gradient descent (and
their continuous time limits) appear to possess anisotropic covariance
properties. To study the privacy risks for the anisotropic noise case, one
requires general results on the relative entropy between the laws of two
Stochastic Differential Equations with different drifts and diffusion
coefficients. Our main contribution is to establish such a bound using
stability estimates for solutions to the Fokker-Planck equations via functional
inequalities. With additional assumptions, the relative entropy bound implies
an $(\epsilon,\delta)$-differential privacy bound or translates to bounds on
the membership inference attack success and we show how anisotropic noise can
lead to better privacy-accuracy trade-offs. Finally, the benefits of
anisotropic noise are illustrated using numerical results in quadratic loss and
neural network setups.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MonoFlow: Rethinking Divergence GANs via the Perspective of Wasserstein
  Gradient Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01075v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01075v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxuan Yi, Zhanxing Zhu, Song Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The conventional understanding of adversarial training in generative
adversarial networks (GANs) is that the discriminator is trained to estimate a
divergence, and the generator learns to minimize this divergence. We argue that
despite the fact that many variants of GANs were developed following this
paradigm, the current theoretical understanding of GANs and their practical
algorithms are inconsistent. In this paper, we leverage Wasserstein gradient
flows which characterize the evolution of particles in the sample space, to
gain theoretical insights and algorithmic inspiration of GANs. We introduce a
unified generative modeling framework - MonoFlow: the particle evolution is
rescaled via a monotonically increasing mapping of the log density ratio. Under
our framework, adversarial training can be viewed as a procedure first
obtaining MonoFlow's vector field via training the discriminator and the
generator learns to draw the particle flow defined by the corresponding vector
field. We also reveal the fundamental difference between variational divergence
minimization and adversarial training. This analysis helps us to identify what
types of generator loss functions can lead to the successful training of GANs
and suggest that GANs may have more loss designs beyond the literature (e.g.,
non-saturated loss), as long as they realize MonoFlow. Consistent empirical
studies are included to validate the effectiveness of our framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Green, Quantized Federated Learning over Wireless Networks: An
  Energy-Efficient Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09387v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09387v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minsu Kim, Walid Saad, Mohammad Mozaffari, Merouane Debbah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a green-quantized FL framework, which represents data with a
finite precision level in both local training and uplink transmission, is
proposed. Here, the finite precision level is captured through the use of
quantized neural networks (QNNs) that quantize weights and activations in
fixed-precision format. In the considered FL model, each device trains its QNN
and transmits a quantized training result to the base station. Energy models
for the local training and the transmission with quantization are rigorously
derived. To minimize the energy consumption and the number of communication
rounds simultaneously, a multi-objective optimization problem is formulated
with respect to the number of local iterations, the number of selected devices,
and the precision levels for both local training and transmission while
ensuring convergence under a target accuracy constraint. To solve this problem,
the convergence rate of the proposed FL system is analytically derived with
respect to the system control variables. Then, the Pareto boundary of the
problem is characterized to provide efficient solutions using the normal
boundary inspection method. Design insights on balancing the tradeoff between
the two objectives while achieving a target accuracy are drawn from using the
Nash bargaining solution and analyzing the derived convergence rate. Simulation
results show that the proposed FL framework can reduce energy consumption until
convergence by up to 70\% compared to a baseline FL algorithm that represents
data with full precision without damaging the convergence rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the IEEE Transactions on Wireless Communications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CodeGen2: Lessons for Training LLMs on Programming and Natural Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.02309v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.02309v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, Yingbo Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable abilities in
representation learning for program synthesis and understanding tasks. The
quality of the learned representations appears to be dictated by the neural
scaling laws as a function of the number of model parameters and observations,
while imposing upper bounds on the model performance by the amount of available
data and compute, which is costly.
  In this study, we attempt to render the training of LLMs for program
synthesis more efficient by unifying four key components: (1) model
architectures, (2) learning methods, (3) infill sampling, and, (4) data
distributions. Specifically, for the model architecture, we attempt to unify
encoder and decoder-based models into a single prefix-LM. For learning methods,
(i) causal language modeling, (ii) span corruption, (iii) infilling are unified
into a simple learning algorithm. For infill sampling, we explore the claim of
a "free lunch" hypothesis. For data distributions, the effect of a mixture
distribution and multi-epoch training of programming and natural languages on
model performance is explored.
  We conduct a comprehensive series of empirical experiments on 1B LLMs, for
which failures and successes of this exploration are distilled into five
lessons. We will provide a final recipe for training and release CodeGen2
models in size 1B, 3.7B, 7B, and, 16B parameters, along with the training
framework as open-source: https://github.com/salesforce/CodeGen.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distilled Pruning: Using Synthetic Data to Win the Lottery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03364v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03364v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luke McDermott, Daniel Cummings
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces a novel approach to pruning deep learning models by
using distilled data. Unlike conventional strategies which primarily focus on
architectural or algorithmic optimization, our method reconsiders the role of
data in these scenarios. Distilled datasets capture essential patterns from
larger datasets, and we demonstrate how to leverage this capability to enable a
computationally efficient pruning process. Our approach can find sparse,
trainable subnetworks (a.k.a. Lottery Tickets) up to 5x faster than Iterative
Magnitude Pruning at comparable sparsity on CIFAR-10. The experimental results
highlight the potential of using distilled data for resource-efficient neural
network pruning, model compression, and neural architecture search.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing the Power of Explanations for Incremental Training: A
  LIME-Based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01413v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01413v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arnab Neelim Mazumder, Niall Lyons, Ashutosh Pandey, Avik Santra, Tinoosh Mohsenin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainability of neural network prediction is essential to understand
feature importance and gain interpretable insight into neural network
performance. However, explanations of neural network outcomes are mostly
limited to visualization, and there is scarce work that looks to use these
explanations as feedback to improve model performance. In this work, model
explanations are fed back to the feed-forward training to help the model
generalize better. To this extent, a custom weighted loss where the weights are
generated by considering the Euclidean distances between true LIME (Local
Interpretable Model-Agnostic Explanations) explanations and model-predicted
LIME explanations is proposed. Also, in practical training scenarios,
developing a solution that can help the model learn sequentially without losing
information on previous data distribution is imperative due to the
unavailability of all the training data at once. Thus, the framework
incorporates the custom weighted loss with Elastic Weight Consolidation (EWC)
to maintain performance in sequential testing sets. The proposed custom
training procedure results in a consistent enhancement of accuracy ranging from
0.5% to 1.5% throughout all phases of the incremental learning setup compared
to traditional loss-based training methods for the keyword spotting task using
the Google Speech Commands dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EUSIPCO 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sparse Modular Activation for Efficient Sequence Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11197v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11197v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liliang Ren, Yang Liu, Shuohang Wang, Yichong Xu, Chenguang Zhu, ChengXiang Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Linear State Space Models (SSMs) have demonstrated strong performance in a
variety of sequence modeling tasks due to their efficient encoding of the
recurrent structure. However, in more comprehensive tasks like language
modeling and machine translation, self-attention-based models still outperform
SSMs. Hybrid models employing both SSM and self-attention generally show
promising performance, but current approaches apply attention modules
statically and uniformly to all elements in the input sequences, leading to
sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse
Modular Activation (SMA), a general mechanism enabling neural networks to
sparsely and dynamically activate sub-modules for sequence elements in a
differentiable manner. Through allowing each element to skip non-activated
sub-modules, SMA reduces computation and memory consumption at both training
and inference stages of sequence modeling. As a specific instantiation of SMA,
we design a novel neural architecture, SeqBoat, which employs SMA to sparsely
activate a Gated Attention Unit (GAU) based on the state representations
learned from an SSM. By constraining the GAU to only conduct local attention on
the activated inputs, SeqBoat can achieve linear inference complexity with
theoretically infinite attention span, and provide substantially better
quality-efficiency trade-off than the chunking-based models. With experiments
on a wide range of tasks, including language modeling, speech classification
and long-range arena, SeqBoat brings new state-of-the-art results among hybrid
models with linear complexity and reveals the amount of attention needed for
each task through the learned sparse activation patterns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active Learning for Single Neuron Models with Lipschitz Non-Linearities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.13601v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.13601v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aarshvi Gajjar, Chinmay Hegde, Christopher Musco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of active learning for single neuron models, also
sometimes called ``ridge functions'', in the agnostic setting (under
adversarial label noise). Such models have been shown to be broadly effective
in modeling physical phenomena, and for constructing surrogate data-driven
models for partial differential equations.
  Surprisingly, we show that for a single neuron model with any Lipschitz
non-linearity (such as the ReLU, sigmoid, absolute value, low-degree
polynomial, among others), strong provable approximation guarantees can be
obtained using a well-known active learning strategy for fitting \emph{linear
functions} in the agnostic setting. % -- i.e. for the case when there is no
non-linearity. Namely, we can collect samples via statistical \emph{leverage
score sampling}, which has been shown to be near-optimal in other active
learning scenarios. We support our theoretical results with empirical
simulations showing that our proposed active learning strategy based on
leverage score sampling outperforms (ordinary) uniform sampling when fitting
single neuron models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In and Out-of-Domain Text Adversarial Robustness via Label Smoothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10258v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10258v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yahan Yang, Soham Dan, Dan Roth, Insup Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently it has been shown that state-of-the-art NLP models are vulnerable to
adversarial attacks, where the predictions of a model can be drastically
altered by slight modifications to the input (such as synonym substitutions).
While several defense techniques have been proposed, and adapted, to the
discrete nature of text adversarial attacks, the benefits of general-purpose
regularization methods such as label smoothing for language models, have not
been studied. In this paper, we study the adversarial robustness provided by
various label smoothing strategies in foundational models for diverse NLP tasks
in both in-domain and out-of-domain settings. Our experiments show that label
smoothing significantly improves adversarial robustness in pre-trained models
like BERT, against various popular attacks. We also analyze the relationship
between prediction confidence and robustness, showing that label smoothing
reduces over-confident errors on adversarial examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Generalized Alternating Method for Bilevel Learning under the
  Polyak-Łojasiewicz Condition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02422v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02422v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Xiao, Songtao Lu, Tianyi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bilevel optimization has recently regained interest owing to its applications
in emerging machine learning fields such as hyperparameter optimization,
meta-learning, and reinforcement learning. Recent results have shown that
simple alternating (implicit) gradient-based algorithms can achieve the same
convergence rate of single-level gradient descent (GD) for bilevel problems
with a strongly convex lower-level objective. However, it remains unclear
whether this result can be generalized to bilevel problems beyond this basic
setting. In this paper, we propose a Generalized ALternating mEthod for bilevel
opTimization (GALET) with a nonconvex lower-level objective that satisfies the
Polyak-{\L}ojasiewicz (PL) condition. We first introduce a stationary metric
for the considered bilevel problems, which generalizes the existing metric. We
then establish that GALET achieves an $\epsilon$-stationary metric for the
considered problem within $\tilde{\cal O}(\epsilon^{-1})$ iterations, which
matches the iteration complexity of GD for smooth nonconvex problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Correct a typo of Figure 2</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benign Overfitting in Multiclass Classification: All Roads Lead to
  Interpolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2106.10865v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2106.10865v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Wang, Vidya Muthukumar, Christos Thrampoulidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The literature on "benign overfitting" in overparameterized models has been
mostly restricted to regression or binary classification; however, modern
machine learning operates in the multiclass setting. Motivated by this
discrepancy, we study benign overfitting in multiclass linear classification.
Specifically, we consider the following training algorithms on separable data:
(i) empirical risk minimization (ERM) with cross-entropy loss, which converges
to the multiclass support vector machine (SVM) solution; (ii) ERM with
least-squares loss, which converges to the min-norm interpolating (MNI)
solution; and, (iii) the one-vs-all SVM classifier. First, we provide a simple
sufficient deterministic condition under which all three algorithms lead to
classifiers that interpolate the training data and have equal accuracy. When
the data is generated from Gaussian mixtures or a multinomial logistic model,
this condition holds under high enough effective overparameterization. We also
show that this sufficient condition is satisfied under "neural collapse", a
phenomenon that is observed in training deep neural networks. Second, we derive
novel bounds on the accuracy of the MNI classifier, thereby showing that all
three training algorithms lead to benign overfitting under sufficient
overparameterization. Ultimately, our analysis shows that good generalization
is possible for SVM solutions beyond the realm in which typical margin-based
bounds apply.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Corrected subtle issue in the proofs of Lemmas 4 an 5. Relaxed
  Assumptions 1 and 2 and added error bound for ETF geometry</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multiple Instance Learning via Iterative Self-Paced Supervised
  Contrastive Learning <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.09452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.09452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangning Liu, Weicheng Zhu, Yiqiu Shen, Sheng Liu, Narges Razavian, Krzysztof J. Geras, Carlos Fernandez-Granda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning representations for individual instances when only bag-level labels
are available is a fundamental challenge in multiple instance learning (MIL).
Recent works have shown promising results using contrastive self-supervised
learning (CSSL), which learns to push apart representations corresponding to
two different randomly-selected instances. Unfortunately, in real-world
applications such as medical image classification, there is often class
imbalance, so randomly-selected instances mostly belong to the same majority
class, which precludes CSSL from learning inter-class differences. To address
this issue, we propose a novel framework, Iterative Self-paced Supervised
Contrastive Learning for MIL Representations (ItS2CLR), which improves the
learned representation by exploiting instance-level pseudo labels derived from
the bag-level labels. The framework employs a novel self-paced sampling
strategy to ensure the accuracy of pseudo labels. We evaluate ItS2CLR on three
medical datasets, showing that it improves the quality of instance-level pseudo
labels and representations, and outperforms existing MIL methods in terms of
both bag and instance level accuracy. Code is available at
https://github.com/Kangningthu/ItS2CLR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023 camera-ready version. The first two authors contribute
  equally. The last two authors are joint last authors</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Feature Extraction for Symbolic Music 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Simonetta, Ana Llorens, Martín Serrano, Eduardo García-Portugués, Álvaro Torrente
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a comprehensive investigation of existing feature
extraction tools for symbolic music and contrasts their performance to
determine the set of features that best characterizes the musical style of a
given music score. In this regard, we propose a novel feature extraction tool,
named musif, and evaluate its efficacy on various repertoires and file formats,
including MIDI, MusicXML, and **kern. Musif approximates existing tools such as
jSymbolic and music21 in terms of computational efficiency while attempting to
enhance the usability for custom feature development. The proposed tool also
enhances classification accuracy when combined with other sets of features. We
demonstrate the contribution of each set of features and the computational
resources they require. Our findings indicate that the optimal tool for feature
extraction is a combination of the best features from each tool rather than
those of a single one. To facilitate future research in music information
retrieval, we release the source code of the tool and benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ISMIR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive Video Question Answering via Video Graph <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13668v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13668v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junbin Xiao, Pan Zhou, Angela Yao, Yicong Li, Richang Hong, Shuicheng Yan, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose to perform video question answering (VideoQA) in a Contrastive
manner via a Video Graph Transformer model (CoVGT). CoVGT's uniqueness and
superiority are three-fold: 1) It proposes a dynamic graph transformer module
which encodes video by explicitly capturing the visual objects, their relations
and dynamics, for complex spatio-temporal reasoning. 2) It designs separate
video and text transformers for contrastive learning between the video and text
to perform QA, instead of multi-modal transformer for answer classification.
Fine-grained video-text communication is done by additional cross-modal
interaction modules. 3) It is optimized by the joint fully- and self-supervised
contrastive objectives between the correct and incorrect answers, as well as
the relevant and irrelevant questions respectively. With superior video
encoding and QA solution, we show that CoVGT can achieve much better
performances than previous arts on video reasoning tasks. Its performances even
surpass those models that are pretrained with millions of external data. We
further show that CoVGT can also benefit from cross-modal pretraining, yet with
orders of magnitude smaller data. The results demonstrate the effectiveness and
superiority of CoVGT, and additionally reveal its potential for more
data-efficient pretraining. We hope our success can advance VideoQA beyond
coarse recognition/description towards fine-grained relation reasoning of video
contents. Our code is available at https://github.com/doc-doc/CoVGT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE T-PAMI'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AMD: Autoregressive Motion Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.09381v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.09381v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Han, Hao Peng, Minjing Dong, Chang Xu, Yi Ren, Yixuan Shen, Yuheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human motion generation aims to produce plausible human motion sequences
according to various conditional inputs, such as text or audio. Despite the
feasibility of existing methods in generating motion based on short prompts and
simple motion patterns, they encounter difficulties when dealing with long
prompts or complex motions. The challenges are two-fold: 1) the scarcity of
human motion-captured data for long prompts and complex motions. 2) the high
diversity of human motions in the temporal domain and the substantial
divergence of distributions from conditional modalities, leading to a
many-to-many mapping problem when generating motion with complex and long
texts. In this work, we address these gaps by 1) elaborating the first dataset
pairing long textual descriptions and 3D complex motions (HumanLong3D), and 2)
proposing an autoregressive motion diffusion model (AMD). Specifically, AMD
integrates the text prompt at the current timestep with the text prompt and
action sequences at the previous timestep as conditional information to predict
the current action sequences in an iterative manner. Furthermore, we present
its generalization for X-to-Motion with "No Modality Left Behind", enabling for
the first time the generation of high-definition and high-fidelity human
motions based on user-defined modality input.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-07-10T00:00:00Z">2023-07-10</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">38</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SimpleMTOD: A Simple Language Model for Multimodal Task-Oriented
  Dialogue with Symbolic Scene Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhathiya Hemanthage, Christian Dondrup, Phil Bartie, Oliver Lemon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SimpleMTOD is a simple language model which recasts several sub-tasks in
multimodal task-oriented dialogues as sequence prediction tasks. SimpleMTOD is
built on a large-scale transformer-based auto-regressive architecture, which
has already proven to be successful in uni-modal task-oriented dialogues, and
effectively leverages transfer learning from pre-trained GPT-2. In-order to
capture the semantics of visual scenes, we introduce both local and
de-localized tokens for objects within a scene. De-localized tokens represent
the type of an object rather than the specific object itself and so possess a
consistent meaning across the dataset. SimpleMTOD achieves a state-of-the-art
BLEU score (0.327) in the Response Generation sub-task of the SIMMC 2.0
test-std dataset while performing on par in other multimodal sub-tasks:
Disambiguation, Coreference Resolution, and Dialog State Tracking. This is
despite taking a minimalist approach for extracting visual (and non-visual)
information. In addition the model does not rely on task-specific architectural
changes such as classification heads.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Entity Identifier: A Natural Text Parsing-based Framework For Entity
  Relation Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        El Mehdi Chouham, Jessica López Espejel, Mahaman Sanoussi Yahaya Alassan, Walid Dahhane, El Hassane Ettifouri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of programming has a diversity of paradigms that are used according
to the working framework. While current neural code generation methods are able
to learn and generate code directly from text, we believe that this approach is
not optimal for certain code tasks, particularly the generation of classes in
an object-oriented project. Specifically, we use natural language processing
techniques to extract structured information from requirements descriptions, in
order to automate the generation of CRUD (Create, Read, Update, Delete) class
code. To facilitate this process, we introduce a pipeline for extracting entity
and relation information, as well as a representation called an "Entity Tree"
to model this information. We also create a dataset to evaluate the
effectiveness of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review for Elsevier's Natural Language Processing Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SITTA: A Semantic Image-Text Alignment for Image Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Paischer, Thomas Adler, Markus Hofmarcher, Sepp Hochreiter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Textual and semantic comprehension of images is essential for generating
proper captions. The comprehension requires detection of objects, modeling of
relations between them, an assessment of the semantics of the scene and,
finally, representing the extracted knowledge in a language space. To achieve
rich language capabilities while ensuring good image-language mappings,
pretrained language models (LMs) were conditioned on pretrained multi-modal
(image-text) models that allow for image inputs. This requires an alignment of
the image representation of the multi-modal model with the language
representations of a generative LM. However, it is not clear how to best
transfer semantics detected by the vision encoder of the multi-modal model to
the LM. We introduce two novel ways of constructing a linear mapping that
successfully transfers semantics between the embedding spaces of the two
pretrained models. The first aligns the embedding space of the multi-modal
language encoder with the embedding space of the pretrained LM via token
correspondences. The latter leverages additional data that consists of
image-text pairs to construct the mapping directly from vision to language
space. Using our semantic mappings, we unlock image captioning for LMs without
access to gradient information. By using different sources of data we achieve
strong captioning performance on MS-COCO and Flickr30k datasets. Even in the
face of limited data, our method partly exceeds the performance of other
zero-shot and even finetuned competitors. Our ablation studies show that even
LMs at a scale of merely 250M parameters can generate decent captions employing
our semantic mappings. Our approach makes image captioning more accessible for
institutions with restricted computational resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages (+ references and appendix), Code:
  https://github.com/ml-jku/semantic-image-text-alignment</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models as General Pattern Machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, Andy Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We observe that pre-trained large language models (LLMs) are capable of
autoregressively completing complex token sequences -- from arbitrary ones
procedurally generated by probabilistic context-free grammars (PCFG), to more
rich spatial patterns found in the Abstract Reasoning Corpus (ARC), a general
AI benchmark, prompted in the style of ASCII art. Surprisingly, pattern
completion proficiency can be partially retained even when the sequences are
expressed using tokens randomly sampled from the vocabulary. These results
suggest that without any additional training, LLMs can serve as general
sequence modelers, driven by in-context learning. In this work, we investigate
how these zero-shot capabilities may be applied to problems in robotics -- from
extrapolating sequences of numbers that represent states over time to complete
simple motions, to least-to-most prompting of reward-conditioned trajectories
that can discover and represent closed-loop policies (e.g., a stabilizing
controller for CartPole). While difficult to deploy today for real systems due
to latency, context size limitations, and compute costs, the approach of using
LLMs to drive low-level control may provide an exciting glimpse into how the
patterns among words could be transferred to actions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 23 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BeaverTails: Towards Improved Safety Alignment of LLM via a
  Human-Preference <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Chi Zhang, Ruiyang Sun, Yizhou Wang, Yaodong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce the BeaverTails dataset, aimed at fostering
research on safety alignment in large language models (LLMs). This dataset
uniquely separates annotations of helpfulness and harmlessness for
question-answering pairs, thus offering distinct perspectives on these crucial
attributes. In total, we have compiled safety meta-labels for 30,207
question-answer (QA) pairs and gathered 30,144 pairs of expert comparison data
for both the helpfulness and harmlessness metrics. We further showcase
applications of BeaverTails in content moderation and reinforcement learning
with human feedback (RLHF), emphasizing its potential for practical safety
measures in LLMs. We believe this dataset provides vital resources for the
community, contributing towards the safe development and deployment of LLMs.
Our project page is available at the following URL:
https://sites.google.com/view/pku-beavertails.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Measuring Lexical Diversity in Texts: The Twofold Length Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yves Bestgen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The impact of text length on the estimation of lexical diversity has captured
the attention of the scientific community for more than a century. Numerous
indices have been proposed, and many studies have been conducted to evaluate
them, but the problem remains. This methodological review provides a critical
analysis not only of the most commonly used indices in language learning
studies, but also of the length problem itself, as well as of the methodology
for evaluating the proposed solutions. The analysis of three datasets of
English language-learners' texts revealed that indices that reduce all texts to
the same length using a probabilistic or an algorithmic approach solve the
length dependency problem; however, all these indices failed to address the
second problem, which is their sensitivity to the parameter that determines the
length to which the texts are reduced. The paper concludes with recommendations
for optimizing lexical diversity analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hate Speech Detection via Dual Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyu Lu, Hongfei Lin, Xiaokun Zhang, Zhaoqing Li, Tongyue Zhang, Linlin Zong, Fenglong Ma, Bo Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fast spread of hate speech on social media impacts the Internet
environment and our society by increasing prejudice and hurting people.
Detecting hate speech has aroused broad attention in the field of natural
language processing. Although hate speech detection has been addressed in
recent work, this task still faces two inherent unsolved challenges. The first
challenge lies in the complex semantic information conveyed in hate speech,
particularly the interference of insulting words in hate speech detection. The
second challenge is the imbalanced distribution of hate speech and non-hate
speech, which may significantly deteriorate the performance of models. To
tackle these challenges, we propose a novel dual contrastive learning (DCL)
framework for hate speech detection. Our framework jointly optimizes the
self-supervised and the supervised contrastive learning loss for capturing
span-level information beyond the token-level emotional semantics used in
existing models, particularly detecting speech containing abusive and insulting
words. Moreover, we integrate the focal loss into the dual contrastive learning
framework to alleviate the problem of data imbalance. We conduct experiments on
two publicly available English datasets, and experimental results show that the
proposed model outperforms the state-of-the-art models and precisely detects
hate speeches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted in IEEE/ACM Transactions on Audio,
  Speech, and Language Processing (TASLP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Factuality of Abstractive Summarization via Contrastive Reward
  Learning <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        I-Chun Chern, Zhiruo Wang, Sanjan Das, Bhavuk Sharma, Pengfei Liu, Graham Neubig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern abstractive summarization models often generate summaries that contain
hallucinated or contradictory information. In this paper, we propose a simple
but effective contrastive learning framework that incorporates recent
developments in reward learning and factuality metrics. Empirical studies
demonstrate that the proposed framework enables summarization models to learn
from feedback of factuality metrics using contrastive reward learning, leading
to more factual summaries by human evaluations. This suggests that further
advances in learning and evaluation algorithms can feed directly into providing
more factual summaries.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TrustNLP @ ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Large Language Model for Graph Data Understanding in Online
  Job Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Likang Wu, Zhaopeng Qiu, Zhi Zheng, Hengshu Zhu, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have revolutionized natural language processing
tasks, demonstrating their exceptional capabilities in various domains.
However, their potential for behavior graph understanding in job
recommendations remains largely unexplored. This paper focuses on unveiling the
capability of large language models in understanding behavior graphs and
leveraging this understanding to enhance recommendations in online recruitment,
including the promotion of out-of-distribution (OOD) application. We present a
novel framework that harnesses the rich contextual information and semantic
representations provided by large language models to analyze behavior graphs
and uncover underlying patterns and relationships. Specifically, we propose a
meta-path prompt constructor that leverages LLM recommender to understand
behavior graphs for the first time and design a corresponding path augmentation
module to alleviate the prompt bias introduced by path-based sequence input. By
leveraging this capability, our framework enables personalized and accurate job
recommendations for individual users. We evaluate the effectiveness of our
approach on a comprehensive dataset and demonstrate its ability to improve the
relevance and quality of recommended quality. This research not only sheds
light on the untapped potential of large language models but also provides
valuable insights for developing advanced recommendation systems in the
recruitment market. The findings contribute to the growing field of natural
language processing and offer practical implications for enhancing job search
experiences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Biomedical Text Summarization and Question-Answering: On the
  Utility of Domain-Specific <span class="highlight-title">Pre-Train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dima Galat, Marian-Andrei Rizoiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biomedical summarization requires large datasets to train for text
generation. We show that while transfer learning offers a viable option for
addressing this challenge, an in-domain pre-training does not always offer
advantages in a BioASQ summarization task. We identify a suitable model
architecture and use it to show a benefit of a general-domain pre-training
followed by a task-specific fine-tuning in the context of a BioASQ
summarization task, leading to a novel three-step fine-tuning approach that
works with only a thousand in-domain examples. Our results indicate that a
Large Language Model without domain-specific pre-training can have a
significant edge in some domain-specific biomedical text generation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unmasking the giant: A comprehensive evaluation of Chat<span class="highlight-title">GPT</span>'s proficiency
  in coding algorithms and data structures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sayed Erfan Arefin, Tasnia Ashrafi Heya, Hasan Al-Qudah, Ynes Ineza, Abdul Serwadda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The transformative influence of Large Language Models (LLMs) is profoundly
reshaping the Artificial Intelligence (AI) technology domain. Notably, ChatGPT
distinguishes itself within these models, demonstrating remarkable performance
in multi-turn conversations and exhibiting code proficiency across an array of
languages. In this paper, we carry out a comprehensive evaluation of ChatGPT's
coding capabilities based on what is to date the largest catalog of coding
challenges. Our focus is on the python programming language and problems
centered on data structures and algorithms, two topics at the very foundations
of Computer Science. We evaluate ChatGPT for its ability to generate correct
solutions to the problems fed to it, its code quality, and nature of run-time
errors thrown by its code. Where ChatGPT code successfully executes, but fails
to solve the problem at hand, we look into patterns in the test cases passed in
order to gain some insights into how wrong ChatGPT code is in these kinds of
situations. To infer whether ChatGPT might have directly memorized some of the
data that was used to train it, we methodically design an experiment to
investigate this phenomena. Making comparisons with human performance whenever
feasible, we investigate all the above questions from the context of both its
underlying learning models (GPT-3.5 and GPT-4), on a vast array sub-topics
within the main topics, and on problems having varying degrees of difficulty.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TIM: Teaching Large Language Models to Translate with Comparison 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiali Zeng, Fandong Meng, Yongjing Yin, Jie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-sourced large language models (LLMs) have demonstrated remarkable
efficacy in various tasks with instruction tuning. However, these models can
sometimes struggle with tasks that require more specialized knowledge such as
translation. One possible reason for such deficiency is that instruction tuning
aims to generate fluent and coherent text that continues from a given
instruction without being constrained by any task-specific requirements.
Moreover, it can be more challenging for tuning smaller LLMs with lower-quality
training data. To address this issue, we propose a novel framework using
examples in comparison to teach LLMs to learn translation. Our approach
involves presenting the model with examples of correct and incorrect
translations and using a preference loss to guide the model's learning. We
evaluate our method on WMT2022 test sets and show that it outperforms existing
methods. Our findings offer a new perspective on fine-tuning LLMs for
translation tasks and provide a promising solution for generating high-quality
translations. Please refer to Github for more details:
https://github.com/lemon0830/TIM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ethicist: Targeted Training Data Extraction Through Loss Smoothed Soft
  <span class="highlight-title">Prompt</span>ing and Calibrated Confidence Estimation <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04401v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04401v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhexin Zhang, Jiaxin Wen, Minlie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large pre-trained language models achieve impressive results across many
tasks. However, recent works point out that pre-trained language models may
memorize a considerable fraction of their training data, leading to the privacy
risk of information leakage. In this paper, we propose a method named Ethicist
for targeted training data extraction through loss smoothed soft prompting and
calibrated confidence estimation, investigating how to recover the suffix in
the training data when given a prefix. To elicit memorization in the attacked
model, we tune soft prompt embeddings while keeping the model fixed. We further
propose a smoothing loss that smooths the loss distribution of the suffix
tokens to make it easier to sample the correct suffix. In order to select the
most probable suffix from a collection of sampled suffixes and estimate the
prediction confidence, we propose a calibrated confidence estimation method,
which normalizes the confidence of the generated suffixes with a local
estimation. We show that Ethicist significantly improves the extraction
performance on a recently proposed public benchmark. We also investigate
several factors influencing the data extraction performance, including decoding
strategy, model scale, prefix length, and suffix length. Our code is available
at https://github.com/thu-coai/Targeted-Data-Extraction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023 Long Paper (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Cross-lingual Transfer via Phonemic Transcription Integration <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoang H. Nguyen, Chenwei Zhang, Tao Zhang, Eugene Rohrbaugh, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous cross-lingual transfer methods are restricted to orthographic
representation learning via textual scripts. This limitation hampers
cross-lingual transfer and is biased towards languages sharing similar
well-known scripts. To alleviate the gap between languages from different
writing scripts, we propose PhoneXL, a framework incorporating phonemic
transcriptions as an additional linguistic modality beyond the traditional
orthographic transcriptions for cross-lingual transfer. Particularly, we
propose unsupervised alignment objectives to capture (1) local one-to-one
alignment between the two different modalities, (2) alignment via
multi-modality contexts to leverage information from additional modalities, and
(3) alignment via multilingual contexts where additional bilingual dictionaries
are incorporated. We also release the first phonemic-orthographic alignment
dataset on two token-level tasks (Named Entity Recognition and Part-of-Speech
Tagging) among the understudied but interconnected
Chinese-Japanese-Korean-Vietnamese (CJKV) languages. Our pilot study reveals
phonemic transcription provides essential information beyond the orthography to
enhance cross-lingual transfer and bridge the gap among CJKV languages, leading
to consistent improvements on cross-lingual token-level tasks over
orthographic-based multilingual PLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages,1 figure, 7 tables. To appear in Findings of ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RLTF: Reinforcement Learning from Unit Test Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao Han, Wei Yang, Deheng Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of program synthesis, or code generation, is to generate executable
code based on given descriptions. Recently, there has been an increasing number
of studies employing reinforcement learning (RL) to improve the performance of
large language models (LLMs) for code. However, these RL methods have only used
offline frameworks, limiting their exploration of new sample spaces.
Additionally, current approaches that utilize unit test signals are rather
simple, not accounting for specific error locations within the code. To address
these issues, we proposed RLTF, i.e., Reinforcement Learning from Unit Test
Feedback, a novel online RL framework with unit test feedback of
multi-granularity for refining code LLMs. Our approach generates data in
real-time during training and simultaneously utilizes fine-grained feedback
signals to guide the model towards producing higher-quality code. Extensive
experiments show that RLTF achieves state-of-the-art performance on the APPS
and the MBPP benchmarks. Our code can be found at:
https://github.com/Zyq-scut/RLTF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Event Extraction as Question Generation and Answering <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05567v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05567v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Di Lu, Shihao Ran, Joel Tetreault, Alejandro Jaimes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work on Event Extraction has reframed the task as Question Answering
(QA), with promising results. The advantage of this approach is that it
addresses the error propagation issue found in traditional token-based
classification approaches by directly predicting event arguments without
extracting candidates first. However, the questions are typically based on
fixed templates and they rarely leverage contextual information such as
relevant arguments. In addition, prior QA-based approaches have difficulty
handling cases where there are multiple arguments for the same role. In this
paper, we propose QGA-EE, which enables a Question Generation (QG) model to
generate questions that incorporate rich contextual information instead of
using fixed templates. We also propose dynamic templates to assist the training
of QG model. Experiments show that QGA-EE outperforms all prior
single-task-based models on the ACE05 English dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Generate Equitable Text in Dialogue from Biased Training
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Sicilia, Malihe Alikhani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ingrained principles of fairness in a dialogue system's decision-making
process and generated responses are crucial for user engagement, satisfaction,
and task achievement. Absence of equitable and inclusive principles can hinder
the formation of common ground, which in turn negatively impacts the overall
performance of the system. For example, misusing pronouns in a user interaction
may cause ambiguity about the intended subject. Yet, there is no comprehensive
study of equitable text generation in dialogue. Aptly, in this work, we use
theories of computational learning to study this problem. We provide formal
definitions of equity in text generation, and further, prove formal connections
between learning human-likeness and learning equity: algorithms for improving
equity ultimately reduce to algorithms for improving human-likeness (on
augmented data). With this insight, we also formulate reasonable conditions
under which text generation algorithms can learn to generate equitable text
without any modifications to the biased training data on which they learn. To
exemplify our theory in practice, we look at a group of algorithms for the
GuessWhat?! visual dialogue game and, using this example, test our theory
empirically. Our theory accurately predicts relative-performance of multiple
algorithms in generating equitable text as measured by both human and automated
evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HistRED: A Historical Document-Level Relation Extraction <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04285v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04285v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soyoung Yang, Minseok Choi, Youngwoo Cho, Jaegul Choo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the extensive applications of relation extraction (RE) tasks in
various domains, little has been explored in the historical context, which
contains promising data across hundreds and thousands of years. To promote the
historical RE research, we present HistRED constructed from Yeonhaengnok.
Yeonhaengnok is a collection of records originally written in Hanja, the
classical Chinese writing, which has later been translated into Korean. HistRED
provides bilingual annotations such that RE can be performed on Korean and
Hanja texts. In addition, HistRED supports various self-contained subtexts with
different lengths, from a sentence level to a document level, supporting
diverse context settings for researchers to evaluate the robustness of their RE
models. To demonstrate the usefulness of our dataset, we propose a bilingual RE
model that leverages both Korean and Hanja contexts to predict relations
between entities. Our model outperforms monolingual baselines on HistRED,
showing that employing multiple language contexts supplements the RE
predictions. The dataset is publicly available at:
https://huggingface.co/datasets/Soyoung/HistRED under CC BY-NC-ND 4.0 license.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Lingual Retrieval Augmented <span class="highlight-title">Prompt</span> for Low-Resource Languages <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09651v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09651v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ercong Nie, Sheng Liang, Helmut Schmid, Hinrich Schütze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual Pretrained Language Models (MPLMs) have shown their strong
multilinguality in recent empirical cross-lingual transfer studies. In this
paper, we propose the Prompts Augmented by Retrieval Crosslingually (PARC)
pipeline to improve the zero-shot performance on low-resource languages (LRLs)
by augmenting the context with semantically similar sentences retrieved from a
high-resource language (HRL) as prompts. PARC improves the zero-shot
performance on three downstream tasks (binary sentiment classification, topic
categorization and natural language inference) with multilingual parallel test
sets across 10 LRLs covering 6 language families in both unlabeled settings
(+5.1%) and labeled settings (+16.3%). PARC-labeled also outperforms the
finetuning baseline by 3.7%. We find a significant positive correlation between
cross-lingual transfer performance on one side, and the similarity between the
high- and low-resource languages as well as the amount of low-resource
pretraining data on the other side. A robustness analysis suggests that PARC
has the potential to achieve even stronger performance with more powerful
MPLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Findings of ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Forming Trees with Treeformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.06960v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.06960v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nilay Patel, Jeffrey Flanigan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human language is known to exhibit a nested, hierarchical structure, allowing
us to form complex sentences out of smaller pieces. However, many
state-of-the-art neural networks models such as Transformers have no explicit
hierarchical structure in its architecture -- that is, they don't have an
inductive bias toward hierarchical structure. Additionally, Transformers are
known to perform poorly on compositional generalization tasks which require
such structures. In this paper, we introduce Treeformer, a general-purpose
encoder module inspired by the CKY algorithm which learns a composition
operator and pooling function to construct hierarchical encodings for phrases
and sentences. Our extensive experiments demonstrate the benefits of
incorporating hierarchical structure into the Transformer and show significant
improvements in compositional generalization as well as in downstream tasks
such as machine translation, abstractive summarization, and various natural
language understanding tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to RANLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I2I: Initializing Adapters with Improvised Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02168v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02168v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tejas Srinivasan, Furong Jia, Mohammad Rostami, Jesse Thomason
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adapters present a promising solution to the catastrophic forgetting problem
in continual learning. However, training independent Adapter modules for every
new task misses an opportunity for cross-task knowledge transfer. We propose
Improvise to Initialize (I2I), a continual learning algorithm that initializes
Adapters for incoming tasks by distilling knowledge from previously-learned
tasks' Adapters. We evaluate I2I on CLiMB, a multimodal continual learning
benchmark, by conducting experiments on sequences of visual question answering
tasks. Adapters trained with I2I consistently achieve better task accuracy than
independently-trained Adapters, demonstrating that our algorithm facilitates
knowledge transfer between task Adapters. I2I also results in better cross-task
knowledge transfer than the state-of-the-art AdapterFusion without incurring
the associated parametric cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2nd Conference on Lifelong Learning Agents (CoLLAs), 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RecallM: An Architecture for Temporal Context Understanding and Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02738v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02738v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brandon Kynoch, Hugo Latapie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ideal long-term memory mechanism for Large Language Model (LLM) based
chatbots, would lay the foundation for continual learning, complex reasoning
and allow sequential and temporal dependencies to be learnt. Creating this type
of memory mechanism is an extremely challenging problem. In this paper we
explore different methods of achieving the effect of long-term memory. We
propose a new architecture focused on creating adaptable and updatable
long-term memory for AGI systems. We demonstrate through various experiments
the benefits of the RecallM architecture, particularly the improved temporal
understanding of knowledge it provides.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures, Our code is publicly available online at:
  https://github.com/cisco-open/DeepVision/tree/main/recallm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">GPT</span> detectors are biased against non-native English writers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02819v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02819v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, James Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid adoption of generative language models has brought about
substantial advancements in digital communication, while simultaneously raising
concerns regarding the potential misuse of AI-generated content. Although
numerous detection methods have been proposed to differentiate between AI and
human-generated content, the fairness and robustness of these detectors remain
underexplored. In this study, we evaluate the performance of several
widely-used GPT detectors using writing samples from native and non-native
English writers. Our findings reveal that these detectors consistently
misclassify non-native English writing samples as AI-generated, whereas native
writing samples are accurately identified. Furthermore, we demonstrate that
simple prompting strategies can not only mitigate this bias but also
effectively bypass GPT detectors, suggesting that GPT detectors may
unintentionally penalize writers with constrained linguistic expressions. Our
results call for a broader conversation about the ethical implications of
deploying ChatGPT content detectors and caution against their use in evaluative
or educational settings, particularly when they may inadvertently penalize or
exclude non-native English speakers from the global discourse. The published
version of this study can be accessed at:
www.cell.com/patterns/fulltext/S2666-3899(23)00130-7
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UNIQORN: Unified Question Answering over RDF Knowledge Graphs and
  Natural Language Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.08614v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.08614v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumajit Pramanik, Jesujoba Oluwadara Alabi, Rishiraj Saha Roy, Gerhard Weikum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering over knowledge graphs and other RDF data has been greatly
advanced, with a number of good techniques providing crisp answers for natural
language questions or telegraphic queries. Some of these systems incorporate
textual sources as additional evidence for the answering process, but cannot
compute answers that are present in text alone. Conversely, techniques from the
IR and NLP communities have addressed QA over text, but such systems barely
utilize semantic data and knowledge. This paper presents a method for complex
questions that can seamlessly operate over a mixture of RDF datasets and text
corpora, or individual sources, in a unified framework. Our method, called
UNIQORN, builds a context graph on-the-fly, by retrieving question-relevant
evidences from the RDF data and/or a text corpus, using fine-tuned BERT models.
The resulting graph typically contains all question-relevant evidences but also
a lot of noise. UNIQORN copes with this input by a graph algorithm for Group
Steiner Trees, that identifies the best answer candidates in the context graph.
Experimental results on several benchmarks of complex questions with multiple
entities and relations, show that UNIQORN significantly outperforms
state-of-the-art methods for heterogeneous QA. The graph-based methodology
provides user-interpretable evidence for the complete answering process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What do End-to-End Speech Models Learn about Speaker, Language and
  Channel Information? A Layer-wise and Neuron-level Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.00439v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.00439v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shammur Absar Chowdhury, Nadir Durrani, Ahmed Ali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks are inherently opaque and challenging to interpret.
Unlike hand-crafted feature-based models, we struggle to comprehend the
concepts learned and how they interact within these models. This understanding
is crucial not only for debugging purposes but also for ensuring fairness in
ethical decision-making. In our study, we conduct a post-hoc functional
interpretability analysis of pretrained speech models using the probing
framework [1]. Specifically, we analyze utterance-level representations of
speech models trained for various tasks such as speaker recognition and dialect
identification. We conduct layer and neuron-wise analyses, probing for speaker,
language, and channel properties. Our study aims to answer the following
questions: i) what information is captured within the representations? ii) how
is it represented and distributed? and iii) can we identify a minimal subset of
the network that possesses this information?
  Our results reveal several novel findings, including: i) channel and gender
information are distributed across the network, ii) the information is
redundantly available in neurons with respect to a task, iii) complex
properties such as dialectal information are encoded only in the task-oriented
pretrained network, iv) and is localised in the upper layers, v) we can extract
a minimal subset of neurons encoding the pre-defined property, vi) salient
neurons are sometimes shared between properties, vii) our analysis highlights
the presence of biases (for example gender) in the network. Our
cross-architectural comparison indicates that: i) the pretrained models capture
speaker-invariant information, and ii) CNN models are competitive with
Transformer models in encoding various understudied properties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in CSL journal. Keywords: Speech, Neuron Analysis,
  Interpretibility, Diagnostic Classifier, AI explainability, End-to-End
  Architecture</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simplicity Bias in <span class="highlight-title">Transformer</span>s and their Ability to Learn Sparse
  Boolean Functions <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12316v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12316v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satwik Bhattamishra, Arkil Patel, Varun Kanade, Phil Blunsom
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the widespread success of Transformers on NLP tasks, recent works
have found that they struggle to model several formal languages when compared
to recurrent models. This raises the question of why Transformers perform well
in practice and whether they have any properties that enable them to generalize
better than recurrent models. In this work, we conduct an extensive empirical
study on Boolean functions to demonstrate the following: (i) Random
Transformers are relatively more biased towards functions of low sensitivity.
(ii) When trained on Boolean functions, both Transformers and LSTMs prioritize
learning functions of low sensitivity, with Transformers ultimately converging
to functions of lower sensitivity. (iii) On sparse Boolean functions which have
low sensitivity, we find that Transformers generalize near perfectly even in
the presence of noisy labels whereas LSTMs overfit and achieve poor
generalization accuracy. Overall, our results provide strong quantifiable
evidence that suggests differences in the inductive biases of Transformers and
recurrent models which may help explain Transformer's effective generalization
performance despite relatively limited expressiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimedia Generative Script Learning for Task Planning <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.12306v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.12306v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyun Wang, Manling Li, Hou Pong Chan, Lifu Huang, Julia Hockenmaier, Girish Chowdhary, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Goal-oriented generative script learning aims to generate subsequent steps to
reach a particular goal, which is an essential task to assist robots or humans
in performing stereotypical activities. An important aspect of this process is
the ability to capture historical states visually, which provides detailed
information that is not covered by text and will guide subsequent steps.
Therefore, we propose a new task, Multimedia Generative Script Learning, to
generate subsequent steps by tracking historical states in both text and vision
modalities, as well as presenting the first benchmark containing 5,652 tasks
and 79,089 multimedia steps. This task is challenging in three aspects: the
multimedia challenge of capturing the visual states in images, the induction
challenge of performing unseen tasks, and the diversity challenge of covering
different information in individual steps. We propose to encode visual state
changes through a selective multimedia encoder to address the multimedia
challenge, transfer knowledge from previously observed tasks using a
retrieval-augmented decoder to overcome the induction challenge, and further
present distinct information at each step by optimizing a diversity-oriented
contrastive learning objective. We define metrics to evaluate both generation
and inductive quality. Experiment results demonstrate that our approach
significantly outperforms strong baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, Accepted by Findings of the Association for Computational
  Linguistics: ACL 2023, Code and Resources at
  https://github.com/EagleW/Multimedia-Generative-Script-Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Biomedical Language Models are Robust to Sub-optimal Tokenization <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.17649v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.17649v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bernal Jiménez Gutiérrez, Huan Sun, Yu Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As opposed to general English, many concepts in biomedical terminology have
been designed in recent history by biomedical professionals with the goal of
being precise and concise. This is often achieved by concatenating meaningful
biomedical morphemes to create new semantic units. Nevertheless, most modern
biomedical language models (LMs) are pre-trained using standard domain-specific
tokenizers derived from large scale biomedical corpus statistics without
explicitly leveraging the agglutinating nature of biomedical language. In this
work, we first find that standard open-domain and biomedical tokenizers are
largely unable to segment biomedical terms into meaningful components.
Therefore, we hypothesize that using a tokenizer which segments biomedical
terminology more accurately would enable biomedical LMs to improve their
performance on downstream biomedical NLP tasks, especially ones which involve
biomedical terms directly such as named entity recognition (NER) and entity
linking. Surprisingly, we find that pre-training a biomedical LM using a more
accurate biomedical tokenizer does not improve the entity representation
quality of a language model as measured by several intrinsic and extrinsic
measures such as masked language modeling prediction (MLM) accuracy as well as
NER and entity linking performance. These quantitative findings, along with a
case study which explores entity representation quality more directly, suggest
that the biomedical pre-training process is quite robust to instances of
sub-optimal tokenization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>BioNLP @ ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Testing of Detection Tools for AI-Generated Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15666v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15666v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debora Weber-Wulff, Alla Anohina-Naumeca, Sonja Bjelobaba, Tomáš Foltýnek, Jean Guerrero-Dib, Olumide Popoola, Petr Šigut, Lorna Waddington
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in generative pre-trained transformer large language models
have emphasised the potential risks of unfair use of artificial intelligence
(AI) generated content in an academic environment and intensified efforts in
searching for solutions to detect such content. The paper examines the general
functionality of detection tools for artificial intelligence generated text and
evaluates them based on accuracy and error type analysis. Specifically, the
study seeks to answer research questions about whether existing detection tools
can reliably differentiate between human-written text and ChatGPT-generated
text, and whether machine translation and content obfuscation techniques affect
the detection of AI-generated text. The research covers 12 publicly available
tools and two commercial systems (Turnitin and PlagiarismCheck) that are widely
used in the academic setting. The researchers conclude that the available
detection tools are neither accurate nor reliable and have a main bias towards
classifying the output as human-written rather than detecting AI-generated
text. Furthermore, content obfuscation techniques significantly worsen the
performance of tools. The study makes several significant contributions. First,
it summarises up-to-date similar scientific and non-scientific efforts in the
field. Second, it presents the result of one of the most comprehensive tests
conducted so far, based on a rigorous research methodology, an original
document set, and a broad coverage of tools. Third, it discusses the
implications and drawbacks of using detection tools for AI-generated text in
academic settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 13 figures and 10 tables, and an appendix with 18 figures.
  Submitted to the International Journal for Educational Integrity</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BLEURT Has Universal Translations: An Analysis of Automatic Metrics by
  Minimum Risk Training <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03131v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03131v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Yan, Tao Wang, Chengqi Zhao, Shujian Huang, Jiajun Chen, Mingxuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic metrics play a crucial role in machine translation. Despite the
widespread use of n-gram-based metrics, there has been a recent surge in the
development of pre-trained model-based metrics that focus on measuring sentence
semantics. However, these neural metrics, while achieving higher correlations
with human evaluations, are often considered to be black boxes with potential
biases that are difficult to detect. In this study, we systematically analyze
and compare various mainstream and cutting-edge automatic metrics from the
perspective of their guidance for training machine translation systems. Through
Minimum Risk Training (MRT), we find that certain metrics exhibit robustness
defects, such as the presence of universal adversarial translations in BLEURT
and BARTScore. In-depth analysis suggests two main causes of these robustness
deficits: distribution biases in the training datasets, and the tendency of the
metric paradigm. By incorporating token-level constraints, we enhance the
robustness of evaluation metrics, which in turn leads to an improvement in the
performance of machine translation systems. Codes are available at
\url{https://github.com/powerpuffpomelo/fairseq_mrt}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2023 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Syntax and Semantics Meet in the "Middle": Probing the Syntax-Semantics
  Interface of LMs Through Agentivity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18185v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18185v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lindia Tjuatja, Emmy Liu, Lori Levin, Graham Neubig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models have prompted researchers to examine
their abilities across a variety of linguistic tasks, but little has been done
to investigate how models handle the interactions in meaning across words and
larger syntactic forms -- i.e. phenomena at the intersection of syntax and
semantics. We present the semantic notion of agentivity as a case study for
probing such interactions. We created a novel evaluation dataset by utilitizing
the unique linguistic properties of a subset of optionally transitive English
verbs. This dataset was used to prompt varying sizes of three model classes to
see if they are sensitive to agentivity at the lexical level, and if they can
appropriately employ these word-level priors given a specific syntactic
context. Overall, GPT-3 text-davinci-003 performs extremely well across all
experiments, outperforming all other models tested by far. In fact, the results
are even better correlated with human judgements than both syntactic and
semantic corpus statistics. This suggests that LMs may potentially serve as
more useful tools for linguistic annotation, theory testing, and discovery than
select corpora for certain tasks. Code is available at
https://github.com/lindiatjuatja/lm_sem
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Testing the Predictions of Surprisal Theory in 11 Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03667v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03667v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Gotlieb Wilcox, Tiago Pimentel, Clara Meister, Ryan Cotterell, Roger P. Levy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental result in psycholinguistics is that less predictable words take
a longer time to process. One theoretical explanation for this finding is
Surprisal Theory (Hale, 2001; Levy, 2008), which quantifies a word's
predictability as its surprisal, i.e. its negative log-probability given a
context. While evidence supporting the predictions of Surprisal Theory have
been replicated widely, most have focused on a very narrow slice of data:
native English speakers reading English texts. Indeed, no comprehensive
multilingual analysis exists. We address this gap in the current literature by
investigating the relationship between surprisal and reading times in eleven
different languages, distributed across five language families. Deriving
estimates from language models trained on monolingual and multilingual corpora,
we test three predictions associated with surprisal theory: (i) whether
surprisal is predictive of reading times; (ii) whether expected surprisal, i.e.
contextual entropy, is predictive of reading times; (iii) and whether the
linking function between surprisal and reading times is linear. We find that
all three predictions are borne out crosslinguistically. By focusing on a more
diverse set of languages, we argue that these results offer the most robust
link to-date between information theory and incremental language processing
across languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is a pre-MIT Press publication version of the paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Chat<span class="highlight-title">GPT</span> pass the Vietnamese National High School Graduation
  Examination? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09170v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09170v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan-Quy Dao, Ngoc-Bich Le, Xuan-Dung Phan, Bac-Bien Ngo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research article highlights the potential of AI-powered chatbots in
education and presents the results of using ChatGPT, a large language model, to
complete the Vietnamese National High School Graduation Examination (VNHSGE).
The study dataset included 30 essays in the literature test case and 1,700
multiple-choice questions designed for other subjects. The results showed that
ChatGPT was able to pass the examination with an average score of 6-7,
demonstrating the technology's potential to revolutionize the educational
landscape. The analysis of ChatGPT performance revealed its proficiency in a
range of subjects, including mathematics, English, physics, chemistry, biology,
history, geography, civic education, and literature, which suggests its
potential to provide effective support for learners. However, further research
is needed to assess ChatGPT performance on more complex exam questions and its
potential to support learners in different contexts. As technology continues to
evolve and improve, we can expect to see the use of AI tools like ChatGPT
become increasingly common in educational settings, ultimately enhancing the
educational experience for both students and educators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 13 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inductive Relation Prediction from Relational Paths and Context with
  Hierarchical <span class="highlight-title">Transformer</span>s <span class="chip">ICASSP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.00215v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.00215v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaang Li, Quan Wang, Zhendong Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation prediction on knowledge graphs (KGs) is a key research topic.
Dominant embedding-based methods mainly focus on the transductive setting and
lack the inductive ability to generalize to new entities for inference.
Existing methods for inductive reasoning mostly mine the connections between
entities, i.e., relational paths, without considering the nature of head and
tail entities contained in the relational context. This paper proposes a novel
method that captures both connections between entities and the intrinsic nature
of entities, by simultaneously aggregating RElational Paths and cOntext with a
unified hieRarchical Transformer framework, namely REPORT. REPORT relies solely
on relation semantics and can naturally generalize to the fully-inductive
setting, where KGs for training and inference have no common entities. In the
experiments, REPORT performs consistently better than all baselines on almost
all the eight version subsets of two fully-inductive datasets. Moreover. REPORT
is interpretable by providing each element's contribution to the prediction
results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2023 (Oral). The code is available at:
  https://github.com/JiaangL/REPORT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Text Embedding Space Generation Using Generative
  Adversarial Networks for Text Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.17181v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.17181v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun-Min Lee, Tae-Bin Ha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Adversarial Networks (GAN) is a model for data synthesis, which
creates plausible data through the competition of generator and discriminator.
Although GAN application to image synthesis is extensively studied, it has
inherent limitations to natural language generation. Because natural language
is composed of discrete tokens, a generator has difficulty updating its
gradient through backpropagation; therefore, most text-GAN studies generate
sentences starting with a random token based on a reward system. Thus, the
generators of previous studies are pre-trained in an autoregressive way before
adversarial training, causing data memorization that synthesized sentences
reproduce the training data. In this paper, we synthesize sentences using a
framework similar to the original GAN. More specifically, we propose Text
Embedding Space Generative Adversarial Networks (TESGAN) which generate
continuous text embedding spaces instead of discrete tokens to solve the
gradient backpropagation problem. Furthermore, TESGAN conducts unsupervised
learning which does not directly refer to the text of the training data to
overcome the data memorization issue. By adopting this novel method, TESGAN can
synthesize new sentences, showing the potential of unsupervised learning for
text synthesis. We expect to see extended research combining Large Language
Models with a new perspective of viewing text as an continuous space.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ValiTex -- a unified validation framework for computational text-based
  measures of social science constructs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02863v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02863v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Birkenmaier, Clemens Lechner, Claudia Wagner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Guidance on how to validate computational text-based measures of social
science constructs is fragmented. Whereas scholars are generally acknowledging
the importance of validating their text-based measures, they often lack common
terminology and a unified framework to do so. This paper introduces a new
validation framework called ValiTex, designed to assist scholars to measure
social science constructs based on textual data. The framework draws on a
long-established tradition within psychometrics while extending the framework
for the purpose of computational text analysis. ValiTex consists of two
components, a conceptual model, and a dynamic checklist. Whereas the conceptual
model provides a general structure along distinct phases on how to approach
validation, the dynamic checklist defines specific validation steps and
provides guidance on which steps might be considered recommendable (i.e.,
providing relevant and necessary validation evidence) or optional (i.e., useful
for providing additional supporting validation evidence. The utility of the
framework is demonstrated by applying it to a use case of detecting sexism from
social media data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mars: Modeling Context & State Representations with Contrastive Learning
  for End-to-End Task-Oriented Dialog <span class="chip">ACL2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.08917v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.08917v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haipeng Sun, Junwei Bao, Youzheng Wu, Xiaodong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional end-to-end task-oriented dialog systems first convert dialog
context into belief state and action state before generating the system
response. The system response performance is significantly affected by the
quality of the belief state and action state. We first explore what dialog
context representation is beneficial to improving the quality of the belief
state and action state, which further enhances the generated response quality.
To tackle our exploration, we propose Mars, an end-to-end task-oriented dialog
system with two contrastive learning strategies to model the relationship
between dialog context and belief/action state representations. Empirical
results show dialog context representations, which are more different from
semantic state representations, are more conducive to multi-turn task-oriented
dialog. Moreover, our proposed Mars achieves state-of-the-art performance on
the MultiWOZ 2.0, CamRest676, and CrossWOZ.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of ACL2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Contrastive Decoding: Open-ended Text Generation as Optimization <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.15097v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.15097v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, Mike Lewis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a language model (LM), maximum probability is a poor decoding objective
for open-ended generation, because it produces short and repetitive text. On
the other hand, sampling can often produce incoherent text that drifts from the
original topics. We propose contrastive decoding (CD), a reliable decoding
approach that optimizes a contrastive objective subject to a plausibility
constraint. The contrastive objective returns the difference between the
likelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM
(called the amateur, e.g. OPT-125M), and the constraint ensures that the
outputs are plausible. CD is inspired by the fact that the failures of larger
LMs (e.g., repetition, incoherence) are even more prevalent in smaller LMs, and
that this difference signals which texts should be preferred. CD requires zero
additional training, and produces higher quality text than decoding from the
larger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and
significantly outperforms four strong decoding algorithms (e.g., nucleus,
top-k) in automatic and human evaluations across wikipedia, news and story
domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main conference long paper at ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ranking with Long-Term Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kianté Brantley, Zhichong Fang, Sarah Dean, Thorsten Joachims
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The feedback that users provide through their choices (e.g., clicks,
purchases) is one of the most common types of data readily available for
training search and recommendation algorithms. However, myopically training
systems based on choice data may only improve short-term engagement, but not
the long-term sustainability of the platform and the long-term benefits to its
users, content providers, and other stakeholders. In this paper, we thus
develop a new framework in which decision makers (e.g., platform operators,
regulators, users) can express long-term goals for the behavior of the platform
(e.g., fairness, revenue distribution, legal requirements). These goals take
the form of exposure or impact targets that go well beyond individual sessions,
and we provide new control-based algorithms to achieve these goals. In
particular, the controllers are designed to achieve the stated long-term goals
with minimum impact on short-term engagement. Beyond the principled theoretical
derivation of the controllers, we evaluate the algorithms on both synthetic and
real-world data. While all controllers perform well, we find that they provide
interesting trade-offs in efficiency, robustness, and the ability to plan
ahead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fairness and Diversity in Recommender Systems: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuying Zhao, Yu Wang, Yunchao Liu, Xueqi Cheng, Charu Aggarwal, Tyler Derr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are effective tools for mitigating information overload
and have seen extensive applications across various domains. However, the
single focus on utility goals proves to be inadequate in addressing real-world
concerns, leading to increasing attention to fairness-aware and diversity-aware
recommender systems. While most existing studies explore fairness and diversity
independently, we identify strong connections between these two domains. In
this survey, we first discuss each of them individually and then dive into
their connections. Additionally, motivated by the concepts of user-level and
item-level fairness, we broaden the understanding of diversity to encompass not
only the item level but also the user level. With this expanded perspective on
user and item-level diversity, we re-interpret fairness studies from the
viewpoint of diversity. This fresh perspective enhances our understanding of
fairness-related work and paves the way for potential future research
directions. Papers discussed in this survey along with public code links are
available at
https://github.com/YuyingZhao/Awesome-Fairness-and-Diversity-Papers-in-Recommender-Systems .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InPars Toolkit: A Unified and Reproducible Synthetic Data Generation
  Pipeline for Neural Information Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Abonizio, Luiz Bonifacio, Vitor Jeronymo, Roberto Lotufo, Jakub Zavrel, Rodrigo Nogueira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has explored Large Language Models (LLMs) to overcome the lack of
training data for Information Retrieval (IR) tasks. The generalization
abilities of these models have enabled the creation of synthetic in-domain data
by providing instructions and a few examples on a prompt. InPars and
Promptagator have pioneered this approach and both methods have demonstrated
the potential of using LLMs as synthetic data generators for IR tasks. This
makes them an attractive solution for IR tasks that suffer from a lack of
annotated data. However, the reproducibility of these methods was limited,
because InPars' training scripts are based on TPUs -- which are not widely
accessible -- and because the code for Promptagator was not released and its
proprietary LLM is not publicly accessible. To fully realize the potential of
these methods and make their impact more widespread in the research community,
the resources need to be accessible and easy to reproduce by researchers and
practitioners. Our main contribution is a unified toolkit for end-to-end
reproducible synthetic data generation research, which includes generation,
filtering, training and evaluation. Additionally, we provide an interface to IR
libraries widely used by the community and support for GPU. Our toolkit not
only reproduces the InPars method and partially reproduces Promptagator, but
also provides a plug-and-play functionality allowing the use of different LLMs,
exploring filtering methods and finetuning various reranker models on the
generated data. We also made available all the synthetic data generated in this
work for the 18 different datasets in the BEIR benchmark which took more than
2,000 GPU hours to be generated as well as the reranker models finetuned on the
synthetic data. Code and data are available at
https://github.com/zetaalphavector/InPars
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Semi-Automated Solution Approach Selection Tool for Any Use Case via
  Scopus and OpenAI: a Case Study for AI/ML in Oncology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deniz Kenan Kılıç, Alex Elkjær Vasegaard, Aurélien Desoeuvres, Peter Nielsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In today's vast literature landscape, a manual review is very time-consuming.
To address this challenge, this paper proposes a semi-automated tool for
solution method review and selection. It caters to researchers, practitioners,
and decision-makers while serving as a benchmark for future work. The tool
comprises three modules: (1) paper selection and scoring, using a keyword
selection scheme to query Scopus API and compute relevancy; (2) solution method
extraction in papers utilizing OpenAI API; (3) sensitivity analysis and
post-analyzes. It reveals trends, relevant papers, and methods. AI in the
oncology case study and several use cases are presented with promising results,
comparing the tool to manual ground truth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper is under review in Expert Systems with Applications,
  Elsevier</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Alleviating Matthew Effect of Offline Reinforcement Learning in
  Interactive Recommendation <span class="chip">SIGIR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chongming Gao, Kexin Huang, Jiawei Chen, Yuan Zhang, Biao Li, Peng Jiang, Shiqi Wang, Zhong Zhang, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline reinforcement learning (RL), a technology that offline learns a
policy from logged data without the need to interact with online environments,
has become a favorable choice in decision-making processes like interactive
recommendation. Offline RL faces the value overestimation problem. To address
it, existing methods employ conservatism, e.g., by constraining the learned
policy to be close to behavior policies or punishing the rarely visited
state-action pairs. However, when applying such offline RL to recommendation,
it will cause a severe Matthew effect, i.e., the rich get richer and the poor
get poorer, by promoting popular items or categories while suppressing the less
popular ones. It is a notorious issue that needs to be addressed in practical
recommender systems.
  In this paper, we aim to alleviate the Matthew effect in offline RL-based
recommendation. Through theoretical analyses, we find that the conservatism of
existing methods fails in pursuing users' long-term satisfaction. It inspires
us to add a penalty term to relax the pessimism on states with high entropy of
the logging policy and indirectly penalizes actions leading to less diverse
states. This leads to the main technical contribution of the work: Debiased
model-based Offline RL (DORL) method. Experiments show that DORL not only
captures user interests well but also alleviates the Matthew effect. The
implementation is available via https://github.com/chongminggao/DORL-codes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGIR 2023 Full Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Large Language Model for Graph Data Understanding in Online
  Job Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Likang Wu, Zhaopeng Qiu, Zhi Zheng, Hengshu Zhu, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have revolutionized natural language processing
tasks, demonstrating their exceptional capabilities in various domains.
However, their potential for behavior graph understanding in job
recommendations remains largely unexplored. This paper focuses on unveiling the
capability of large language models in understanding behavior graphs and
leveraging this understanding to enhance recommendations in online recruitment,
including the promotion of out-of-distribution (OOD) application. We present a
novel framework that harnesses the rich contextual information and semantic
representations provided by large language models to analyze behavior graphs
and uncover underlying patterns and relationships. Specifically, we propose a
meta-path prompt constructor that leverages LLM recommender to understand
behavior graphs for the first time and design a corresponding path augmentation
module to alleviate the prompt bias introduced by path-based sequence input. By
leveraging this capability, our framework enables personalized and accurate job
recommendations for individual users. We evaluate the effectiveness of our
approach on a comprehensive dataset and demonstrate its ability to improve the
relevance and quality of recommended quality. This research not only sheds
light on the untapped potential of large language models but also provides
valuable insights for developing advanced recommendation systems in the
recruitment market. The findings contribute to the growing field of natural
language processing and offer practical implications for enhancing job search
experiences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Counterfactual Explanation for Fairness in Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04386v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04386v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangmeng Wang, Qian Li, Dianer Yu, Qing Li, Guandong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fairness-aware recommendation eliminates discrimination issues to build
trustworthy recommendation systems.Explaining the causes of unfair
recommendations is critical, as it promotes fairness diagnostics, and thus
secures users' trust in recommendation models. Existing fairness explanation
methods suffer high computation burdens due to the large-scale search space and
the greedy nature of the explanation search process. Besides, they perform
score-based optimizations with continuous values, which are not applicable to
discrete attributes such as gender and race. In this work, we adopt the novel
paradigm of counterfactual explanation from causal inference to explore how
minimal alterations in explanations change model fairness, to abandon the
greedy search for explanations. We use real-world attributes from Heterogeneous
Information Networks (HINs) to empower counterfactual reasoning on discrete
attributes. We propose a novel Counterfactual Explanation for Fairness
(CFairER) that generates attribute-level counterfactual explanations from HINs
for recommendation fairness. Our CFairER conducts off-policy reinforcement
learning to seek high-quality counterfactual explanations, with an attentive
action pruning reducing the search space of candidate counterfactuals. The
counterfactual explanations help to provide rational and proximate explanations
for model fairness, while the attentive action pruning narrows the search space
of attributes. Extensive experiments demonstrate our proposed model can
generate faithful explanations while maintaining favorable recommendation
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Neural Graph Collaborative Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangmeng Wang, Qian Li, Dianer Yu, Wei Huang, Guandong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph collaborative filtering (GCF) has gained considerable attention in
recommendation systems by leveraging graph learning techniques to enhance
collaborative filtering (CF) models. One classical approach in GCF is to learn
user and item embeddings by modeling complex graph relations and utilizing
these embeddings for CF models. However, the quality of the embeddings
significantly impacts the recommendation performance of GCF models. In this
paper, we argue that existing graph learning methods are insufficient in
generating satisfactory embeddings for CF models. This is because they
aggregate neighboring node messages directly, which can result in incorrect
estimations of user-item correlations. To overcome this limitation, we propose
a novel approach that incorporates causal modeling to explicitly encode the
causal effects of neighboring nodes on the target node. This approach enables
us to identify spurious correlations and uncover the root causes of user
preferences. We introduce Causal Neural Graph Collaborative Filtering (CNGCF),
the first causality-aware graph learning framework for CF. CNGCF integrates
causal modeling into the graph representation learning process, explicitly
coupling causal effects between node pairs into the core message-passing
process of graph learning. As a result, CNGCF yields causality-aware embeddings
that promote robust recommendations. Our extensive experiments demonstrate that
CNGCF provides precise recommendations that align with user preferences.
Therefore, our proposed framework can address the limitations of existing GCF
models and offer a more effective solution for recommendation systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Contrastive Learning with Multi-Objective for Personalized Product
  Retrieval in Taobao Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longbin Li, Chao Zhang, Sen Li, Yun Zhong, Qingwen Liu, Xiaoyi Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In e-commerce search, personalized retrieval is a crucial technique for
improving user shopping experience. Recent works in this domain have achieved
significant improvements by the representation learning paradigm, e.g.,
embedding-based retrieval (EBR) and collaborative filtering (CF). EBR methods
do not sufficiently exploit the useful collaborative signal and are difficult
to learn the representations of long-tail item well. Graph-based CF methods
improve personalization by modeling collaborative signal within the user click
graph. However, existing Graph-based methods ignore user's multiple behaviours,
such as click/purchase and the relevance constraint between user behaviours and
items.In this paper, we propose a Graph Contrastive Learning with
Multi-Objective (GCL-MO) collaborative filtering model, which solves the
problems of weak relevance and incomplete personalization in e-commerce search.
Specifically, GCL-MO builds a homogeneous graph of items and then optimizes a
multi-objective function of personalization and relevance. Moreover, we propose
a modified contrastive loss for multi-objectives graph learning, which avoids
the mutual suppression among positive samples and thus improves the
generalization and robustness of long-tail item representations. These learned
item embeddings are then used for personalized retrieval by constructing an
efficient offline-to-online inverted table. GCL-MO outperforms the online
collaborative filtering baseline in both offline/online experimental metrics
and shows a significant improvement in the online A/B testing of Taobao search.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Ad Procurement in Non-stationary Autobidding Worlds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jason Cheuk Nam Liang, Haihao Lu, Baoyu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today's online advertisers procure digital ad impressions through interacting
with autobidding platforms: advertisers convey high level procurement goals via
setting levers such as budget, target return-on-investment, max cost per click,
etc.. Then ads platforms subsequently procure impressions on advertisers'
behalf, and report final procurement conversions (e.g. click) to advertisers.
In practice, advertisers may receive minimal information on platforms'
procurement details, and procurement outcomes are subject to non-stationary
factors like seasonal patterns, occasional system corruptions, and market
trends which make it difficult for advertisers to optimize lever decisions
effectively. Motivated by this, we present an online learning framework that
helps advertisers dynamically optimize ad platform lever decisions while
subject to general long-term constraints in a realistic bandit feedback
environment with non-stationary procurement outcomes. In particular, we
introduce a primal-dual algorithm for online decision making with
multi-dimension decision variables, bandit feedback and long-term uncertain
constraints. We show that our algorithm achieves low regret in many worlds when
procurement outcomes are generated through procedures that are stochastic,
adversarial, adversarially corrupted, periodic, and ergodic, respectively,
without having to know which procedure is the ground truth. Finally, we
emphasize that our proposed algorithm and theoretical results extend beyond the
applications of online advertising.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collective Privacy Recovery: Data-sharing Coordination via Decentralized
  Artificial Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.05995v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.05995v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evangelos Pournaras, Mark Christopher Ballandies, Stefano Bennati, Chien-fei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collective privacy loss becomes a colossal problem, an emergency for personal
freedoms and democracy. But, are we prepared to handle personal data as scarce
resource and collectively share data under the doctrine: as little as possible,
as much as necessary? We hypothesize a significant privacy recovery if a
population of individuals, the data collective, coordinates to share minimum
data for running online services with the required quality. Here we show how to
automate and scale-up complex collective arrangements for privacy recovery
using decentralized artificial intelligence. For this, we compare for first
time attitudinal, intrinsic, rewarded and coordinated data sharing in a
rigorous living-lab experiment of high realism involving >27,000 real data
disclosures. Using causal inference and cluster analysis, we differentiate
criteria predicting privacy and five key data-sharing behaviors. Strikingly,
data-sharing coordination proves to be a win-win for all: remarkable privacy
recovery for people with evident costs reduction for service providers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Contains Supplementary Information</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UNIQORN: Unified Question Answering over RDF Knowledge Graphs and
  Natural Language Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.08614v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.08614v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumajit Pramanik, Jesujoba Oluwadara Alabi, Rishiraj Saha Roy, Gerhard Weikum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering over knowledge graphs and other RDF data has been greatly
advanced, with a number of good techniques providing crisp answers for natural
language questions or telegraphic queries. Some of these systems incorporate
textual sources as additional evidence for the answering process, but cannot
compute answers that are present in text alone. Conversely, techniques from the
IR and NLP communities have addressed QA over text, but such systems barely
utilize semantic data and knowledge. This paper presents a method for complex
questions that can seamlessly operate over a mixture of RDF datasets and text
corpora, or individual sources, in a unified framework. Our method, called
UNIQORN, builds a context graph on-the-fly, by retrieving question-relevant
evidences from the RDF data and/or a text corpus, using fine-tuned BERT models.
The resulting graph typically contains all question-relevant evidences but also
a lot of noise. UNIQORN copes with this input by a graph algorithm for Group
Steiner Trees, that identifies the best answer candidates in the context graph.
Experimental results on several benchmarks of complex questions with multiple
entities and relations, show that UNIQORN significantly outperforms
state-of-the-art methods for heterogeneous QA. The graph-based methodology
provides user-interpretable evidence for the complete answering process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Consistent Collaborative Filtering via Tensor Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.11936v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.11936v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiwen Zhao, Charles Crissman, Guillermo R Sapiro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative filtering is the de facto standard for analyzing users'
activities and building recommendation systems for items. In this work we
develop Sliced Anti-symmetric Decomposition (SAD), a new model for
collaborative filtering based on implicit feedback. In contrast to traditional
techniques where a latent representation of users (user vectors) and items
(item vectors) are estimated, SAD introduces one additional latent vector to
each item, using a novel three-way tensor view of user-item interactions. This
new vector extends user-item preferences calculated by standard dot products to
general inner products, producing interactions between items when evaluating
their relative preferences. SAD reduces to state-of-the-art (SOTA)
collaborative filtering models when the vector collapses to 1, while in this
paper we allow its value to be estimated from data. Allowing the values of the
new item vector to be different from 1 has profound implications. It suggests
users may have nonlinear mental models when evaluating items, allowing the
existence of cycles in pairwise comparisons. We demonstrate the efficiency of
SAD in both simulated and real world datasets containing over 1M user-item
interactions. By comparing with seven SOTA collaborative filtering models with
implicit feedbacks, SAD produces the most consistent personalized preferences,
in the meanwhile maintaining top-level of accuracy in personalized
recommendations. We release the model and inference algorithms in a Python
library https://github.com/apple/ml-sad.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image
  Alignment with Iterative VQA Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaskirat Singh, Liang Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of text-conditioned image generation has made unparalleled progress
with the recent advent of latent diffusion models. While remarkable, as the
complexity of given text input increases, the state-of-the-art diffusion models
may still fail in generating images which accurately convey the semantics of
the given prompt. Furthermore, it has been observed that such misalignments are
often left undetected by pretrained multi-modal models such as CLIP. To address
these problems, in this paper we explore a simple yet effective decompositional
approach towards both evaluation and improvement of text-to-image alignment. In
particular, we first introduce a Decompositional-Alignment-Score which given a
complex prompt decomposes it into a set of disjoint assertions. The alignment
of each assertion with generated images is then measured using a VQA model.
Finally, alignment scores for different assertions are combined aposteriori to
give the final text-to-image alignment score. Experimental analysis reveals
that the proposed alignment metric shows significantly higher correlation with
human ratings as opposed to traditional CLIP, BLIP scores. Furthermore, we also
find that the assertion level alignment scores provide a useful feedback which
can then be used in a simple iterative procedure to gradually increase the
expression of different assertions in the final image outputs. Human user
studies indicate that the proposed approach surpasses previous state-of-the-art
by 8.7% in overall text-to-image alignment accuracy. Project page for our paper
is available at https://1jsingh.github.io/divide-evaluate-and-refine
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emotion-Conditioned Melody Harmonization with Hierarchical Variational
  Autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03718v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03718v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shulei Ji, Xinyu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing melody harmonization models have made great progress in improving
the quality of generated harmonies, but most of them ignored the emotions
beneath the music. Meanwhile, the variability of harmonies generated by
previous methods is insufficient. To solve these problems, we propose a novel
LSTM-based Hierarchical Variational Auto-Encoder (LHVAE) to investigate the
influence of emotional conditions on melody harmonization, while improving the
quality of generated harmonies and capturing the abundant variability of chord
progressions. Specifically, LHVAE incorporates latent variables and emotional
conditions at different levels (piece- and bar-level) to model the global and
local music properties. Additionally, we introduce an attention-based melody
context vector at each step to better learn the correspondence between melodies
and harmonies. Objective experimental results show that our proposed model
outperforms other LSTM-based models. Through subjective evaluation, we conclude
that only altering the type of chord hardly changes the overall emotion of the
music. The qualitative analysis demonstrates the ability of our model to
generate variable harmonies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE SMC 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-07-09T00:00:00Z">2023-07-09</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">29</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Essay Scoring in Argumentative Writing: De<span class="highlight-title">BERT</span>eachingAssistant 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yann Hicke, Tonghua Tian, Karan Jha, Choong Hee Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated Essay scoring has been explored as a research and industry problem
for over 50 years. It has drawn a lot of attention from the NLP community
because of its clear educational value as a research area that can engender the
creation of valuable time-saving tools for educators around the world. Yet,
these tools are generally focused on detecting good grammar, spelling mistakes,
and organization quality but tend to fail at incorporating persuasiveness
features in their final assessment. The responsibility to give actionable
feedback to the student to improve the strength of their arguments is left
solely on the teacher's shoulders. In this work, we present a transformer-based
architecture capable of achieving above-human accuracy in annotating
argumentative writing discourse elements for their persuasiveness quality and
we expand on planned future work investigating the explainability of our model
so that actionable feedback can be offered to the student and thus potentially
enable a partnership between the teacher's advice and the machine's advice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Augmenters at SemEval-2023 Task 1: Enhancing CLIP in Handling
  Compositionality and Ambiguity for Zero-Shot Visual WSD through <span class="highlight-title">Prompt</span>
  Augmentation and Text-To-Image Diffusion <span class="chip">SemEval-2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie S. Li, Yow-Ting Shiue, Yong-Siang Shih, Jonas Geiping
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes our zero-shot approaches for the Visual Word Sense
Disambiguation (VWSD) Task in English. Our preliminary study shows that the
simple approach of matching candidate images with the phrase using CLIP suffers
from the many-to-many nature of image-text pairs. We find that the CLIP text
encoder may have limited abilities in capturing the compositionality in natural
language. Conversely, the descriptive focus of the phrase varies from instance
to instance. We address these issues in our two systems, Augment-CLIP and
Stable Diffusion Sampling (SD Sampling). Augment-CLIP augments the text prompt
by generating sentences that contain the context phrase with the help of large
language models (LLMs). We further explore CLIP models in other languages, as
the an ambiguous word may be translated into an unambiguous one in the other
language. SD Sampling uses text-to-image Stable Diffusion to generate multiple
images from the given phrase, increasing the likelihood that a subset of images
match the one that paired with the text.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 17th International Workshop on Semantic Evaluation
  (SemEval-2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessing the efficacy of large language models in generating accurate
  teacher responses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yann Hicke, Abhishek Masand, Wentao Guo, Tushaar Gangavarapu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  (Tack et al., 2023) organized the shared task hosted by the 18th Workshop on
Innovative Use of NLP for Building Educational Applications on generation of
teacher language in educational dialogues. Following the structure of the
shared task, in this study, we attempt to assess the generative abilities of
large language models in providing informative and helpful insights to
students, thereby simulating the role of a knowledgeable teacher. To this end,
we present an extensive evaluation of several benchmarking generative models,
including GPT-4 (few-shot, in-context learning), fine-tuned GPT-2, and
fine-tuned DialoGPT. Additionally, to optimize for pedagogical quality, we
fine-tuned the Flan-T5 model using reinforcement learning. Our experimental
findings on the Teacher-Student Chatroom Corpus subset indicate the efficacy of
GPT-4 over other fine-tuned models, measured using BERTScore and DialogRPT.
  We hypothesize that several dataset characteristics, including sampling,
representativeness, and dialog completeness, pose significant challenges to
fine-tuning, thus contributing to the poor generalizability of the fine-tuned
models. Finally, we note the need for these generative models to be evaluated
with a metric that relies not only on dialog coherence and matched language
modeling distribution but also on the model's ability to showcase pedagogical
skills.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chat<span class="highlight-title">GPT</span> in the Age of Generative AI and Large Language Models: A Concise
  <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salman Mohamadi, Ghulam Mujtaba, Ngan Le, Gianfranco Doretto, Donald A. Adjeroh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT is a large language model (LLM) created by OpenAI that has been
carefully trained on a large amount of data. It has revolutionized the field of
natural language processing (NLP) and has pushed the boundaries of LLM
capabilities. ChatGPT has played a pivotal role in enabling widespread public
interaction with generative artificial intelligence (GAI) on a large scale. It
has also sparked research interest in developing similar technologies and
investigating their applications and implications. In this paper, our primary
goal is to provide a concise survey on the current lines of research on ChatGPT
and its evolution. We considered both the glass box and black box views of
ChatGPT, encompassing the components and foundational elements of the
technology, as well as its applications, impacts, and implications. The glass
box approach focuses on understanding the inner workings of the technology, and
the black box approach embraces it as a complex system, and thus examines its
inputs, outputs, and effects. This paves the way for a comprehensive
exploration of the technology and provides a road map for further research and
experimentation. We also lay out essential foundational literature on LLMs and
GAI in general and their connection with ChatGPT. This overview sheds light on
existing and missing research lines in the emerging field of LLMs, benefiting
both public users and developers. Furthermore, the paper delves into the broad
spectrum of applications and significant concerns in fields such as education,
research, healthcare, finance, etc.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Coding at Scale: Design and Deployment of a Nationwide System
  for Normalizing Referrals in the Chilean Public Healthcare System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabián Villena, Matías Rojas, Felipe Arias, Jorge Pacheco, Paulina Vera, Jocelyn Dunstan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The disease coding task involves assigning a unique identifier from a
controlled vocabulary to each disease mentioned in a clinical document. This
task is relevant since it allows information extraction from unstructured data
to perform, for example, epidemiological studies about the incidence and
prevalence of diseases in a determined context. However, the manual coding
process is subject to errors as it requires medical personnel to be competent
in coding rules and terminology. In addition, this process consumes a lot of
time and energy, which could be allocated to more clinically relevant tasks.
These difficulties can be addressed by developing computational systems that
automatically assign codes to diseases. In this way, we propose a two-step
system for automatically coding diseases in referrals from the Chilean public
healthcare system. Specifically, our model uses a state-of-the-art NER model
for recognizing disease mentions and a search engine system based on
Elasticsearch for assigning the most relevant codes associated with these
disease mentions. The system's performance was evaluated on referrals manually
coded by clinical experts. Our system obtained a MAP score of 0.63 for the
subcategory level and 0.83 for the category level, close to the best-performing
models in the literature. This system could be a support tool for health
professionals, optimizing the coding and management process. Finally, to
guarantee reproducibility, we publicly release the code of our models and
experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAS Video-QA: Self-Adaptive Sampling for Efficient Video
  Question-Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Han, Hui Chen, Min-Yen Kan, Soujanya Poria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video question--answering is a fundamental task in the field of video
understanding. Although current vision--language models (VLMs) equipped with
Video Transformers have enabled temporal modeling and yielded superior results,
they are at the cost of huge computational power and thus too expensive to
deploy in real-time application scenarios. An economical workaround only
samples a small portion of frames to represent the main content of that video
and tune an image--text model on these sampled frames. Recent video
understanding models usually randomly sample a set of frames or clips,
regardless of internal correlations between their visual contents, nor their
relevance to the problem. We argue that such kinds of aimless sampling may omit
the key frames from which the correct answer can be deduced, and the situation
gets worse when the sampling sparsity increases, which always happens as the
video lengths increase. To mitigate this issue, we propose two frame sampling
strategies, namely the most domain frames (MDF) and most implied frames (MIF),
to maximally preserve those frames that are most likely vital to the given
questions. MDF passively minimizes the risk of key frame omission in a
bootstrap manner, while MIS actively searches key frames customized for each
video--question pair with the assistance of auxiliary models. The experimental
results on three public datasets from three advanced VLMs (CLIP, GIT and
All-in-one) demonstrate that our proposed strategies can boost the performance
for image--text pretrained models. The source codes pertaining to the method
proposed in this paper are publicly available at
https://github.com/declare-lab/sas-vqa.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Generative Large Language Models Perform ASR Error Correction? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rao Ma, Mengjie Qian, Potsawee Manakul, Mark Gales, Kate Knill
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ASR error correction continues to serve as an important part of
post-processing for speech recognition systems. Traditionally, these models are
trained with supervised training using the decoding results of the underlying
ASR system and the reference text. This approach is computationally intensive
and the model needs to be re-trained when switching the underlying ASR model.
Recent years have seen the development of large language models and their
ability to perform natural language processing tasks in a zero-shot manner. In
this paper, we take ChatGPT as an example to examine its ability to perform ASR
error correction in the zero-shot or 1-shot settings. We use the ASR N-best
list as model input and propose unconstrained error correction and N-best
constrained error correction methods. Results on a Conformer-Transducer model
and the pre-trained Whisper model show that we can largely improve the ASR
system performance with error correction using the powerful ChatGPT model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dream Content Discovery from Reddit with an Unsupervised Mixed-Method
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anubhab Das, Sanja Šćepanović, Luca Maria Aiello, Remington Mallett, Deirdre Barrett, Daniele Quercia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dreaming is a fundamental but not fully understood part of human experience
that can shed light on our thought patterns. Traditional dream analysis
practices, while popular and aided by over 130 unique scales and rating
systems, have limitations. Mostly based on retrospective surveys or lab
studies, they struggle to be applied on a large scale or to show the importance
and connections between different dream themes. To overcome these issues, we
developed a new, data-driven mixed-method approach for identifying topics in
free-form dream reports through natural language processing. We tested this
method on 44,213 dream reports from Reddit's r/Dreams subreddit, where we found
217 topics, grouped into 22 larger themes: the most extensive collection of
dream topics to date. We validated our topics by comparing it to the
widely-used Hall and van de Castle scale. Going beyond traditional scales, our
method can find unique patterns in different dream types (like nightmares or
recurring dreams), understand topic importance and connections, and observe
changes in collective dream experiences over time and around major events, like
the COVID-19 pandemic and the recent Russo-Ukrainian war. We envision that the
applications of our method will provide valuable insights into the intricate
nature of dreaming.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 6 figures, 4 tables, 4 pages of supplementary information</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Review</span> of feedback in Automated Essay Scoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        You-Jin Jong, Yong-Jin Kim, Ok-Chol Ri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The first automated essay scoring system was developed 50 years ago.
Automated essay scoring systems are developing into systems with richer
functions than the previous simple scoring systems. Its purpose is not only to
score essays but also as a learning tool to improve the writing skill of users.
Feedback is the most important aspect of making an automated essay scoring
system useful in real life. The importance of feedback was already emphasized
in the first AES system. This paper reviews research on feedback including
different feedback types and essay traits on automated essay scoring. We also
reviewed the latest case studies of the automated essay scoring system that
provides feedback.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 5 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards cross-language prosody transfer for dialog 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan E. Avila, Nigel G. Ward
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech-to-speech translation systems today do not adequately support use for
dialog purposes. In particular, nuances of speaker intent and stance can be
lost due to improper prosody transfer. We present an exploration of what needs
to be done to overcome this. First, we developed a data collection protocol in
which bilingual speakers re-enact utterances from an earlier conversation in
their other language, and used this to collect an English-Spanish corpus, so
far comprising 1871 matched utterance pairs. Second, we developed a simple
prosodic dissimilarity metric based on Euclidean distance over a broad set of
prosodic features. We then used these to investigate cross-language prosodic
differences, measure the likely utility of three simple baseline models, and
identify phenomena which will require more powerful modeling. Our findings
should inform future research on cross-language prosody and the design of
speech-to-speech translation systems capable of effective prosody transfer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Interspeech 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FILM: How can Few-Shot Image Classification Benefit from <span class="highlight-title">Pre-Train</span>ed
  Language Models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04114v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04114v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Jiang, Yunkai Dang, Dong Pang, Huishuai Zhang, Weiran Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot learning aims to train models that can be generalized to novel
classes with only a few samples. Recently, a line of works are proposed to
enhance few-shot learning with accessible semantic information from class
names. However, these works focus on improving existing modules such as visual
prototypes and feature extractors of the standard few-shot learning framework.
This limits the full potential use of semantic information. In this paper, we
propose a novel few-shot learning framework that uses pre-trained language
models based on contrastive learning. To address the challenge of alignment
between visual features and textual embeddings obtained from text-based
pre-trained language model, we carefully design the textual branch of our
framework and introduce a metric module to generalize the cosine similarity.
For better transferability, we let the metric module adapt to different
few-shot tasks and adopt MAML to train the model via bi-level optimization.
Moreover, we conduct extensive experiments on multiple benchmarks to
demonstrate the effectiveness of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Transport Posterior Alignment for Cross-lingual Semantic Parsing <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Sherborne, Tom Hosking, Mirella Lapata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-lingual semantic parsing transfers parsing capability from a
high-resource language (e.g., English) to low-resource languages with scarce
training data. Previous work has primarily considered silver-standard data
augmentation or zero-shot methods, however, exploiting few-shot gold data is
comparatively unexplored. We propose a new approach to cross-lingual semantic
parsing by explicitly minimizing cross-lingual divergence between probabilistic
latent variables using Optimal Transport. We demonstrate how this direct
guidance improves parsing from natural languages using fewer examples and less
training. We evaluate our method on two datasets, MTOP and MultiATIS++SQL,
establishing state-of-the-art results under a few-shot cross-lingual regime.
Ablation studies further reveal that our method improves performance even
without parallel input translations. In addition, we show that our model better
captures cross-lingual structure in the latent space to improve semantic
representation similarity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TACL 2023. Pre-MIT Press publication. 17 pages, 3
  figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge
  Graphs <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04090v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04090v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Allen Roush
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work within the Argument Mining community has shown the applicability
of Natural Language Processing systems for solving problems found within
competitive debate. One of the most important tasks within competitive debate
is for debaters to create high quality debate cases. We show that effective
debate cases can be constructed using constrained shortest path traversals on
Argumentative Semantic Knowledge Graphs. We study this potential in the context
of a type of American Competitive Debate, called Policy Debate, which already
has a large scale dataset targeting it called DebateSum. We significantly
improve upon DebateSum by introducing 53180 new examples, as well as further
useful metadata for every example, to the dataset. We leverage the txtai
semantic search and knowledge graph toolchain to produce and contribute 9
semantic knowledge graphs built on this dataset. We create a unique method for
evaluating which knowledge graphs are better in the context of producing policy
debate cases. A demo which automatically generates debate cases, along with all
other code and the Knowledge Graphs, are open-sourced and made available to the
public here: https://github.com/Hellisotherpeople/DebateKG
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, knife-edge reject from EACL 2023 and workshops, System
  Demonstration paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Personalized Reinforcement Learning Summarization Service for Learning
  Structure from Unstructured Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samira Ghodratnama, Amin Beheshti, Mehrdad Zakershahrak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exponential growth of textual data has created a crucial need for tools
that assist users in extracting meaningful insights. Traditional document
summarization approaches often fail to meet individual user requirements and
lack structure for efficient information processing. To address these
limitations, we propose Summation, a hierarchical personalized concept-based
summarization approach. It synthesizes documents into a concise hierarchical
concept map and actively engages users by learning and adapting to their
preferences. Using a Reinforcement Learning algorithm, Summation generates
personalized summaries for unseen documents on specific topics. This framework
enhances comprehension, enables effective navigation, and empowers users to
extract meaningful insights from large document collections aligned with their
unique requirements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2108.09443</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Supervised Adversarial Contrastive Learning for Emotion Recognition in
  Conversations <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01505v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01505v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dou Hu, Yinan Bao, Lingwei Wei, Wei Zhou, Songlin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting generalized and robust representations is a major challenge in
emotion recognition in conversations (ERC). To address this, we propose a
supervised adversarial contrastive learning (SACL) framework for learning
class-spread structured representations in a supervised manner. SACL applies
contrast-aware adversarial training to generate worst-case samples and uses
joint class-spread contrastive learning to extract structured representations.
It can effectively utilize label-level feature consistency and retain
fine-grained intra-class features. To avoid the negative impact of adversarial
perturbations on context-dependent data, we design a contextual adversarial
training (CAT) strategy to learn more diverse features from context and enhance
the model's context robustness. Under the framework with CAT, we develop a
sequence-based SACL-LSTM to learn label-consistent and context-robust features
for ERC. Experiments on three datasets show that SACL-LSTM achieves
state-of-the-art performance on ERC. Extended experiments prove the
effectiveness of SACL and CAT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, accepted by ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identity Construction in a Misogynist Incels Forum <span class="chip">WOAH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.15745v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.15745v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Miller Yoder, Chloe Perry, David West Brown, Kathleen M. Carley, Meredith L. Pruden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online communities of involuntary celibates (incels) are a prominent source
of misogynist hate speech. In this paper, we use quantitative text and network
analysis approaches to examine how identity groups are discussed on
incels-dot-is, the largest black-pilled incels forum. We find that this
community produces a wide range of novel identity terms and, while terms for
women are most common, mentions of other minoritized identities are increasing.
An analysis of the associations made with identity groups suggests an
essentialist ideology where physical appearance, as well as gender and racial
hierarchies, determine human value. We discuss implications for research into
automated misogynist hate speech detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Workshop on Online Abuse and Harms (WOAH) 2023; Minor edits to author
  names and abstracts in most recent version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Using Large Language Models to Simulate Multiple Humans and Replicate
  Human Subject Studies <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.10264v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.10264v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gati Aher, Rosa I. Arriaga, Adam Tauman Kalai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new type of test, called a Turing Experiment (TE), for
evaluating to what extent a given language model, such as GPT models, can
simulate different aspects of human behavior. A TE can also reveal consistent
distortions in a language model's simulation of a specific human behavior.
Unlike the Turing Test, which involves simulating a single arbitrary
individual, a TE requires simulating a representative sample of participants in
human subject research. We carry out TEs that attempt to replicate
well-established findings from prior studies. We design a methodology for
simulating TEs and illustrate its use to compare how well different language
models are able to reproduce classic economic, psycholinguistic, and social
psychology experiments: Ultimatum Game, Garden Path Sentences, Milgram Shock
Experiment, and Wisdom of Crowds. In the first three TEs, the existing findings
were replicated using recent models, while the last TE reveals a
"hyper-accuracy distortion" present in some language models (including ChatGPT
and GPT-4), which could affect downstream applications in education and the
arts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for oral presentation at International Conference on Machine
  Learning (ICML) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DataComp: In search of the next generation of multimodal <span class="highlight-title">dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.14108v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.14108v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, Ludwig Schmidt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal datasets are a critical component in recent breakthroughs such as
Stable Diffusion and GPT-4, yet their design does not receive the same research
attention as model architectures or training algorithms. To address this
shortcoming in the ML ecosystem, we introduce DataComp, a testbed for dataset
experiments centered around a new candidate pool of 12.8 billion image-text
pairs from Common Crawl. Participants in our benchmark design new filtering
techniques or curate new data sources and then evaluate their new dataset by
running our standardized CLIP training code and testing the resulting model on
38 downstream test sets. Our benchmark consists of multiple compute scales
spanning four orders of magnitude, which enables the study of scaling trends
and makes the benchmark accessible to researchers with varying resources. Our
baseline experiments show that the DataComp workflow leads to better training
sets. In particular, our best baseline, DataComp-1B, enables training a CLIP
ViT-L/14 from scratch to 79.2% zero-shot accuracy on ImageNet, outperforming
OpenAI's CLIP ViT-L/14 by 3.7 percentage points while using the same training
procedure and compute. We release DataComp and all accompanying code at
www.datacomp.ai.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Creativity of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.00008v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.00008v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgio Franceschelli, Mirco Musolesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are revolutionizing several areas of Artificial
Intelligence. One of the most remarkable applications is creative writing,
e.g., poetry or storytelling: the generated outputs are often of astonishing
quality. However, a natural question arises: can LLMs be really considered
creative? In this article we firstly analyze the development of LLMs under the
lens of creativity theories, investigating the key open questions and
challenges. In particular, we focus our discussion around the dimensions of
value, novelty and surprise as proposed by Margaret Boden in her work. Then, we
consider different classic perspectives, namely product, process, press and
person. We discuss a set of ``easy'' and ``hard'' problems in machine
creativity, presenting them in relation to LLMs. Finally, we examine the
societal impact of these technologies with a particular focus on the creative
industries, analyzing the opportunities offered by them, the challenges arising
by them and the potential associated risks, from both legal and ethical points
of view.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Cross-Linguistic Pressure for Uniform Information Density in Word
  Order 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03734v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03734v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Hikaru Clark, Clara Meister, Tiago Pimentel, Michael Hahn, Ryan Cotterell, Richard Futrell, Roger Levy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While natural languages differ widely in both canonical word order and word
order flexibility, their word orders still follow shared cross-linguistic
statistical patterns, often attributed to functional pressures. In the effort
to identify these pressures, prior work has compared real and counterfactual
word orders. Yet one functional pressure has been overlooked in such
investigations: the uniform information density (UID) hypothesis, which holds
that information should be spread evenly throughout an utterance. Here, we ask
whether a pressure for UID may have influenced word order patterns
cross-linguistically. To this end, we use computational models to test whether
real orders lead to greater information uniformity than counterfactual orders.
In our empirical study of 10 typologically diverse languages, we find that: (i)
among SVO languages, real word orders consistently have greater uniformity than
reverse word orders, and (ii) only linguistically implausible counterfactual
orders consistently exceed the uniformity of real orders. These findings are
compatible with a pressure for information uniformity in the development and
usage of natural languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Student's t-Distribution: On Measuring the Inter-Rater Reliability When
  the Observations are Scarce 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04526v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04526v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Serge Gladkoff, Lifeng Han, Goran Nenadic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In natural language processing (NLP) we always rely on human judgement as the
golden quality evaluation method. However, there has been an ongoing debate on
how to better evaluate inter-rater reliability (IRR) levels for certain
evaluation tasks, such as translation quality evaluation (TQE), especially when
the data samples (observations) are very scarce. In this work, we first
introduce the study on how to estimate the confidence interval for the
measurement value when only one data (evaluation) point is available. Then,
this leads to our example with two human-generated observational scores, for
which, we introduce ``Student's \textit{t}-Distribution'' method and explain
how to use it to measure the IRR score using only these two data points, as
well as the confidence intervals (CIs) of the quality evaluation. We give
quantitative analysis on how the evaluation confidence can be greatly improved
by introducing more observations, even if only one extra observation. We
encourage researchers to report their IRR scores in all possible means, e.g.
using Student's \textit{t}-Distribution method whenever possible; thus making
the NLP evaluation more meaningful, transparent, and trustworthy. This
\textit{t}-Distribution method can be also used outside of NLP fields to
measure IRR level for trustworthy evaluation of experimental investigations,
whenever the observational data is scarce.
  Keywords: Inter-Rater Reliability (IRR); Scarce Observations; Confidence
Intervals (CIs); Natural Language Processing (NLP); Translation Quality
Evaluation (TQE); Student's \textit{t}-Distribution
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to RANLP2023: Recent Advances in Natural Language
  Processing, Varna, Bulgaria. 30 Aug - 8 Sep
  \url{https://ranlp.org/ranlp2023/}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multilingual Language Models are not Multicultural: A Case Study in
  Emotion <span class="chip">WASSA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.01370v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.01370v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreya Havaldar, Sunny Rai, Bhumika Singhal, Langchen Liu, Sharath Chandra Guntuku, Lyle Ungar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotions are experienced and expressed differently across the world. In order
to use Large Language Models (LMs) for multilingual tasks that require
emotional sensitivity, LMs must reflect this cultural variation in emotion. In
this study, we investigate whether the widely-used multilingual LMs in 2023
reflect differences in emotional expressions across cultures and languages. We
find that embeddings obtained from LMs (e.g., XLM-RoBERTa) are Anglocentric,
and generative LMs (e.g., ChatGPT) reflect Western norms, even when responding
to prompts in other languages. Our results show that multilingual LMs do not
successfully learn the culturally appropriate nuances of emotion and we
highlight possible research directions towards correcting this.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WASSA at ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KenSwQuAD -- A Question Answering <span class="highlight-title">Dataset</span> for Swahili Low Resource
  Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.02364v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.02364v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Barack W. Wanjawa, Lilian D. A. Wanzare, Florence Indede, Owen McOnyango, Lawrence Muchemi, Edward Ombui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The need for Question Answering datasets in low resource languages is the
motivation of this research, leading to the development of Kencorpus Swahili
Question Answering Dataset, KenSwQuAD. This dataset is annotated from raw story
texts of Swahili low resource language, which is a predominantly spoken in
Eastern African and in other parts of the world. Question Answering (QA)
datasets are important for machine comprehension of natural language for tasks
such as internet search and dialog systems. Machine learning systems need
training data such as the gold standard Question Answering set developed in
this research. The research engaged annotators to formulate QA pairs from
Swahili texts collected by the Kencorpus project, a Kenyan languages corpus.
The project annotated 1,445 texts from the total 2,585 texts with at least 5 QA
pairs each, resulting into a final dataset of 7,526 QA pairs. A quality
assurance set of 12.5% of the annotated texts confirmed that the QA pairs were
all correctly annotated. A proof of concept on applying the set to the QA task
confirmed that the dataset can be usable for such tasks. KenSwQuAD has also
contributed to resourcing of the Swahili language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 1 figure, 10 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nano: Nested Human-in-the-Loop Reward Learning for Few-shot Language
  Model Control <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.05750v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.05750v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Fan, Yiwei Lyu, Paul Pu Liang, Ruslan Salakhutdinov, Louis-Philippe Morency
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretrained language models have demonstrated extraordinary capabilities in
language generation. However, real-world tasks often require controlling the
distribution of generated text in order to mitigate bias, promote fairness, and
achieve personalization. Existing techniques for controlling the distribution
of generated text only work with quantified distributions, which require
pre-defined categories, proportions of the distribution, or an existing corpus
following the desired distributions. However, many important distributions,
such as personal preferences, are unquantified. In this work, we tackle the
problem of generating text following arbitrary distributions (quantified and
unquantified) by proposing Nano, a few-shot human-in-the-loop training
algorithm that continuously learns from human feedback. Nano achieves
state-of-the-art results on single topic/attribute as well as quantified
distribution control compared to previous works. We also show that Nano is able
to learn unquantified distributions, achieves personalization, and captures
differences between different individuals' personal preferences with high
sample efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL Findings 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffusEmp: A Diffusion Model-Based Framework with Multi-Grained Control
  for Empathetic Response Generation <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01657v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01657v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanqun Bi, Lei Shen, Yanan Cao, Meng Chen, Yuqiang Xie, Zheng Lin, Xiaodong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Empathy is a crucial factor in open-domain conversations, which naturally
shows one's caring and understanding to others. Though several methods have
been proposed to generate empathetic responses, existing works often lead to
monotonous empathy that refers to generic and safe expressions. In this paper,
we propose to use explicit control to guide the empathy expression and design a
framework DiffusEmp based on conditional diffusion language model to unify the
utilization of dialogue context and attribute-oriented control signals.
Specifically, communication mechanism, intent, and semantic frame are imported
as multi-grained signals that control the empathy realization from coarse to
fine levels. We then design a specific masking strategy to reflect the
relationship between multi-grained signals and response tokens, and integrate
it into the diffusion model to influence the generative process. Experimental
results on a benchmark dataset EmpatheticDialogue show that our framework
outperforms competitive baselines in terms of controllability, informativeness,
and diversity without the loss of context-relatedness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by ACL 2023 main conference (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task-Agnostic Structured Pruning of Speech Representation Models <span class="chip">INTERSPEECH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01385v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01385v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Wang, Siyuan Wang, Wei-Qiang Zhang, Hongbin Suo, Yulong Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised pre-trained models such as Wav2vec2, Hubert, and WavLM have
been shown to significantly improve many speech tasks. However, their large
memory and strong computational requirements hinder their industrial
applicability. Structured pruning is a hardware-friendly model compression
technique but usually results in a larger loss of accuracy. In this paper, we
propose a fine-grained attention head pruning method to compensate for the
performance degradation. In addition, we also introduce the straight through
estimator into the L0 regularization to further accelerate the pruned model.
Experiments on the SUPERB benchmark show that our model can achieve comparable
performance to the dense model in multiple tasks and outperforms the Wav2vec
2.0 base model on average, with 72% fewer parameters and 2 times faster
inference speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by INTERSPEECH 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio
  <span class="highlight-title">Pretrain</span>ing for Speech Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07848v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07848v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Pan, Yanni Hu, Yuguang Yang, Jixun Yao, Wen Fei, Lei Ma, Heng Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning based pretraining methods have recently exhibited
impressive success in diverse fields. In this paper, we propose GEmo-CLAP, a
kind of efficient gender-attribute-enhanced contrastive language-audio
pretraining (CLAP) model for speech emotion recognition. To be specific, we
first build an effective emotion CLAP model Emo-CLAP for emotion recognition,
utilizing various self-supervised learning based pre-trained models. Then,
considering the importance of the gender attribute in speech emotion modeling,
two GEmo-CLAP approaches are further proposed to integrate the emotion and
gender information of speech signals, forming more reasonable objectives.
Extensive experiments on the IEMOCAP corpus demonstrate that our proposed two
GEmo-CLAP approaches consistently outperform the baseline Emo-CLAP with
different pre-trained models, while also achieving superior recognition
performance compared with other state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multilingual Coreference Resolution in Multiparty Dialogue 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.01307v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.01307v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyuan Zheng, Patrick Xia, Mahsa Yarmohammadi, Benjamin Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing multiparty dialogue datasets for entity coreference resolution are
nascent, and many challenges are still unaddressed. We create a large-scale
dataset, Multilingual Multiparty Coref (MMC), for this task based on TV
transcripts. Due to the availability of gold-quality subtitles in multiple
languages, we propose reusing the annotations to create silver coreference
resolution data in other languages (Chinese and Farsi) via annotation
projection. On the gold (English) data, off-the-shelf models perform relatively
poorly on MMC, suggesting that MMC has broader coverage of multiparty
coreference than prior datasets. On the silver data, we find success both using
it for data augmentation and training from scratch, which effectively simulates
the zero-shot cross-lingual setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating the Zero-shot Robustness of Instruction-tuned Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiuding Sun, Chantal Shaib, Byron C. Wallace
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction fine-tuning has recently emerged as a promising approach for
improving the zero-shot capabilities of Large Language Models (LLMs) on new
tasks. This technique has shown particular strength in improving the
performance of modestly sized LLMs, sometimes inducing performance competitive
with much larger model variants. In this paper we ask two questions: (1) How
sensitive are instruction-tuned models to the particular phrasings of
instructions, and, (2) How can we make them more robust to such natural
language variation? To answer the former, we collect a set of 319 instructions
manually written by NLP practitioners for over 80 unique tasks included in
widely used benchmarks, and we evaluate the variance and average performance of
these instructions as compared to instruction phrasings observed during
instruction fine-tuning. We find that using novel (unobserved) but appropriate
instruction phrasings consistently degrades model performance, sometimes
substantially so. Further, such natural instructions yield a wide variance in
downstream performance, despite their semantic equivalence. Put another way,
instruction-tuned models are not especially robust to instruction re-phrasings.
We propose a simple method to mitigate this issue by introducing ``soft
prompt'' embedding parameters and optimizing these to maximize the similarity
between representations of semantically equivalent instructions. We show that
this method consistently improves the robustness of instruction-tuned models.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Figure Classification Techniques in Scientific Documents <span class="chip">ICDAR
  2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anurag Dhote, Mohammed Javed, David S Doermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Figures visually represent an essential piece of information and provide an
effective means to communicate scientific facts. Recently there have been many
efforts toward extracting data directly from figures, specifically from tables,
diagrams, and plots, using different Artificial Intelligence and Machine
Learning techniques. This is because removing information from figures could
lead to deeper insights into the concepts highlighted in the scientific
documents. In this survey paper, we systematically categorize figures into five
classes - tables, photos, diagrams, maps, and plots, and subsequently present a
critical review of the existing methodologies and data sets that address the
problem of figure classification. Finally, we identify the current research
gaps and provide possible directions for further research on figure
classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Some contents of this paper appears in the accepted paper - "A Survey
  and Approach to Chart Classification" at 15th IAPR GREC 2023 at 17th ICDAR
  2023, August 21-26, San Jose, USA. arXiv admin note: text overlap with
  arXiv:2307.04147</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DebateKG: Automatic Policy Debate Case Creation with Semantic Knowledge
  Graphs <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04090v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04090v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Allen Roush
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work within the Argument Mining community has shown the applicability
of Natural Language Processing systems for solving problems found within
competitive debate. One of the most important tasks within competitive debate
is for debaters to create high quality debate cases. We show that effective
debate cases can be constructed using constrained shortest path traversals on
Argumentative Semantic Knowledge Graphs. We study this potential in the context
of a type of American Competitive Debate, called Policy Debate, which already
has a large scale dataset targeting it called DebateSum. We significantly
improve upon DebateSum by introducing 53180 new examples, as well as further
useful metadata for every example, to the dataset. We leverage the txtai
semantic search and knowledge graph toolchain to produce and contribute 9
semantic knowledge graphs built on this dataset. We create a unique method for
evaluating which knowledge graphs are better in the context of producing policy
debate cases. A demo which automatically generates debate cases, along with all
other code and the Knowledge Graphs, are open-sourced and made available to the
public here: https://github.com/Hellisotherpeople/DebateKG
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, knife-edge reject from EACL 2023 and workshops, System
  Demonstration paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Personalized Reinforcement Learning Summarization Service for Learning
  Structure from Unstructured Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05696v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05696v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samira Ghodratnama, Amin Beheshti, Mehrdad Zakershahrak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exponential growth of textual data has created a crucial need for tools
that assist users in extracting meaningful insights. Traditional document
summarization approaches often fail to meet individual user requirements and
lack structure for efficient information processing. To address these
limitations, we propose Summation, a hierarchical personalized concept-based
summarization approach. It synthesizes documents into a concise hierarchical
concept map and actively engages users by learning and adapting to their
preferences. Using a Reinforcement Learning algorithm, Summation generates
personalized summaries for unseen documents on specific topics. This framework
enhances comprehension, enables effective navigation, and empowers users to
extract meaningful insights from large document collections aligned with their
unique requirements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2108.09443</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human Activity Behavioural Pattern Recognition in Smarthome with
  Long-hour Data Collection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.13374v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.13374v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ranjit Kolkar, Geetha V
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The research on human activity recognition has provided novel solutions to
many applications like healthcare, sports, and user profiling. Considering the
complex nature of human activities, it is still challenging even after
effective and efficient sensors are available. The existing works on human
activity recognition using smartphone sensors focus on recognizing basic human
activities like sitting, sleeping, standing, stair up and down and running.
However, more than these basic activities is needed to analyze human
behavioural pattern. The proposed framework recognizes basic human activities
using deep learning models. Also, ambient sensors like PIR, pressure sensors,
and smartphone-based sensors like accelerometers and gyroscopes are combined to
make it hybrid-sensor-based human activity recognition. The hybrid approach
helped derive more activities than the basic ones, which also helped derive
human activity patterns or user profiling. User profiling provides sufficient
information to identify daily living activity patterns and predict whether any
anomaly exists. The framework provides the base for applications such as
elderly monitoring when they are alone at home. The GRU model's accuracy of
95\% is observed to recognize the basic activities. Finally, Human activity
patterns over time are recognized based on the duration and frequency of the
activities. It is observed that human activity pattern, like, morning walking
duration, varies depending on the day of the week.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cascading Residual Graph Convolutional Network for Multi-Behavior
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.13128v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.13128v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingshi Yan, Zhiyong Cheng, Chen Gao, Jing Sun, Fan Liu, Fuming Sun, Haojie Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-behavior recommendation exploits multiple types of user-item
interactions to alleviate the data sparsity problem faced by the traditional
models that often utilize only one type of interaction for recommendation. In
real scenarios, users often take a sequence of actions to interact with an
item, in order to get more information about the item and thus accurately
evaluate whether an item fits personal preference. Those interaction behaviors
often obey a certain order, and different behaviors reveal different
information or aspects of user preferences towards the target item. Most
existing multi-behavior recommendation methods take the strategy to first
extract information from different behaviors separately and then fuse them for
final prediction. However, they have not exploited the connections between
different behaviors to learn user preferences. Besides, they often introduce
complex model structures and more parameters to model multiple behaviors,
largely increasing the space and time complexity. In this work, we propose a
lightweight multi-behavior recommendation model named Cascading Residual Graph
Convolutional Network (CRGCN for short), which can explicitly exploit the
connections between different behaviors into the embedding learning process
without introducing any additional parameters. In particular, we design a
cascading residual graph convolutional network structure, which enables our
model to learn user preferences by continuously refining user embeddings across
different types of behaviors. The multi-task learning method is adopted to
jointly optimize our model based on different behaviors. Extensive experimental
results on two real-world benchmark datasets show that CRGCN can substantially
outperform state-of-the-art methods. Further studies also analyze the effects
of leveraging multi-behaviors in different numbers and orders on the final
performance.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAS Video-QA: Self-Adaptive Sampling for Efficient Video
  Question-Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Han, Hui Chen, Min-Yen Kan, Soujanya Poria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video question--answering is a fundamental task in the field of video
understanding. Although current vision--language models (VLMs) equipped with
Video Transformers have enabled temporal modeling and yielded superior results,
they are at the cost of huge computational power and thus too expensive to
deploy in real-time application scenarios. An economical workaround only
samples a small portion of frames to represent the main content of that video
and tune an image--text model on these sampled frames. Recent video
understanding models usually randomly sample a set of frames or clips,
regardless of internal correlations between their visual contents, nor their
relevance to the problem. We argue that such kinds of aimless sampling may omit
the key frames from which the correct answer can be deduced, and the situation
gets worse when the sampling sparsity increases, which always happens as the
video lengths increase. To mitigate this issue, we propose two frame sampling
strategies, namely the most domain frames (MDF) and most implied frames (MIF),
to maximally preserve those frames that are most likely vital to the given
questions. MDF passively minimizes the risk of key frame omission in a
bootstrap manner, while MIS actively searches key frames customized for each
video--question pair with the assistance of auxiliary models. The experimental
results on three public datasets from three advanced VLMs (CLIP, GIT and
All-in-one) demonstrate that our proposed strategies can boost the performance
for image--text pretrained models. The source codes pertaining to the method
proposed in this paper are publicly available at
https://github.com/declare-lab/sas-vqa.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predictive Coding For Animation-Based Video Compression <span class="chip">ICIP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Goluck Konuko, Stéphane Lathuilière, Giuseppe Valenzise
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of efficiently compressing video for conferencing-type
applications. We build on recent approaches based on image animation, which can
achieve good reconstruction quality at very low bitrate by representing face
motions with a compact set of sparse keypoints. However, these methods encode
video in a frame-by-frame fashion, i.e. each frame is reconstructed from a
reference frame, which limits the reconstruction quality when the bandwidth is
larger. Instead, we propose a predictive coding scheme which uses image
animation as a predictor, and codes the residual with respect to the actual
target frame. The residuals can be in turn coded in a predictive manner, thus
removing efficiently temporal dependencies. Our experiments indicate a
significant bitrate gain, in excess of 70% compared to the HEVC video standard
and over 30% compared to VVC, on a datasetof talking-head videos
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted paper: ICIP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FILM: How can Few-Shot Image Classification Benefit from <span class="highlight-title">Pre-Train</span>ed
  Language Models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04114v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04114v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Jiang, Yunkai Dang, Dong Pang, Huishuai Zhang, Weiran Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot learning aims to train models that can be generalized to novel
classes with only a few samples. Recently, a line of works are proposed to
enhance few-shot learning with accessible semantic information from class
names. However, these works focus on improving existing modules such as visual
prototypes and feature extractors of the standard few-shot learning framework.
This limits the full potential use of semantic information. In this paper, we
propose a novel few-shot learning framework that uses pre-trained language
models based on contrastive learning. To address the challenge of alignment
between visual features and textual embeddings obtained from text-based
pre-trained language model, we carefully design the textual branch of our
framework and introduce a metric module to generalize the cosine similarity.
For better transferability, we let the metric module adapt to different
few-shot tasks and adopt MAML to train the model via bi-level optimization.
Moreover, we conduct extensive experiments on multiple benchmarks to
demonstrate the effectiveness of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio
  <span class="highlight-title">Pretrain</span>ing for Speech Emotion Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07848v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07848v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Pan, Yanni Hu, Yuguang Yang, Jixun Yao, Wen Fei, Lei Ma, Heng Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning based pretraining methods have recently exhibited
impressive success in diverse fields. In this paper, we propose GEmo-CLAP, a
kind of efficient gender-attribute-enhanced contrastive language-audio
pretraining (CLAP) model for speech emotion recognition. To be specific, we
first build an effective emotion CLAP model Emo-CLAP for emotion recognition,
utilizing various self-supervised learning based pre-trained models. Then,
considering the importance of the gender attribute in speech emotion modeling,
two GEmo-CLAP approaches are further proposed to integrate the emotion and
gender information of speech signals, forming more reasonable objectives.
Extensive experiments on the IEMOCAP corpus demonstrate that our proposed two
GEmo-CLAP approaches consistently outperform the baseline Emo-CLAP with
different pre-trained models, while also achieving superior recognition
performance compared with other state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-07-08T00:00:00Z">2023-07-08</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bidirectional Attention as a Mixture of Continuous Word Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04057v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04057v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Christian Wibisono, Yixin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bidirectional attention $\unicode{x2013}$ composed of self-attention with
positional encodings and the masked language model (MLM) objective
$\unicode{x2013}$ has emerged as a key component of modern large language
models (LLMs). Despite its empirical success, few studies have examined its
statistical underpinnings: What statistical model is bidirectional attention
implicitly fitting? What sets it apart from its non-attention predecessors? We
explore these questions in this paper. The key observation is that fitting a
single-layer single-head bidirectional attention, upon reparameterization, is
equivalent to fitting a continuous bag of words (CBOW) model with
mixture-of-experts (MoE) weights. Further, bidirectional attention with
multiple heads and multiple layers is equivalent to stacked MoEs and a mixture
of MoEs, respectively. This statistical viewpoint reveals the distinct use of
MoE in bidirectional attention, which aligns with its practical effectiveness
in handling heterogeneous data. It also suggests an immediate extension to
categorical tabular data, if we view each word location in a sentence as a
tabular feature. Across empirical studies, we find that this extension
outperforms existing tabular extensions of transformers in out-of-distribution
(OOD) generalization. Finally, this statistical perspective of bidirectional
attention enables us to theoretically characterize when linear word analogies
are present in its word embeddings. These analyses show that bidirectional
attention can require much stronger assumptions to exhibit linear word
analogies than its non-attention predecessors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How is Fatherhood Framed Online in Singapore? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tran Hien Van, Abhay Goyal, Muhammad Siddique, Lam Yin Cheung, Nimay Parekh, Jonathan Y Huang, Keri McCrickerd, Edson C Tandoc Jr., Gerard Chung, Navin Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of discussion about fatherhood in Singapore attests to its
significance, indicating the need for an exploration of how fatherhood is
framed, aiding policy-making around fatherhood in Singapore. Sound and holistic
policy around fatherhood in Singapore may reduce stigma and apprehension around
being a parent, critical to improving the nations flagging birth rate. We
analyzed 15,705 articles and 56,221 posts to study how fatherhood is framed in
Singapore across a range of online platforms (news outlets, parenting forums,
Twitter). We used NLP techniques to understand these differences. While
fatherhood was framed in a range of ways on the Singaporean online environment,
it did not seem that fathers were framed as central to the Singaporean family
unit. A strength of our work is how the different techniques we have applied
validate each other.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CausalDialogue: Modeling Utterance-level Causality in Conversations <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10515v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10515v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Lin Tuan, Alon Albalak, Wenda Xu, Michael Saxon, Connor Pryor, Lise Getoor, William Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their widespread adoption, neural conversation models have yet to
exhibit natural chat capabilities with humans. In this research, we examine
user utterances as causes and generated responses as effects, recognizing that
changes in a cause should produce a different effect. To further explore this
concept, we have compiled and expanded upon a new dataset called CausalDialogue
through crowd-sourcing. This dataset includes multiple cause-effect pairs
within a directed acyclic graph (DAG) structure. Our analysis reveals that
traditional loss functions struggle to effectively incorporate the DAG
structure, leading us to propose a causality-enhanced method called Exponential
Maximum Average Treatment Effect (ExMATE) to enhance the impact of causality at
the utterance level in training neural conversation models. To evaluate the
needs of considering causality in dialogue generation, we built a comprehensive
benchmark on CausalDialogue dataset using different models, inference, and
training methods. Through experiments, we find that a causality-inspired loss
like ExMATE can improve the diversity and agility of conventional loss function
and there is still room for improvement to reach human-level quality on this
new dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL-Findings 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Talk the Walk: Synthetic Data Generation for Conversational Music
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11489v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11489v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Megan Leszczynski, Ravi Ganti, Shu Zhang, Krisztian Balog, Filip Radlinski, Fernando Pereira, Arun Tejasvi Chaganty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation systems are ubiquitous yet often difficult for users to
control and adjust when recommendation quality is poor. This has motivated the
development of conversational recommendation systems (CRSs), with control over
recommendations provided through natural language feedback. However, building
conversational recommendation systems requires conversational training data
involving user utterances paired with items that cover a diverse range of
preferences. Such data has proved challenging to collect scalably using
conventional methods like crowdsourcing. We address it in the context of
item-set recommendation, noting the increasing attention to this task motivated
by use cases like music, news and recipe recommendation. We present a new
technique, TalkTheWalk, that synthesizes realistic high-quality conversational
data by leveraging domain expertise encoded in widely available curated item
collections, showing how these can be transformed into corresponding item set
curation conversations. Specifically, TalkTheWalk generates a sequence of
hypothetical yet plausible item sets returned by a system, then uses a language
model to produce corresponding user utterances. Applying TalkTheWalk to music
recommendation, we generate over one million diverse playlist curation
conversations. A human evaluation shows that the conversations contain
consistent utterances with relevant item sets, nearly matching the quality of
small human-collected conversational data for this task. At the same time, when
the synthetic corpus is used to train a CRS, it improves Hits@100 by 10.5
points on a benchmark dataset over standard baselines and is preferred over the
top-performing baseline in an online evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kencorpus: A Kenyan Language Corpus of Swahili, Dholuo and Luhya for
  Natural Language Processing Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.12081v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.12081v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Barack Wanjawa, Lilian Wanzare, Florence Indede, Owen McOnyango, Edward Ombui, Lawrence Muchemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Indigenous African languages are categorized as under-served in Natural
Language Processing. They therefore experience poor digital inclusivity and
information access. The processing challenge with such languages has been how
to use machine learning and deep learning models without the requisite data.
The Kencorpus project intends to bridge this gap by collecting and storing text
and speech data that is good enough for data-driven solutions in applications
such as machine translation, question answering and transcription in
multilingual communities. The Kencorpus dataset is a text and speech corpus for
three languages predominantly spoken in Kenya: Swahili, Dholuo and Luhya. Data
collection was done by researchers from communities, schools, media, and
publishers. The Kencorpus' dataset has a collection of 5,594 items - 4,442
texts (5.6M words) and 1,152 speech files (177hrs). Based on this data, Part of
Speech tagging sets for Dholuo and Luhya (50,000 and 93,000 words respectively)
were developed. We developed 7,537 Question-Answer pairs for Swahili and
created a text translation set of 13,400 sentences from Dholuo and Luhya into
Swahili. The datasets are useful for downstream machine learning tasks such as
model training and translation. We also developed two proof of concept systems:
for Kiswahili speech-to-text and machine learning system for Question Answering
task, with results of 18.87% word error rate and 80% Exact Match (EM)
respectively. These initial results give great promise to the usability of
Kencorpus to the machine learning community. Kencorpus is one of few public
domain corpora for these three low resource languages and forms a basis of
learning and sharing experiences for similar works especially for low resource
languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fairness-Aware Graph Neural Networks: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        April Chen, Ryan A. Rossi, Namyong Park, Puja Trivedi, Yu Wang, Tong Yu, Sungchul Kim, Franck Dernoncourt, Nesreen K. Ahmed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have become increasingly important due to their
representational power and state-of-the-art predictive performance on many
fundamental learning tasks. Despite this success, GNNs suffer from fairness
issues that arise as a result of the underlying graph data and the fundamental
aggregation mechanism that lies at the heart of the large class of GNN models.
In this article, we examine and categorize fairness techniques for improving
the fairness of GNNs. Previous work on fair GNN models and techniques are
discussed in terms of whether they focus on improving fairness during a
preprocessing step, during training, or in a post-processing phase.
Furthermore, we discuss how such techniques can be used together whenever
appropriate, and highlight the advantages and intuition as well. We also
introduce an intuitive taxonomy for fairness evaluation metrics including
graph-level fairness, neighborhood-level fairness, embedding-level fairness,
and prediction-level fairness metrics. In addition, graph datasets that are
useful for benchmarking the fairness of GNN models are summarized succinctly.
Finally, we highlight key open problems and challenges that remain to be
addressed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embedding Mental Health Discourse for Community Recommendation <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03892v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03892v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hy Dang, Bang Nguyen, Noah Ziems, Meng Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our paper investigates the use of discourse embedding techniques to develop a
community recommendation system that focuses on mental health support groups on
social media. Social media platforms provide a means for users to anonymously
connect with communities that cater to their specific interests. However, with
the vast number of online communities available, users may face difficulties in
identifying relevant groups to address their mental health concerns. To address
this challenge, we explore the integration of discourse information from
various subreddit communities using embedding techniques to develop an
effective recommendation system. Our approach involves the use of content-based
and collaborative filtering techniques to enhance the performance of the
recommendation system. Our findings indicate that the proposed approach
outperforms the use of each technique separately and provides interpretability
in the recommendation process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 4th workshop on Computational Approaches to Discourse
  (CODI-2023) at ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unconfounded Propensity Estimation for Unbiased Ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.09918v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.09918v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dan Luo, Lixin Zou, Qingyao Ai, Zhiyu Chen, Chenliang Li, Dawei Yin, Brian D. Davison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of unbiased learning to rank (ULTR) is to leverage implicit user
feedback for optimizing learning-to-rank systems. Among existing solutions,
automatic ULTR algorithms that jointly learn user bias models (i.e., propensity
models) with unbiased rankers have received a lot of attention due to their
superior performance and low deployment cost in practice. Despite their
theoretical soundness, the effectiveness is usually justified under a weak
logging policy, where the ranking model can barely rank documents according to
their relevance to the query. However, when the logging policy is strong, e.g.,
an industry-deployed ranking policy, the reported effectiveness cannot be
reproduced. In this paper, we first investigate ULTR from a causal perspective
and uncover a negative result: existing ULTR algorithms fail to address the
issue of propensity overestimation caused by the query-document relevance
confounder. Then, we propose a new learning objective based on backdoor
adjustment and highlight its differences from conventional propensity models,
which reveal the prevalence of propensity overestimation. On top of that, we
introduce a novel propensity model called Logging-Policy-aware Propensity (LPP)
model and its distinctive two-step optimization strategy, which allows for the
joint learning of LPP and ranking models within the automatic ULTR framework,
and actualize the unconfounded propensity estimation for ULTR. Extensive
experiments on two benchmarks demonstrate the effectiveness and
generalizability of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Talk the Walk: Synthetic Data Generation for Conversational Music
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11489v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11489v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Megan Leszczynski, Ravi Ganti, Shu Zhang, Krisztian Balog, Filip Radlinski, Fernando Pereira, Arun Tejasvi Chaganty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation systems are ubiquitous yet often difficult for users to
control and adjust when recommendation quality is poor. This has motivated the
development of conversational recommendation systems (CRSs), with control over
recommendations provided through natural language feedback. However, building
conversational recommendation systems requires conversational training data
involving user utterances paired with items that cover a diverse range of
preferences. Such data has proved challenging to collect scalably using
conventional methods like crowdsourcing. We address it in the context of
item-set recommendation, noting the increasing attention to this task motivated
by use cases like music, news and recipe recommendation. We present a new
technique, TalkTheWalk, that synthesizes realistic high-quality conversational
data by leveraging domain expertise encoded in widely available curated item
collections, showing how these can be transformed into corresponding item set
curation conversations. Specifically, TalkTheWalk generates a sequence of
hypothetical yet plausible item sets returned by a system, then uses a language
model to produce corresponding user utterances. Applying TalkTheWalk to music
recommendation, we generate over one million diverse playlist curation
conversations. A human evaluation shows that the conversations contain
consistent utterances with relevant item sets, nearly matching the quality of
small human-collected conversational data for this task. At the same time, when
the synthetic corpus is used to train a CRS, it improves Hits@100 by 10.5
points on a benchmark dataset over standard baselines and is preferred over the
top-performing baseline in an online evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Result Diversification in Search and Recommendation: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.14464v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.14464v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haolun Wu, Yansen Zhang, Chen Ma, Fuyuan Lyu, Bowei He, Bhaskar Mitra, Xue Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diversifying return results is an important research topic in retrieval
systems in order to satisfy both the various interests of customers and the
equal market exposure of providers. There has been growing attention on
diversity-aware research during recent years, accompanied by a proliferation of
literature on methods to promote diversity in search and recommendation.
However, diversity-aware studies in retrieval systems lack a systematic
organization and are rather fragmented. In this survey, we are the first to
propose a unified taxonomy for classifying the metrics and approaches of
diversification in both search and recommendation, which are two of the most
extensively researched fields of retrieval systems. We begin the survey with a
brief discussion of why diversity is important in retrieval systems, followed
by a summary of the various diversity concerns in search and recommendation,
highlighting their relationship and differences. For the survey's main body, we
present a unified taxonomy of diversification metrics and approaches in
retrieval systems, from both the search and recommendation perspectives. In the
later part of the survey, we discuss the open research questions of
diversity-aware research in search and recommendation in an effort to inspire
future innovations and encourage the implementation of diversity in real-world
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How to Index Item IDs for Recommendation Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.06569v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.06569v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyue Hua, Shuyuan Xu, Yingqiang Ge, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation foundation model utilizes large language models (LLM) for
recommendation by converting recommendation tasks into natural language tasks.
It enables generative recommendation which directly generates the item(s) to
recommend rather than calculating a ranking score for each and every candidate
item in traditional recommendation models, simplifying the recommendation
pipeline from multi-stage filtering to single-stage filtering. To avoid
generating excessively long text when deciding which item(s) to recommend,
creating LLM-compatible item IDs is essential for recommendation foundation
models. In this study, we systematically examine the item indexing problem for
recommendation foundation models, using P5 as the representative backbone model
and replicating its results with various indexing methods. To emphasize the
importance of item indexing, we first discuss the issues of several trivial
item indexing methods, such as independent indexing, title indexing, and random
indexing. We then propose four simple yet effective solutions, including
sequential indexing, collaborative indexing, semantic (content-based) indexing,
and hybrid indexing. Our reproducibility study of P5 highlights the significant
influence of item indexing methods on the model performance, and our results on
real-world datasets validate the effectiveness of our proposed solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Graph Contrastive Learning for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.10837v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.10837v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangqin Jiang, Chao Huang, Lianghao Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) have recently emerged as an effective
collaborative filtering (CF) approaches for recommender systems. The key idea
of GNN-based recommender systems is to recursively perform message passing
along user-item interaction edges to refine encoded embeddings, relying on
sufficient and high-quality training data. However, user behavior data in
practical recommendation scenarios is often noisy and exhibits skewed
distribution. To address these issues, some recommendation approaches, such as
SGL, leverage self-supervised learning to improve user representations. These
approaches conduct self-supervised learning through creating contrastive views,
but they depend on the tedious trial-and-error selection of augmentation
methods. In this paper, we propose a novel Adaptive Graph Contrastive Learning
(AdaGCL) framework that conducts data augmentation with two adaptive
contrastive view generators to better empower the CF paradigm. Specifically, we
use two trainable view generators - a graph generative model and a graph
denoising model - to create adaptive contrastive views. With two adaptive
contrastive views, AdaGCL introduces additional high-quality training signals
into the CF paradigm, helping to alleviate data sparsity and noise issues.
Extensive experiments on three real-world datasets demonstrate the superiority
of our model over various state-of-the-art recommendation methods. Our model
implementation codes are available at the link https://github.com/HKUDS/AdaGCL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Known by the Company it Keeps: Proximity-Based Indexing for Physical
  Content in Archival Repositories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18683v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18683v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Douglas W. Oard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the plethora of born-digital content, vast troves of important
content remain accessible only on physical media such as paper or microfilm.
The traditional approach to indexing undigitized content is using manually
created metadata that describes it at some level of aggregation (e.g., folder,
box, or collection). Searchers led in this way to some subset of the content
often must then manually examine substantial quantities of physical media to
find what they are looking for. This paper proposes a complementary approach,
in which selective digitization of a small portion of the content is used as a
basis for proximity-based indexing as a way of bringing the user closer to the
specific content for which they are looking. Experiments with 35 boxes of
partially digitized US State Department records indicate that box-level indexes
built in this way can provide a useful basis for search.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in Theory and Practice of Digital Libraries (TPDL)
  2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emotion-Guided Music Accompaniment Generation Based on Variational
  Autoencoder <span class="chip">IJCNN2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Wang, Shubing Zhang, Li Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music accompaniment generation is a crucial aspect in the composition
process. Deep neural networks have made significant strides in this field, but
it remains a challenge for AI to effectively incorporate human emotions to
create beautiful accompaniments. Existing models struggle to effectively
characterize human emotions within neural network models while composing music.
To address this issue, we propose the use of an easy-to-represent emotion flow
model, the Valence/Arousal Curve, which allows for the compatibility of
emotional information within the model through data transformation and enhances
interpretability of emotional factors by utilizing a Variational Autoencoder as
the model structure. Further, we used relative self-attention to maintain the
structure of the music at music phrase level and to generate a richer
accompaniment when combined with the rules of music theory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted By International Joint Conference on Neural Networks
  2023(IJCNN2023)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-07-07T00:00:00Z">2023-07-07</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How does AI chat change search behaviors? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Capra, Jaime Arguello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI tools such as chatGPT are poised to change the way people
engage with online information. Recently, Microsoft announced their "new Bing"
search system which incorporates chat and generative AI technology from OpenAI.
Google has announced plans to deploy search interfaces that incorporate similar
types of technology. These new technologies will transform how people can
search for information. The research presented here is an early investigation
into how people make use of a generative AI chat system (referred to simply as
chat from here on) as part of a search process, and how the incorporation of
chat systems with existing search tools may effect users search behaviors and
strategies.
  We report on an exploratory user study with 10 participants who used a
combined Chat+Search system that utilized the OpenAI GPT-3.5 API and the Bing
Web Search v5 API. Participants completed three search tasks. In this pre-print
paper of preliminary results, we report on ways that users integrated AI chat
into their search process, things they liked and disliked about the chat
system, their trust in the chat responses, and their mental models of how the
chat system generated responses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaptiveRec: Adaptively Construct Pairs for Contrastive Learning in
  Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaeheyoung Jeon, Jung Hyun Ryu, Jewoong Cho, Myungjoo Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a solution to the challenges faced by contrastive
learning in sequential recommendation systems. In particular, it addresses the
issue of false negative, which limits the effectiveness of recommendation
algorithms. By introducing an advanced approach to contrastive learning, the
proposed method improves the quality of item embeddings and mitigates the
problem of falsely categorizing similar instances as dissimilar. Experimental
results demonstrate performance enhancements compared to existing systems. The
flexibility and applicability of the proposed approach across various
recommendation scenarios further highlight its value in enhancing sequential
recommendation systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Network Resource Allocation Recommendation Method with An Improved
  Similarity Measure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huiyu Li, Pei Liang, Junhua Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems have been acknowledged as efficacious tools for managing
information overload. Nevertheless, conventional algorithms adopted in such
systems primarily emphasize precise recommendations and, consequently, overlook
other vital aspects like the coverage, diversity, and novelty of items. This
approach results in less exposure for long-tail items. In this paper, to
personalize the recommendations and allocate recommendation resources more
purposively, a method named PIM+RA is proposed. This method utilizes a
bipartite network that incorporates self-connecting edges and weights.
Furthermore, an improved Pearson correlation coefficient is employed for better
redistribution. The evaluation of PIM+RA demonstrates a significant enhancement
not only in accuracy but also in coverage, diversity, and novelty of the
recommendation. It leads to a better balance in recommendation frequency by
providing effective exposure to long-tail items, while allowing customized
parameters to adjust the recommendation list bias.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ JEPOO: Highly Accurate Joint Estimation of Pitch, Onset and Offset for
  Music Information Retrieval <span class="chip">IJCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01304v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01304v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haojie Wei, Jun Yuan, Rui Zhang, Yueguo Chen, Gang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Melody extraction is a core task in music information retrieval, and the
estimation of pitch, onset and offset are key sub-tasks in melody extraction.
Existing methods have limited accuracy, and work for only one type of data,
either single-pitch or multipitch. In this paper, we propose a highly accurate
method for joint estimation of pitch, onset and offset, named JEPOO. We address
the challenges of joint learning optimization and handling both single-pitch
and multi-pitch data through novel model design and a new optimization
technique named Pareto modulated loss with loss weight regularization. This is
the first method that can accurately handle both single-pitch and multi-pitch
music data, and even a mix of them. A comprehensive experimental study on a
wide range of real datasets shows that JEPOO outperforms state-ofthe-art
methods by up to 10.6%, 8.3% and 10.3% for the prediction of Pitch, Onset and
Offset, respectively, and JEPOO is robust for various types of data and
instruments. The ablation study shows the effectiveness of each component of
JEPOO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by IJCAI 2023; 11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging the Gap Between Indexing and Retrieval for Differentiable
  Search Index with Query Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.10128v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.10128v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengyao Zhuang, Houxing Ren, Linjun Shou, Jian Pei, Ming Gong, Guido Zuccon, Daxin Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Differentiable Search Index (DSI) is an emerging paradigm for information
retrieval. Unlike traditional retrieval architectures where index and retrieval
are two different and separate components, DSI uses a single transformer model
to perform both indexing and retrieval.
  In this paper, we identify and tackle an important issue of current DSI
models: the data distribution mismatch that occurs between the DSI indexing and
retrieval processes. Specifically, we argue that, at indexing, current DSI
methods learn to build connections between the text of long documents and the
identifier of the documents, but then retrieval of document identifiers is
based on queries that are commonly much shorter than the indexed documents.
This problem is further exacerbated when using DSI for cross-lingual retrieval,
where document text and query text are in different languages.
  To address this fundamental problem of current DSI models, we propose a
simple yet effective indexing framework for DSI, called DSI-QG. When indexing,
DSI-QG represents documents with a number of potentially relevant queries
generated by a query generation model and re-ranked and filtered by a
cross-encoder ranker. The presence of these queries at indexing allows the DSI
models to connect a document identifier to a set of queries, hence mitigating
data distribution mismatches present between the indexing and the retrieval
phases. Empirical results on popular mono-lingual and cross-lingual passage
retrieval datasets show that DSI-QG significantly outperforms the original DSI
model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">5</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Launchpad<span class="highlight-title">GPT</span>: Language Model as Music Visualization Designer on
  Launchpad 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.04827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.04827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siting Xu, Yunlong Tang, Feng Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Launchpad is a musical instrument that allows users to create and perform
music by pressing illuminated buttons. To assist and inspire the design of the
Launchpad light effect, and provide a more accessible approach for beginners to
create music visualization with this instrument, we proposed the LaunchpadGPT
model to generate music visualization designs on Launchpad automatically. Based
on the language model with excellent generation ability, our proposed
LaunchpadGPT takes an audio piece of music as input and outputs the lighting
effects of Launchpad-playing in the form of a video (Launchpad-playing video).
We collect Launchpad-playing videos and process them to obtain music and
corresponding video frame of Launchpad-playing as prompt-completion pairs, to
train the language model. The experiment result shows the proposed method can
create better music visualization than random generation methods and hold the
potential for a broader range of music visualization applications. Our code is
available at https://github.com/yunlong10/LaunchpadGPT/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by International Computer Music Conference (ICMC) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physical-aware Cross-modal Adversarial Network for Wearable Sensor-based
  Human Action Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianyuan Ni, Hao Tang, Anne H. H. Ngu, Gaowen Liu, Yan Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wearable sensor-based Human Action Recognition (HAR) has made significant
strides in recent times. However, the accuracy performance of wearable
sensor-based HAR is currently still lagging behind that of visual
modalities-based systems, such as RGB video and depth data. Although diverse
input modalities can provide complementary cues and improve the accuracy
performance of HAR, wearable devices can only capture limited kinds of
non-visual time series input, such as accelerometers and gyroscopes. This
limitation hinders the deployment of multimodal simultaneously using visual and
non-visual modality data in parallel on current wearable devices. To address
this issue, we propose a novel Physical-aware Cross-modal Adversarial (PCA)
framework that utilizes only time-series accelerometer data from four inertial
sensors for the wearable sensor-based HAR problem. Specifically, we propose an
effective IMU2SKELETON network to produce corresponding synthetic skeleton
joints from accelerometer data. Subsequently, we imposed additional constraints
on the synthetic skeleton data from a physical perspective, as accelerometer
data can be regarded as the second derivative of the skeleton sequence
coordinates. After that, the original accelerometer as well as the constrained
skeleton sequence were fused together to make the final classification. In this
way, when individuals wear wearable devices, the devices can not only capture
accelerometer data, but can also generate synthetic skeleton sequences for
real-time wearable sensor-based HAR applications that need to be conducted
anytime and anywhere. To demonstrate the effectiveness of our proposed PCA
framework, we conduct extensive experiments on Berkeley-MHAD, UTD-MHAD, and
MMAct datasets. The results confirm that the proposed PCA approach has
competitive performance compared to the previous methods on the mono
sensor-based HAR classification problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First IMU2SKELETON GANs approach for wearable HAR problem. arXiv
  admin note: text overlap with arXiv:2208.08090</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anableps: Adapting Bitrate for Real-Time Communication Using VBR-encoded
  Video <span class="chip">ICME 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zicheng Zhang, Hao Chen, Xun Cao, Zhan Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Content providers increasingly replace traditional constant bitrate with
variable bitrate (VBR) encoding in real-time video communication systems for
better video quality. However, VBR encoding often leads to large and frequent
bitrate fluctuation, inevitably deteriorating the efficiency of existing
adaptive bitrate (ABR) methods. To tackle it, we propose the Anableps to
consider the network dynamics and VBR-encoding-induced video bitrate
fluctuations jointly for deploying the best ABR policy. With this aim, Anableps
uses sender-side information from the past to predict the video bitrate range
of upcoming frames. Such bitrate range is then combined with the receiver-side
observations to set the proper bitrate target for video encoding using a
reinforcement-learning-based ABR model. As revealed by extensive experiments on
a real-world trace-driven testbed, our Anableps outperforms the GCC with
significant improvement of quality of experience, e.g., 1.88x video quality,
57% less bitrate consumption, 85% less stalling, and 74% shorter interaction
delay.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper will be presented at IEEE ICME 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Robust SDRTV-to-HDRTV via Dual Inverse Degradation Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03394v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03394v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kepeng Xu, Gang He, Li Xu, Xingchao Yang, Ming Sun, Yuzhi Wang, Zijia Ma, Haoqiang Fan, Xing Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the transformation of standard dynamic range TV (SDRTV) to high
dynamic range TV (HDRTV) is in high demand due to the scarcity of HDRTV
content. However, the conversion of SDRTV to HDRTV often amplifies the existing
coding artifacts in SDRTV which deteriorate the visual quality of the output.
In this study, we propose a dual inverse degradation SDRTV-to-HDRTV network
DIDNet to address the issue of coding artifact restoration in converted HDRTV,
which has not been previously studied. Specifically, we propose a
temporal-spatial feature alignment module and dual modulation convolution to
remove coding artifacts and enhance color restoration ability. Furthermore, a
wavelet attention module is proposed to improve SDRTV features in the frequency
domain. An auxiliary loss is introduced to decouple the learning process for
effectively restoring from dual degradation. The proposed method outperforms
the current state-of-the-art method in terms of quantitative results, visual
quality, and inference times, thus enhancing the performance of the
SDRTV-to-HDRTV method in real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ JEPOO: Highly Accurate Joint Estimation of Pitch, Onset and Offset for
  Music Information Retrieval <span class="chip">IJCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01304v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01304v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haojie Wei, Jun Yuan, Rui Zhang, Yueguo Chen, Gang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Melody extraction is a core task in music information retrieval, and the
estimation of pitch, onset and offset are key sub-tasks in melody extraction.
Existing methods have limited accuracy, and work for only one type of data,
either single-pitch or multipitch. In this paper, we propose a highly accurate
method for joint estimation of pitch, onset and offset, named JEPOO. We address
the challenges of joint learning optimization and handling both single-pitch
and multi-pitch data through novel model design and a new optimization
technique named Pareto modulated loss with loss weight regularization. This is
the first method that can accurately handle both single-pitch and multi-pitch
music data, and even a mix of them. A comprehensive experimental study on a
wide range of real datasets shows that JEPOO outperforms state-ofthe-art
methods by up to 10.6%, 8.3% and 10.3% for the prediction of Pitch, Onset and
Offset, respectively, and JEPOO is robust for various types of data and
instruments. The ablation study shows the effectiveness of each component of
JEPOO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by IJCAI 2023; 11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-07-06T00:00:00Z">2023-07-06</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">19</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InfoSync: Information Synchronization across Multilingual
  Semi-structured Tables <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Khincha, Chelsi Jain, Vivek Gupta, Tushar Kataria, Shuo Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information Synchronization of semi-structured data across languages is
challenging. For instance, Wikipedia tables in one language should be
synchronized across languages. To address this problem, we introduce a new
dataset InfoSyncC and a two-step method for tabular synchronization. InfoSync
contains 100K entity-centric tables (Wikipedia Infoboxes) across 14 languages,
of which a subset (3.5K pairs) are manually annotated. The proposed method
includes 1) Information Alignment to map rows and 2) Information Update for
updating missing/outdated information for aligned tables across multilingual
tables. When evaluated on InfoSync, information alignment achieves an F1 score
of 87.91 (en <-> non-en). To evaluate information updation, we perform
human-assisted Wikipedia edits on Infoboxes for 603 table pairs. Our approach
obtains an acceptance rate of 77.28% on Wikipedia, showing the effectiveness of
the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 7 figures, 20 tables, ACL 2023 (Toronto, Canada)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiVENT: Multilingual Videos of Events with Aligned Natural Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03153v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03153v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kate Sanders, David Etter, Reno Kriz, Benjamin Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Everyday news coverage has shifted from traditional broadcasts towards a wide
range of presentation formats such as first-hand, unedited video footage.
Datasets that reflect the diverse array of multimodal, multilingual news
sources available online could be used to teach models to benefit from this
shift, but existing news video datasets focus on traditional news broadcasts
produced for English-speaking audiences. We address this limitation by
constructing MultiVENT, a dataset of multilingual, event-centric videos
grounded in text documents across five target languages. MultiVENT includes
both news broadcast videos and non-professional event footage, which we use to
analyze the state of online news videos and how they can be leveraged to build
robust, factually accurate models. Finally, we provide a model for complex,
multilingual video retrieval to serve as a baseline for information retrieval
using MultiVENT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structure Guided Multi-modal <span class="highlight-title">Pre-train</span>ed <span class="highlight-title">Transformer</span> for Knowledge Graph
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Liang, Sihang Zhou, Yue Liu, Lingyuan Meng, Meng Liu, Xinwang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal knowledge graphs (MKGs), which intuitively organize information in
various modalities, can benefit multiple practical downstream tasks, such as
recommendation systems, and visual question answering. However, most MKGs are
still far from complete, which motivates the flourishing of MKG reasoning
models. Recently, with the development of general artificial architectures, the
pretrained transformer models have drawn increasing attention, especially for
multimodal scenarios. However, the research of multimodal pretrained
transformer (MPT) for knowledge graph reasoning (KGR) is still at an early
stage. As the biggest difference between MKG and other multimodal data, the
rich structural information underlying the MKG still cannot be fully leveraged
in existing MPT models. Most of them only utilize the graph structure as a
retrieval map for matching images and texts connected with the same entity.
This manner hinders their reasoning performances. To this end, we propose the
graph Structure Guided Multimodal Pretrained Transformer for knowledge graph
reasoning, termed SGMPT. Specifically, the graph structure encoder is adopted
for structural feature encoding. Then, a structure-guided fusion module with
two different strategies, i.e., weighted summation and alignment constraint, is
first designed to inject the structural information into both the textual and
visual features. To the best of our knowledge, SGMPT is the first MPT model for
multimodal KGR, which mines the structural information underlying the knowledge
graph. Extensive experiments on FB15k-237-IMG and WN18-IMG, demonstrate that
our SGMPT outperforms existing state-of-the-art models, and prove the
effectiveness of the designed strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Track Mix Generation on Music Streaming Services using <span class="highlight-title">Transformer</span>s <span class="chip">RecSys 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03045v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03045v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Walid Bendada, Théo Bontempelli, Mathieu Morlon, Benjamin Chapus, Thibault Cador, Thomas Bouabça, Guillaume Salha-Galvan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Track Mix, a personalized playlist generation system
released in 2022 on the music streaming service Deezer. Track Mix automatically
generates "mix" playlists inspired by initial music tracks, allowing users to
discover music similar to their favorite content. To generate these mixes, we
consider a Transformer model trained on millions of track sequences from user
playlists. In light of the growing popularity of Transformers in recent years,
we analyze the advantages, drawbacks, and technical challenges of using such a
model for mix generation on the service, compared to a more traditional
collaborative filtering approach. Since its release, Track Mix has been
generating playlists for millions of users daily, enhancing their music
discovery experience on Deezer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>RecSys 2023 - Industry track with oral presentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Retrieval-Augmented Large Language Models via Data Importance
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaozhong Lyu, Stefan Grafberger, Samantha Biegel, Shaopeng Wei, Meng Cao, Sebastian Schelter, Ce Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval augmentation enables large language models to take advantage of
external knowledge, for example on tasks like question answering and data
imputation. However, the performance of such retrieval-augmented models is
limited by the data quality of their underlying retrieval corpus. In this
paper, we propose an algorithm based on multilinear extension for evaluating
the data importance of retrieved data points. There are exponentially many
terms in the multilinear extension, and one key contribution of this paper is a
polynomial time algorithm that computes exactly, given a retrieval-augmented
model with an additive utility function and a validation set, the data
importance of data points in the retrieval corpus using the multilinear
extension of the model's utility function. We further proposed an even more
efficient ({\epsilon}, {\delta})-approximation algorithm. Our experimental
results illustrate that we can enhance the performance of large language models
by only pruning or reweighting the retrieval corpus, without requiring further
training. For some tasks, this even allows a small model (e.g., GPT-JT),
augmented with a search engine API, to outperform GPT-3.5 (without retrieval
augmentation). Moreover, we show that weights based on multilinear extension
can be computed efficiently in practice (e.g., in less than ten minutes for a
corpus with 100 million elements).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Meta-Evaluation of C/W/L/A Metrics: System Ranking Similarity, System
  Ranking Consistency and Discriminative Power 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nuo Chen, Tetsuya Sakai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Moffat et al. proposed an analytic framework, namely C/W/L/A, for
offline evaluation metrics. This framework allows information retrieval (IR)
researchers to design evaluation metrics through the flexible combination of
user browsing models and user gain aggregations. However, the statistical
stability of C/W/L/A metrics with different aggregations is not yet
investigated. In this study, we investigate the statistical stability of
C/W/L/A metrics from the perspective of: (1) the system ranking similarity
among aggregations, (2) the system ranking consistency of aggregations and (3)
the discriminative power of aggregations. More specifically, we combined
various aggregation functions with the browsing model of Precision, Discounted
Cumulative Gain (DCG), Rank-Biased Precision (RBP), INST, Average Precision
(AP) and Expected Reciprocal Rank (ERR), examing their performances in terms of
system ranking similarity, system ranking consistency and discriminative power
on two offline test collections. Our experimental result suggests that, in
terms of system ranking consistency and discriminative power, the aggregation
function of expected rate of gain (ERG) has an outstanding performance while
the aggregation function of maximum relevance usually has an insufficient
performance. The result also suggests that Precision, DCG, RBP, INST and AP
with their canonical aggregation all have favourable performances in system
ranking consistency and discriminative power; but for ERR, replacing its
canonical aggregation with ERG can further strengthen the discriminative power
while obtaining a system ranking list similar to the canonical version at the
same time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Machine-Learned Ranking Algorithm for Dynamic and Personalised Car
  Pooling Services <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mattia Giovanni Campana, Franca Delmastro, Raffaele Bruno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Car pooling is expected to significantly help in reducing traffic congestion
and pollution in cities by enabling drivers to share their cars with travellers
with similar itineraries and time schedules. A number of car pooling matching
services have been designed in order to efficiently find successful ride
matches in a given pool of drivers and potential passengers. However, it is now
recognised that many non-monetary aspects and social considerations, besides
simple mobility needs, may influence the individual willingness of sharing a
ride, which are difficult to predict. To address this problem, in this study we
propose GoTogether, a recommender system for car pooling services that
leverages on learning-to-rank techniques to automatically derive the
personalised ranking model of each user from the history of her choices (i.e.,
the type of accepted or rejected shared rides). Then, GoTogether builds the
list of recommended rides in order to maximise the success rate of the offered
matches. To test the performance of our scheme we use real data from Twitter
and Foursquare sources in order to generate a dataset of plausible mobility
patterns and ride requests in a metropolitan area. The results show that the
proposed solution quickly obtain an accurate prediction of the personalised
user's choice model both in static and dynamic conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted from the IEEE 19th International Conference on Intelligent
  Transportation Systems (ITSC), 2016</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PLIERS: a Popularity-Based Recommender System for Content Dissemination
  in Online Social Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valerio Arnaboldi, Mattia Giovanni Campana, Franca Delmastro, Elena Pagani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel tag-based recommender system called PLIERS,
which relies on the assumption that users are mainly interested in items and
tags with similar popularity to those they already own. PLIERS is aimed at
reaching a good tradeoff between algorithmic complexity and the level of
personalization of recommended items. To evaluate PLIERS, we performed a set of
experiments on real OSN datasets, demonstrating that it outperforms
state-of-the-art solutions in terms of personalization, relevance, and novelty
of recommendations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in SAC '16: Proceedings of the 31st Annual ACM Symposium on
  Applied Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BHEISR: Nudging from Bias to Balance -- Promoting Belief Harmony by
  Eliminating Ideological Segregation in Knowledge-based Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengyan Wang, Yuxuan Hu, Zihan Yuan, Chenting Jiang, Weihua Li, Shiqing Wu, Quan Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of personalized recommendation systems, the increasing concern
is the amplification of belief imbalance and user biases, a phenomenon
primarily attributed to the filter bubble. Addressing this critical issue, we
introduce an innovative intermediate agency (BHEISR) between users and existing
recommendation systems to attenuate the negative repercussions of the filter
bubble effect in extant recommendation systems. The main objective is to strike
a belief balance for users while minimizing the detrimental influence caused by
filter bubbles. The BHEISR model amalgamates principles from nudge theory while
upholding democratic and transparent principles. It harnesses user-specific
category information to stimulate curiosity, even in areas users might
initially deem uninteresting. By progressively stimulating interest in novel
categories, the model encourages users to broaden their belief horizons and
explore the information they typically overlook. Our model is time-sensitive
and operates on a user feedback loop. It utilizes the existing recommendation
algorithm of the model and incorporates user feedback from the prior time
frame. This approach endeavors to transcend the constraints of the filter
bubble, enrich recommendation diversity, and strike a belief balance among
users while also catering to user preferences and system-specific business
requirements. To validate the effectiveness and reliability of the BHEISR
model, we conducted a series of comprehensive experiments with real-world
datasets. These experiments compared the performance of the BHEISR model
against several baseline models using nearly 200 filter bubble-impacted users
as test subjects. Our experimental results conclusively illustrate the superior
performance of the BHEISR model in mitigating filter bubbles and balancing user
perspectives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Undecimated Wavelet Transform for Word Embedded Semantic Marginal
  Autoencoder in Security improvement and Denoising different Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreyanth S
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By combining the undecimated wavelet transform within a Word Embedded
Semantic Marginal Autoencoder (WESMA), this research study provides a novel
strategy for improving security measures and denoising multiple languages. The
incorporation of these strategies is intended to address the issues of
robustness, privacy, and multilingualism in data processing applications. The
undecimated wavelet transform is used as a feature extraction tool to identify
prominent language patterns and structural qualities in the input data. The
proposed system may successfully capture significant information while
preserving the temporal and geographical links within the data by employing
this transform. This improves security measures by increasing the system's
ability to detect abnormalities, discover hidden patterns, and distinguish
between legitimate content and dangerous threats. The Word Embedded Semantic
Marginal Autoencoder also functions as an intelligent framework for
dimensionality and noise reduction. The autoencoder effectively learns the
underlying semantics of the data and reduces noise components by exploiting
word embeddings and semantic context. As a result, data quality and accuracy
are increased in following processing stages. The suggested methodology is
tested using a diversified dataset that includes several languages and security
scenarios. The experimental results show that the proposed approach is
effective in attaining security enhancement and denoising capabilities across
multiple languages. The system is strong in dealing with linguistic variances,
producing consistent outcomes regardless of the language used. Furthermore,
incorporating the undecimated wavelet transform considerably improves the
system's ability to efficiently address complex security concerns
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Bandwidth Selection for DENCLUE 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In modern day industry, clustering algorithms are daily routines of algorithm
engineers. Although clustering algorithms experienced rapid growth before 2010.
Innovation related to the research topic has stagnated after deep learning
became the de facto industrial standard for machine learning applications. In
2007, a density-based clustering algorithm named DENCLUE was invented to solve
clustering problem for nonlinear data structures. However, its parameter
selection problem was largely neglected until 2011. In this paper, we propose a
new approach to compute the optimal parameters for the DENCLUE algorithm, and
discuss its performance in the experiment section.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Modal Content Inference and Feature Enrichment for Cold-Start
  Recommendation <span class="chip">IJCNN</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haokai Ma, Zhuang Qi, Xinxin Dong, Xiangxian Li, Yuze Zheng, Xiangxu Mengand Lei Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimedia recommendation aims to fuse the multi-modal information of items
for feature enrichment to improve the recommendation performance. However,
existing methods typically introduce multi-modal information based on
collaborative information to improve the overall recommendation precision,
while failing to explore its cold-start recommendation performance. Meanwhile,
these above methods are only applicable when such multi-modal data is
available. To address this problem, this paper proposes a recommendation
framework, named Cross-modal Content Inference and Feature Enrichment
Recommendation (CIERec), which exploits the multi-modal information to improve
its cold-start recommendation performance. Specifically, CIERec first
introduces image annotation as the privileged information to help guide the
mapping of unified features from the visual space to the semantic space in the
training phase. And then CIERec enriches the content representation with the
fusion of collaborative, visual, and cross-modal inferred representations, so
as to improve its cold-start recommendation performance. Experimental results
on two real-world datasets show that the content representations learned by
CIERec are able to achieve superior cold-start recommendation performance over
existing visually-aware recommendation algorithms. More importantly, CIERec can
consistently achieve significant improvements with different conventional
visually-aware backbones, which verifies its universality and effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by International Joint Conference on Neural Networks 2023
  (IJCNN)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge Graph <span class="highlight-title">Self-Supervised</span> Rationalization for Recommendation <span class="chip">KDD'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Yang, Chao Huang, Lianghao Xia, Chunzhen Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a new self-supervised rationalization method,
called KGRec, for knowledge-aware recommender systems. To effectively identify
informative knowledge connections, we propose an attentive knowledge
rationalization mechanism that generates rational scores for knowledge
triplets. With these scores, KGRec integrates generative and contrastive
self-supervised tasks for recommendation through rational masking. To highlight
rationales in the knowledge graph, we design a novel generative task in the
form of masking-reconstructing. By masking important knowledge with high
rational scores, KGRec is trained to rebuild and highlight useful knowledge
connections that serve as rationales. To further rationalize the effect of
collaborative interactions on knowledge graph learning, we introduce a
contrastive learning task that aligns signals from knowledge and user-item
interaction views. To ensure noise-resistant contrasting, potential noisy edges
in both graphs judged by the rational scores are masked. Extensive experiments
on three real-world datasets demonstrate that KGRec outperforms
state-of-the-art methods. We also provide the implementation codes for our
approach at https://github.com/HKUDS/KGRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by KDD'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LogitMat : Zeroshot Learning Algorithm for Recommender Systems without
  Transfer Learning or <span class="highlight-title">Pretrain</span>ed Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender system is adored in the internet industry as one of the most
profitable technologies. Unlike other sectors such as fraud detection in the
Fintech industry, recommender system is both deep and broad. In recent years,
many researchers start to focus on the cold-start problem of recommender
systems. In spite of the large volume of research literature, the majority of
the research utilizes transfer learning / meta learning and pretrained model to
solve the problem. Although the researchers claim the effectiveness of the
approaches, everyone of them does rely on extra input data from other sources.
In 2021 and 2022, several zeroshot learning algorithm for recommender system
such as ZeroMat, DotMat, PoissonMat and PowerMat were invented. They are the
first batch of the algorithms that rely on no transfer learning or pretrained
models to tackle the problem. In this paper, we follow this line and invent a
new zeroshot learning algorithm named LogitMat. We take advantage of the Zipf
Law property of the user item rating values and logistic regression model to
tackle the cold-start problem and generate competitive results with other
competing techniques. We prove in experiments that our algorithm is fast,
robust and effective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dense Retrieval Adaptation using Target Domain Description 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Helia Hashemi, Yong Zhuang, Sachith Sri Ram Kothur, Srivas Prasad, Edgar Meij, W. Bruce Croft
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In information retrieval (IR), domain adaptation is the process of adapting a
retrieval model to a new domain whose data distribution is different from the
source domain. Existing methods in this area focus on unsupervised domain
adaptation where they have access to the target document collection or
supervised (often few-shot) domain adaptation where they additionally have
access to (limited) labeled data in the target domain. There also exists
research on improving zero-shot performance of retrieval models with no
adaptation. This paper introduces a new category of domain adaptation in IR
that is as-yet unexplored. Here, similar to the zero-shot setting, we assume
the retrieval model does not have access to the target document collection. In
contrast, it does have access to a brief textual description that explains the
target domain. We define a taxonomy of domain attributes in retrieval tasks to
understand different properties of a source domain that can be adapted to a
target domain. We introduce a novel automatic data construction pipeline that
produces a synthetic document collection, query set, and pseudo relevance
labels, given a textual domain description. Extensive experiments on five
diverse target domains show that adapting dense retrieval models using the
constructed synthetic data leads to effective retrieval performance on the
target domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Embedding APIs for Information Retrieval <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.06300v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.06300v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ehsan Kamalloo, Xinyu Zhang, Odunayo Ogundepo, Nandan Thakur, David Alfonso-Hermelo, Mehdi Rezagholizadeh, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ever-increasing size of language models curtails their widespread
availability to the community, thereby galvanizing many companies into offering
access to large language models through APIs. One particular type, suitable for
dense retrieval, is a semantic embedding service that builds vector
representations of input text. With a growing number of publicly available
APIs, our goal in this paper is to analyze existing offerings in realistic
retrieval scenarios, to assist practitioners and researchers in finding
suitable services according to their needs. Specifically, we investigate the
capabilities of existing semantic embedding APIs on domain generalization and
multilingual retrieval. For this purpose, we evaluate these services on two
standard benchmarks, BEIR and MIRACL. We find that re-ranking BM25 results
using the APIs is a budget-friendly approach and is most effective in English,
in contrast to the standard practice of employing them as first-stage
retrievers. For non-English retrieval, re-ranking still improves the results,
but a hybrid model with BM25 works best, albeit at a higher cost. We hope our
work lays the groundwork for evaluating semantic embedding APIs that are
critical in search and more broadly, for information access.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visualizing Relation Between (De)Motivating Topics and Public Stance
  toward COVID-19 Vaccine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.12118v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.12118v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashiqur Rahman, Hamed Alhoori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While social media plays a vital role in communication nowadays,
misinformation and trolls can easily take over the conversation and steer
public opinion on these platforms. We saw the effect of misinformation during
the COVID-19 pandemic when public health officials faced significant push-back
while trying to motivate the public to vaccinate. To tackle the current and any
future threats in emergencies and motivate the public towards a common goal, it
is essential to understand how public motivation shifts and which topics
resonate among the general population. In this study, we proposed an
interactive visualization tool to inspect and analyze the topics that resonated
among Twitter-sphere during the COVID-19 pandemic and understand the key
factors that shifted public stance for vaccination. This tool can easily be
generalized for any scenario for visual analysis and to increase the
transparency of social media data for researchers and the general population
alike.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Co-design Hardware and Algorithm for Vector Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11182v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11182v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Jiang, Shigang Li, Yu Zhu, Johannes de Fine Licht, Zhenhao He, Runbin Shi, Cedric Renggli, Shuai Zhang, Theodoros Rekatsinas, Torsten Hoefler, Gustavo Alonso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vector search has emerged as the foundation for large-scale information
retrieval and machine learning systems, with search engines like Google and
Bing processing tens of thousands of queries per second on petabyte-scale
document datasets by evaluating vector similarities between encoded query texts
and web documents. As performance demands for vector search systems surge,
accelerated hardware offers a promising solution in the post-Moore's Law era.
We introduce \textit{FANNS}, an end-to-end and scalable vector search framework
on FPGAs. Given a user-provided recall requirement on a dataset and a hardware
resource budget, \textit{FANNS} automatically co-designs hardware and
algorithm, subsequently generating the corresponding accelerator. The framework
also supports scale-out by incorporating a hardware TCP/IP stack in the
accelerator. \textit{FANNS} attains up to 23.0$\times$ and 37.2$\times$ speedup
compared to FPGA and CPU baselines, respectively, and demonstrates superior
scalability to GPUs, achieving 5.5$\times$ and 7.6$\times$ speedup in median
and 95\textsuperscript{th} percentile (P95) latency within an eight-accelerator
configuration. The remarkable performance of \textit{FANNS} lays a robust
groundwork for future FPGA integration in data centers and AI supercomputers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modeling Content Creator Incentives on Algorithm-Curated Platforms <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.13102v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.13102v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiri Hron, Karl Krauth, Michael I. Jordan, Niki Kilbertus, Sarah Dean
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Content creators compete for user attention. Their reach crucially depends on
algorithmic choices made by developers on online platforms. To maximize
exposure, many creators adapt strategically, as evidenced by examples like the
sprawling search engine optimization industry. This begets competition for the
finite user attention pool. We formalize these dynamics in what we call an
exposure game, a model of incentives induced by algorithms, including modern
factorization and (deep) two-tower architectures. We prove that seemingly
innocuous algorithmic choices, e.g., non-negative vs. unconstrained
factorization, significantly affect the existence and character of (Nash)
equilibria in exposure games. We proffer use of creator behavior models, like
exposure games, for an (ex-ante) pre-deployment audit. Such an audit can
identify misalignment between desirable and incentivized content, and thus
complement post-hoc measures like content filtering and moderation. To this
end, we propose tools for numerically finding equilibria in exposure games, and
illustrate results of an audit on the MovieLens and LastFM datasets. Among
else, we find that the strategically produced content exhibits strong
dependence between algorithmic exploration and content diversity, and between
model expressivity and bias towards gender-based user and creator groups.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>presented at ICLR 2023 (top 5%)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiVENT: Multilingual Videos of Events with Aligned Natural Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03153v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03153v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kate Sanders, David Etter, Reno Kriz, Benjamin Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Everyday news coverage has shifted from traditional broadcasts towards a wide
range of presentation formats such as first-hand, unedited video footage.
Datasets that reflect the diverse array of multimodal, multilingual news
sources available online could be used to teach models to benefit from this
shift, but existing news video datasets focus on traditional news broadcasts
produced for English-speaking audiences. We address this limitation by
constructing MultiVENT, a dataset of multilingual, event-centric videos
grounded in text documents across five target languages. MultiVENT includes
both news broadcast videos and non-professional event footage, which we use to
analyze the state of online news videos and how they can be leveraged to build
robust, factually accurate models. Finally, we provide a model for complex,
multilingual video retrieval to serve as a baseline for information retrieval
using MultiVENT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unifying Multimodal Source and Propagation Graph for Rumour Detection on
  Social Media with Missing Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.04832v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.04832v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tsun-Hin Cheung, Kin-Man Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of online social media platforms, the spread of
rumours has become a critical societal concern. Current methods for rumour
detection can be categorized into image-text pair classification and
source-reply graph classification. In this paper, we propose a novel approach
that combines multimodal source and propagation graph features for rumour
classification. We introduce the Unified Multimodal Graph Transformer Network
(UMGTN) which integrates Transformer encoders to fuse these features. Given
that not every message in social media is associated with an image and
community responses in propagation graphs do not immediately follow source
messages, our aim is to build a network architecture that handles missing
features such as images or replies. To enhance the model's robustness to data
with missing features, we adopt a multitask learning framework that
simultaneously learns representations between samples with complete and missing
features. We evaluate our proposed method on four real-world datasets,
augmenting them by recovering images and replies from Twitter and Weibo.
Experimental results demonstrate that our UMGTN with multitask learning
achieves state-of-the-art performance, improving F1-score by 1.0% to 4.0%,
while maintaining detection robustness to missing features within 2% accuracy
and F1-score compared to models trained without the multitask learning
framework. We have made our models and datasets publicly available at:
https://thcheung.github.io/umgtn/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ INDCOR white paper 4: Evaluation of Interactive Narrative Design For
  Complexity Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.09817v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.09817v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Roth, Breanne Pitt, Lāsma Šķestere, Jonathan Barbara, Agnes Karolina Bakk, Kirsty Dunlop, Maria del Mar Grandio, Miguel Barreda, Despoina Sampatakou, Michael Schlauch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While a strength of Interactive Digital Narratives (IDN) is its support for
multiperspectivity, this also poses a substantial challenge to its evaluation.
Moreover, evaluation has to assess the system's ability to represent a complex
reality as well as the user's understanding of that complex reality as a result
of the experience of interacting with the system. This is needed to measure an
IDN's efficiency and effectiveness in representing the chosen complex
phenomenon. We here present some empirical methods employed by INDCOR members
in their research including UX toolkits and scales. Particularly, we consider
the impact of IDN on transformative learning and its evaluation through
self-reporting and other alternatives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2010.10135</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Errorless Robust JPEG Steganography using Outputs of JPEG Coders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.04750v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.04750v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Butora, Pauline Puteaux, Patrick Bas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust steganography is a technique of hiding secret messages in images so
that the message can be recovered after additional image processing. One of the
most popular processing operations is JPEG recompression. Unfortunately, most
of today's steganographic methods addressing this issue only provide a
probabilistic guarantee of recovering the secret and are consequently not
errorless. That is unacceptable since even a single unexpected change can make
the whole message unreadable if it is encrypted. We propose to create a robust
set of DCT coefficients by inspecting their behavior during recompression,
which requires access to the targeted JPEG compressor. This is done by dividing
the DCT coefficients into 64 non-overlapping lattices because one embedding
change can potentially affect many other coefficients from the same DCT block
during recompression. The robustness is then combined with standard
steganographic costs creating a lattice embedding scheme robust against JPEG
recompression. Through experiments, we show that the size of the robust set and
the scheme's security depends on the ordering of lattices during embedding. We
verify the validity of the proposed method with three typical JPEG compressors
and the {\it Slack} instant messaging application. We benchmark its security
for various embedding payloads, three different ways of ordering the lattices,
and a range of Quality Factors. Finally, this method is errorless by
construction, meaning the embedded message will always be readable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 13 figures, 5 tables, submitted to IEEE Transactions on
  Dependable and Secure Computing</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-07-05T00:00:00Z">2023-07-05</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">11</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparing Apples to Apples: Generating Aspect-Aware Comparative
  Sentences from User <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jessica Echterhoff, An Yan, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is time-consuming to find the best product among many similar
alternatives. Comparative sentences can help to contrast one item from others
in a way that highlights important features of an item that stand out. Given
reviews of one or multiple items and relevant item features, we generate
comparative review sentences to aid users to find the best fit. Specifically,
our model consists of three successive components in a transformer: (i) an item
encoding module to encode an item for comparison, (ii) a comparison generation
module that generates comparative sentences in an autoregressive manner, (iii)
a novel decoding method for user personalization. We show that our pipeline
generates fluent and diverse comparative sentences. We run experiments on the
relevance and fidelity of our generated sentences in a human evaluation study
and find that our algorithm creates comparative review sentences that are
relevant and truthful.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Address Matching using Siamese <span class="highlight-title">Transformer</span> Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André V. Duarte, Arlindo L. Oliveira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Matching addresses is a critical task for companies and post offices involved
in the processing and delivery of packages. The ramifications of incorrectly
delivering a package to the wrong recipient are numerous, ranging from harm to
the company's reputation to economic and environmental costs. This research
introduces a deep learning-based model designed to increase the efficiency of
address matching for Portuguese addresses. The model comprises two parts: (i) a
bi-encoder, which is fine-tuned to create meaningful embeddings of Portuguese
postal addresses, utilized to retrieve the top 10 likely matches of the
un-normalized target address from a normalized database, and (ii) a
cross-encoder, which is fine-tuned to accurately rerank the 10 addresses
obtained by the bi-encoder. The model has been tested on a real-case scenario
of Portuguese addresses and exhibits a high degree of accuracy, exceeding 95%
at the door level. When utilized with GPU computations, the inference speed is
about 4.5 times quicker than other traditional approaches such as BM25. An
implementation of this system in a real-world scenario would substantially
increase the effectiveness of the distribution process. Such an implementation
is currently under investigation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in the 22nd EPIA Conference on Artificial
  Intelligence, EPIA 2023, Faial Island - Azores, Portugal, 5-8 September 2023,
  Proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Equivalent Graph Reconstruction Model and its Application in
  Recommendation Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangrui Yang, Lihua Yang, Qing Zhang, Zhihua Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation algorithm plays an important role in recommendation system
(RS), which predicts users' interests and preferences for some given items
based on their known information. Recently, a recommendation algorithm based on
the graph Laplacian regularization was proposed, which treats the prediction
problem of the recommendation system as a reconstruction issue of small samples
of the graph signal under the same graph model. Such a technique takes into
account both known and unknown labeled samples information, thereby obtaining
good prediction accuracy. However, when the data size is large, solving the
reconstruction model is computationally expensive even with an approximate
strategy. In this paper, we propose an equivalent reconstruction model that can
be solved exactly with extremely low computational cost. Finally, a final
prediction algorithm is proposed. We find in the experiments that the proposed
method significantly reduces the computational cost while maintaining a good
prediction accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Job Recommendations with Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02157v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02157v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi Zheng, Zhaopeng Qiu, Xiao Hu, Likang Wu, Hengshu Zhu, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of online recruitment services has encouraged the
utilization of recommender systems to streamline the job seeking process.
Predominantly, current job recommendations deploy either collaborative
filtering or person-job matching strategies. However, these models tend to
operate as "black-box" systems and lack the capacity to offer explainable
guidance to job seekers. Moreover, conventional matching-based recommendation
methods are limited to retrieving and ranking existing jobs in the database,
restricting their potential as comprehensive career AI advisors. To this end,
here we present GIRL (GeneratIve job Recommendation based on Large language
models), a novel approach inspired by recent advancements in the field of Large
Language Models (LLMs). We initially employ a Supervised Fine-Tuning (SFT)
strategy to instruct the LLM-based generator in crafting suitable Job
Descriptions (JDs) based on the Curriculum Vitae (CV) of a job seeker.
Moreover, we propose to train a model which can evaluate the matching degree
between CVs and JDs as a reward model, and we use Proximal Policy Optimization
(PPO)-based Reinforcement Learning (RL) method to further fine-tine the
generator. This aligns the generator with recruiter feedback, tailoring the
output to better meet employer preferences. In particular, GIRL serves as a job
seeker-centric generative model, providing job suggestions without the need of
a candidate set. This capability also enhances the performance of existing job
recommendation models by supplementing job seeking features with generated
content. With extensive experiments on a large-scale real-world dataset, we
demonstrate the substantial effectiveness of our approach. We believe that GIRL
introduces a paradigm-shifting approach to job recommendation systems,
fostering a more personalized and comprehensive job-seeking experience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recommendation Unlearning via Influence Function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zhang, Zhiyu Hu, Yimeng Bai, Fuli Feng, Jiancan Wu, Qifan Wang, Xiangnan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation unlearning is an emerging task to serve users for erasing
unusable data (e.g., some historical behaviors) from a well-trained recommender
model. Existing methods process unlearning requests by fully or partially
retraining the model after removing the unusable data. However, these methods
are impractical due to the high computation cost of full retraining and the
highly possible performance damage of partial training. In this light, a
desired recommendation unlearning method should obtain a similar model as full
retraining in a more efficient manner, i.e., achieving complete, efficient and
innocuous unlearning. In this work, we propose an Influence Function-based
Recommendation Unlearning (IFRU) framework, which efficiently updates the model
without retraining by estimating the influence of the unusable data on the
model via the influence function. In the light that recent recommender models
use historical data for both the constructions of the optimization loss and the
computational graph (e.g., neighborhood aggregation), IFRU jointly estimates
the direct influence of unusable data on optimization loss and the spillover
influence on the computational graph to pursue complete unlearning.
Furthermore, we propose an importance-based pruning algorithm to reduce the
cost of the influence function. IFRU is innocuous and applicable to mainstream
differentiable models. Extensive experiments demonstrate that IFRU achieves
more than250times acceleration compared to retraining-based methods with
recommendation performance comparable to full retraining.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recommender Systems in the Era of Large Language Models (LLMs) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei, Yiqi Wang, Jiliang Tang, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the prosperity of e-commerce and web applications, Recommender Systems
(RecSys) have become an important component of our daily life, providing
personalized suggestions that cater to user preferences. While Deep Neural
Networks (DNNs) have made significant advancements in enhancing recommender
systems by modeling user-item interactions and incorporating textual side
information, DNN-based methods still face limitations, such as difficulties in
understanding users' interests and capturing textual side information,
inabilities in generalizing to various recommendation scenarios and reasoning
on their predictions, etc. Meanwhile, the emergence of Large Language Models
(LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural
Language Processing (NLP) and Artificial Intelligence (AI), due to their
remarkable abilities in fundamental responsibilities of language understanding
and generation, as well as impressive generalization and reasoning
capabilities. As a result, recent studies have attempted to harness the power
of LLMs to enhance recommender systems. Given the rapid evolution of this
research direction in recommender systems, there is a pressing need for a
systematic overview that summarizes existing LLM-empowered recommender systems,
to provide researchers in relevant fields with an in-depth understanding.
Therefore, in this paper, we conduct a comprehensive review of LLM-empowered
recommender systems from various aspects including Pre-training, Fine-tuning,
and Prompting. More specifically, we first introduce representative methods to
harness the power of LLMs (as a feature encoder) for learning representations
of users and items. Then, we review recent techniques of LLMs for enhancing
recommender systems from three paradigms, namely pre-training, fine-tuning, and
prompting. Finally, we comprehensively discuss future directions in this
emerging field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fisher-Weighted Merge of Contrastive Learning Models in Sequential
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jung Hyun Ryu, Jaeheyoung Jeon, Jewoong Cho, Myungjoo Kang 1
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Along with the exponential growth of online platforms and services,
recommendation systems have become essential for identifying relevant items
based on user preferences. The domain of sequential recommendation aims to
capture evolving user preferences over time. To address dynamic preference,
various contrastive learning methods have been proposed to target data
sparsity, a challenge in recommendation systems due to the limited user-item
interactions. In this paper, we are the first to apply the Fisher-Merging
method to Sequential Recommendation, addressing and resolving practical
challenges associated with it. This approach ensures robust fine-tuning by
merging the parameters of multiple models, resulting in improved overall
performance. Through extensive experiments, we demonstrate the effectiveness of
our proposed methods, highlighting their potential to advance the
state-of-the-art in sequential learning and recommendation systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Remote Sensing Image Change Detection with Graph Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02007v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02007v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenglong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern remote sensing image change detection has witnessed substantial
advancements by harnessing the potent feature extraction capabilities of CNNs
and Transforms.Yet,prevailing change detection techniques consistently
prioritize extracting semantic features related to significant
alterations,overlooking the viability of directly interacting with bitemporal
image features.In this letter,we propose a bitemporal image graph Interaction
network for remote sensing change detection,namely BGINet-CD. More
specifically,by leveraging the concept of non-local operations and mapping the
features obtained from the backbone network to the graph structure space,we
propose a unified self-focus mechanism for bitemporal images.This approach
enhances the information coupling between the two temporal images while
effectively suppressing task-irrelevant interference,Based on a streamlined
backbone architecture,namely ResNet18,our model demonstrates superior
performance compared to other state-of-the-art methods (SOTA) on the GZ CD
dataset. Moreover,the model exhibits an enhanced trade-off between accuracy and
computational efficiency,further improving its overall effectiveness
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Utilizing Human Memory Processes to Model Genre Preferences for
  Personalized Music Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2003.10699v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2003.10699v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Kowald, Elisabeth Lex, Markus Schedl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a psychology-inspired approach to model and
predict the music genre preferences of different groups of users by utilizing
human memory processes. These processes describe how humans access information
units in their memory by considering the factors of (i) past usage frequency,
(ii) past usage recency, and (iii) the current context. Using a publicly
available dataset of more than a billion music listening records shared on the
music streaming platform Last.fm, we find that our approach provides
significantly better prediction accuracy results than various baseline
algorithms for all evaluated user groups, i.e., (i) low-mainstream music
listeners, (ii) medium-mainstream music listeners, and (iii) high-mainstream
music listeners. Furthermore, our approach is based on a simple psychological
model, which contributes to the transparency and explainability of the
calculated predictions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Dominik Kowald and Elisabeth Lex contributed equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Personalized Cold-Start Recommendation with <span class="highlight-title">Prompt</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.17256v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.17256v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuansheng Wu, Huachi Zhou, Wenlin Yao, Xiao Huang, Ninghao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems play a crucial role in helping users discover information
that aligns with their interests based on their past behaviors. However,
developing personalized recommendation systems becomes challenging when
historical records of user-item interactions are unavailable, leading to what
is known as the system cold-start recommendation problem. This issue is
particularly prominent in start-up businesses or platforms with insufficient
user engagement history. Previous studies focus on user or item cold-start
scenarios, where systems could make recommendations for new users or items but
are still trained with historical user-item interactions in the same domain,
which cannot solve our problem. To bridge the gap, our research introduces an
innovative and effective approach, capitalizing on the capabilities of
pre-trained language models. We transform the recommendation process into
sentiment analysis of natural languages containing information of user profiles
and item attributes, where the sentiment polarity is predicted with prompt
learning. By harnessing the extensive knowledge housed within language models,
the prediction can be made without historical user-item interaction records. A
benchmark is also introduced to evaluate the proposed method under the
cold-start setting, and the results demonstrate the effectiveness of our
method. To the best of our knowledge, this is the first study to tackle the
system cold-start recommendation problem. The benchmark and implementation of
the method are available at https://github.com/JacksonWuxs/PromptRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ensemble Knowledge Distillation for CTR Prediction <span class="chip">CIKM'2020</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2011.04106v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2011.04106v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jieming Zhu, Jinyang Liu, Weiqi Li, Jincai Lai, Xiuqiang He, Liang Chen, Zibin Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, deep learning-based models have been widely studied for
click-through rate (CTR) prediction and lead to improved prediction accuracy in
many industrial applications. However, current research focuses primarily on
building complex network architectures to better capture sophisticated feature
interactions and dynamic user behaviors. The increased model complexity may
slow down online inference and hinder its adoption in real-time applications.
Instead, our work targets at a new model training strategy based on knowledge
distillation (KD). KD is a teacher-student learning framework to transfer
knowledge learned from a teacher model to a student model. The KD strategy not
only allows us to simplify the student model as a vanilla DNN model but also
achieves significant accuracy improvements over the state-of-the-art teacher
models. The benefits thus motivate us to further explore the use of a powerful
ensemble of teachers for more accurate student model training. We also propose
some novel techniques to facilitate ensembled CTR prediction, including teacher
gating and early stopping by distillation loss. We conduct comprehensive
experiments against 12 existing models and across three industrial datasets.
Both offline and online A/B testing results show the effectiveness of our
KD-based training strategy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in CIKM'2020</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeSRA: Detect and Delete the Artifacts of GAN-based Real-World
  Super-Resolution Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangbin Xie, Xintao Wang, Xiangyu Chen, Gen Li, Ying Shan, Jiantao Zhou, Chao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image super-resolution (SR) with generative adversarial networks (GAN) has
achieved great success in restoring realistic details. However, it is notorious
that GAN-based SR models will inevitably produce unpleasant and undesirable
artifacts, especially in practical scenarios. Previous works typically suppress
artifacts with an extra loss penalty in the training phase. They only work for
in-distribution artifact types generated during training. When applied in
real-world scenarios, we observe that those improved methods still generate
obviously annoying artifacts during inference. In this paper, we analyze the
cause and characteristics of the GAN artifacts produced in unseen test data
without ground-truths. We then develop a novel method, namely, DeSRA, to Detect
and then Delete those SR Artifacts in practice. Specifically, we propose to
measure a relative local variance distance from MSE-SR results and GAN-SR
results, and locate the problematic areas based on the above distance and
semantic-aware thresholds. After detecting the artifact regions, we develop a
finetune procedure to improve GAN-based SR models with a few samples, so that
they can deal with similar types of artifacts in more unseen real data.
Equipped with our DeSRA, we can successfully eliminate artifacts from inference
and improve the ability of SR models to be applied in real-world scenarios. The
code will be available at https://github.com/TencentARC/DeSRA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code and models will be made publicly at
  https://github.com/TencentARC/DeSRA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAE-DFER: Efficient Masked Autoencoder for <span class="highlight-title">Self-supervised</span> Dynamic
  Facial Expression Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02227v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02227v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Licai Sun, Zheng Lian, Bin Liu, Jianhua Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic facial expression recognition (DFER) is essential to the development
of intelligent and empathetic machines. Prior efforts in this field mainly fall
into supervised learning paradigm, which is restricted by the limited labeled
data in existing datasets. Inspired by recent unprecedented success of masked
autoencoders (e.g., VideoMAE), this paper proposes MAE-DFER, a novel
self-supervised method which leverages large-scale self-supervised pre-training
on abundant unlabeled data to advance the development of DFER. Since the
vanilla Vision Transformer (ViT) employed in VideoMAE requires substantial
computation during fine-tuning, MAE-DFER develops an efficient local-global
interaction Transformer (LGI-Former) as the encoder. LGI-Former first
constrains self-attention in local spatiotemporal regions and then utilizes a
small set of learnable representative tokens to achieve efficient local-global
information exchange, thus avoiding the expensive computation of global
space-time self-attention in ViT. Moreover, in addition to the standalone
appearance content reconstruction in VideoMAE, MAE-DFER also introduces
explicit facial motion modeling to encourage LGI-Former to excavate both static
appearance and dynamic motion information. Extensive experiments on six
datasets show that MAE-DFER consistently outperforms state-of-the-art
supervised methods by significant margins, verifying that it can learn powerful
dynamic facial representations via large-scale self-supervised pre-training.
Besides, it has comparable or even better performance than VideoMAE, while
largely reducing the computational cost (about 38\% FLOPs). We believe MAE-DFER
has paved a new way for the advancement of DFER and can inspire more relavant
research in this field and even other related tasks. Codes and models are
publicly available at https://github.com/sunlicai/MAE-DFER.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 11 figures, 14 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sandwiched Video Compression: Efficiently Extending the Reach of
  Standard Codecs with Neural Wrappers <span class="chip">ICIP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11473v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11473v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Berivan Isik, Onur G. Guleryuz, Danhang Tang, Jonathan Taylor, Philip A. Chou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose sandwiched video compression -- a video compression system that
wraps neural networks around a standard video codec. The sandwich framework
consists of a neural pre- and post-processor with a standard video codec
between them. The networks are trained jointly to optimize a rate-distortion
loss function with the goal of significantly improving over the standard codec
in various compression scenarios. End-to-end training in this setting requires
a differentiable proxy for the standard video codec, which incorporates
temporal processing with motion compensation, inter/intra mode decisions, and
in-loop filtering. We propose differentiable approximations to key video codec
components and demonstrate that, in addition to providing meaningful
compression improvements over the standard codec, the neural codes of the
sandwich lead to significantly better rate-distortion performance in two
important scenarios.When transporting high-resolution video via low-resolution
HEVC, the sandwich system obtains 6.5 dB improvements over standard HEVC. More
importantly, using the well-known perceptual similarity metric, LPIPS, we
observe 30% improvements in rate at the same quality over HEVC. Last but not
least, we show that pre- and post-processors formed by very
modestly-parameterized, light-weight networks can closely approximate these
results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at the International Conference on Image Processing (ICIP),
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Artificial ASMR: A Cyber-Psychological Approach <span class="chip">SP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.14321v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.14321v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zexin Fang, Bin Han, C. Clark Cao, Hans. D. Schotten
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The popularity of Autonomous Sensory Meridian Response (ASMR) has skyrockted
over the past decade, but scientific studies on what exactly triggered ASMR
effect remain few and immature, one most commonly acknowledged trigger is that
ASMR clips typically provide rich semantic information. With our attention
caught by the common acoustic patterns in ASMR audios, we investigate the
correlation between the cyclic features of audio signals and their
effectiveness in triggering ASMR effects. A cyber-psychological approach that
combines signal processing, artificial intelligence, and experimental
psychology is taken, with which we are able to quantize ASMR-related acoustic
features, and therewith synthesize ASMR clips with random cyclic patterns but
not delivering identifiably scenarios to the audience, which were proven to be
effective in triggering ASMR effects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE MLSP 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2023-07-13T05:27:12.184277587Z">
            2023-07-13 05:27:12 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
